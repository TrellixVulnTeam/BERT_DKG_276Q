Social media has become almost ubiquitous in present times.
Such proliferation leads to automatic information processing need and has various challenges.
The nature of social media content is mostly informal.
Additionally while talking about Indian social media, users often prefer to use Roman transliterations of their native languages and English embedding.
Therefore Information retrieval (IR) on such Indian social media data is a challenging and difficult task when the documents and the queries are a mixture of two or more languages written in either the native scripts and/or in the Roman transliterated form.
Here in this paper we have emphasized issues related with Information Retrieval (IR) for Code-Mixed Indian social media texts, particularly texts from twitter.
We describe a corpus collection process, reported limitations of available state-of-the-art IR systems on such data and formalize the problem of Code-Mixed Information Retrieval on informal texts.

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.

Qanda uses a general architecture for human language technology called Catalyst Burger Mardis 2002, Nyberg et al to appear Developed at MITRE for the DARPA TIDES program, the Catalyst architecture is specifically designed for fast processing and for combining the strengths of Information Retrieval and Natural Language Processing into a single framework.
Catalyst uses a dataflow architecture in which standoff annotations are passed from one component to another, with the components connected in arbitrary (acyclic) topologies.
The use of standoff annotation permits components to be optimized for just those pieces of information they require for their processing.

We propose a new application of the Friedman statistical test of significance to compare multiple retrieval methods.
After measuring the average precision at the eleven standard levels of recall, our application of the Friedman test provides a global comparison of the methods.
In some experiments this test provides additional and useful information to decide if methods are different.

One of the key components of designing usable and useful collaborative information retrieval systems is to understand the needs of the users of these systems.
Our research team has been exploring collaborative information behavior in a variety of organizational settings.
Our research goals have been two-fold:
First, to develop a conceptual understanding of collaborative information behavior and second, gather requirements for the design of collaborative information retrieval systems.
In this paper, we present a brief overview of our fieldwork in a three different organizational settings, discuss our methodology for collecting data on collaborative information behavior, and highlight some lessons that we are learning about potential users of collaborative information retrieval systems in these domains.

Recently there have been appearing new applications of genetic algorithms to information retrieval, most of them specifically to relevance feedback.
The evolution of the possible solutions are guided by fitness functions that are designed as measures of the goodness of the solutions.
These functions are naturally the key to achieving a reasonable improvement, and which function is chosen most distinguishes one experiment from another.
In previous work, we found that, among the functions implemented in the literature, the ones that yield the best results are those that take into account not only when documents are retrieved, but also the order in which they are retrieved.
Here, we therefore evaluate the efficacy of a genetic algorithm with various order-based fitness functions for relevance feedback (some of them of our own design and compare the results with the Ide dec-hi method, one of the best traditional methods.

This work describes an algorithm which aims at increasing the quantity of relevant documents retrieved from a Peer-To-Peer (P2P) network.
The algorithm is based on a statistical model used for ranking documents, peers and ultra-peers, and on a “piggybacking” technique performed when the query is routed across the network.
The algorithm “amplifies
” the statistical information about the neighborhood stored in each ultra-peer.
The preliminary experiments provided encouraging results as the quantity of relevant documents retrieved through the network almost doubles once query piggybacking is exploited.

However we can distinguish three basic recommendation approaches: demographic, contentbased and collaborative [7 they are using many different general reasoning methods know from many other disciplines such as Artificial Intelligence, Expert Systems or Information Retrieval.
Recently, the recommender systems also adopt some nature inspired methods such as artificial immune system [4,7 In the Biology, the immune system is defined as “the system of specialized cells and organs that protect an organism from outside biological influences 12
In the literature we can find some application of AIS for collaborative filtering
[1 We are going to develop, implement and verify hybrid recommendation method for layout, structure and content recommendation in the existing wiki-based system (for example Wikitravel or Wikinews 12
Besides AIS we are going to implement some consensus-based methods for content-based recommendation [8,9,10] and fuzzy inference rules for demographic filtering [7].

The papers in this special section are devoted to the growing field of acoustic scene classification and acoustic event recognition.
Machine listening systems still have difficulties to reach the ability of human listeners in the analysis of realistic acoustic scenes.
If sustained research efforts have been made for decades in speech recognition, speaker identification and to a lesser extent in music information retrieval, the analysis of other types of sounds, such as environmental sounds, is the subject of growing interest from the community and is targeting an ever increasing set of audio categories.
This problem appears to be particularly challenging due to the large variety of potential sound sources in the scene, which may in addition have highly different acoustic characteristics, especially in bioacoustics.
Furthermore, in realistic environments, multiple sources are often present simultaneously, and in reverberant conditions.

Sentiment Analysis is a two level task.
The first one is Identifying Topic and the second is, classifying sentiment related to that topic.
Sentiment Analysis starts with “What other people thinks Sentiment Extraction deals with the retrieval of the opinion or mood conveyed in a block of Unstructured text in relation to the domain of the document being analyzed.
Although a lot of research has gone in the NLP, machine learning and web mining community on extracting structured data from unstructured sources, most of the4 proposed methods depend on tediously labeled unstructured data.
The World Wide Web has been dominated by unstructured content and searching the web has been based on techniques from Information Retrieval.
Supervised learning algorithm analyzes the training data and produces an inferred function which is called classification.

This thesis presents a system for web-based information retrieval that supports precise and informative post-query organization (automated document clustering by topic) to decrease real search time on the part of the user.
Most existing Information Retrieval systems depend on the user to perform intelligent, specific queries with Boolean operators in order to minimize the set of returned documents.
The user essentially must guess the appropriate keywords before performing the query.
Other systems use a vector space model which is more suitable to performing the document similarity operations which permit hierarchical clustering of returned documents by topic.
This allows "post query" refinement by the user.
The system we propose is a hybrid beween these two systems, compatibile with the former, while providing the enhanced document organization permissable by the latter.

Classification of reusable software components is essential to successful software reuse initiatives and a critical feature of library development.
This paper provides a survey of storage and retrieval methods and highlights the main characteristics of each class of methods.
The work focuses on information retrieval methods with emphasis on Component Rank and Latent Semantic Analysis models that are applied to component classification.
These models have shown to provide efficient storage and retrieval algorithms as they narrow the results of a user query and provide accurate component selection.
Thus, leading to efficient reusable repository.
Reuse Libraries, Asset Classification, Asset Retrieval, and Retrieval Methods.

This paper reports on a study that explored the needs and challenges with respect to the creation of a collaboratory for li­ brary and information science practitioners.
To identify needs and challenges interviews were conducted with practitioners at a variety of institutions.
The results suggest that there is a need for a collaboratory to facilitate on-demand, personalized knowledge sharing.
The collaboratory should also be well integrated into the everyday practice of library and information science practitioners.

Product or company names used in this set are for identification purposes only.
Inclusion of the names of the products or companies does not indicate a claim of ownership by IGI Global of the trademark or registered trademark.
Encyclopedia of information science and technology Mehdi Khosrow-Pour, editor.-2nd ed.
Includes bibliographical references and index.
This set of books represents a detailed compendium of authoritative, research-based entries that define the contemporary state of knowledge on technology"-Provided by publisher.
All work contributed to this encyclopedia set is original material.
The views expressed in this encyclopedia set are those of the authors, but not necessarily of the publisher.
Note to Librarians: If your institution has purchased a print edition of this publication, please go to http www.igi-global.com/agree-ment for information on activating the library's complimentary online access.

The usage of single geographical footprint model in existing Geographical Information Retrieval (GIR) system will cause many problems like overestimation or underestimation of the geographical scopes for documents to search.
To be honest, the single geographical footprint model is not applicable in modern GIR system although it is simple, fast and widely applied today.
In order to improve the quality of answers given to a spatial query, a new model as well as a dedicated algorithm will be proposed to study the geographical information attached to documents.
Besides, a dedicated algorithm based on network graph, which is inspired by the Google PageRank and Bayesian network theory, is also invented to estimate the geographical similarity between document

This paper describes a system which enables users to create on-the-fly queries which involve not just keywords, but also sortal constraints and linguistic constraints.
The user can specify how the results should be presented e.g. in terms of links to documents, or as table entries.
The aim is to bridge the gap between keyword based Information Retrieval and pattern based Information Extraction.

We proposed in [7] a nested relational calculus and a nested relational algebra based on structural recursion [6,5] and on monads [27,16
In this report, we describe relative set abstraction as our third nested relational query language.
This query language is similar to the well known list comprehension mechanism in functional programming languages such as Haskell [ll Miranda [24 KRC [23 etc.
This language is equivalent to our earlier query languages both in terms of semantics and in terms of equational theories.
This strong sense of equivalence allows our three query languages to be freely combined into a nested relational query language that is robust and user-friendly.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report
No. MSCIS-92-59.
This technical report is available at ScholarlyCommons:
http repository.upenn.edu/cis_reports/472
A Conservative Property Of
A Nested Relational Query Language MS-CIS-92-59 LOGIC COMPUTATION 48

On 5 August 2003, our beloved colleague Anthony E. Cawkell passed away from cancer.
Tony was a longtime member of the Editorial Board of the Journal of Information Science as well as Honorary Fellow of the Institute of Information Scientists, now the Chartered Institute for Librarians and Information Professionals
(CILIP Before ‘retiring’ to a private consultancy, CITECH Ltd, he had been ISI’s man in London.
For thirty years he not only directed their educational and marketing efforts in Europe but also served as a Vice President of Research and Development which was his forte.
Space and time do not permit me to provide an adequate treatment of Tony’s contributions to information science and technology.
In a brief essay in Current

In a User-Private Information Retrieval (UPIR) scheme, a set of users collaborate to retrieve files from a database without revealing to observers which participant in the scheme requested the file.
Protocols have been proposed based on pairwise balanced designs and symmetric designs.
We propose a new class of UPIR schemes based on generalised quadrangles (GQ
We prove that while the privacy of users in the previously proposed schemes could be compromised by a single user, the new GQ-UPIR schemes proposed in this paper maintain privacy with high probability even when up to O(n1/4 users collude, where n is the total number of users in the scheme.

Automatic classification of text documents has become an important research issue now days.
Proper classification of text documents requires information retrieval, machine learning and Natural language processing (NLP) techniques.
Our aim is to focus on important approaches to automatic text classification based on machine learning techniques viz.
supervised, unsupervised and semi supervised.
In this paper we present a review of various text classification approaches under machine learning paradigm.
We expect our research efforts provide useful insights on the relationships among various text classification techniques as well as sheds light on the future research trend in this domain.

Xerox participated in the Cross Language Information Retrieval (CLIR) track of TREC-6.
This track examines the problem of retrieving documents written in one language using queries written in another language.
Our approach is to use a bilingual dictionary at query time to construct a target language version of the original query.
We concentrate our experiments this year on manual query construction based on a weighted boolean model and on an automatic method for the translation of multi-word units.
We also introduce a new derivational stemming algorithm whose word classes are generated automatically from a monolingual lexicon.
We present our results on the 22 TREC-6 CLIR topics which have been assessed and brieey discuss the problems inherent in the cross-language IR task.

Dr. Abdur Chowdhury serves as Twitter’s Chief Scientist.
Prior to that, Dr. Chowdhury co-founded Summize, a real-time search engine sold to Twitter in 2008.
Dr. Chowdhury has held positions at AOL as their Chief Architect for Search, Georgetown’s Computer Science Department and University of Maryland’s Institute for Systems Research.
His research interest lays in Information Retrieval focusing on making information accessible.

This paper describes a designed and implemented system for efficient storage, indexing and search in collections of spoken documents that takes advantage of automatic speech recognition.
As the quality of current speech recognizers is not sufficient for a great deal of applications, it is necessary to index the ambiguous output of the recognition,
i. e. the acyclic graphs of word hypotheses recognition lattices.
Then, it is not possible to directly apply the standard methods known from text-based systems.
The paper discusses an optimized indexing system for efficient search in the complex and large data structure that has been developed by our group.
The search engine works as a server.
The meeting browser JFerret, developed withing the European AMI project, is used as a client to browse search results.

One important problem in Music Information Retrieval is Automatic Music Transcription, which is an automated conversion process from played music to a symbolic notation such as sheet music.
Since the accuracy of previous audiobased transcription systems is not satisfactory, we propose an innovative visual-based automatic music transcription system named claVision to perform piano music transcription.
Instead of processing the music audio, the system performs the transcription only from the video performance captured by a camera mounted over the piano keyboard.
claVision can be used as a transcription tool, but it also has other applications such as music education.
The claVision software has a very high accuracy (over 95 and a very low latency in real-time music transcription, even under di↵erent illumination conditions.

There has been growing interest in dealing with large-seale geographic data bases because of the growing capacities of computer systems and because of the increasing interest in environmental and social systems.
This paper outlines the problems associated with these data bases from data acquisition to simulators and information retrieval.
It outlines an overall approach to the problem using polygons, point data, and common coordinate systems.
It explains a method of grid independent data observation.
It also deals with a variety of specific problems of file management and information handling which arise in large-scale information systems dependent on data of a two-dimensional structure, as opposed to data of a linear or hierarchical structure.

Collaborative tagging provides exceptional performance in the domains of IF (Information Filtering) and IR (Information Retrieval Based on various studies regarding the tagging behavior of users, it can be concluded that there is potential for expansion of this domain to the area of ratings.
The paper presents Qtag, a qualitative tagging system that allows users to tag in order to rate and express opinions in more sharable vocabulary.
A conceptual model and evaluation are presented.

Social media platforms have opened new dimensions within the information retrieval domain leading to a novel concept known as Social Information Retrieval.
We argue that the concept of Social Information Retrieval can be extended by augmenting the huge amount of content on the traditional Web with the ever-growing rich Social Web content to increase the information richness of today’s search engines.
This paper proposes a subjectivity detection framework which can lead towards a proposed emotion-aware search engine interface.
Our proposed method differs from previous subjectivity analysis approaches in that it is the first method that takes into account social features of social media platforms for the subjectivity classification task.
Through experimental evaluations, we observe the accuracy of the proposed method to be 86.21% which demonstrates a promising outcome for large-scale application of our proposed subjectivity analysis technique.

The database and information retrieval communities have long been recognized as being irreconcilable.
Today, however, we witness a surprising convergence of the techniques used by both communities in decentralized, large-scale environments.
The newly emerging field of reputation based trust management, borrowing techniques from both communities, best demonstrates this claim.
We argue that incomplete knowledge and increasing autonomy of the participating entities are the driving forces behind this convergence, pushing the adoption of probabilistic techniques typically borrowed from an information retrieval context.
We argue that using a common probabilistic framework would be an important step in furthering this convergence and enabling a common treatment and analysis of distributed complex systems.
We will provide a first sketch of such a framework and illustrate it with examples from our previous work on information retrieval, structured search and trust

Several studies have identified clinical questions posed by health care professionals to understand the nature of information needs during clinical practice.
To support access to digital information sources, it is necessary to integrate the information needs with a computer system.
We have developed a conceptual guidance approach in information retrieval, based on a knowledge base that contains the patterns of information needs.
The knowledge base uses a formal representation of clinical questions based on the UMLS knowledge sources, called the Generic Query model.
To improve the coverage of the knowledge base, we investigated a method for extracting plausible clinical questions from the medical literature.
This poster presents the Generic Query model, shows how it is used to represent the patterns of clinical questions, and describes the framework used to extract knowledge from the medical literature.

The natural language processing has a set of phases that evolves from lexical text analysis to the pragmatic one in which the author’s intentions are shown.
The ambiguity problem appears in all of these tasks.
Previous works tries to do word sense disambiguation, the process of assign a sense to a word inside a specific context, creating algorithms under a supervised or unsupervised approach, which means that those algorithms use or not an external lexical resource.
This paper presents an approximated approach that combines not supervised algorithms by the use of a classifiers set, the result will be a learning algorithm based on unsupervised methods for word sense disambiguation process.
It begins with an introduction to word sense disambiguation concepts and then analyzes some unsupervised algorithms in order to extract the best of them, and combines them under a supervised approach making use of some classifiers.

In the Dempster-Shafer's theory of evidence, for incorporating uncertainty, the valuation assigns to the data tables the degrees of belief for these data.
Firstly, we are looking for the answers to the following questions.
Is there a valuation-based system in which combination and marginalization operate on valuations?
Has this system prosperities analogical to the t-norm system?
In the t-norm system of the valuation for the specific database attributes configuration can be described the algebra of possible data set in which can be interpreted the Information Retrieval Logic.

We participated in the Cross-Language Information Retrieval evaluation at NTCIR-3 for the EnglishChinese and English-Japanese tasks.
We examined several approaches to query translation, including the use of a commercial machine translation system, a thesaurus that is automatically extracted from a parallel corpus, and a general-purpose online dictionary.
The MT-based approach was most effective among these alternatives in our experiments for English-Chinese retrieval on the NTCIR-2 and 3 data.
Combined use of machine translation and thesaurus extraction yielded further improvement.

The aim of this work is to present methods some of the ongoing research done as a part of development of Semantically Enhanced Intellectual Property Protection System SEIPro2S. Main focus is on description of methods that allow for creation of more concise documents preserving semantically the same meaning as their originals.
Thus, compacting methods are denoted as a semantic compression.

As an alternative to the usual Mean Average Precision, some use is currently being made of the Geometric Mean Average Precision (GMAP) as a measure of average search effectiveness across topics.
GMAP is specifically used to emphasise the lower end of the average precision scale, in order to shed light on poor performance of search engines.
This paper discusses the status of this measure and how it should be understood.

In this paper, we propose a retrieval model for news-based information contents over the WWW.
We have integrated web-tickers with a navigation browser.
It works as an adaptable dynamic information retrieval tool embedded with the browser for the web navigation.
The system has been implemented using both mobile and static agents, but provides inter-operability between the agents and the conventional requestresponse environment.

Efficient management of a wireless local area network is one of the goals set by faculty of information science and technology (FTSM) UKM, in order to provide wireless network connection to its staffs, students and visitors.
An investigation was conducted to measure the signal strength and overlaps in order to ascertain the performance and dimension measures of the current wireless network and its future requirement.
The existing area under study covers six buildings inclusive of a lecture theatre and the experiments were done on several different days and time slots.
A comparison of the network performance was also measured by taking into accounts the number of connected users at any one time.
The results from the research shows that wireless access points can be more efficiently located in strategic places for continuous near excellent connectivity and also allows for more effective systematic planning and placement of access points in the future expansion of the network.

The effect of using paat queries to improve automatic query expansion was examined in the TREC environment.
Automatic feedback of documents identified from similar past queries was compared with standard top-document feedback and with no feedback.
A new query similarity metric was used based on comparing result lists and using probability of relevance.
Our top-document feedback method showed small improvements over no feedback method consistent with past studies.
On recall-precision and average precision measures, past query feedback yielded performance superior to that of top-document feedback.
The past query feedback method also lends itself to tunable thresholds such that better performance can be obtained by automatically deciding when, and when not, to apply the expansion.
Automatic past-query feedback actually improved top document precision in this experiment.

With Ihe growth of digital libraries and electronic publishing, many structured document sources are appearing and their efleclive mediation is an imporlanl research topic.
In this paper, we propose a wrapping architecture for externally maintained struclured document sources.
Our wrapping target is information retrieval systems (IRSs) that provide access to strucaured documenk We describe a wrapper construction method for such IRSs with limited functionality.
A constructed wrapper enhances retneval facilities of Ihe underlying IRS and provides an object database view lo the mediator.
We focus on determining whether the underlying IRS can support a given query.
Then we discuss some research issues related to OUT wrapping architecture.

This paper describes a model for optimum information retrieval over a distributed document collection.
The model stems from Robertson's Probability Ranking Principle: Having computed individual document rankings correlated to diierent subcollections, these local rankings are stepwise merged into a nal ranking list where the documents are ordered according to their probability of relevance.
Here, a full dissemination of subcollection-wide information is not required.
The documents of diierent subcollec-tions are assumed to be indexed using diierent indexing vocabularies.
Moreover, local rankings may be computed by individual probabilistic retrieval methods.
The underlying data volume is arbitrarily scalable.
A criterion for eeectively limiting the ranking process to a subset of subcollections extends the model.

This paper discusses the Visual Information Retrieval (VIR) process and emphasizes the need for more natural interfaces and a better understanding of the user.
The central role of user modelling and the importance of the communication channel in a VIR scenario are discussed.
We identify areas of research and possible approaches for dealing with the problem of creating effective, user responsive VIR systems.

By synthesizing information common to retrieved documents, multi-document summarization can help users of information retrieval systems to find relevant documents with a minimal amount of reading.
We are developing a multidocument summarization system to automatically generate a concise summary by identifying and synthesizing similarities across a set of related documents.
Our approach is unique in its integration of machine learning and statistical techniques to identify similar paragraphs, intersection of similar phrases within paragraphs, and language generation to reformulate the wording of the summary.
Our evaluation of system components shows that learning over multiple extracted linguistic features is more effective than information retrieval approaches at identifying similar text units for summarization and that it is possible to generate a fluent summary that conveys similarities among documents even when full semantic interpretations of the input text are not available.

The detection of synonyms is a challenge that has attracted many contributions for the possible applications in many areas, including Semantic Web and Information Retrieval.
An open challenge is to identify synonyms of a term that are appropriate for a specific domain, not just all the synonyms.
Moreover, the execution time is critical when handling big data.
Therefore, it is needed an algorithm which can perform accurately and fast in detecting domain-appropriate synonyms on-thefly.
This contribution presents SynFinder which uses WordNet and the web of data.
Given a term and a domain in input, WordNet is used for the retrieval of all the synonyms of the term.
Then, synonyms which do not appear in web pages related to the domain are eliminated.
Our experimentation shows a very good accuracy and computation performance of SynFinder, reporting a mean precision of 0.94 and an average execution time lower than 1 s.

The FIRE 2017 IRLeD Track focused on creating a framework for evaluating different methods of Information Retrieval from legal documents.
Therewere two tasks for this track i)
Catchphrase Extraction task, and (ii) Precedence Retrieval task.
In the catchphrase extraction task, the participants had to extract catchphrases (legal keywords) from Indian Supreme Court case documents.
In the second task of Precedence Retrieval, the participants were to retrieve relevant or cite-able documents for particular Indian SupremeCourt cases from a set of prior case documents.

In information retrieval systems search quality is directly related to the number of relevant retrieved documents.
Database and data archive that these systems are implemented on can include variety of documents with different sizes.
In this case, the chance of retrieving the longer documents can be more than the shorter ones.
To avoid this and giving equal chance to all documents be retrieved and increasing the quality and integrity in information retrieval, the length of documents must be normalized.
In this study, both conventional and proposed methods for normalization, Cosine Similarity normalization and Pivoted Unique normalization are used for normalizing the length of documents.
Their performance is tested on Wikipedia MM2008 data archive and compared to each other.
Finally the best model has been introduced.

We present an approach for picture indexing and abstraction.
Picture indexing facilitates information retrieval from a pictorial database consisting of picture objects and picture relations.
To construct picture indexes, abstraction operations to perform picture object clustering and classification are formulated.
To substantiate the abstraction operations, we also formalize syntactic abstraction rules and semantic abstraction rules.
We then illustrate by examples how to apply these abstraction operations to obtain various picture indexes, and how to construct icons to facilitate accessing of pictorial data.

This paper describes our participation in monolingual tasks at CLEF-2005.
In this research we have worked in the following languages: English, French, Portuguese, Bulgarian and Hungarian.
Our task has been focused on using combined different size passages to improve the Information Retrieval process.
Once we have studied the experiments which have been carried out and the official results at CLEF, we have realized that this combining model gets better the achieved scores considerably.

The paper presents the innovative use of cluster based search for e cient mathematical information retreival.
The search is realized by applying multiple clustering techniques on the mathematical markup documents.
The technique makes use of cluster oriented search to speed up math information retrieval.
Impressive results have been obtained as compared to similarity based search.
With the use of cluster based search, the retrieval time has been reduced from multiple seconds to about 1 second.
The quality of the result set has been found to be comparable to similarity based search.

In ranking with the pairwise classification approach, the loss associated to a predicted ranked list is the mean of the pairwise classification losses.
This loss is inadequate for tasks like information retrieval where we prefer ranked lists with high precision on the top of the list.
We propose to optimize a larger class of loss functions for ranking, based on an ordered weighted average (OWA Yager, 1988) of the classification losses.
Convex OWA aggregation operators range from the max to the mean depending on their weights, and can be used to focus on the top ranked elements as they give more weight to the largest losses.
When aggregating hinge losses, the optimization problem is similar to the SVM for interdependent output spaces.
Moreover, we show that OWA aggregates of margin-based classification losses have good generalization properties.
Experiments on the Letor 3.0 benchmark dataset for information retrieval validate our approach.

This paper proposes a novel Chinese-English Cross-Lingual Information Retrieval (CECLIR) model PME, in which bilingual dictionary and comparable corpora are used to translate the query terms.
The proximity and mutual information of the term-pairs in the Chinese and English comparable corpora are employed not only to resolve the translation ambiguities but also to perform the query expansion so as to deal with the out-of-vocabulary issues in the CECLIR.
The evaluation results show that the query precision of PME algorithm is about 84.4% of the monolingual information retrieval.

The labyrinthine abundance of educational resources on the Web has greatly expanded the challenge of helping students find, organize, and use resources that will best match their individual goals, interests, and current knowledge.
Map-based navigation, using technologies such as the Self-Organizing Maps
(SOM is one solution to this growing challenge.
However, as the number of documents organized by SOM increases, the number of documents within each cell becomes too large for the user to make meaningful choices, overwhelming his ability to make accurate decisions.
Combining interactivity with the ability to organize a large number of documents, we have developed two-level heterogeneous maps that are augmented with social navigation support.
We implemented our idea within the Knowledge Sea II system and ran a pilot study using this system in an Information Retrieval course.

World Wide Web is considered the most valuable place for Information Retrieval and Knowledge Discovery.
While retrieving information through user queries, a search engine results in a large and unmanageable collection of documents.
A more efficient way to organize the documents can be a combination of clustering and ranking, where clustering can group the documents and ranking can be applied for ordering the pages within each cluster.
This paper proposes an approach to co-clustering web documents and queries.
When user issues a query, we construct a Query-Document Bipartite Graph from click log data.
Then, we co-cluster the web documents and queries simultaneous based on the bipartite spectral graph partitioning which uses the second singular vectors of an appropriately scaled query-document matrix to yield good bipartition and rank the queries and documents on the bipartite graph via an iterative process like HITS.
The results of experiments show promising improvement.

We developed a music information retrieval system based on singing voice timbre, i.e a system that can search for songs in a database that have similar vocal timbres.
To achieve this, we developed a method for extracting feature vectors that represent characteristics of singing voices and calculating the vocal-timbre similarity between two songs by using a mutual information content of their feature vectors.
We operated the system using 75 songs and confirmed that the system worked appropriately.
According to the results of a subjective experiment, 80% of subjects judged that compared with a conventional method using MFCC, our method finds more appropriate songs that have similar vocal timbres.

The inverted file is the most popular indexing mechanism used for document search in an information retrieval system (IRS
However, the disk I/O for accessing the inverted file becomes a bottleneck in an IRS.
To avoid using the disk
I/O, we propose a caching mechanism for accessing the inverted file, called the inverted file cache (IF cache In this cache, a proposed hashing scheme using a linked list structure to handle collisions in the hash table speeds up entry indexing.
Furthermore, the replacement and storage mechanisms of this cache are designed specifically for the inverted file structure.
We experimentally verify our design, based on documents collected from the TREC (Text REtrieval Conference) and search requests generated by the Zipf-like distribution.
Simulation results show that the IF cache can improve the performance of a test IRS by about 60% in terms of the average searching response time.

In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets.
Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together.
We solve how to arrange numerous visualizations onto a meta-visualization display, so that their similarities and differences can be analyzed.
We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization.
Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other.

In this paper we describe our Oromo-English retrieval experiments that we have conducted at IIITHyderabad (India) and submitted to the ad hoc retrieval task of CLEF 2007.
We participated in the bilingual subtask of CLEF campaign for the second time by designing and submitting four official runs.
The experiments differ from one another in terms of topic fields used for query construction and the application of stemmer for normalization of query terms.
One of our major objectives was to assess the overall performance of our dictionary-based Oromo-English CLIR system on a new English test collection that has been provided by CLEF this year.
We are also interested in exploring and assessing the impacts of Afaan Oromo light stemmer on the overall performances of our experimental CLIR system.
After a brief description of the research contexts of our Oromo-English CLIR system, we will present and discuss the evaluation results of our official runs.

In order to explicate the applicability of importing Web Service to digital library information retrieval system, this paper firstly briefly introduces Web Service.
Then the authors construct an information retrieval model based on Web Service to improve the current retrieval system.
In this paper, we comprehensively present the model from overall designing, process designing, interface implementation and others.

Motivated by the hypothesis that the retrieval performance of a weighting model is independent of the language in which queries and collection are expressed, we compared the retrieval performance of three weighting models, i.e Okapi, statistical language modeling (SLM and deviation from randomness (DFR on three monolingual test collections, i.e French, Italian, and Spanish.
The DFR model was found to consistently achieve better results than both Okapi and SLM, whose performance was comparable.
We also evaluated whether the use of retrieval feedback improved retrieval performance; retrieval feedback was beneficial for DFR and Okapi and detrimental for SLM.
Besides relative performance, DFR with retrieval feedback achieved excellent absolute results: best run for Italian and Spanish, third run for French.

We have developed efficient methods to score structured hypotheses from technologies that fuse evidence from massive data streams to detect threat phenomena.
We have generalized metrics (precision, recall, F-value, and area under the precision-recall curve) traditionally used in the information retrieval and machine learning communities to realize object-oriented versions that accommodate inexact matching over structured hypotheses with weighted attributes.
We also exploit the object-oriented precision and recall metrics in additional metrics that account for the costs of false-positive and false-negative threat reporting.
We have reported on our scoring methods more fully previously; the present brief presentation is offered to help make this work accessible to the machine learning community.

In urban areas including shopping malls and stations with many people, it is important to utilize various information which those people have obtained.
In this paper, we propose a method for information registration and retrieval in MANET which achieves small communication cost and short response time.
In our method, we divide the whole application field into multiple sub-areas and classify records into several categories so that mobile terminals in an area holds records with a category.
Each area is associated with a category so that the number of queries for the category becomes the largest in the area.
Thus, mobile users search records with a certain category by sending a query to nodes in the particular area using existing protocol such as LBM (Location-Based Multicast Through simulations supposing actual urban area near Osaka station, we have confirmed that our method achieves practical communication cost and performance for information retrieval in MANET.

In applications including chemoinformatics, bioinformatics, information retrieval, text classification, computer vision and others, a variety of common issues have been identified involving frequency of occurrence, variation and similarities of instances, and lack of precise class labels.
These issues continue to be important hurdles in machine intelligence and my doctoral thesis focuses on developing robust machine learning models that address the same.

Merging search results from different servers is a major problem in Distributed Information Retrieval.
We used Regression-SVM and Ranking-SVM which would learn a function that merges results based on information that is readily available: i.e. the ranks, titles, summaries and URLs contained in the results pages.
By not downloading additional information, such as the full document, we decrease bandwidth usage.
CORI and Round Robin merging were used as our baselines; surprisingly, our results show that the SVMmethods do not improve over those baselines.

The new frontier of mobile information retrieval will combine context awareness and content adaptation.

This paper studies the problem of end-to-end windows mining directly from detection output.
Traditional object detection systems approach this problem in an ad-hoc manner, say, Non-
Maximum Suppression
(NMS Beyond NMS, multi-class context modeling has been explored thoroughly recent years.
But all these methods put their emphasis on eliminating false positive windows rather than improving recall.
To address this problem, we firstly study this problem and propose semantic windows mining.
To improve recall, we propose Selective Forward Search (SFS) which keeps most of the semantic windows while substantially reduces the number of false positives.
After SFS, to improve precision, we present the end-to-end windows mining by means of similarity refining optimized for mean Average Precision (mAP) and overlap regression.
We show a noticeable improvement on the PASCAL VOC datasets in both recall and precision.

In FIPA-style multi-agent systems, agents coordinate their activities by sending messages representing particular communicative acts (or performatives Agent communication languages must strike a balance between simplicity and expressiveness by defining a limited set of communicative act types that fit the communication needs of a wide set of problems.
More complex requirements for particular problems must then be handled by defining domain-specific predicates and actions within ontologies.
This paper examines the communication needs of a multi-agent distributed information retrieval system and discusses how well these are met by the FIPA ACL.

The Knowledge Base Discovery Tool (KBDT) is a suite of tools and components to improve the indexing of and search for documents.
KBDT extracts and displays content from documents and builds knowledge indexes based on meaning, rather than keywords.
KBDT uses the indexes to perform more intelligent searches.
It also includes visualization technology to display relevant results using multi-media, rather than plain text.
This paper describes prototypes of two tools in this suite that use components for searching, extraction, and display of requested information.
The tools are the Knowledge Base Editor and the Intelligent Information Retrieval Engine.

For the last few years peer-to-peer (p2p) networks have become widely used tools for sharing any kind of information from multimedia data to text documents.
The vast amount of shared information leads issues on finding relevant information over p2p networks.
Existing p2p file search and information retrieval techniques are based on the name of files, which is insufficient when searching relevant documents.
In this paper we present a method to perform semantic information retrieval over p2p networks.
Our method semantically inspects the content of shared data in peers to generate conceptual information about documents and general information about the peer.

Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision.
Exact inference in realworld applications of these problems is intractable, making efficient approximation methods essential for learning and inference.
In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings.
This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes.
We present experimental results with bipartite matching problems
—ranking and image correspondence—which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches.

As Internet resources become accessible to more and more countries, there is a need to develop efficient methods for information retrieval across languages.
In the present paper, we focus on query expansion techniques to improve the effectiveness of an information retrieval.
A combination to a dictionary-based translation and statistical-based disambiguation is indispensable to overcome translation’s ambiguity.
We propose a model using multiple sources for query reformulation and expansion to select expansion terms and retrieve information needed by a user.
Relevance feedback, thesaurus-based expansion, as well as a new feedback strategy, based on the extraction of domain keywords to expand user’s query, are introduced and evaluated.
We evaluated the effectiveness of the proposed combined method, by an application to a FrenchEnglish Information Retrieval.

Participation from music librarians has been sparse in the first six ISMIR conferences, despite many potential areas of common interest.
This paper makes an argument for the benefit to both the library and Music IR communities of increased representation of librarians at ISMIR.
An analysis of conference programs and primary publications of two music library organizations to determine topics from the library literature relevant to Music IR research is presented.
A discussion follows of expertise music librarians could potentially contribute to Music IR research and the ways in which Music IR research could further the work of music librarians, in each of the topics represented in the library literature.

In this paper we address the combination of query translation approaches for cross-language information retrieval (CLIR
We translate queries with Google Translate and extend them with new translations obtained by mapping noun phrases in the query to concepts in the target language using Wikipedia.
For two CLIR collections, we show that the proposed model provides meaningful translations that improve the strong baseline CLIR model based on a top performing SMT system.

The digital image over the network is inevitably affected by the channel additive noise.
However the existing fragile watermarking techniques with recovery are susceptible to random noise.
To overcome this problem, this paper presents a chaos-based fragile watermarking scheme with recovery.
In the proposed algorithm, the original image is divided into 2times2 blocks.
The watermark embedding position of every image block is randomly generated based on chaotic system.
These strategies can effectively improve the ability of our algorithm against not only random noise but also the synchronous counterfeiting attack.
The experiment demonstrates that the proposed scheme can detect and localize any malicious alterations and can recover a tampered image to an intelligible one even if the tested image is polluted by the random noise.

A large amount of information in the form of text, audio, video and other documents is available on the web.
Users should be able to find relevant information in these documents.
Information Retrieval (IR) refers to the task of searching relevant documents and information from the contents of a data set such as the World Wide Web (
WWW A web search engine is an IR system that is designed to search for information on the World Wide Web.
There are various components involved in information retrieval.
IR system has following components:

Multilingual Digital Libraries ask for Cross-Language search engines able to disambiguate the search terms across languages.
In this paper, we report on the development and first evaluation of a Cross-Language Information Retrieval (CLIR) system enriched with a Word To Category Module: we automatically extract a mapping from words in the meta data of the Catalog’s records to the associated Classification Categories and exploit it to help the CLIR system retrieve only books in different languages about the topic actually queried by the user.

In this work, we investigate utilizing the structure of a website to increase the effectiveness of document retrieval within a structured domain.
In particular we examine various methods to combine evidence within the website in order to improve the quality of pages returned.

This paper introduces a novel evaluation framework for question series and employs it to explore the effectiveness of QA and IR systems at addressing users’ information needs.
The framework is based on the notion of recall curves, which characterize the amount of relevant information contained within a fixed-length text segment.
Although it is widely assumed that QA technology provides more efficient access to information than IR systems, our experiments show that a simple IR baseline is quite competitive.
These results help us better understand the role of NLP technology in QA systems and suggest directions for future research.

Trapped atomic ions are a well-advanced physical system for investigating fundamental questions of quantum physics and for quantum information science and its applications.
When contemplating the scalability of trapped ions for quantum information science one notes that the use of laser light for coherent operations gives rise to technical and also physical issues that can be remedied by replacing laser light by long wavelength radiation in the microwave (MW) and radio-frequency (RF) regime radiation employing suitably modified ion traps.

While similarity has gained in importance in research about information retrieval on the (geospatial) semantic Web, information retrieval paradigms and their integration into existing spatial data infrastructures have not been examined in detail so far.
In this paper, intensional and extensional paradigms for similarity-based information retrieval are introduced.
The differences between these paradigms with respect to the query and results are pointed out.
Web user interfaces implementing two of these paradigms are presented, and steps towards the integration of the SIM-DL similarity theory into a spatial data infrastructure are discussed.
Remaining difficulties are highlighted and directions of further work are given.

Time is an important dimension of any information space and can be very useful in information retrieval.
Current information retrieval systems and applications do not take advantage of all the time information available in the content of documents to provide better search results and user experience.
In this paper we show some of the areas that can benefit from exploiting such temporal information.

Plagiarism is an increasing problem in the digital world.
The sheer amount of digital data calls for automation of plagirism discovery.
In this paper we evaluate an Information Retrieval approach of dealing with plagiarism through Vector Spaces.
This will allow us to detect similarities that are not result of naive copy&paste.
We also consider the extension of Vector Spaces where input documents are analyzed for term co-occurence, allowing us to introduce some semantics into our approach beyond mere word matching.
The approach is evaluated on a real-world collection of mathematical documents as part of the DML-CZ project.

Standards are important because they make a field more open to small and medium businesses and to academic players.
We review a number of standards that apply to information retrieval and web search, and discuss the role that they play.
We also discuss some areas where there is potential for the development of standards, where for instance information retrieval would benefit, and where standards development appears feasible.

The aim of this study was to implement a supervised codebook learning methodology for optimizing the feature based Bag-of-Words(Bow) representation towards data Retrieval.
Following the cluster hypothesis, that states that points within the same cluster square measure seemingly to fulfill a similar data.
The Bow model are often applied to different domains furthermore, so also judge our approach employing a assortment of for five time series datasets, a text dataset and a video dataset.
The gains square measure three fold since the EO-Bow will improve the mean Average preciseness, whereas reducing the coding time and therefore the info storage requirements The Bow model treats each imageas a document that contains a number of different “visual “words.
Then an image is represented as a histogram over aset of representative words, known as dictionary or codebook.
These histograms describe the corresponding images andthey can be used for the subsequent retrieval tasks.

We present four approaches to the Amharic French bilingual track at CLEF 2005.
All experiments use a dictionary based approach to translate the Amharic queries into French Bags-of-words, but while one approach uses word sense discrimination on the translated side of the queries, the other one includes all senses of a translated word in the query for searching.
We used two search engines: The SICS experimental engine and Lucene, hence four runs with the two approaches. Non-content bearing words were removed both before and after the dictionary lookup.
TF/IDF values supplemented by a heuristic function was used to remove the stop words from the Amharic queries and two French stopwords lists were used to remove them from the French translations.
In our experiments, we found that the SICS search engine performs better than Lucene and that using the word sense discriminated keywords produce a slightly better result than the full set of non discriminated keywords.

Research on ontology is becoming increasingly widespread in the computer science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction.
We shall use the generic term “information systems in its broadest sense, to collectively refer to these application perspectives.
We argue in this paper that so-called ontologies present their own methodological and architectural peculiarities: on the methodological side, their main peculiarity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an information system, leading to the perspective of ontology-driven information systems.

In the age of digital information more and more digital libraries and historical archives are using information systems in order to facilitate the document retrieval and provide better visualization of the search results and document presentation.
Much research has been done in the field of digital libraries, but in the case of historical archives, which have particular needs, this is not the case.
To this end, we investigate the use of new tools, which are based on the ontology of the historical archive in order to provide a new and effective method for document retrieval in a dynamic environment which will take into account the collaboration needs of the users.

Finding software for reuse is a problem that programmers face.
To reuse code that has been proven to work can increase any programmer’s productivity, benefit corporate productivity, and also increase the stability of software programs.
This paper shows that fuzzy retrieval has an improved retrieval performance over typical Boolean retrieval.
Various methods of fuzzy information retrieval implementation and their use for software reuse will be examined.
A deeper explanation of the fundamentals of designing a fuzzy information retrieval system for software reuse will be examined.
Future research options and necessary data storage systems are explored.

A q query Locally Decodable Code (LDC) encodes an n-bit message x as an N -bit codeword C(x such that one can probabilistically recover any bit xi of the message by querying only q bits of the codeword C(x even after some constant fraction of codeword bits has been corrupted.
We give new constructions of three query LDCs of vastly shorter length than that of previous constructions.
Specifically, given any Mersenne prime p 2t 1, we design three query LDCs of length
N exp n1/t for every n.
Based on the largest known Mersenne prime, this translates to a length of less than exp (

This paper presents our contribution to enhance literature-based discovery with information retrieval techniques.
We propose an approach that combines flexible Information Retrieval and Concepts indexing for knowledge discovery in biomedical literature.
Flexible information retrieval contributes to filter MEDLINE biomedical literature to the most relevant documents, whereas concept-based indexing allows to quickly identifying candidate concepts that could potentially validate a hypothesis.
We have tested our approach by replicating the Swanson’s first discovery on fish oil and Raynaud's disease correlation.
The obtained results show the effectiveness of our approach.

This paper will first briefly survey the existing impact of multimedia information retrieval (MIR) in applications.
It will then analyze the current trends of MIR research which can have an influence on future applications.
It will then detail the future possibilities and bottlenecks in applying the MIR research results in the main target application areas, such as the consumer (e.g personal video recorders, web information retrieval public safety (e.g automated smart surveillance systems and professional world (e.g automated meeting capture and summarization
In particular, recommendations will be made to the research community regarding the challenges that need to be met to make the knowledge transfer towards the applications more efficient and effective.
It will also attempt to study the trends in the applications which can inform the MIR community on directing intellectual resources towards MIR problems which can have a maximal real-world impact.

Much of the research in Information Retrieval has concerned improvements to similarity computations, statistics gathering, and term extraction, with the goal of improving effectiveness.
However, a simple examination of user characteristics can readily show, the method of computing similarity is less important than the behaviour of the system interface and environmental factors.
It is hypothesised that there must be knowledge of the relationship between a query, its user, the environment, and the query and user instantiation in the real world.
This hypothesis and others are demonstrated.
With facilities for interaction and feedback appropriately incorporated, effectiveness of 100% can be achieved.

In this paper, we introduce an approach for training a Named Entity Recognizer (NER) from a set of seed entities on the web.
Creating training data for NERs is tedious, time consuming, and becomes more difficult with a growing set of entity types that should be learned and recognized.
Named Entity Recognition is a building block in natural language processing and is widely used in fields such as question answering, tagging, and information retrieval.
Our NER can be trained on a set of entity names of different types and can be extended whenever a new entity type should be recognized.
This feature increases the practical applications of the NER.

The heuristics employed in information retrieval systems have traditionally been document-based, and have judged similarity holistically based upon entire documents.
In this work we present a locality-based paradigm for information retrieval, in which every word location in each document is scored.
The locality-based similarity heuristic provides retrieval e ectiveness as good as the documentbased technique, and has the additional advantage of allowing the matching section or sections of retrieved documents to be shown to the user when they are sifting the results of their query.
This is a considerable improvement upon the conventional presentation mechanism, in which the user must manually search each document for the passage if any such passage exists at all that suggested to the retrieval mechanism that this document is an answer.
We also describe an improved index representation that supports the required operations.

Based on bibliometric methods, the article makes an analysis on 455 papers published by the scholars of Mainland China (ML) in 66 foreign periodicals in the field of Information Science &amp; Library Science (ISLS) included in Social Sciences Citation Index from 2000 to 2010.
A series of quantitative analysis has been made from the aspects of publications, periodicals, affiliation to reflect the current research status and academic ability of Mainland China in the field of ISLS.

We describe our approach and results towards the Hyperlinking sub-task at MediaEval 2012.
We approached this as an Information Retrieval task and used re-ranking strategies for finding relevant videos.
A three-step approach was then applied on results to extract the most relevant part of the video regarding the query content.
Our results show that reranking strategies and integration of metadata information both improve the system performance.

The F-measure, originally introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction.
Optimizing this measure remains a statistically and computationally challenging problem, since no closed-form maximizer exists.
Current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables.
In this paper, we present an algorithm which is not only computationally efficient but also exact, regardless of the underlying distribution.
The algorithm requires only a quadratic number of parameters of the joint distribution (with respect to the number of binary responses We illustrate its practical performance by means of experimental results for multi-label classification.

As the email service is becoming an important communication way on the Network, the spam is increasing every day.
This paper describes a new filtering model based on email content by using Back-Propagation Neural Networks (BPNN
And for the Chinese email, it uses Natural Language Processing Information Retrieval Sharing Platform (NLPIR) system to perform Chinese word segmentation.
The simulation results show that this model can precisely filter the Chinese spam.

Earlier work has identified the potential for reuse and reproducibility when applying workflow systems to audio analysis and Music Information Retrieval.
In this paper we extend this approach with the introduction of Research Objects to capture semantic information about the use of workflows within the audio research and development process.
Once aggregated, the metadata encapsulated in a Research Object can be used to manage and disseminate research output, providing a well structured foundation for meeting the needs of reproducibility.
We report on the development and deployment of a software suite that practically applies this notion of Research Objects to capture the semantics surrounding the use of an audio processing workflow, and reflect upon how this might be further integrated with lower level semantics from the audio processing

We have developed a distributed search engine, Cooperative Search Engine (CSE) to retrieve fresh information.
In CSE, a local search engine located in each web server makes an index of local pages.
And, a Meta search server integrates these local search engines to realize a global search engine.
In such a way, the communication delay occurs at retrieval.
So, we have developed several speedup techniques in order to realize real time retrieval.
In addition, the meta server is a single point of failure in CSE.
So, we introduce redundancy of the meta search server increase availability of CSE.
In this paper, we describe scalability and reliability of CSE and their evaluations.

A major problem concerning the reusability of software is the retrieval of software components.
Different approaches have been followed to solve this problem.
In this paper we present the Reuse Assistant, a hybrid approach to support the retrieval of software components from a library of object classes.
The Reuse Assistant consists of two subsystems that follow two different approaches: information retrieval techniques based on statistical methods, and knowledge-based techniques using some of the representation and indexing mechanisms found in case-based systems.
The Information Retrieval approach grants system extendibility, and permits the use of a natural language interface.
The Case-Based approach enables reasoning about concepts, allowing the retrieval of “approximate” components.
Both subsystems can be operated from a common interface, where free-text and form-filling queries can be posed.

With the development of wireless networks and mobile terminals, mobile commerce has become a research hotspot in recent years for its commercial value and has been considered to be significant supplement and potential substitute of traditional e-commerce.
Tourism, which has the feature of mobility, is a typical mobile commerce application.
In this paper, we propose a RFID and personalized recommendation based tourism information system.
In this system, RFID tags together with RFID readers provide user identification.
And implicit feedbacks (including operations on short messages and web pages, and user locations and purchase records) based recommendation improves information retrieval efficiency and user satisfaction greatly.
System overview and issues on RFID, personalized recommendation and scalable multimedia coding in proposed tourism information system are discussed respectively.
Furthermore, some possible future work is presented.

Nowadays, one of crucial problems of the Semantic Web is to offer a simple and convenient access to knowledge bases and ontologies.
Advances in semantic search have been delayed because of the complexity of nRQL like query languages, as well as the ambiguities of the Natural Language (NL To go beyond these difficulties, we propose an approach that aims to support mechanisms and techniques for analyzing and processing a natural language query.
As result, we obtain an intermediate representation in description logics, easy to be interpreted in any query language oriented information retrieval, as nRQL.
This system is composed of four basic modules that provide a sequential processing of the query expressed in NL until the expected results.

In recent years, the information retrieval (IR) community has witnessed the first successful applications of deep neural network models to short-text matching and ad-hoc retrieval tasks.
However, the two communities focused on deep neural networks and on IR have less in common when it comes to the choice of programming languages.
Indri, an indexing framework popularly used by the IR community, is written in C while Torch, a popular machine learning library for deep learning, is written in the light-weight scripting language Lua.
To bridge this gap, we introduce Luandri (pronounced i>laundry</i a simple interface for exposing the search capabilities of Indri to Torch models implemented in Lua.

This paper presents the 2007 MIRACLE’s team approach to the AdHoc Information Retrieval track.
The main work carried out for this campaign has been around monolingual experiments, in the standard and in the robust tracks.
The most important contributions have been the general introduction of automatic named-entities extraction and the use of wikipedia resources.
For the 2007 campaign, runs were submitted for the following languages and tracks: a) Monolingual: Bulgarian, Hungarian, and Czech.
Robust monolingual: French, English and Portuguese.

Ontology has a richer internal structure as it includes relations and constraints between the concepts.
Ontology can be used for information retrieval.
Ontology is a halfway determination of a conceptual vocabulary to be utilized for formulating knowledge-level hypotheses around a domain of discourse.
The key part of ontology is to help knowledge sharing and reuse.
The process of allotting descriptions to documents in an IRS is called indexing.
In previous system zone based indexing is introduced which has certain drawbacks.
It helps finding results of user&apos;s query with exact match.
A new technique is proposed which improves results.
In this technique web pages are stored in xml database.
Zones are formed in database.
In case exact match is not found in xml database using zone based indexing then proximity of keyword

The growing usage of mobile devices has led to proliferation of many mobile applications.
A growing trend in mobile applications is centered on mobile landmark recognition.
It is a new mobile application that recognizes a captured landmark using the mobile device and retrieves related information.
This paper will present a survey on mobile landmark recognition for information retrieval.
A general overview of existing mobile landmark recognition systems will be summarized.
The techniques and algorithms used in the literatures, including content analysis of landmarks and classification methods for recognition, will be described.

Lexical taxonomies are tree or directed acyclic graph-like structures where each node represents a concept and each edge encodes a binary hypernymic (is-a) relation.
These lexical resources are useful for AI tasks like Information Retrieval or Machine Translation.
Two main trends exist in the construction and exploitation of these resources: On one hand, general purpose taxonomies like WordNet, and on the other, domain-specific databases such as the CheBi chemical ontology, or MusicBrainz in the music domain.
In both cases these are based on finding correct hypernymic relations between pairs of concepts.
In this paper, we propose a generic framework for hypernym discovery, based on exploiting linear relations between (term, hypernym) pairs in Wikidata, and apply it to the domain of music.
Our promising results, based on several metrics used in Information Retrieval, show that in several cases we are able to discover the correct hypernym for a given novel term.

The Problem: Text retrieval is a difficult problem that has become both more difficult and more important in recent years.
This is because of the increased amount of electronic information available and the greater demand for text search as a result of the World Wide Web.
People are surrounded with large quantities of information, but unable to use that information effectively because of its overabundance.

This study investigates if and why assessing relevance of clinical records for a clinical retrieval task is cognitively demanding.
Previous research has highlighted the challenges and issues information retrieval systems are faced with when determining the relevance of documents in this domain, e.g the vocabulary mismatch problem.
Determining if this assessment imposes cognitive load on human assessors, and why this is the case, may shed lights on what are the (cognitive) processes that assessors use for determining document relevance (in this domain High cognitive load may impair the ability of the user to make accurate relevance judgements and hence the design of IR mechanisms may need to take this into account in order to reduce the load.

In addition to the human-readable contents of the shared documents, the new generation of WWW requires machinereadable formalization of data.
We present an ontology-based information management system called knOWLer, targeting semantic integration into large-scale information systems.
In this paper, we are specially focusing on large-scale information retrieval systems.
The semantic is provided through an ontology language (OWL showing that ontological reasoning can be scaled to sizes of standard IR systems.
We propose an information management system for automatic document manipulation based on the semantics added by an ontology to the raw data.
The main capabilities of our application are expressivity provided by the ontology language and scalability induced by the persistent storage mechanism.

This paper discusses an email discovery and information retrieval tool based on formal concept analysis.
The ECA program allows its users to navigate email using a visual lattice metaphor rather than a tree.
It implements a virtual file structure over email where files and entire directories can appear in multiple positions in a given view.
The content and shape of the lattice formed by the conceptual ontology can assist in email discovery.
The system described provides more flexibility in retrieving stored emails than what is normally availableq in email clients and the approach can be generalised to any electronic document type.
The paper discusses how conceptual ontologies can leverage traditional IR systems.

Music information retrieval (MIR) as a nascent discipline is blessed with a multi-disciplinary group of people endeavoring to bring their respective knowledge-bases and research paradigms to bear on MIR problems.
Communication difficulties across disciplinary boundaries, however, threaten to impede the maturation of MIR into a full-fledge discipline.
The principal causes of the communications breakdown among members of the MIR community are a)
the lack of bibliographic control of the MIR literature; and, b) the use of discipline-specific languages and methodologies throughout that literature.
This poster abstract reports upon the background, framework, goals and ongoing development of the MIR Annotated Bibliography Website Project.
This project is being undertaken to specifically address and overcome these bibliographic control and communications issues.

This paper addresses the issue of devising a new document prior for the language modeling (LM) approach for Information Retrieval.
The prior is based on term statistics, derived in a probabilistic fashion and portrays a novel way of considering document length.
Furthermore, we developed a new way of combining document length priors with the query likelihood estimation based on the risk of accepting the latter as a score.
This prior has been combined with a document retrieval language model that uses Jelinek-Mercer (JM a smoothing technique which does not take into account document length.
The combination of the prior boosts the retrieval performance, so that it outperforms a LM with a document length dependent smoothing component (Dirichlet prior) and other state of the art high-performing scoring function (BM25 Improvements are significant, robust across different collections and query sizes.

This paper addresses the problem of information and service accessibility in mobile devices with limited resources.
A solution is developed and tested through a prototype that applies state-of-the-art Distributed Speech Recognition (DSR) and knowledge-based Information Retrieval (IR) processing for spoken query answering.
For the DSR part, a configurable DSR system is implemented on the basis of the ETSI-DSR advanced front-end and the SPHINX IV recognizer.
For the knowledge-based IR part, a distributed system solution is developed for fast retrieval of the most relevant documents, with a text window focused over the part which most likely contains an answer to the query.
The two systems are integrated into a full spoken query answering system.
The prototype can answer queries and questions within the chosen football (soccer) test domain, but the system has the flexibility for being ported to other domains.

Exhaustivity and Specificity in logical Information Retrieval framework were introduced by Nie [16
However, even with some attempts, they are still theoretical notions without a clear idea of how to be implemented.
In this study, we present a new approach to deal with them.
We use propositional logic and lattice theory in order to redefine the two implications and their uncertainty P(d 8594; q) and P(q 8594; d We also show how to integrate the two notions into a concrete IR model for building a new effective model.
Our proposal is validated against six corpora, and using two types of terms (words and concepts The experimental results showed the validity of our viewpoint, which state: the explicit integration of Exhaustivity and Specificity into IR models will improve the retrieval performance of these models.
Moreover, there should be a type of balance between the two notions.

In this work, we describe a system for classification of propositions from legal judgements of the Supreme Court of India.
The system was submitted for participation to the Information Access in the Legal Domain track at the Forum for Information Retrieval Evaluation (FIRE) 2013.
The system uses a multi-class Maximum Entropy classifier and various specially designed features to capture the underlying characteristics of the legal propositions.
The best performing feature set was chosen by 10-fold cross-validation over the training set.
The system achieved an accuracy of 65.03% on the training set and an accuracy of 51.02% on the test set.

We built two Information Retrieval systems that were targeted for the TREC-6 “aspect oriented” retrieval track.
The systems were built to test the usefulness of different visualizations in an interactive IR setting-in particular, an “aspect window” for the chosen task, and a 3-D visualization of document inter-relationships.
We studied 24 users of the system in order to investigate: whether the systems were more effective than a control system, whether experienced users outperformed novices, whether spatial reasoning ability was a good predictor of effective use of 3-D, and whether the systems could be compared indirectly via a control system.
Our results show substantial differences in user performance are related to spatial reasoning ability and to a lesser degree other traits.
We also obtained markedly different results from the direct and indirect comparisons.

Dissent is an anonymous communication system that offers much stronger anonymity guarantees than Tor.
Dissent clients efficiently publish anonymous messages, but any response must be broadcast to all clients.
This approach does not scale to many clients or large responses.
We’ve developed a private information retrieval scheme that enables clients to receieve responses without the bandwidth burden of broadcasting.
Our protocol reduces bandwidth usage from quadratic in the number of clients to linear in the number of clients, while maintaining the same anonymity guarantees as Dissent.

It is a period of information explosion.
Especially for spatial information science, information can be acquired through many ways, such as man-made planet, aeroplane, laser, digital photogrammetry and so on.
Spatial data sources are usually distributed and heterogeneous.
Federated database is the best resolution for the share and interoperation of spatial database.
In this paper, the concepts of federated database and interoperability are introduced.
Three heterogeneous kinds of spatial data, vector, image and DEM are used to create integrated database.
A data model of federated spatial databases is given.

In this paper, we describe a novel approach for acquiring and managing digital models of archaeological sites.
More in detail, we present an approach to digitization based on a robotic platform and a cloud-based information system.
Our robot is the result of over two years of efforts by a group of cultural heritage experts, computer scientists and roboticists.
Exploiting the large and heterogeneous amount data provided by the robotic platform requires this data to be managed, organized and analyzed.
To this extent we developed ARIS (ARchaeological Information System a software that exploits modern information retrieval and machine learning systems.

This paper presents a genetic relevance optimisation process performed in an information retrieval system.
The process uses genetic techniques for solving multimodal problems (niching) and query reformulation techniques commonly used in information retrieval.
The niching technique allows the process to reach different relevance regions of the document space.
Query reformulation techniques represent domain knowledge integrated in the genetic operators structure in order to improve the convergence conditions of the algorithm.
Experimental analysis performed using a TREC sub-collection validates our

This tutorial aims to provide attendees with a detailed understanding of end-to-end evaluation pipeline based on human judgments (offline measurement The tutorial will give an overview of the state of the art methods, techniques, and metrics necessary for each stage of evaluation process.
We will mostly focus on evaluating an information retrieval (search) system, but the other tasks such as recommendation and classification will also be discussed.
Practical examples will be drawn both from the literature and from real world usage scenarios in industry.

The diverse data types of musical information domain including binary and text-based structures create semantic gaps between the entities of different data formats.
This leads to difficulties in analyzing, capturing and managing entities of the domain.
In this paper, we present a semantic knowledge management system, called SEMU, to efficiently managing musical information.
We propose SEMU ontology to capture information extracted from various data types and sources.
In order to extract information from raw data, we use Musical Information Retrieval techniques for audio files and Natural Language Processing techniques for text-based formats.
We develop a rule-based solution to enrich the system knowledge base.
Later, we provide a web application with seamless integration between SEMU knowledge base and user interface to enable users to benefit from the advantages of the SEMU system.
2015 Elsevier B.V.
All rights reserved.

The compositional mechanisms involved in the comprehension and creation of concepts is of much interest to the communities studying Cognitive Science and Artificial Intelligence.
Nevertheless, comprehension has been largely studied while the creation or production of novel concepts has been somewhat forgotten.
We present a model for concept generation using a well known lexical ontology WordNet along with the results of our experiments that evaluate the creative characteristics of the generated concepts.
We also explain how these ideas may be applied to other areas of research, namely to Information Retrieval systems.

While the field of Information Retrieval originally had the search for the most relevant documents in mind, it has become increasingly clear that in many instances, what the user wants is a piece of coherent information, derived from a set of relevant documents and possibly other sources.
Reducing relevant documents, passages, and sentences to their core is the task of text summarization or information condensation.
Applying text-based technologies to speech is not always workable and often not enough to capture speech specific phenomena.
In this paper, we will contrast speech summarization with text summarization, give an overview of the history of speech summarization, its current state, and, finally, sketch possible avenues as well as remaining challenges in future research.

The effects of word recognition errors (WRE) in Spoken Document Retrieval have been well studied and well reported in recent Information Retrieval (IR) literature.
Much less experimental work has been devoted to studying the effects of WRE in Spoken Query Processing in IR.
It is easy to hypothesize that given the typical length of the user query, the effects of WRE in spoken queries on the performance of IR systems must be destructive.
The experimental work reported in this paper intends to test that.
The paper reports on the background of such a study, on the construction of a suitable test collection, on the first experimental results obtained and on the limitations of the study.
The results show that classical IR techniques are quite robust to considerably high levels of WRE rates in spoken queries (roughly below 40 in particular for long queries.

PDF documents form a rich resource repository of knowledge on the Internet both for academia and for business.
The lack of logical structure information of PDF documents, however, limits the possibility of automatic information retrieval in many ways.
In order to enhance its usability, the logical components of a PDF document, such as title, heading, paragraph, or reference for academic articles, need to be detected.
This project builds on previous work that extracts the physical layout information of conference papers from PDF files, and aims to detect the logical structure from their physical layouts.
We designed and implemented two algorithms for homogeneous block aggregation and for logical structure detection in software using objectedoriented technology.
They will be evaluated on an unseen test set of conference articles from the Association for Computational Linguistics (ACL) Anthology.

EXTENDED ABSTRACT In cross-language information retrieval (CLIR relevance feedback (RF) has been demonstrated to be effective in improving retrieval results, especially when reliable RF information can be obtained from users.
Though query expansion (QE) is the leading RF approach in CLIR and it can take place before or/and after translating the query, it should not be the only possible RF method.
In our demonstration, besides an implementation of posttranslation QE, we also implement a novel RF approach called translation enhancement (TE) and the integration of TE and QE.

Although methods exist to identify well-defined relations, such as is_a or part_of, existing tools rarely support a user who wants to define new, domain-specific relations.
We conducted a situated case study in plant science and introduce four new domain-specific relations that are of interest to domain scientists but have not been explored in information science.
Results show that precision varies between relations and ranges from 0.73 to 0.91 for the manufacturer location category, 0.89 and 0.93 for the seed donor-bank relation, 0.29 and 0.67 for the seed origin location, and 0.32 and 0.77 for the field experiment location.
The manufacturer location category recall varies from 0.91 to 0.94, the seed bank-donor location recall ranges between 0.93 and 1, the seed origin relation from 0.33 to 0.82 while the field experiment location from 0.67 to 0.83 depending on the classifier and using a combination of lexical and syntactic features in the background.

Common people often experience difficulties in accessing relevant, correct, accurate and understandable health information online.
Developing search techniques that aid these information needs is challenging.
In this paper we present the datasets created by CLEF eHealth Lab from 2013-2015 for evaluation of search solutions to support common people finding health information online.
Specifically, the CLEF eHealth information retrieval (IR) task of this Lab has provided the research community with benchmarks for evaluating consumer-centered health information retrieval, thus fostering research and development aimed to address this challenging problem.
Given consumer queries, the goal of the task is to retrieve relevant documents from the provided collection of web pages.
The shared datasets provide a large health web crawl, queries representing people’s real world information needs, and relevance assessment judgements for the queries.

Ranking is likely the most important process of an Information Retrieval (IR) system that will be used to evaluate and measure the effectiveness of an IR system.
This paper aims to produce the implementation of Fuzzy Logic Controller of Mamdani-type Fuzzy Inference System for defining the ranking function by using the BM25 Model in the Malay IR System that also includes the Malay Stemmer.
The result of the ranking function then will be compared to the result of Vector Space Model that is also applied in Malay IR System and be evaluated using relevant document by the Hadith expert.
The results showed that FBMIR has slightly outperformed Vector Space Model on 3 Topic Set of query results such as x201C;Iman&#x201D
x201C;Ilmu&#x201D; and
x201C;Wuduk&#x201D; on the Precision at Rank 10 and the percentage of no relevant document in the top ten retrieved measures.

To address the challenge of adapting Information Retrieval (IR) to the constantly evolving user tasks and needs and to adjust it to user interactions and preferences we develop a new model of user behavior based on Markov chains.
We aim at integrating the proposed model into several aspects of IR, i.e. evaluation measures, systems and collections.
Firstly, we studied IR evaluation measures and we propose a theoretical framework to describe their properties.
Then, we presented a new family of evaluation measures, called Markov Precision (MP based on the proposed model and able to explicitly link lab-style and on-line evaluation metrics.
Future work will include the presented model into Learning to Rank (LtR) algorithms and will define a collection for evaluation and comparison of Personalized Information Retrieval (PIR) systems.

Document clustering has been a particularly active research field within the Information Retrieval (IR) community.
Among the numerous clustering algorithms proposed, single-pass clustering stands out in terms of both time and space efficiency.
However, it is generally acknowledged that single-pass clustering has a major defect, namely its output depends on the order in which documents are presented.
Building on our previous work, and having identified single-pass clustering as potentially useful for P2P IR, we study the extent to which this is true in practical terms.
We do so by experimenting with two large web-based testbeds, which are suitable for Peer-to-Peer IR evaluation.
The results of our study show that document ordering does not practically matter for single-pass clustering.

Aging populations have a huge demand of searching health information online.
However, for their relatively worse physical ability and cognitive ability, normal searching interface may not be able to fulfill aging people's special demands.
In our work, we point out the problem which aging populations are facing when they use the normal online searching system.
Then, we propose our interactive health information retrieval framework for aging populations using actual pages from the WebMD.com, a popular website where people search for health information.
There are three phases in our proposed framework: the retrieval model design, the interface design and the evaluation design.
We hope our interface could help aging users obtain better experience when they search for healthcare information online.

This paper describes the participation of the IXA NLP group at the CLEF 2008 Robust-WSD Task.
This is our first time at CLEF, and we participated at both the monolingual (English) and the bilingual (Spanish to English) subtasks.
We tried several query and document expansion and translation strategies, with and without the use of the word sense disambiguation results provided by the organizers.
All expansions and translations were done using the English and Spanish wordnets as provided by the organizers and no other resource was used.
We used Indri as the search engine, which we tuned in the training part.
Our main goal was to improve (Cross Lingual)
Information Retrieval results using WSD information, and we attained improvements in both mono and bilingual subtasks, although the improvement was only significant for the bilingual subtask.
As a secondary goal, our best systems ranked 4th overall and 3rd overall in the monolingual and bilingual subtasks, respectively.

In multimedia information retrieval applications, contentbased image retrieval is essential for retrieving relevant mnltimedia documents.
The purpose of our paper is to provide both effective representation and efficient retrieval of images when a pixel-level original image is antomaticahy or mannally transformed into its iconic image containing meaningful graphic descriptions, called icon objects.
For this, we propose a new spatial match representation scheme to describe spatial relationships between icon objects preciseIy by using accurate positional operators as well as by expressing objects as rectangles, rather than pointer.
In order to accelerate image searching, we also design an efficient retrieval method using a signature file technique.
Finally, we show from our experiments that our representation scheme achieves better retrieval effectiveness than the 9-DLT scheme.

The last decade has seen a great progress on the research and applications of information retrieval.
The major improvement has been made to combine traditional keyword-based search with implicit inputs such as eye movements or fixations, mouse clicks and voice commands, thus gradually forming a new branch under the name of proactive information retrieval.
This paper focuses on the the study of eye movement tracking in the document retrieval by constructing a universal online research platform that merges all the research steps collecting data, feature selection, model selection and testing into a flexible and extendible cross-platform software system.

This paper discusses some main difficulties of restricted-domain question-answering systems, in particular the problem of precision performance.
We propose methods for improving the precision, which can be classified into two main approaches: improving the Information Retrieval module, and improving its results.
We present the application of these methods in a real QA system for a large company, which yielded very good results.

The paper presents a text classification approach for classifying tweets into two classes: availability/ need, based on the content of the tweets.
The approach uses a language model for classification based on word-embedding of fixed length to get the semantic relationship among words.
The approach uses logistic regression for actual classification.
The logistic regression measures the relationship between the categorical dependent variable (tweet label) and a fixed length words embedding of the tweetcontent(words by estimating the probabilities of tweets produced by embedding words.
The regression function is estimated by maximum likelihood estimation of composition of tweets by these embedding words.
The approach produced 84% accurate classification for the two classes on the training set provided for shared task on "Information Retrieval from Microblogs during Disasters (IRMiDis as a part of, The 9th meeting of Forum for Information Retrieval Evaluation (FIRE 2017).

This paper describes a system designed to retrieve melodies from a database on the basis of a few notes sung into a microphone.
The system first accepts acoustic input from the user, transcribes it into common music notation, then searches a database of 9400 folk tunes for those containing the sung pattern, or patterns similar to the sung pattern; retrieval is ranked according to the closeness of the match.
The paper presents an analysis of the performance of the system using different search criteria involving melodic contour, musical intervals and rhythm; tests were carried out using both exact and approximate string matching.
Approximate matching used a dynamic programming algorithm designed for comparing musical sequences.
Current work focuses on developing a faster algorithm.

This article reports research that explored children’s information retrieval behavior using an online public access catalog (OPAC) in an elementary school library.
The study considers the impact of a variety of factors including user characteristics, the school setting, interface usability, and information access features on children’s information retrieval success and breakdown.
The study reports the overall patterns of children’s behavior that influence success and breakdown in information retrieval as well as findings about the intentions, moves, plans, strategies, and search terms of children in grades one through six.

While it is generally agreed that an elliptical Verb Phrase must be identical to its antecedent, the precise formulation of the identity condition is controversial.
I present a semantic identity condition on VP ellipsis: the elided VP must have the same meaning as its antecedent.
I argue that a semantic identity condition is superior to a syntactic condition on both empirical and theoretical grounds.
In addition, I show that the proposed condition differs significantly from previously proposed semantic conditions, in that other approaches do not take into account the dynamic nature of semantic representation.

This paper presents a unified utility framework for resource selection of distributed text information retrieval.
This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases.
With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications.
Specifically, when used for database recommendation, the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents
This new model provides a more solid framework for distributed information retrieval.
Empirical studies show that it is at least as effective as other state-of-the-art algorithms.

Users nowadays need to manage large amounts of information, including documents, e-mails, contacts, and multimedia content.
To facilitate the tasks of organisation, maintenance, and retrieval of personal information, a number of semantics-based methods have emerged; these methods employ (personal) ontologies as an underlying infrastructure for organising and querying the personal information space.
In this paper we present OntoFM, a novel personal information management tool that offers a mindmapinspired interface to facilitate user interactions with the information base.
Besides serving as an information retrieval aid, OntoFM allows the user to specify and update the semantic links between information items, constituting thus a complete personal information management tool.

Chaotic special properties make the chaotic encryption technology to be an important research field of information science and technology.
However, as digital chaos is affected by the limited precision of computer, chaos system properties present degradation--short periodicity of output sequence.
This paper proposes K-L transform method for Logistic 0/1 sequence, and then analyses time-autocorrelation, complexity, frequency spectrum and time-frequency of the sequence.
Simulation results prove that this method can effectively improve the complexity of key sequence.
More than that, it also can increase the period of Logistic sequence, which makes up the short periodicity phenomenon of digital chaotic sequence.
So this kind of digital chaotic key sequence can be applied safely to encryption system.

We propose a simple, scalable, and non-parametric approach for short text classification.
Leveraging the well studied and scalable Information Retrieval (IR) framework, our approach mimics human labeling process for a piece of short text.
It first selects the most representative and topical-indicative words from a given short text as query words, and then searches for a small set of labeled short texts best matching the query words.
The predicted category label is the majority vote of the search results.
Evaluated on a collection of more than 12K Web snippets, the proposed approach achieves comparable classification accuracy with the baseline Maximum Entropy classifier using as few as 3 query words and top-5 best matching search hits.
Among the four query word selection schemes proposed and evaluated in our experiments, term frequency together with clarity gives the best classification accuracy.

We have proposed a document representation model based on query and content, this model uses users&#x02019; query behavior and content information of documents for representing document.
The pragmatic information from users&#x02019; implicit feedback and the semantic and syntactic information from documents are integrated to dynamically regulate the term-weight of index database.
In this paper, our document representation model is applied to website search engine and the key parameters of this model are determined by experiments.
The results show that this model can consequently improve precision ratio in information retrieval.

Automatic text classification is one of the most important tools in Information Retrieval.
As the traditional methods for text classification cannot find the best feature set, the GA is applied to the feature selection because it can get the global optimal solution.
This paper presents a novel text classifier from positive and unlabeled documents based on GA.
Firstly, we identify reliable negative documents by improved 1-DNF algorithm.
Secondly, we build a set of classifiers by iteratively applying SVM algorithm on training example sets.
Thirdly, we discuss an approach to evaluate the weighted vote of all classifiers generated in the iteration steps to construct the final classifier based on GA instead of choosing one of the classifiers as the final classifier.
GA evolving process can discover the best combination of the weights.
The experimental result on the Reuter data set shows that the performance is exciting.

An application of mathematical lattice theory, called relationship lattices, is utilized to attack problems of operational bibliographic information retrieval.
The proposed solution offers an interface to the information searcher enabling operation in a world of concepts, authors, and document records and their relationships.
This hides the complexities of query language and database structures, and it allows to use a personally preferred terminology and to browse, query and download document records in a convenient way.
The main component of the proposed solution is a personal thesaurus built up as a relationship lattice.

The objective of Information Retrieval is to retrieve all relevant documents for a user query and only those relevant documents.
Much research has focused on achieving this objective with little regard for storage overhead or performance.
In this paper we evaluate the use of Part of Speech Tagging to improve the index storage overhead and general speed of the system with only a minimal increment in precision and recall measurements.
We tagged 242 abstracts of Arabic documents using the Proceedings of the Saudi Arabian National Conferences as a source.
All these abstracts involve computer science We also built an automatic information retrieval system to handle Arabic data.
We then did a series of experiments to identify the most relevant part of speech indexing method.

Image retrieval in general and content based image retrieval in particular are well known research fields in information retrieval management.
An image contains several types of visual information which are difficult to extract and combine manually by humans.
The main goal of this paper is to show multimedia information retrieval task using the combination of textual pre-filtering and image re-ranking.
The combination of textual and visual techniques and retrieval processes used to develop the multimedia information retrieval system by which we solves the problem of the semantic gap of the given query.
Five late semantic fusion approaches are used for text based and content based image retrieval of any dataset.
The logistic regression relevance feedback algorithm is used to determine the similarity between the images from the dataset to the query.

A Treebank is a linguistic resource that is composed of a large collection of manually annotated and verified syntactically analyzed sentences.
Statistical Natural Language Processing (NLP) approaches have been successful in using these annotations for developing basic NLP tasks such as tokenization, diacritization, part-of-speech tagging, parsing, among others.
In this paper, we address the problem of exploiting Treebank resources for statistical parsing of Modern Standard Arabic (MSA) sentences.
Statistical parsing is significant for NLP tasks that use parsed text as an input such as Information Retrieval, and Machine Translation.
We conducted an experiment on Pen Arabic Treebank (PATB) and the parsing performance obtained in terms of Precision, Recall, and F-measure was 82.4 86.6 84.4 respectively.

In Korean information retrieval, compound nouns play an important role in improving precision in search experiments.
There are two major approaches to compound noun indexing in Korean: statistical and linguistic.
Each method, however, has its own shortcomings, such as limitations when indexing diverse types of compound nouns, over-generation of compound nouns, and data sparseness in training.
In this paper, we propose a corpus-based learning method, which can index diverse types of compound nouns using rules automatically extracted from a large corpus.
The automatic learning method is more portable and requires less human effort, although it exhibits a performance level similar to the manual-linguistic approach.
We also present a new filtering method to solve the problems of compound noun over-generation and data sparseness.

With the emerging focus of search engines on semantic search, there is a growing need to understand queries and documents not only syntactically, but semantically as well.
Over the recent years, major search engines have redesigned their search results to accommodate some semantic information, particularly recognized entities in queries and search results.
Recent information retrieval studies in SIGIR have also paid significant amount of attention on entity-related research.
However, techniques for accurate entity recognition and disambiguation are still far from perfect.
The motivation of the workshop is to advance the state of the art on entity recognition and disambiguation for both long and short web documents.

We describe an instance-based reasoning solution to a variety of spatial reasoning problems.
The solution centers on identifying an isomorphic mapping between labelled graphs that represent some problem data and a known solution instance.
We describe a number of spatial reasoning problems that are solved by generating non-deductive inferences, integrating topology with area (and other) features.
We report the accuracy of our algorithm on different categories of spatial reasoning tasks from the domain of Geographical Information Science.
The generality of our approach is illustrated by also solving geometric proportional (IQ-test type) analogy problems.

User profiles and interests have become essential for personalizing information search and retrieval.
Indeed, traditional Information Retrieval Systems (IRS) don't integrate the user in the search process.
Also, users do not always find what they need after a single query.
Instead, they often issue multiple queries, incorporating what they learned from the previous results to iterate and refine how they express their information needs.
So we rely on this process to learn the user information needs without asking him explicitly.
This is achieved by capturing his judgments on the retrieved results.
We consider also, in the construction of the user interests, what he is looking for and what the user doesn't want to find in the future results to build interests that best match his information needs.

Distributional word similarity is most commonly perceived as a symmetric relation.
Yet, directional relations are abundant in lexical semantics and in many Natural Language Processing (NLP) settings that require lexical inference, making symmetric similarity measures less suitable for their identification.
This paper investigates the nature of directional (asymmetric) similarity measures that aim to quantify distributional feature inclusion.
We identify desired properties of such measures for lexical inference, specify a particular measure based on Average Precision that addresses these properties, and demonstrate the empirical benefit of directional measures for two different NLP datasets.

The last few years have seen the emergence of the information retrieval services industry.
This industry consists of three tiers: data base producers, retrieval service vendors, and information centers who service the end user.
The experience of Lockheed Information Systems as a retrieval service vendor is recounted as it pertains to its system and to its relationship with the data base suppliers.
Government competition is seen as a potential threat to the industry.

Retrieving entities instead of just documents has become an important task for search engines.
In this paper we study entity retrieval for news applications, and in particular the importance of the news trail history (i.e past related articles) in determining the relevant entities in current articles.
This is an important problem in applications that display retrieved entities to the user, together with the news article We analyze and discuss some statistics about entities in news trails, unveiling some unknown findings such as the persistence of relevance over time.
We focus on the task of query dependent entity retrieval over time.
For this task we evaluate several features, and show that their combinations significantly improves performance.

In this paper, we propose a simple and .exible spell checker using e0cient associative matching in the AURA modular neural system.
Our approach aims to provide a pre-processor for an information retrieval (IR) system allowing the user’s query to be checked against a lexicon and any spelling errors corrected, to prevent wasted searching.
IR searching is computationally intensive so much so that if we can prevent futile searches we can minimise computational cost.
We evaluate our approach against several commonly used spell checking techniques for memory-use, retrieval speed and recall accuracy.
The proposed methodology has low memory use, high speed for word presence checking, reasonable speed for spell checking and a high recall rate 2002 Pattern Recognition Society.

Modeling term dependence has been shown to have a significant positive impact on retrieval.
Current models, however, use sequential term dependencies, leading to an increased query latency, especially for long queries.
In this paper, we examine two query segmentation models that reduce the number of dependencies.
We find that two-stage segmentation based on both query syntactic structure and external information sources such as query logs, attains retrieval performance comparable to the sequential dependence model, while achieving a 50% reduction in query latency.

A visual term discrimination value analysis method is introduced using a document density space within a distance–angle-based visual information retrieval environment.
The term discrimination capacity is analyzed using the comparison of the distance and angle-based visual representations with and without a specified term, thereby allowing the user to see the impact of the term on individual documents within the density space.
Next, the concept of a “term density space” is introduced for term discrimination analysis.
Using this concept, a term discrimination capacity for distinguishing one term from others in the space can also be visualized within the visual space.
Applications of these methods facilitate more effective assignment of term weights to index terms within documents and may assist searchers in the selection of search terms.

Multimedia information retrieval is a highly diverse field.
A variety of data types, research problems,methodologies are involved.
Researchers in the field come from very different disciplines, ranging from mathematical and physical sciences, computational sciences and engineering, to application domains.
The panel, consisting of highly visible active researchers from both academia and the industry,opens a discussion on the importance of diversity to the healthy growth of the field.
This paper records their opinions expressed at the panel.

Essential for the success of FAQ systems is their ability to systematically manage knowledge including the intelligent retrieval of useful FAQ documents and the continuous evolution of the knowledge base.
Based on our experiences, we propose a hybrid approach for the management of FAQ documents on programming languages written in Portuguese, Spanish or other latin languages.
The approach integrates various types of knowledge and provides intelligent mechanisms for knowledge access as well as the continuous evolution and improvement of the FAQ system throughout its life cycle.
The principal strength of the approach lies in the integration of techniques from Case-Based Reasoning and Information Retrieval customized to the specific requirements and characteristics of the management of FAQ documents.
The approach is currently being implemented and evaluated in the context of an international research project.

In this paper, a new method for English-Chinese cross-lingual information retrieval is proposed and evaluated in NTCIR-II project.
We use the bilingual resources and contextual information to deal with the word sense disambiguation (WSD) and translation disambiguation for query translation.
An EnglishChinese WordNet and a synset co-occurrence model are adopted to solve the problem of word sense ambiguity.
And the translation ambiguity and target polysemy are also resolved using such co-occurrence relationship of synsets.
The experimental results are discussed to analyze the effects of ambiguity in source language and target language.

Here we discuss how to look for <i>similar</i> melody in music databases by giving monophonic melody in sheet.
In this work, we utilize text expression (or sheet music) to describe music and introduce <i>pitch</i> spectrum of melodies.
By this feature, we concisely distinguish music from tempo, transposition or other arbitrary expressions.
We show the usefulness by experimental results.

In this article, we propose to evaluate the lexical similarity information provided by word representations against several opinion resources using traditional Information Retrieval tools.
Word representation have been used to build and to extend opinion resources such as lexicon, and ontology and their performance have been evaluated on sentiment analysis tasks.
We question this method by measuring the correlation between the sentiment proximity provided by opinion resources and the semantic similarity provided by word representations using different correlation coefficients.
We also compare the neighbors found in word representations and list of similar opinion words.
Our results show that the proximity of words in state-of-the-art word representations is not very effective to build sentiment similarity.

Being such a vast resource of information, World Wide Web has become irreplaceable but the outburst of information available over the internet has made web search a time consuming and a very complex process.
Now days if someone need to retrieve any information using internet they come across huge number of web pages because of large amount of data on web.
It becomes difficult task to find information so in order to retrieve meaningful and intelligent information we have many information retrieval (IR) techniques.
To gather the significant information from such a vast available resource Information Retrieval (IR a method of retrieving such information resources which are relevant to an information need is applied.
This paper would review few of these methods for intelligent information retrieval systems on web based on the Ontology information retrieval techniques.

Knowledge intensive organizations have vast array of information contained in large document repositories.
With the advent of E-commerce and corporate intranets/extranets, these repositories are expected to grow at a fast pace.
This explosive growth has led to huge, fragmented, and unstructured document collections.
Although it has become easier to collect and store information in document collections, it has become increasingly difficult to retrieve relevant information from these large document collections.
This paper addresses the issue of improving retrieval performance (in terms of precision and recall) for retrieval from document

This paper discusses the first-hand experiences of the author in developing and teaching courses in neural networks, evolutionary computing and fuzzy logic.
While these courses are offered at the graduate level in a school of engineering, they have attracted a variety of students from many engineering disciplines, from medical studies, from business school, from mathematics and statistics, and from information sciences.
The paper discusses course content, textbooks and written material, software and computer projects, and grading and evaluation.

It is recognized that data, information, knowledge, and intelligence are the fundamental cognitive objects in the brain and cognitive systems.
However, there is a lack of formal studies and rigorous models towards them.
This paper explores the cognitive and mathematical models of the cognitive objects.
The taxonomy and cognitive foundations of abstract mental objects are explored.
A set of mathematical models of data, information, knowledge, and intelligence is formally created.
On the basis of the cognitive and mathematical models of the cognitive objects, formal properties and relationship of contemporary data, information, knowledge, and intelligence are rigorously explained.
Key-Words: Cognitive informatics, brain science, mathematical models, formal theories, data science, information science, knowledge science, intelligence science, and system science

In this paper, we present a new approach to ranking that considers the reading ability (and motivation) of the user.
Web pages can be, increasingly, badly written with unfamiliar words, poor use of syntax, ambiguous phrases and so on.
Readability research suggests that experts and motivated readers may overcome confusingly written text, but nevertheless find it an irritation.
We investigate using readability to re-rank web pages.
We take an extended view of readability that considers the reading level of retrieved web pages using techniques that consider both textual and cognitive factors.
Readability of a selection of query results is examined, and a re-ranking on readability is compared to the original ranking.
Results to date suggest that considering a view of readability for each reader may increase the probability of relevance to a particular user.

Semantic similarity is an essential part for question answering, it is used various fields such as Artificial Intelligence, Natural Language Processing, information retrieval, Document Retrieval and Automatic evaluations.
This paper mainly focuses on similarity measure based on the posted query, and finding the appropriate meaning between the words.
Accessing an accurate answer from the web document is challenging task.
The proposed approach is used to analyze and measuring the similarity between the words.
It presents the Web And semantic knowledge-Driven automatic question answering system (WAD It encompasses three phases to enhance the performance of QA system using the web as well as the semantic knowledge.
Initially, the WAD approach determines the user query, query expansion technique and entity linking method.
The ontology based information is used in WAD to rank the answers and experimental results provide the result with high accuracy than the baseline method.

Music Information Retrieval (MIR) and modern data mining techniques are applied to identify style markers in midi music for stylometric analysis and author attribution.
Over 100 attributes are extracted from a library of 2830 songs then mined using supervised learning data mining techniques.
Two attributes are identified that provide high informational gain.
These attributes are then used as style markers to predict authorship.
Using these style markers the authors are able to correctly distinguish songs written by the Beatles from those that were not with a precision and accuracy of over 98 per cent.
The identification of these style markers as well as the architecture for this research provides a foundation for future research in musical stylometry.
Keywords—Music Information Retrieval, Music Data Mining, Stylometry.

Significant amount of text-based knowledge is created in collaborative web-based environments in the context of education.
In order to efficiently utilize all this information, there is a need to provide the users with an easy access to the information they are interested in.
To achieve this, various methods of information retrieval (IR) can be used.
We analyze six different algorithms for seeking similar sections of text in context of a social mindtool called the Woven Stories.

In this work I propose to describe a new model of information retrieval.
I study the effects of using weighted index terms recognition mistakes in a document indexing system and I evaluate indexing performance when short and long requests are used.
The effect of weighting index terms in the document collection and in the requests is analyzed.
Given the typical requests submitted to term indexing system, it seems easy to consider that the effects of term recognition mistakes in user requests must be severely destructive on the effectiveness of the system.
The experimental study reported in this paper shows that the use of classical term Indexing technique for processing this kind of request is robust to considerably high levels of term recognition mistakes, in particular for long requests.
Moreover, both standard pertinence feedback and pseudo pertinence feedback can be employed to improve the effectiveness of user request processing.

The Information Retrieval and Advertising Workshop (IRA 2009) was held on July 23, 2009 in Boston, Massachusetts, in conjunction with the 32nd Annual ACM SIGIR Conference.
The workshop covered theoretical and empirical issues in several research areas that span the intersection of computational advertising, information retrieval, and economics.
The workshop consisted of 3 invited talks, 6 refereed paper presentations, 2 positions statement presentations and several discussion sessions.

A long standing problem in information retrieval is how to treat queries that are best answered by two or more distinct sets of documents.
Existing methods average across the words or terms in a user’s query, and consequently, perform poorly with multimodal queries, such as Show me documents about French art and American jazz
We propose a new method, the Relevance Density Method for selecting documents relevant to a user’s query.
The method can be used whenever the documents and the terms are represented by vectors in a multi-dimensional space, such that the vectors corresponding to documents and terms dealing with closely related topics are close to each other.
We show that the Relevance Density Method performs better for multimodal as well as single mode queries than an averaging method.
In addition, we show that retrieval is substantially faster for the new method.

In this paper we describe Taisc eala a web-based system which provides content-based retrieval on an up-to-date archive of RT E radio news bulletins.
Taisc eala automatically records and indexes news bulletins twice daily using a stream of phones recognised from the raw audio data.
A user's typed query is matched against xed length windows from the broadcasts.
A user interface allows the news bulletins most likely to be relevant to be presented and the user to select sub-parts of the bulletins to be played.
Many of the parameters we have chosen to use such as the size and amount of overlap of windows and the weighting of phones within those windows, have been determined within the framework of the TREC Spoken Document Retrieval track and are thus well-founded.
We conclude the paper with a walkthrough of a worked example retrieval and an outline of our plans for extending Taisc eala into an integrated digital library for news.

Research in the probabilistic theory of information retrieval involves the construction of mathematical models based on statistical assumptions.
One of the hazards inherent in this kind of theory construction is that the assumptions laid down maybe inconsmtent in unanticipated ways with the data to which they are applied.
Another hazard is that the stated assumptions may not be those on which the derived modeling equations or resulting experiments are actually based.
Both kinds of mistakes have been made m past research on probabihstic reformation retrieval.
One consequence of these errors is that the statistical character of certain probabilistic IR models, including the so-called Binary Independence model, has been seriously misapprehended

With the emergence of more and more XML documents, effectively and efficiently retrieving information from XML documents has become an active research area.
Since XML documents lie between structured data and unstructured data which describe both content and structure, it is a huge challenge for effectively and efficiently retrieving information from XML documents.
This paper develops a novel retrieval model named as Extend Vector Space Model which effectively combines XPath and Vector Space Model for XML information retrieval.
A prototype system for XML information retrieval based on this retrieval model has been implemented, and several corresponding algorithms have been introduced.
The experiments show that this model has effectively improved recall and precision.

Some recent studies on information retrieval and information seeking have examined the utility of serendipitous discovery.
This research argues that serendipitous discovery has a positive impact on information retrieval and can happen most frequently in semantic web built on the framework of topic maps.
This paper discusses the components of topic map that influence serendipitous discovery as well as the elements of topic map designs that may enhance serendipitous discovery.
To that end the results of a study on the effects of serendipitous discovery in topic-map-based ontology systems are discussed in the context of information seeking.

Relation graphs, in which multi-type (or single type) nodes are related to each other, frequently arise in many important applications, such as Web mining, information retrieval, bioinformatics, and epidemiology.
In this study, We propose a general framework for clustering on relation graphs.
Under this framework, we derive a family of clustering algorithms including both hard and soft versions, which are capable of learning cluster patterns from relation graphs with various structures and statistical properties.
A number of classic approaches on special cases of relation graphs, such as traditional graphs with singly-type nodes and bi-type relation graphs with two types of nodes, can be viewed as special cases of the proposed framework.
The theoretic analysis and experiments demonstrate the great potential and effectiveness of the proposed framework and algorithm.

This paper develops a general and formal frame-work for the ranking of web documents by considering the multimedia information contained in these documents.
Multimedia information is mostly related to images and videos.
Ranking can be treated as a combination of static and dynamic ranking.
In this paper, we have described a static ranking method based on the analysis of the images present in the web document and integrated it into the ranking framework which is based on Markov Random Field Model, which combines both static and dynamic ranking.
We have described a novel metric DQEV(Document Quality Enhancing Value based on the images present in a web document and the value of DQEV indicates to what extent the images in a web document increase its value.
Integration of an Image Search Engine has also been proposed for the computation of DQEV.
It has also been shown that the integration of DQEV can increase the effectiveness of the ranking system.

Nearest neighbor search is one of the most fundamental problem in machine learning, machine vision, clustering, information retrieval, etc.
To handle a dataset of million or more records, efficient storing and retrieval techniques are needed.
Binary code is an efficient method to address these two problems.
Recently, the problem of finding good binary code has been formulated and solved, resulting in a technique called spectral hashing [21
In this work we analyze the spectral hashing, its possible shortcomings and solutions.
Experimental results are promising.

The VCM (Vector-Correlative Model) is one of the currently popular models for information searching.
In this paper, author inducted Fuzzy Sets theory and approach for to construct information searching models, and presented a new VCM, which is called Fuzzy Vector-correlative Model (FVCM
So, its theory structure and user's searching quizzing mode has been dimming improved.
We have show with some fringe experimentation that the new model has overcome some intrinsic defects of the exciting ones, and has raised the precision and recall of information search system.

We examine issues in the design of fully dynamic information retrieval systems supporting both document insertions and deletions.
The two main components of such a system, index maintenance and query processing, affect each other, as high query performance is usually paid for by additional work during update operations.
Two aspects of the system incremental updates and garbage collection for delayed document deletions are discussed, with a focus on the respective indexing vs. query performance trade-offs.
Depending on the relative number of queries and update operations, different strategies lead to optimal overall performance.

This paper first discusses the method of attribute reduction to determine the discernibility matrix and the discernibility function, which lead to some questions being asked.
To find the answers, a new discernibility function is introduced based on information systems and a logical formula defined in the information system.
Because each formula produces a granule, the new discernibility function also corresponds to a granule viewed as the semantics.
Formulas and granules make it possible to connect the discernibility function with granular computing, which is a current topic of data processing in information science.
It sets the stage for research on the new discernibility function using a granular computing method.
Accordingly, a conclusion is reached which shows the granule produced by the new discernibility function is equal to the union of all discernibility relations generated by the attributes.
Some theorems are proved based on the conclusion, which are answers to the questions.

Natural language processing techniques may hold a tremendous potential for overcoming the inadequacies of purely quantitative methods of text information retrieval, but the empirical evidence to support such predictions has thus far been inadequate, and appropriate scale evaluations have been slow to emerge.
In this chapter, we report on the progress of the Natural Language Information Retrieval project, a joint eeort of several sites led by GE Research, and its evaluation in the 6th Text Retrieval Conferences (TREC-6).

Up to now, there are so many CLIR systems has been researched and built.
Generally, These CLIR systems are built upon some search engine to skip building a crawler, an indexer and a searcher component.
By this way, these CLIR systems do not have enough documents gathered for identifying pairs of similar content documents in languages and they have to send and receive too much data to and from the search engine and web sites while processing user queries.
This is a big disadvantage which makes the CLIR systems inefficient.
In this paper, we would like to introduce a model of Cross-lingual information retrieval system for Vietnamese-English web sites which include a crawler, an indexer and a searcher and show how gathered documents are processed to efficiently identify and retrieve the similar documents in languages.

In this paper we compare two types of corpus, focusing on the lexical ambiguity of each of them.
The first corpus consists mainly of general newspaper articles and literature excerpts, while the second belongs to the medical domain.
To conduct the study, we have used two different disambiguation tools.
First, each tool was validated in its respective application area.
We then use these systems in order to assess and compare both the general ambiguity rate and the particularities of each domain.
Quantitative results show that medical documents are lexically less ambiguous than unrestricted documents.
Our conclusions emphasize the importance of the application area in the design of NLP tools.

Clustering hypertext document collection is an important task in Information Retrieval.
Most clustering methods are based on document content and do not take into account the hyper-text links.
Here we propose a novel PageRank based clustering (PRC)
algorithm which uses the hypertext structure.
The PRC algorithm produces graph partitioning with high modularity and coverage.
The comparison of the PRC algorithm with two content based clustering algorithms shows that there is a good match between PRC clustering and content based clustering.

In this article, we report on the study of Terrier search engine for Quranic verse retrieval problem.
However, the results produced by Terrier search engine are not yet completely satisfying; an extensible framework has been proposed that incorporates feedback from the user to generate a better, query.
In this article, we integrate the feedback framework with the Terrier search engine and extend the existing browser-based interface of Terrier search engine to support explicit relevance feedback, reevaluate the query when new feedback is available.
We also modify the existing Terrier search engine to support queries; the evaluation of the query should reuse the partial results from the evaluation of the original query.

We construct a scheme for private information retrieval with k databases and communication complexity O(n

Private as well as commercial music collections keep growing in size and diversity.
With an increasing number of tracks and the resulting complexity users quickly face problems in handling their collections in an adequate way.
At the same time, new business models of online vendors arise and the inherent industry interest in new ways of distribution channels and devices becomes immanent.
In this paper, we show alternative ways of interacting with large music collections, based on the Self-Organising Map clustering algorithm applied to an audio feature representation of audio files.
We therein focus on the presentation of full desktop applications as well as applications for mobile devices like PDAs and Smartphones with the goal of bringing Music Information Retrieval technologies closer to end users.
Further, the presented interfaces give an outlook to means of access to other types of media in streaming environments, e.g. video.

Children experience several difficulties retrieving information using current Information Retrieval (IR) systems.
Particularly, children struggle to find the right keywords to construct queries given their lack of domain knowledge.
This problem is even more critical in the case of the specialized health domain.
In this work we present a novel method to address this problem using a cross-media search interface in which the textual data is searched through visual images.
This solution aims to solve the recall and recognition problem which is salient for health information, by replacing the need for a vocabulary with the easy task of recognising the different body parts.

High-throughput biological data acquisition and processing technologies have shifted the focus of biological research from the realm of traditional experimental science (wet biology) to that of information science (in silico biology Powerful computation and communication means can be applied to the very large amount of apparently incoherent data coming from biomedical research.
Part of the experiments and tests that we used to do in vitro and/or in vivo can now be done in silico.
The concept of Laboratory (Lab) on Chip (LoC) is the natural evolution of System on Chip (SoC) by using an array of heterogeneous technologies.
This paper reviews first hardware supports and technologies that allow the studies of whole bodies of molecules through the different steps of the biological information flow, and
then some of the algorithms and softwares for data processing and integration

In this paper we present our structured information retrieval model based on subgraphs similarity.
Our approach combines a content propagation technique which handles sibling relationships with a document query matching process on structure.
The latter is based on tree edit distance (TED) which is the minimum set of insert, delete, and replace operations to turn one tree to another.
As the effectiveness of TED relies both on the input tree and the edit costs, we experimented various subtree extraction techniques as well as different costs based on the DTD associated to the Datacentric collection.

We present a method to translate queries from an arbitrary source language to retrieve documents in a destination language merely with easily obtainable instruments such as a machine readable dictionary and monolingual corpora in both languages.
The key is to infer probabilistical information about the query and structuring the destination language terms accordingly.
Though the results compare unfavourably with those obtained with more sophisticated but difficult to obtain IRmethods using Part-of-Speech-Tagging and/or Phrase dictionaries, our work shows the successful deployment and combination of related work to crosslingual Information Retrieval.

Current Information Retrieval systems are often based on topicality.
They estimate relevance by comparing the similarity between the user query and each document.
These systems do not take into account important contextual information.
More specifically, they do not often apply mechanisms to filter out redundant information.
We interpret context here as the set of chunks of text from the ranked set of documents that the user has already seen.
This is a valuable contextual information to guide the retrieval processes in a way that avoids redundancy.
It is desirable that the ranking of results is composed by relevant but also novel material.
This means that each document must provide to the user unseen information which is related to his need
In this work we study different novelty detection approaches that make good use of this contextual information.
We show that these techniques can be applied effectively and efficiently at the sentence level.

The current exponential growth of the Internet precipitates a need for new tools to help people cope with the volume of information.
To complement recent work on creating searchable indexes of the World-Wide Web and systems for filtering incoming e-mail and Usenet news articles, we describe a system which helps users keep abreast of new and interesting information.
Every day it presents a selection of interesting web pages.
The user evaluates each page, and given this feedback the system adapts and attempts to produce better pages the following day.
We present some early results from an AI programming class to whom this was set as a project, and then describe our current implementation.
Over the course of 24 days the output of our system was compared to both randomly-selected and human-selected pages.
It consistently performed better than the random pages, and was better than the human-selected pages half of the time.

In this research, we focus on tracking topics that originate and evolve from a specific event.
Intuitively, a few key elements of a target event, such as date, location, and persons involved, would be enough for making a decision on whether a test story is on-topic.
Consequently, a profile-based event tracking method is proposed.
We attempt to build an event profile from the given on-topic stories by robust information retrieval technologies.
A feature selection metric and a recognized event clause are utilized to determine most (if not all) key semantic elements of the target event.
Preliminary experiments on the TDT2 mandarin corpus show that this profile-based event tracking method is promising.

The performance of distributed text document retrieval systems is strongly influenced by the organization of the inverted text.
This article compares the performance impact on query processing of various physical organizations for inverted lists.
We present a new probabilistic model of the database and queries.
Simulation experiments determine those variables that most strongly influence response time and throughput.
This leads to a set of design trade-offs over a wide range of hardware configurations and new parallel query processing strategies.

The paper makes three points of significance for IR research 1)
The Cranfield paradigm of IR evaluation seems to lose power when one looks at human instead of system performance 2)
Searchers using IR systems in reallife use rather short queries, which individually often have poor performance.
However, when used in sessions, they may be surprisingly effective.
The searcher’s strategies have not been sufficiently described and cannot therefore be properly understood, supported nor evaluated 3) Searchers in real-life seek to optimize the entire information access process, not just result quality.
Evaluation of output alone is insufficient to explain searcher behavior.

The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, phrasal translation as well as evaluations on Cross-Language Information Retrieval.
A two-stages translation model is proposed for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives according to their linguistics-based knowledge.
Different rescoring techniques are proposed and evaluated in order to select best phrasal translation alternatives.
Results demonstrate that the proposed translation model yields better translations and retrieval effectiveness could be achieved across JapaneseEnglish language pair.

Text mining is a technique to discover or find interesting patterns from the available text documents, which is more prevalent in text mining applications.
It is also known as knowledge discovery from text (KDT deals with the machine supported analysis of text.
The pattern discovery from the collection of text documents is a well-known problem in text mining.
Analysis of terms or text content and categorization of the documents is a complex task of data mining.
Various techniques of text categorization and classification have been developed in order to efficiently and effectively handle the task of text mining.
Some of them are based on supervised and others unsupervised methods.
Various approaches to carry out text mining are presented in this paper.

Public health informatics is an evolving domain in which practices constantly change to meet the demands of a highly complex public health and healthcare delivery system.
Given the emergence of various concepts, such as learning health systems, smart health systems, and adaptive complex health systems, health informatics professionals would benefit from a common set of measures and capabilities to inform our modeling, measuring, and managing of health system "smartness Here, we introduce the concepts of organizational complexity, problem/issue complexity, and situational awareness as three codependent drivers of smart public health systems characteristics.
We also propose seven smart public health systems measures and capabilities that are important in a public health informatics professional's toolkit.

This paper describes the design, implementation and evaluation of Parallel Multimedia Information Retrieval systems.
PAMIR is implemented on a network of Pentium PC based workstations to exploit the I/O and computation parallelism in both insertion and processing phase of complex similarity queries in multimedia Information retrieval.
The system is capable to index and query multimedia objects concurrently based on more than one features (i.e global color, local color, texture, shape) and each features of an object queried by a different workstation using multithreaded version of M-tree.
The system was tested for a collection of approximately 68000 images and each image described by three features; color moments, histogram layouts and texture.
Experimentation has been shown that the system provides a good reduction in CPU times on both insertion and query processing phase.
Detailed explanation of the system and experimentation results also presented.

This paper and panel discussion will cover the growing and exciting new area of Music Information Retrieval (MIR as well as the more general topic of Audio Information Retrieval (AIR
The main topics, challenges and future directions of MIR research will be identified and four projects from industry and academia are described.

The main idea of pervasive computing is to embed computer into the living environment or tool of human, so as to make the computer invisible from userpsilas sight and the users can focus on their task itself, instead of the computer.
As having natural relationship with the spatial distribution and mobile, the pervasive computing has been applied in geospatial information areas widely.
However, the pervasive computing has brought challenges to the geospatial information science, and the first step of the solution to this challenge should be the modeling of geospatial context .This
paper summarizes and analyses typical geospatial context, builds up a formal context model for geospatial information service based on ontology.
The model is designed hierarchically and the context is expressed using the Web ontology language (OWL which improves context expressing ability.
Finally, this paper designs a context-aware computing model for geospatial information.

In the course of the WedelMusic project [15 we are currently implementing retrieval engines based on musical content automatically extracted from a musical score.
By musical content, we mean not only main melodic motives, but also harmony, or tonality.
In this paper, we first review previous research in the domain of harmonic analysis of tonal music.
We then present a method for automated harmonic analysis of a music score based on the extraction of a figured bass.
The figured bass is determined by means of a template-matching algorithm, where templates for chords can be entirely and easily redefined by the end-user.
We also address the problem of tonality recognition with a simple algorithm based on the figured bass.
Limitations of the method are discussed.
Results are shown and compared to previous research.
Finally, potential uses for Music Information Retrieval are discussed.

The design of systems for automatic audio feature extraction is a central aspect of the field of Music Information Retrieval.
However, feature extraction systems often do not provide an indication of the reliability of the corresponding feature.
Nevertheless, the provision of a reliability or confidence measure can be critical for the usage of a given feature in complex systems and real-world applications.
In the present study we investigate the relationship between the entropy of a rhythmogram, which has been proposed as a descriptor of tempo salience in previous work, and the reliability of the extraction of multiple high level rhythm related features.
The results show that this single descriptor is viable for simultaneously estimating the reliability of multiple rhythm features extraction.
The results also provide quantitative insight that is consistent with qualitative observations extensively reported in the literature on a qualitative basis.

Dictionaries have often been used for query translation in cross-language information retrieval (CLIR
However, we are faced with the problem of translation ambiguity, i.e. multiple translations are stored in a dictionary for a word.
In addition, a word-by-word query translation is not precise enough.
In this paper, we explore several methods to improve the previous dictionary-based query translation.
First, as many as possible, noun phrases are recognized and translated as a whole by using statistical models and phrase translation patterns.
Second, the best word translations are selected based on the cohesion of the translation words.
Our experimental results on TREC English-Chinese CLIR collection show that these techniques result in significant improvements over the simple dictionary approaches, and achieve even better performance than a high-quality machine translation system.

The easy production of data with geographic context has enabled a deeper engagement with people and has led to the emergence of LocationBased Social Networks LBSNs.
Such environments have proved to be very useful in the context of smart cities, however, one of the main challenges has been how to keep users willing to contribute and keep the LBSNs in a continuous operation.
Concerning this problem, we propose an automated production of Volunteered Geographic Information VGI based on Geographic Information Retrieval techniques with the aim of providing valuable and up-to-date information for LBSN environments taking advantage of social media messages around the web.
A prototype software was developed and evaluated through a case study using microtexts.

π-calculus is a calculus for modeling dynamically changing configurations of a network of communicating agents.
This paper studies the suitability of π-calculus as a unifying framework to model the operational semantics of the three paradigms of programming: functional, logic and imperative paradigms.
In doing so, the attempt is to demonstrate that π-calculus models a primitive that is pervasive in the three paradigms and to illustrate that the three forms of sequential computing are special instances of concurrent computing.

In past decades, the precedent for businesses looking for new retail sites has been to choose locations based on intuition or personal experience.
However, in today’s struggling economy and crowded market place, many businesses are looking for more analytical methods.
This research provides a geographic method for prioritizing potential new retail locations.
By using demographic data and Geographic Information Science (GIS the time salespeople spend pursuing unsuitable store location will be reduced.
This research focuses on the Chicago, Illinois seven county metro area, but these same methods could be applied across the United States, perhaps across the world.

A document retrieval system should rank documents in order of their usefulness or satisfaction to the users.
This principle was first explicated in the classic paper by Maron and Kuhns (1 Additional considerations concerning document ranking have been suggested by other researchers (2,3 Particular attention will be given here to the ranking algorithm appropriate for those presenting the same request, but having different information needs.
The research on which this report is based identifies limitations associated with sequencing rules that use a probability ranking technique (4 Three basic and somewhat interdependent limitations will be discussed.

In this paper, we present a novel approach to language independent, ranked document retrieval using our new self-index search engine, Newt.
To our knowledge, this is the first experimental study of ranked self-indexing for multilingual Information Retrieval tasks.
We evaluate the query effectiveness of our indexes using Japanese and English.
We explore the impact that linguistic processing, stemming and stopping have on our character-aligned indexes, and present advantages and challenges discovered during our initial evaluation.

The document titles and their abstracts are frequently used to express the content of the documents.
Many information retrieval systems obtain document keywords from them.
We propose to also extract from the titles some extra-topical details about the content of the documents, for instance, the document intentions.
We use special information extraction techniques for the identification of document intentions and for the construction of the extra-topical representations of the documents.
A possible use for these extra-topical representations in the information retrieval is described.

Word sense ambiguity is recognized as having a detrimental effect on the precision of information retrieval systems in general and web search systems in particular, due to the sparse nature of the queries involved.
Despite continued research into the application of automated word sense disambiguation, the question remains as to whether less than 90% accurate automated word sense disambiguation can lead to improvements in retrieval effectiveness.
In this study we explore the development and subsequent evaluation of a statistical word sense disambiguation system which demonstrates increased precision from a sense based vector space retrieval model over traditional TF*IDF techniques.

This article gives an overview of the problems of information retrieval systems that search court decisions.
Several solutions and new research directions are suggested.
The solutions are inspired by the technologies of current case-based reasoning systems.

We describe the Eurospider component for Cross-Language Information Retrieval (CLIR) that has been employed for experiments at all three CLEF campaigns to date.
The central aspect of our efforts is the use of combination approaches, effectively combining multiple language pairs, translation resources and translation methods into one multilingual retrieval system.
We discuss the implications of building a system that allows flexible combination, give details of the various translation resources and methods, and investigate the impact of merging intermediate results generated by the individual steps.
An analysis of the resulting combination system is given which also takes into account additional requirements when deploying the system as a component in an operational, commercial setting.

We present an overview of requirements for XML query languages, gathered from three application area's: the database community, the traditional SGML/XML community, and the Information Retrieval community.
Differences and similarities of these requirements are discussed, and we show to what extent XQuery and XSLT conform to these requirements.

This study strives to improve medical information search in the CISMEF system by including a conversational agent to interact with the user in natural language.
Experimentation has been set up to obtain human dialogues between a user dealing with medical information search and a CISMEF expert refining the request.
We extend the GODIS dialogue system with dialogue strategies in order to support system digressions.
A model of an artificial agent has been implemented.

Our research presented in this paper concerns the problem of fusing the results returned by the underlying systems to a mediating retrieval system, also called meta-retrieval system, meta-search engine, or mediator.
We propose a fusion technique which is based solely on the actual results returned by each system for each query.
The final (fused) ordering of documents is derived by aggregating the orderings of each system in a democratic manner.
In addition, the fused ordering is accompanied by a level of democracy (alternatively construed as the level of confidence Our method does not require any prior knowledge about the underlying systems, therefore it is appropriate for environments where the underlying systems are heterogeneous and autonomous, thus our method is appropriate for the web.

The study of relevance is one of the central themes in information science where the concern is to match information objects with expressed information needs of the users.
Despite substantial advances in search engines and information retrieval (IR) systems in the past decades, this seemingly intuitive concept of relevance remains to be an illusive one to define and even more challenging to model computationally [5, 13 Geographical information retrieval extends and advances traditional IR methods with a spatial (or geographical dimension) of document representation and relevance measures.

With the population of information technique and Web application, Web information retrieval has became an essential part of work, study and life.
As the problems of retrieval increasing, many researches pay more attention on retrieval technique.
The study presented in this paper proposes the query expansion technique based on ontology.
It uses the rich semantic knowledge of ontology to upgrade the retrieval based on keywords to concepts, and combines it with the specialized engine to improve retrieval effect and efficiency.
The paper also takes patent information for example to explain its application at the end.
Keywords-Ontolog; Query Expansion; Information Retrieval; Conceptual Retrieval; Precision formatting

With the increasing number of documents that are available in digital form, also the number of digital historical documents is increasing (Berkvens, 2001 It cannot be assumed that standard IR systems perform well on historical documents: historical texts differ from modern texts in three ways (Hüning, 1996; Van Der Horst and Marschall, 1989 a) vocabularies have changed b) spelling has changed (sometimes to the extent that two variants of the same word are not even recognizable as such and (c) spelling used to be highly inconsistent (in the Netherlands until the 19th century The goals of this research were to identify the bottlenecks of information retrieval from historical corpora and to find solutions for these bottlenecks.
Section 1 describes our efforts to further identify the bottlenecks, section 2 examines potential solutions, and section 3 sketches our approach to alleviate the bottlenecks.
Finally section 4 draws conclusions and provides directions for future research.

Indexing is an important Information Retrieval (IR) operation, which must be parallelised to support large-scale document corpora.
We propose a novel adaptation of the state-of-the-art single-pass indexing algorithm in terms of the MapReduce programming model.
We then experiment with this adaptation, in the context of the Hadoop MapReduce implementation.
In particular, we explore the scale of improvements that can be achieved when using firstly more processing hardware and secondly larger corpora.
Our results show that indexing speed increases in a close to linear fashion when scaling corpus size or number of processing machines.
This suggests that the proposed indexing implementation is viable to support upcoming large-scale corpora.

This paper describes an application of statistical techniques that have yielded fruitful results in many fields including artificial intelligence and information retrieval to the problem of establishing relationships among organisms.
A combination of these techniques constitutes a new method of comparing organisms based on their whole genomic sequences.
The method represents genomes as sets of short overlapping nucleotide subsequences and employs latent structure modeling to capture correlations in the observed patterns of their distribution.
Factor scores computed to measure the correlations serve as the input to a Ward&#146;s hierarchical cluster analysis method, which produces a tree of their relationships.
The runtime results indicate that this method allows for the fast and efficient comparison that scales well as the number of organisms increases.

Query expansion can improve word statistical based information retrieval, but problems occur due to the heterogeneous nature of underlying documents, and use of thesauri and ontologies not specifically designed for query expansion purposes.
This paper performs a failure analysis on word statistical information retrieval, and uses this data to discover high value query expansions.
This process uses a medical thesaurus (UMLS) and a medical document test collection (OHSUMED The UMLS based medical concepts within the OHSUMED documents and test queries are identified.
Then, using the document-query relevance judgements, we explore the set of UMLS relationships that connect the concepts in the queries with the concepts in the relevant documents.
From this, we increase the specificity of the query expansion process.

Part of speech tagging (POS tagging) has a crucial role in different fields of natural language processing (NLP) including Speech Recognition, Natural Language Parsing, Information Retrieval and Multi Words Term Extraction.
This paper describes a set of experiments involving the application of three state-of the-art part-of-speech taggers to Amazigh texts, using a tagset of 28 tags.
The taggers showed encourageous performance, in particular having problems with unknown words.
The best results were obtained using a decision tree approach, whille CRF and SVM based taggers got comparable results.

In many real-word scenarios, e.g multimedia applications, data often originates from multiple heterogeneous sources or are represented by diverse types of representation, which is often referred to as "multi-modal data
The definition of distance between any two objects/items on multi-modal data is a key challenge encountered by many real-world applications, including multimedia retrieval.
In this paper, we present a novel online learning framework for learning distance functions on multi-modal data through the combination of multiple kernels.
In order to attack large-scale multimedia applications, we propose Online Multi-modal Distance Learning (OMDL) algorithms, which are significantly more efficient and scalable than the state-of-the-art techniques.
We conducted an extensive set of experiments on multi-modal image retrieval applications, in which encouraging results validate the efficacy of the proposed technique.

Relevance, as shown by various cognitive and experimental studies, depends on a number of situational factors other than topicality.
Formal IR models, on the other hand, only consider relevance from a topicality point of view.
There is an urgent need of appropriate models which are able to consider relevance in a broader context.
This paper is an attempt to find a theoretical tool suitable to the broader context.
The Boolean model and the probabilistic model are reviewed with respect to some situational factors, showing their inability for the task.
Conditional logic is then suggested as a potential alternative.
Relevance is interpreted as a causal relation between a document and a query in a given context.

This report documents the program and the outcomes of Dagstuhl Seminar 15512
“Debating Technologies
The seminar brought together leading researchers from computational linguistics, information retrieval, semantic web, and database communities to discuss the possibilities, implications, and necessary actions for the establishment of a new interdisciplinary research community around debating technologies.
31 participants from 22 different institutions took part in 16 sessions that included 34 talks, 13 themed discussions, three system demonstrations, and a hands-on “unshared” task.
Seminar December 13–18, 2015 http www.dagstuhl.de/15512 1998 ACM Subject Classification I.2 Artificial Intelligence, I.2.7 Natural Language Processing, H.3.3 Information Search and Retrieval, I.2.9 Robotics

Digital Library; Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCIRUS; SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory; Web of Science Emerging Sources Citation Index (ESCI) Letter from the Editor

Ranking model construction is an important topic in information retrieval and web mining.
Recently, many approaches based on the idea of x201C;learning to rank&#x201D; have been proposed for this task and most of them attempt to score all documents of different queries by resorting to a single function.
In this paper, we propose a distributional similarity measure for query-dependent ranking.
In the query-dependent ranking framework, an individual ranking model is constructed for each training query with associated documents.
When a new query is asked, the documents retrieved for the new query are ranked according to the scores determined by a joint ranking model which is combined from the individual models of similar training queries.
The distributional similarity measure is used to calculate the similarities between queries.
Experimental results show that our method is more effective than other approaches.

Using context to improve retrieval performance is a current challenge within the discipline and presents a major challenge to the research community.
In this thesis, the context of documents formed via semantic associations is utilized within the Language Modeling (LM) approach for <i>ad hoc</i
> Information Retrieval.

Users of commercial legal information retrieval (IR) systems often want argument retrieval (AR retrieving not merely sentences with highlighted terms, but arguments and argument-related information.
Using a corpus of argumentannotated legal cases, we conducted a baseline study of current legal IR systems in responding to standard queries.
We identify ways in which they cannot meet the need for AR and illustrate how additional argument-relevant information could address some of those inadequacies.
We conclude by indicating our approach to developing an AR system to retrieve arguments from legal decisions.

This chapter provides a survey of the state-of-the-art in the field of Visual Information Retrieval (VIR) systems, particularly Content-Based Visual Information Retrieval (CBVIR) systems.
It presents the main concepts and system design issues, reviews many research prototypes and commercial solutions currently available and points out promising research directions in this area.

The PIKM 2012 workshop is the 5th of its kind after 4 successful PhD workshops at ACM CIKM.
This PhD workshop invites papers that describe the Ph.D. dissertation proposals of doctoral students in any of the CIKM areas: databases, information retrieval, data mining and knowledge management.
Interdisciplinary work across these tracks is particularly encouraged.
This year PIKM has received around 25 submissions from over 12 countries across the globe, among which 10 have been accepted as full papers for oral presentation while 4 have been accepted as short ones for poster presentation.
The selection has been conducted based on reviews submitted by an expert team comprising 21 PC members spanning 12 countries and 6 continents with a good balance of industry and academia.

Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning.
IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning.
In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways.
There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users.
FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab.
This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented.

GiveALink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely.
The bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend, search, and personalize the Web.
GiveALink does not use tags, content, or links in the submitted Web pages.
Instead we present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure in the bookmark files of individual users, and of collaborative filtering across users.
In addition, we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network.
Search results can be personalized using the bookmarks submitted by a user.
We evaluate a subset of the proposed ranking measures by conducting a study with human subjects.

This work presents an Information Retrieval technique based on algorithmic information theory (using the normalized compression distance statistical data outlier detection, and a novel database structure.
The paper shows how they all can be integrated to retrieve information from generic databases using long text-based queries.
Two important problems are addressed.
On the one hand, we analyze and tyr to solve the detection of a particular case of false positives: when the distance among two documents is outlyingly low but there is not actual similarity.
On the other hand, we propose a way to structure the database such that the similarity distance estimation scales well with the length of the size of the query.
All design choices are justified with an experimental evaluation.

In practice the most advanced field of quantum information science is quantum cryptography.
The best known example of quantum cryptography is quantum key distribution (QKD) which offers an information-theoretically secure solution to the key exchange problem.
There are QKD protocols which rely on a quantum channel that is able to transmit single photons.
This can be implemented by using very low-power coherent light field, where values of bits are encoded into the polarization state of photons.
Therefore, it is important to know the photon number distribution of the light source.
In this paper we present the reconstruction of photon statistics for different pulsed light sources radiating at 850 nm (a common choice for free space quantum communication using only a single on-off photon counting detector.

The methodological approach to achieve the elicited research objectives is presented in this study.
The study will employ an experimental methodology involving testing of algorithms to identify the suitable algorithm for an optimized process of information retrieval.
The algorithm that presents a better precision in the personalized IR in web log mining represents the core value of the architecture that this study presents.

In this paper, we describe the WIRE (Web Information Retrieval Environment) project and focus on some details of its crawler component.
The WIRE crawler is a scalable, highly configurable, high performance, open-source Web crawler which we have used to study the characteristics of large Web collections.

Question classification is a key task in many question answering applications.
Nearly all previous work on question classification has used machine learning and knowledge-based methods.
This working note presents an embedding based Bag-ofWords method and Recurrent Neural Network to achieve an automatic question classification in the code-mixed BengaliEnglish text.
We build two systems that classify questions mostly at the sentence level.
We used a recurrent neural network for extracting features from the questions and Logistic regression for classification.
We conduct experiments on Mixed Script Information Retrieval (MSIR) Task 1 dataset at FIRE2016.
The experimental result shows that the proposed method is appropriate for the question classification task.
CCS Concepts Information systems~Clustering and classification Information systems~Question answering Computing methodologies~Learning latent representations

Xiongfeng Ma, 2 Feihu Xu He Xu, Xiaoqing Tan, 2 Bing Qi and Hoi-Kwong Lo Center for Quantum Information, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China Center for Quantum Information and Quantum Control, Department of Physics and Department of Electrical Computer Engineering, University of Toronto, Toronto, Ontario, Canada Department of Mathematics, College of Information Science and Technology, Jinan University, Guangzhou, Guangdong, China

There is a growing realisation that relevant information will be accessible increasingly across media and genres, across languages and across modalities.
The retrieval of such information will depend on time, place, history of interaction, task in hand, and a range of other factors that are not given explicitly but are implicit in the interaction and ambient environment, namely the <i>context</i IR research is now conducted in multimedia, multi-lingual, and multi-modal environments but largely out of context (
Jarvelin &amp; Ingwersen, 2004; Ingwersen &amp; Jarvelin, forthcoming However, such contextual data can be used effectively to constrain retrieval of information thereby reducing the complexity of the retrieval process.
To achieve this, context models for different modalities will need to be developed so that they can be deployed effectively to enhance retrieval performance.
Thus truly context-aware and -dependent retrieval will become feasible.

We have been in a web information stage, by new information management technologies, we can get better agricultural development.
The paper introduces the research work on agricultural thesaurus and ontology; it could improve the agricultural information retrieval.
Main work include to convert Chinese Agricultural Thesaurus (CAT) to the agricultural ontology, this can use traditional domain knowledge directly; research on the thesauri mapping between different languages, this can be help to develop cross languages search engine, and improve the ontologies mapping research; Based on all these works, the paper plans to develop cross languages search engine to index and search web information.

Information retrieval models have been studied for decades, leading to a huge body of literature on the topic.
In this paper, we briefly review this body of literature along with a discussion of some recent trends.

Shoji Itoh 1 Shao-Liang Zhang Yoshio Oyanagi 3 and Makoto Natori 4 1 Doctral Program in Engineering on Information Sciences and Electronics, University of Tsukuba, Tsukuba, Ibaraki,
Japan E-mail: itosho@nalab.is.tsukuba.ac.jp 2 Department of Applied Physics, University of Tokyo, Bunkyo, Tokyo, Japan E-mail: zhang@zzz.t.u-tokyo.ac.jp 3 Department of Information Science, University of Tokyo, Bunkyo, Tokyo,
Japan E-mail: oyanagi@is.s.u-tokyo.ac.jp 4 Institute of Information Sciences and Electronics, University of Tsukuba, Tsukuba, Ibaraki, Japan E-mail: natori@is.tsukuba.ac.jp

Dealing with verbose (or long) queries poses a new challenge for information retrieval.
Selecting a subset of the original query (a "sub-query has been shown to be an effective method for improving these queries.
In this paper, the distribution of sub-queries subset distribution is formally modeled within a well-grounded framework.
Specifically, sub-query selection is considered as a sequential labeling problem, where each query word in a verbose query is assigned a label of "keep" or "don't keep A novel Conditional Random Field model is proposed to generate the distribution of sub-queries.
This model captures the local and global dependencies between query words and directly optimizes the expected retrieval performance on a training set.
The experiments, based on different retrieval models and performance measures, show that the proposed model can generate high-quality sub-query distributions and can significantly outperform state-of-the-art techniques.

Proximity-based term dependencies have been proposed and used in a variety of effective retrieval models.
The execution of these dependency models is commonly supported through the use of positional inverted indexes.
However, few of these models detail how instances of proximate terms should be extracted from the lists of positional data.
In this study, we investigate three algorithms for the extraction of windows that span a range of assumptions about the reuse of terms in multiple window instances.
We observe that computed collection statistics of unordered windows are significantly affected by the choice of algorithm.
We also observe that retrieval efficiency and effectiveness of a state-of-the-art dependence model are not significantly affected by the selection of window extraction algorithm.

Program Committee Peter Fritzson, PELAB, Department of Computer and Information Science, Linköping University, Sweden (Chairman of the committee Bernhard Bachmann, Fachhochschule Bielefeld, Bielefeld, Germany.
Hilding Elmqvist, Dynasim AB, Sweden.
Martin Otter, Institute of Robotics and Mechatronics at DLR Research Center, Oberpfaffenhofen, Germany.
Michael Tiller, Ford Motor Company, Dearborn, USA.
Hubertus Tummescheit, UTRC, Hartford, USA, and PELAB, Department of Computer and Information Science, Linköping University, Sweden.

Scoring documents with learning-to-rank (LtR) models based on large ensembles of regression trees is currently deemed one of the best solutions to effectively rank query results to be returned by large scale Information Retrieval systems.
This paper investigates the opportunities given by SIMD capabilities of modern CPUs to the end of efficiently evaluating regression trees ensembles.
We propose V-QuickScorer (vQS which exploits SIMD extensions to vectorize the document scoring, i.e to perform the ensemble traversal by evaluating multiple documents simultaneously.
We provide a comprehensive evaluation of vQS against the state of the art on three publicly available datasets.
Experiments show that vQS provides speed-ups up to a factor of 3.2x.

The attraction of hundreds of millions of web searches per day provides significant incentive for many content providers to do whatever is necessary to rank highly in search engine results, while search engine providers want to provide the most accurate results.
The conflicting goals of search and content providers are adversarial, and the use of techniques that push rankings higher than they belong is often called search engine spam.
Such methods typically include textual as well as link-based techniques, or their combination.

In this paper, we describe our microblog realtime filtering system developed and submitted for the Text Retrieval Conference (TREC 2015) microblog track.
We submitted six runs for two tasks related to real-time filtering by using various Information Retrieval (IR and Machine Learning (ML) techniques to analyze the Twitter sample live stream and match relevant tweets corresponding to specific user interest profiles.
Evaluation results demonstrate the effectiveness of our approach as we achieved 3 of the top 7 best scores among automatic submissions across all participants and obtained the best (or close to best) scores in more than 25% of the evaluated topics for the real-time mobile push notification task.

The yellow pages service of GTE SuperPages enables Web users to flexibly search through liitings of 11 million businesses in over 17000 categories.
To achieve the flexibility desired it uses an Information Retrieval (IR) engine to search through complex listing objects.
The objects themselves are stored in an object database.
The use of the IR engine enables us to create an index that spans all the components of a complex object.

In web search, understanding the user intent plays an important role in improving search experience of the end users.
Such an intent can be represented by the categories which the user query belongs to.
In this work, we propose an information retrieval based approach to query categorization with an emphasis on learning category rankings.
To carry out categorization we first represent a category by web documents (from Open Directory Project) that describe the semantics of the category.
Then, we learn the category rankings for the queries using 'learning to rank' techniques.
To show that the results obtained are consistent and do not vary across datasets, we evaluate our approach on two datasets including the publicly available KDD Cup dataset.
We report an overall improvement of 20% on all evaluation metrics (precision, recall and F-measure) over two baselines: a text categorization baseline and an unsupervised IR baseline.

This study used citation analysis method to identify the 40 classics published in the Journal of the American Society for Information Science and Technology from 1956 to 2007.
Yhe year and subject distributions of these classic references reflect the history and the current status of information science.

The current challenges in the world are search and retrieve accurate information from the massive web.
The general term used for searching and retrieving data from the web is &apos;query&apos; and keyword-matching.
The existing structure uses Personalized user information system, recommender system and wordnet ontology.
The Personalized user information system used to increase the speed and required response.
To extract user likings, the personalized user information system explore the acquirement of user reviews by supervising their browsing behavior.
In Recommender system the people rate web pages as interesting and not interesting and it responses according to the relevant feedback.
The wordnet ontology uses to retrieve information by means of Synonymy, Antonymy, Hyponymy

Chapter I OpenDLib:
A Digital Library Service System 1 Leonardo Candela, Istituto di Scienza e Tecnologie dell’Informazione “A. Faedo ISTI-CNR Italy Donatella Castelli, Istituto di Scienza e Tecnologie dell’Informazione “A. Faedo ISTI-CNR Italy Pasquale Pagano, Istituto di Scienza e Tecnologie dell’Informazione “A. Faedo ISTI-CNR Italy Manuele Simi, Istituto di Scienza e Tecnologie dell’Informazione
“A. Faedo ISTI-CNR Italy

This paper addresses the task of providing extended responses to questions regarding specialized topics.
This task is an amalgam of information retrieval, topical summarization, and Information Extraction (IE We present an approach which draws on methods from each of these areas, and compare the effectiveness of this approach with a query-focused summarization approach.
The two systems are evaluated in the context of the prosecution queries like those in the DARPA GALE distillation evaluation.

In this paper, we apply an Information Retrieval model for the writer identification task.
A set of local features is defined by clustering the graphemes produced by a segmentation procedure.
Then a textual based Information Retrieval model is applied.
After a first indexation step, this model no longuer requires image access to the database for responding to a specific query, thus making the process particularly effective.
Image query are handwritten documents projected on the feature space prior to the retrieval of the suitable responses.
The method is tested on a database of 88 writers and proves to give interesting results.

This paper presents a method for building concept lattices by learning concepts from RDF annotations of Web documents.
It consists in extracting conceptual descriptions of the Web resources from the RDF graph gathering all the resource annotations and then forming concepts from all possible subsets of resources each such subset being associated with a set of descriptions shared by the resources belonging to it.
The concept hierarchy is the concept lattice built upon a context built from the power context family representing the RDF graph.
In the framework of the CoMMA European IST project dedicated to ontologyguided Information Retrieval in a corporate memory, the hierarchy of the so learned concepts will enrich the ontology of primitive concepts, organize the documents of the organization’s Intranet and then improve Information Retrieval.
The RDF Model is close to the Simple Conceptual Graph Model; our method can be thus generalized to Simple Conceptual Graphs.

An extended Boolean retrieval strategy has previously been introduced in which the individual Boolean operators can be treated more or less strictly, depending on the perceived strength of association of the query terms.
The extended Boolean system is illustrated by examples and evaluation output is used to demonstrate the effectiveness of the operations.

This paper describes a new distinguishing type attack to identify block ciphers, which grounded in a neural network, by means of a linguistic approach and an information retrieval approach, from patterns which is found on a ciphertexts set collection.
The ideas were performed on a set of ciphertexts, which were encrypted by the finalist algorithms of AES contest: MARS, RC6, Rijndael, Serpent and Twofish; each one has a unique 128-bit key.
This experiment shows the processes of clustering and classification were successful, which allows the formation of well-formed and well-defined groups, here ciphertexts encrypted by the same algorithm stayed close to each other.

This paper describes a prototype dissertation information system.
The various components of the system include the database, the information retrieval engine, dynamic Web pages via markup languages, and client side JavaScripts.
Managerial and information retrieval issues associated with a digital dissertation information system are discussed and the database configurations and codes written for performing various operations online are described.
The paper ends with a discussion of the project results and future developmental opportunities.

Short Text Semantic Similarity measurement is a new and rapidly growing field of research Short texts” are typically sentence length but are not required to be grammatically correct.
There is great potential for applying these measures in fields such as Information Retrieval, Dialogue Management and Question Answering.
A dataset of 65 sentence pairs, with similarity ratings, produced in 2006 has become adopted as a de facto Gold Standard benchmark.
This paper discusses the adoption of the 2006 dataset, lays down a number of criteria that can be used to determine whether a dataset should be awarded a “Gold Standard” accolade and illustrates its use as a benchmark.
Procedures for the generation of further Gold Standard datasets in this field are recommended.

A common problem of expert combination approaches in Information Retrieval (IR) is the selection of both, the experts to be combined and the combination function.
In most studies the experts are selected from a rather small set of candidates using some heuristics.
Thus, only a reduced number of possible combinations is considered and other possibly better solutions are left out.
In this paper we propose the use of genetic algorithms to find a suboptimal combination of experts for a document collection.
Our system automatically determines both, the experts to be combined and the parameters of the combination function.
We test and evaluate the approach on four classical text collections.
The results show that the learnt combination strategies perform better than any of the individual methods and that genetic algorithms provide a viable method to learn expert combinations.

Finding significant contextual features is a challenging task in the development of interactive information retrieval (IR) systems.
This paper investigated a simple method to facilitate such a task by looking at aggregated relevance judgements of retrieved documents.
Our study suggested that the agreement on relevance judgements can indicate the effectiveness of retrieved documents as the source of significant features.
The effect of <i>highly agreed documents</i> gives us practical implication for the design of adaptive search models in interactive IR systems.

This work describes the emerging standard for information retrieval on computational grids, GridIR.
GridIR is based on the work of the Global Grid Forum and offers a security model for end-to-end data channel encryption, system authentication, and logging.
GridIR implements a multi-tiered security model at the collection, query and datum level.
Unlike monolithic search engines and customized small-scale systems, GridIR provides a standard method for federating data sets with multiple data types.
Each of the data sets may be part of a virtual organization in which policy decisions dictate what data items may be shared.

Developing intelligent tools to extract information from data collections has long been of critical importance in fields such as knowledge discovery, information retrieval, pattern recognition, and databases.
With the advent of electronic medical records and medical data repositories there is new potential to apply these techniques to the analysis of biomedical data sets.
Looking for complex patterns within large biomedical data repositories and discovering previously unexpected associations can be of particular interest for understanding the physiology and functionality of the human body as well as tracing the roots of diseases.
In the context of a research hospital these analyses may lead to further directed research, better diagnostic capabilities, and improved patient outcomes.
This paper describes an implementation of a knowledge discovery algorithm aimed at such data sets.

Current text indexing and retrieval techniques have their roots in the field of Information Retrieval where the task is to extract documents that best match a query.
With an ever increasing number of documents available due to the easy access through the Internet, the challenge is to provide users with concise and relevant information.
We are proposing here a novel, yet simple approach, which indexes the named entities in the documents, such as to improve the relevance of documents retrieved.
Experiments performed in finding information related to a set of 75 input questions, from a large collection of 125,000 documents, show that this new technique reduces the number of retrieved documents by a factor of 2, while still retrieving the relevant documents.

The main goal of the bilingual and monolingual participation of the MIRACLE team in CLEF 2004 was to test the effect of combination approaches on information retrieval.
The starting point was a set of basic components: stemming, transformation, filtering, generation of n-grams, weighting and relevance feedback.
Some of these basic components were used in different combinations and order of application for document indexing and for query processing.
A second order combination was also tested, mainly by averaging or selective combination of the documents retrieved by different approaches for a particular query.

The quantity of biomedical publications is growing at an exponential rate.
With such explosive growth of the content, it is more and more difficult to locate, retrieve and manage the resulting information.
This is why text mining has become a necessity.
The main goal of biomedical research is to put knowledge to practical use in the form of diagnoses, prevention, and treatment.
It is important to pool the resources between the different individuals researching results.
The objective of this paper is to discuss the variety of issues and challenges surrounding the perspectives regarding the use of Information Retrieval and Text Mining methods in biomedicine.
The article will first look at the directions in biomedical Text Mining and then describe the work done for the BIAM project, the French on-line Medical Data Base.

Since the filed of word sense disambiguation and information retrieval has long history, the literature on this field is growing fast through the recent years.
In this literature we tried collect some of research and studies about the relationship between the word sense disambiguation and information retrieve.
Cover all the studies in this field is so difficult, for those we reading different papers and submitted as literature review and we hope that we can add little of contribution in this field.

Application of Bradford's Law of Scattering is one of the bibliometric law, which is used most commonly in bibliometric research.
The paper gives a review of the contribution in Library Information Science theoretical aspects of the law.
A study carried out on the data of journals cited by Ph.D. research scholars at the universities in Maharashtra for their doctoral research.
To examine the applicability of Bradford's Law of Scattering, study include 798 periodical containing 5467 references collected form 138 theses during the period 1982-2010.
Rank list was prepared and Annals of Library Science and Documentation took top place with 207 citations followed by College and Research Libraries with 184 and Herold of Library Science with 160 citations were the most preferred journals.
Applicability of Bradford's Law in various method was tested.

Leveraging Biological Identifier Relationships and Related Documents to Enhance Information Retrieval for Proteomics Andrew Smith, Kei Cheung, Michael Krauthammer, Martin Schultz and Mark Gerstein Department of Molecular Biophysics and Biochemistry, Department of Computer Science, Center for Medical Informatics, Department of Genetics, Program in Computational Biology and Bioinformatics, Department of Anesthesiology, Department of Pathology, Yale University, New Haven, CT USA Corresponding Author

This paper presents a Foreign-Language Search Assistant that uses noun phrases as fundamental units for document translation and query formulation, translation and refinement.
The system (a) supports the foreign-language document selection task providing a cross-language indicative summary based on noun phrase translations, and (b) supports query formulation and refinement using the information displayed in the cross-language document summaries.
Our results challenge two implicit assumptions in most of cross-language Information Retrieval research: first, that once documents in the target language are found, Machine Translation is the optimal way of informing the user about their contents; and second, that in an interactive setting the optimal way of formulating and refining the query is helping the user to choose appropriate translations for the query terms.
2004 Elsevier Ltd.
All rights reserved.

Cross-Language Information Retrieval (CLIR) resources, such as dictionaries and parallel corpora, are scarce for special domains.
Obtaining comparable corpora automatically for such domains could be an answer to this problem.
The Web, with its vast volumes of data, offers a natural source for this.
We experimented with focused crawling as a means to acquire comparable corpora in the genomics domain.
The acquired corpora were used to statistically translate domain-specific words.
The same words were also translated using a high-quality, but non-genomics-related parallel corpus, which fared considerably worse.
We also evaluated our system with standard information retrieval (IR) experiments, combining statistical translation using the Web corpora with dictionary-based translation.
The results showed improvement over pure dictionary-based translation.
Therefore, mining the Web for comparable corpora seems promising.

Linguistically-augmented perplexity-based data selection for language models Antonio Toral a Pavel Pecina b, Longyue Wang c,1, Josef van Genabith d a School of Computing, Dublin City University, Dublin, Ireland b Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic c Natural Language Processing Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R China d DFKI GmbH, Multilingual Technologies, Campus D3 2, D-66123 Saarbrücken, Germany

This paper considers an iterative user centered design of Information Retrieval Systems (IRS An iterative design based on experimental evaluations with the end-users implies a high modifiability of the software.
To achieve this software quality criterion, we apply the PAC-Amodeus model.
PAC-Amodeus is a conceptual model useful for devising architectures driven by quality criteria including modifiability and portability.
We illustrate the applicability of the model with the implementation of an IRS that we have developed: TIAPRI.

In this work, we describe an automatic entity extraction system for social media content in English as part of our participation in the shared task on Entity Extraction from Social Media Text in Indian Languages
(ESM-IL) organized by Forum for Information Retrieval Evaluation (FIRE) in 2015.
Our method uses simple features such as window of words, capitalization, dictionary word, part of speech tags, hashtag, etc.
The performance of the system has been evaluated against the testset released in the FIRE 2015 shared task on ESM-IL.
Experimental results show encouraging performance in terms of precision, recall and F-measure.
CCS Concepts •Computing methodologies
Artificial intelligence; Natural language processing Information systems Information extraction;

Twitter is most popular microblogging site.
It provide us with real time data.
This article Provide survey of techniques for retrieving information from twitter stream.
This techniques aim is finding real world and most relevant information with respect to the query.
For retrieve most relevant information used query expansion techniques.
Twitter data contain large amount of information.
Information rank retrieval techniques find important data and gives the final score to that information with respect to user interest profile

This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty.
The solution to the problem is formulated as a combination of the opinions of different experts.
The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses latent semantic indexing.
The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment.
The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently

New standards in document representation, like for example SGML, XML, and MPEG-7, compel Information Retrieval to design and implement models and tools to index, retrieve and present documents according to the given document structure.
The paper presents the design of an Information Retrieval system for multimedia structured documents, like for example journal articles, e-books, and MPEG-7 videos.
The system is based on Bayesian Networks, since this class of mathematical models enable to represent and quantify the relations between the structural components of the document.
Some preliminary results on the system implementation are also presented.

Heuristic methods based on biological aspects have been successfully used in many computer science areas of research, including information retrieval (IR This let us ask: why there is no biological environment for the problem of information retrieval.
In this paper, we tried to introduce a new biological environment for information retrieval that broad the modeling concept from the mathematical formula that simulates the basic elements of IR problem to their dual schemas in biology.
It's a new way of thinking of, and dealing with, the problem of information retrieval

Die klassische Aufgabenstellung des Information Retrieval (IR) ist durch das
Internet für jeden Nutzer begreifbar geworden, denn es gehört mittlerweile zur Alltagserfahrung, dass die prinzipielle Verfügbarkeit von Information keineswegs gleichbedeutend damit ist, dass diese auch mit vertretbarem Aufwand gefunden werden kann.
Das Problem, relevante Dokumente aus einer unübersehbaren Zahl von ähnlichem, aber (aus Sicht des Suchenden)
herauszufiltern, ist vielschichtig und wird von den klassischen Werkzeugen (Suchmaschinen, Katalogen) nur mit Einschränkungen gemeistert.

Relevance-Based Language Models are an effective IR approach which explicitly introduces the concept of relevance in the statistical Language Modelling framework of Information Retrieval.
These models have shown to achieve state-of-the-art retrieval performance in the pseudo relevance feedback task.
In this paper we propose a novel adaptation of this language modeling approach to rating-based Collaborative Filtering.
In a memory-based approach, we apply the model to the formation of user neighbourhoods, and the generation of recommendations based on such neighbourhoods.
We report experimental results where our method outperforms other standard memory-based algorithms in terms of ranking precision.

This paper introduces a novel representation, called the <b>Info Crystal</b>#8482 that can be used as a <b>visualization tool</b> as well as a <b>visual query language</b> to help users search for information.
The InfoCrystal visualizes all the possible relationships among N concepts.
Users can assign relevance weights to the concepts and use thresholding to select relationships of interest.
The InfoCrystal allows users to specify <b>Boolean</b> as well as <b>vectorspace</b> queries graphically.
Arbitrarily complex queries can be created by using the InfoCrystals as building blocks and organizing them in a hierarchical structure.
The InfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.

Resource selection is an important topic in distributed information retrieval research.
It can be a component of a distributed information retrieval task and can also serve as an independent application of database recommendation system together with the resource representation part.
There is a large body of valuable prior research on resource selection but very little has studied about the effects of different database size distributions on resource selection.
In this paper, we propose extended versions of two well-known resource selection algorithms: CORI and KL divergence in order to consider the factors of database size distributions, and compare them with the lately proposed Relevant Document Distribution Estimation (ReDDE) resource selection algorithm.
Experiments were done on four testbeds with different characteristics, and the ReDDE and the extended KL divergence resource selection algorithm have been shown to be more robust in various environments.

Enterprise search differs from Internet search in many ways.
The overwhelming majority of information in an enterprise is unstructured.
Together with information in relational and proprietary databases, these documents constitute the enterprise information ecosystem.
The information in an enterprise is distributed.
A centric search engine does not satisfy the security requirement in enterprise information retrieval.
In this paper, a multi-agent based distributed secure information retrieval approach has been presented.
In this approach, a department or a unit in an enterprise has a search agent, so, the department can keep their information privately and securely.
A role based access control strategy is adapted for global security for an enterprise.
We have developed a prototype system by this approach.

ACM Digital Library; Applied Social Sciences Index Abstracts (ASSIA Bacon’s Media Directory; Burrelle’s Media Directory; Cabell’s
Directories; CSA Illumina; DBLP; Gale Directory of Publications Broadcast Media; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD PsycINFO SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Research Articles

An Information Retrieval approach to the prediction of values of time series is suggested.
Instead of trying to model the data, a pattern is searched for in a history file serving as “text and relevant occurrences are used as basis for extrapolation.
The application is to meteorological data, providing short-term predictions for certain weather features.
The experiments show that the algorithm has a good ability of predicting the requested information, and that the predictions are far more accurate than those provided by an analytical prediction procedure, based on approximations using Fourier transforms.

This paper discusses field research in an interdisciplinary public health organization with a small in-house library.
The fieldwork revealed how users in the organization often ignored the library (a "formal" information system) in favor of "informal" information sources such as colleagues, search engines, and personal files.
The fieldwork thereby inverted a design problem, shifting the focus from building a better information retrieval system to understanding and supporting the information ecology of the organization.
The fieldwork demonstrates some of the complexities of information use in practice, and the challenge of designing formally structured information systems (such as libraries) that match how users employ information in their ordinary work practices.
We argue that future design efforts could build upon users natural streams of information" to develop more effective information architectures and capabilities for supporting users' work.

Multilingual translations play a vital role multilingual or cross-lingual information retrieval and extraction.
In this paper, we describe a new method mine translations from bilingual web pages based on our former research.
Two new features are introduced, one is the iterative mining process in order to increase the number of translation pairs; the other is the filtering step which deletes language-specific prefix and postfix in hyperlinks.
Experiments show that the precision has been greatly improved due to the filtering step and the number of translation pairs increased after six iterations.

Duplicated information in today's Web has serious negative impact to search engines in that it increases the size of the index and results in low efficiency for Web information retrieval.
A large amount of Web content duplication happens at block level in addition to page level.
Besides, when searching the web, in most cases the desired information is located at the center block of a page.
Based on these two observations, we propose a block level duplication detection algorithm, and index center blocks instead of entire Web pages for Web information retrieval.
Experiments show that these strategies can effectively reduce index size and indexing time without sacrificing the effectiveness of Web information retrieval.

In order to reduce the dimension of VSM (Vector Space Model) for information retrieval and clustering, this paper proposes a new method, Semantic-VSM, which uses the Semantic Attribute System defined by ”A-Japanese-Lexicon” instead of literal words used in conventional VSM.
The attribute system consists of a tree structure with 2,710 attributes, which includes 400 thousand literal words.
Using this attribute system, the generalization of vector elements can be performed easily based on upper-lower relationships of semantic attributes, so that the dimension can easily be reduced at very low cost.
Synonyms are automatically assessed through semantic attributes to improve the recall performance of retrieval systems.
Experimental results applying it to BMIR-J2 database of 5,079 newspaper articles showed that the dimension can be reduced from 2,710 to 300 or 600 with only a small degradation in performance.
High recall performance was also shown compared with conventional VSM.

Information retrieval on the Internet is particularly challenging for the non-expert user seeking technical information with specialized terminology.
The user can be assisted during the required search tasks with intelligent agent technology delivered through a decision making support system.
This paper describes the technology and its application to suicide prevention searches performed for the National Institute of Mental Health.

Metasearch and data-fusion techniques combine the rank lists of multiple document retrieval systems with the aim of improving search coverage and precision.
We propose a new fusion method that partitions the rank lists of document retrieval systems into chunks.
The size of chunks grows exponentially in the rank list.
Using a small number of training queries, the probabilities of relevance of documents in different chunks are approximated for each search system.
The estimated probabilities and normalized document scores are used to compute the final document ranks in the merged list.
We show that our proposed method produces higher average precision values than previous systems across a range of testbeds.

For Web page retrieval, even if the user uses the same keywords, different kinds of pages tend to be mixed in retrieval result because of polysemy or ambiguity of words.
We already proposed the system of having improved the problem, which classifies retrieval result to groups automatically according to page contents using the vector space model method, the frequency of word appearance and the fuzzy reasoning.
In our system, to reduce dimension of vector space, index words are selected based on the importance of a word measuring by fuzzy reasoning.
On the other hand, TF-IDF is often used as a measure of the importance of a word in Information Retrieval.
Then we thought that it was necessary to compare with the performances of TF-IDF and fuzzy reasoning.
From the experiments, we confirmed that similar classification for retrieval pages in terms of human sense by the system with fuzzy reasoning rather than with modified FT-IDF.

At the core of any information retrieval system is its method for ranking results in response to a user's query.
Geographic Information Retrieval (GIR) systems have an added complexity for this task since the information to be included in the ranking process goes beyond text and word frequency information to encompass geographic proximity, containment and other spatial operations.
The need to combine both geographic and text components into GIR systems has led to some interesting hybrid approaches in addition to the "pure" spatial ranking methods based on spatial similarity.
In this short survey I will look at some of the methods that have been reported in the literature and used in GIR evaluations including GeoCLEF and NTCIR GeoTime.

Script identification has always been a topic of much research interest in the field of document analysis.
The accurate determination of the identity of the script is paramount to many post-processing steps such as document sorting, translation and in determining the choice of linguistic resources to use for OCR or handwriting recognition.
However, few works exist with regards to the identification of online handwritten scripts, partly due to the large variations and challenges innate in handwritten scripts.
This paper proposes a novel approach for online handwritten script identification based on the Information Retrieval model.
We attempt to identify among three script families; Arabic, Roman and Tamil scripts, which attained an average accuracy of 93.3&#x025; from our results.
This signifies promising potential in utilizing Information Retrieval models for script identification.

Traditional Information Retrieval (IR) systems are designed to provide uniform access to centralized corpora by large numbers of people.
The Haystack project emphasizes the relationship between a particular individual and his corpus.
An individual's own haystack priviliges information with which that user interacts, gathers data about those interactions, and uses this metadata to further personalize the retrieval process.
This paper describes the prototype Haystack system.

Concepts are often used in Medical Information Retrieval.
Previous studies showed that exact concept matching (using either the exact concept expression or concept IDs) is not effective, while using concept expressions for proximity matching is more effective.
In our participation in Clinical Decision Support Track 2015 task 1a, we investigated the utilization of proximity matching based on concepts.
Our results suggest that this matching strategy can be helpful.
In this report, we describe the methods tested as well as their results.

An increasing number of recent information retrieval systems make use of ontologies to help the users clarify their information needs and come up with semantic representations of documents.
A particular concern here is the integration of these semantic approaches with traditional search technology.
The research presented in this paper examines how ontologies can be efficiently applied to large-scale search systems for the web.
We describe how these systems can be enriched with adapted ontologies to provide both an in-depth understanding of the user's needs as well as an easy integration with standard vector-space retrieval systems.
The ontology concepts are adapted to the domain terminology by computing a feature vector for each concept.
Later, the feature vectors are used to enrich a provided query.
The whole retrieval system is under development as part of a larger Semantic Web standardization project for the Norwegian oil gas sector.

ii Foreword
These lecture notes are the first part of a series on Information Retrieval and Hypertext.
This part provides a formal introduction to Information Retrieval and Hypertext.
The second part covers aspects of natural language, both as query and characterization language.
A summary (in dutch) of these notes can be found on Internet:

In the Semantic Web information would be retrieved, processed, combined, shared and reused in the maximum automatic way possible.
Obviously, such procedures involve a high degree of uncertainty and imprecision.
For example ontology alignment or information retrieval are rarely true or false procedures but usually involve confidence degrees or provide rankings.
Furthermore, it is often the case that information itself is imprecise and vague like the concept of a “tall” person, a “hot” place and many more.
In order to be able to represent and reason with such type of information in the Semantic Web (SW as well as, enhance SW applications we present an extension of the Description Logic SHIN with fuzzy set theory.
We present the semantics as well as detailed reasoning algorithms for the extended language.

Most of the Information Retrieval Systems uses counts of frequencies of the words that occur in documents.
Such counts entail the need of normalizing these terms.
A simple normalization of characters (upper/lowercase, accents and other diacritical ones) seems insufficient, since many words, by morphologic inflection or derivation, could be grouped under an only form, when having very near semantic mean.
Several algorithms of normalization are analyzed and tested experimentally to evaluate their effectiveness.

Our objective is to enhance the effectiveness of retrieval and routing operations for large scale textbases.
Retrieval concerns the processing of ad hoc queries against a static document collection, while muting concerns the processing of static, trained queries against a document stream.
Both may be viewed as trying to rank relevant answer documents high in the output.
Our text processing and retrieval system PIRCS is based on the probabilistic model and extended with the concept of document components.
Components are regarded as single content-bearing terms as an approximation.
Considering documents and queries as constituted of conceptual components allows one to define initial term weights naturally, to make use of nonbinary term weights, and to facilitate different types of retrieval processes.
The approach is automatic, based mainly on statistical techniques, and is generally language and domain independent.

In-text frequency-weighted citation counting has been seen as a particularly promising solution to the wellknown problem of citation analysis that it treats all citations equally, be they crucial to the citing paper or perfunctory.
But what is a good weighting scheme?
We compare 12 different in-text citation frequencyweighting schemes in the field of library and information science (LIS) and explore author citation impact patterns based on their performance in these schemes.
Our results show that the ranks of authors vary widely with different weighting schemes that favor or are biased against common citation impact patterns— substantiated, applied, or noted.
These variations separate LIS authors quite clearly into groups with these impact patterns.
With consensus rank limits, the hard upper and lower bounds for reasonable author ranks that they provide suggest that author citation ranks may be subject to something like an uncertainty principle.

In this paper, we propose a novel method that performs Cross Language Text Categorization (CLTC) from the perspective of Information Retrieval.
We present an input document in target language in the form of a query in source language.
Then we retrieve the training documents in source language and find K most relevant results.
At last, we use the class labels of the K results to predict the class of the input document.
The only external resource required by our method is a bilingual dictionary.
Experimental results show that our method gives promising performance, which is better than translation-based method.

Despite one and a half decade of research and an impressive body of knowledge on how to represent and process musical audio signals, the discipline of Music Information Retrieval still does not enjoy broad recognition outside of computer science.
In music cognition and neuroscience in particular, where MIR’s contribution could be most needed, MIR technologies are scarcely ever utilized—when they’re not simply brushed aside as irrelevant.
This, we contend here, is the result of a series of misunderstandings between the two fields, about deeply different methodologies and assumptions that are not often made explicit.
A collaboration between a MIR researcher and a music psychologist, this article attempts to clarify some of these assumptions, and offers some suggestions on how to adapt some of MIR’s most emblematic signal processing paradigms, evaluation procedures and application scenarios to the new challenges brought forth by the natural sciences of music.

The Web N-gram Workshop was held on July 23, 2010 in Geneva, Switzerland, in conjunction with the 33rd Annual ACM SIGIR Conference.
The workshop brought together leaders in information retrieval and language modeling to discuss the challenges in information retrieval and how language modeling approaches may help address some of these challenges, with a focus on using n-gram model to help solve them.
The workshop consisted of 1 invited talk, 1 tutorial, 1 panel, 10 refereed paper presentations along with discussion sessions built-in in the agenda.

IdSay is an open domain Question Answering (QA) system for Portuguese.
Its current version can be considered a baseline version, using mainly techniques from the area of Information Retrieval (IR
The only external information it uses besides the text collections is lexical information for Portuguese.
It was submitted to the monolingual Portuguese task of the QA track of the Cross-Language Evaluation Forum 2008 (QA@CLEF) for the first time, and it answered correctly to 65 of the 200 questions in the first answer, and to 85 answers considering the three answers that could be returned per question.
Generally, the types of questions that are answered better by IdSay system are measure factoids, count factoids and definitions, but there is still work to be done in these areas, as well as in the treatment of time.
List questions, location and people/organization factoids are the types of question with more room for improvement.

Ontology is the backbone of the semantic web and can overcome semantic barriers.
A domain ontology provides a common understanding of the knowledge of a particular domain.
Information Science, meanwhile, is an interdisciplinary science that is yet to be defined.
It is necessary to develop an Ontology of Information Science (OIS) to represent the knowledge in the field.
This paper presents a representation of specific domain knowledge by providing a definition, scope, and boundaries of Information Science (IS The OIS has fourteen facets: actors, method, practice, studies, mediate, kinds, domains, resources, legislation, philosophy and theories, societal, tools, time and space.
The methodology followed is Methontology, which is based on the IEEE standard for the development of a software life-cycle process.
The paper then discusses the OIS ontology, particularly its structure.

The present paper describes a method of fully automated cross-language information retrieval, which does not require any query translation.
Namely, monolingual queries retrieve documents from a multilingual collection, which includes items from the query's source language.
This is achieved by a method, which combines the construction of a multilingual semantic space using latent semantic indexing (LSI) and the clustering ability of self-organizing maps (SOM) for the generation of multilingual semantic categories.
Tests on an English-Greek corpus reveal the effectiveness and robustness of the proposed method.

This paper deals with a practical solution for finding and integrating information from the Web.
Since some of the ideas of our BUSTER system are already known we focus on two issues: we introduce the Comprehensive Source Description (CSD a necessary description for information sources that allows extra services such as integration or translation and a new feature that allows a combined search for concepts at a certain location, introducing the concept@location query.
We provide an example for better understanding and discuss the results.

The 4th International Workshop on Patent Information Retrieval builds on the experiences of the first three workshops, to provide its participants an exciting, scientifically challenging and interactive event, where specific issues of patent retrieval may be put into the general context of Information Retrieval and Knowledge Management, in order to explore innovative solutions to new and old problems, but also to evaluate and adapt traditional or classic approaches to new problems.
This year, we observe an increase in the use of standardized test collections in the contributions received, and, at the same time, new discussion points on how to make such standardized evaluation exercises more accessible to the larger IP community.

Fei Xia Department of Computer and Information Science University of Pennsylvania
3401 Walnut Street, Suite 400A Philadelphia PA 19104,
USA fxia@linc.cis.upenn.edu Abstract
In this paper, we report our work on extracting lexicalized tree adjoining grammars (LTAGs) from partially bracketed corpora.
The algorithm rst fully brackets the corpora, then extracts elementary trees (etrees and nally lters out invalid etrees using linguistic knowledge.
We show that the set of extracted etrees may not be complete enough to cover the whole language, but this will not have a big impact on parsing.

The growth of the Semantic Web has seen a rapid increase in the amount of Resource Description Framework (RDF) data.
Meanwhile, the demand for access to RDF data without detailed knowledge of RDF query languages is increasing.
In this study, an approach enabling keyword-based semantic information query over RDF data is proposed.
The approach sets up a keyword-inverted index and a relation index based on the r-radius+ graph and searches the connecting nodes to provide an answer for keyword query.
Moreover, the approach uses an improved scoring function based on textual relevancy and relation popularity and supports top-k queries.
Experimental results show that the proposed approach can achieve good query performance.

With the increasing importance of the "Global Information Society" and as the world's depositories of online collections proliferate, there is a growing need for systems that enable access to information of interest wherever and however it is stored, regardless of form or language.
In recognition of this, five years ago, the DELOS Network for Digital Libraries launched the CrossLanguage Evaluation Forum (CLEF with the objective of promoting multilingual information access by providing the research community with an infrastructure for testing and evaluating systems operating in multilingual contexts and a common platform for the comparison of methodologies and results.
In this paper, we outline the various activities initiated by CLEF over the years in order to meet the emerging needs of the application communities, and trace the impact of these activities on advances in multilingual system development.

The WebDB workshop has been held thirteen times so far: the first WebDB workshop was colocated with EDBT’1998, whereas the other twelve were co-located with the annual SIGMOD/PODS conference.
The WebDB workshop provides a forum where researchers, theoreticians, and practitioners can share their knowledge and opinions about problems and solutions at the intersection of data management and the Web.
WebDB has had a high impact and has been a forum in which a number of seminal papers have been presented.
Since 2002, the workshop always had a theme.
In 2010 WebDB focused on Quality of Web Data and on Linked Data, but papers on all aspects of the web and databases were solicited, such as unstructured and semi-structured data management, data -extraction integration cleansing, and -mining, web applications and privacy, search and information retrieval, and distributed data management.

In this work the authors propose a classification method based on Support Vector Machine (SVM) and key frames features extraction to classify historical sport video contents.
In the context of the Italian Project, IRMA (Information Retrieval in Multimedia Archives with the goal to recover and preserve historical videos of proven cultural interest, a data set made up of several hours of videos from the 1960 Olympic games, provided by RAI and Teche RAI, is adopted as testbed.
Each video is summarized by its key frames and represented by the features vectors computed in the Laguerre Gauss transformed domain.
The high-level video classification starts from these vectors that are the input of the SVM classifier.
The experimental results show the effectiveness of the proposed method.

Query expansion has been a staple of the TREC ad hoc task dating back almost to the inception of TREC, showing consistent benefit when added to a wide variety of baseline techniques, e.g 1, 2
In the biomedical domain, however, results have been mixed.
While Srinivasan obtained improved retrieval using retrieval feedback (automatic relevance feedback) in a small test collection [3 Hersh et. al. did not find improved retrieval when queries were expanded using thesaurus relationships in the Unified Medical Language System (UMLS)
Metathesaurus [4 Query expansion may be feasible in the genomics domain due to the considerable effort being devoted to creating useful cross-linkages across data sources.
The most prominent example is the collection of databases maintained by the National Center for Biotechnology Information (NCBI, www.ncbi.nlm.nih.gov a division of the National Library of Medicine (NLM, www.nlm.nih.gov 5].

Semantic search has been one of the motivations of the Semantic Web since it was envisioned.
We propose a model for the exploitation of ontology-based KBs to improve search over large document repositories.
The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm.
Semantic search is combined with keyword-based search to achieve tolerance to KB incompleteness.
Our proposal has been tested on corpora of significant size, showing promising results with respect to keyword-based search, and providing ground for further analysis and research.

How assessors and end users judge the relevance of images has been studied in information science and information retrieval for a considerable time.
The criteria by which assessors' judge relevance has been intensively studied, and there has been a large amount of work which has investigated how relevance judgments for test collections can be more cheaply generated, such as through crowd sourcing.
Relatively little work has investigated the process individual assessors go through to judge the relevance of an image.
In this paper, we focus on the process by which relevance is judged for images, and in particular, the degree of effort a user must expend to judge relevance for different topics.
Results suggest that topic difficulty and how semantic/visual a topic is impact user performance and perceived effort.

Ontologies are an important component in many areas, such as knowledge management and organization, electronic commerce and information retrieval and extraction.
Several methodologies for ontology building have been proposed.
In this article, we provide an overview of ontology building.
We start by characterizing the ontology building process and its life cycle.
We present the most representative methodologies for building ontologies from scratch, and the proposed techniques, guidelines and methods to help in the construction task.
We analyze and compare these methodologies.
We describe current research issues in ontology reuse.
Finally, we discuss the current trends in ontology building and its future challenges, namely, the new issues for building ontologies for the Semantic Web.

Link analysis is the most important application of web structure mining and serves as a new knowledge source in web information retrieval.
However, the mono-dimensional analysis of links neglects many other structural aspects.
The results presented in this article show that the structure of a site affects the in-links for its pages.
Link analysis algorithms need to be refined in order to account for that fact.

Digitization is the main feature of modern Information Science.
Conjoining the digits and the coordinates, the relation between Information Science and high-dimensional space is consanguineous, and the information issues are transformed to the geometry problems in some high-dimensional spaces.
From this basic idea, we propose Computational Information Geometry (CIG) to make information analysis and processing.
Two kinds of applications of CIG are given, which are blurred image restoration and pattern recognition.
Experimental results are satisfying.
and in this paper, how to combine with groups of simple operators in some 2D planes to implement the geometrical computations in high-dimensional space is also introduced.
Lots of the algorithms have been realized using software.

In this paper we investigate the use of conceptual descriptions based on description logics for contentbased information retrieval and present several innovative contributions.
We provide a query-byexamples retrieval framework which avoids the drawback of a sophisticated query language.
We extend an existing DL to deal with spatial concepts.
We provide a content-based similarity measure based on the least common subsumer which extracts conceptual similarities of examples.

Multilingual Information Retrieval System (MLIR) provides users to apply query in one language and retrieve the resultant documents in more than one language.
Assessing the performance of these systems is a challenging task.
Amongst the available metrics Precision is considered as the basic measure for assessing the performance of IR/MLIR systems.
In the literatures a few metrics are available to analyse the performance of MLIR System.
Therefore in this paper Precision oriented metrics are proposed based on the weight of languages and the results are very promising and demonstrated their existence in MLIR research domain.

In the early years of information retrieval, the focus of research was on systems aspects such as crawling, indexing, and relevancy ranking.
Over the years, more and more user-related information such as click information or search history has entered the equation creating more and more personalized search experiences, though still within the scope of the same overall system.
Though fully personalized search is probably desirable, this individualistic perspective does not exploit the fact that a lot of a users behavior can be explained through their group membership.
Children, despite individual differences, share many challenges and needs; as do men, Republicans, Chinese or any user group.
This workshop takes a group-centric approach to IR and invites contributions that either (i) propose and evaluate IR systems for a particular user group or that (ii) describe how the search behavior of specific groups differ, potentially requiring a different way of addressing their needs.

In May 2000 the Board of Directors changed the name of the American Society of Information Science by adding the words: and Technology.
Today this change may be considered minor, but for many involved in the society at the time it was a change that had purpose and meaning.
The intent of this study was to investigate the society’s transition toward a professional association more inclusive of practitioners and applied research.
The study was conducted in two stages, using both quantitative and qualitative research methods.
Stage 1 compared the research content both prior to and following the society’s name change in 2000.
Stage 2 built upon the assumption that a professional association is a reflection of its membership.
This study begins to scratch the surface toward helping to define the society and its research.
A clear vision that is supported by the membership will guide the activities of the society and, in turn, will effectively serve the membership and its professional needs.

High amount of relevant information is contained in reports stored in the electronic patient records and associated metadata.
R-oogle is a project aiming at developing information retrieval engines adapted to these reports and designed for clinicians.
The system consists in a data warehouse (full-text reports and structured data) imported from two different hospital information systems.
Information retrieval is performed using metadata-based semantic and full-text search methods (as Google Applications may be biomarkers identification in a translational approach, search of specific cases, and constitution of cohorts, professional practice evaluation, and quality control assessment.

We incorporate the Latent Semantic Indexing (LSl) technique into a competition-based neural network model for information retrieval.
The original neural network model was based on a causal inference network, incorporating Roget’s Thesaurus, that connects the index terms and related documents.
Since the pmcIess of creating or updating a thesaurus is rather expensive, we apply the LSI technique to provide an automated procedure that captures the semantic relationship between the doctrments and

Knowledge graphs have been used throughout the history of information retrieval for a variety of tasks.
Technological advances in knowledge acquisition and alignment technology from the last few years gave rise to a body of new approaches for utilizing knowledge graphs in text retrieval tasks.
It is therefore time to consolidate the community efforts in studying how knowledge graph technology can be employed in information retrieval systems in the most effective way.
It is also time to start a dialogue with researchers working on knowledge acquisition and alignment to ensure that resulting technologies and algorithms meet the demands posed by information retrieval tasks.
The goal of this workshop is to bring together a community of researchers and practitioners who are interested in using, aligning, and constructing knowledge graphs and similar semantic resources for information retrieval applications.

A common problem in organizations is to identify people with the right competencies to form a specialized team, in academic or industrial environments, which is capable of executing a self-managed project of research and development (R &amp; D This work presents a technique that allows for identifying people who have the most appropriated competencies to form a R &amp; D team extracting information from their curriculum vitas (CVs Information extraction in this work is carried out by means of textual retrieval techniques in document databases.
The system was evaluated with data of real projects producing expressive results when identifying people to participate in research projects

In this article, our interest is focused on the automatic learning of Boolean queries in information retrieval systems (IRSs) by means of multi-objective evolutionary algorithms considering the classic performance criteria, precision and recall.
We present a comparative study of four well-known, general-purpose, multi-objective evolutionary algorithms to learn Boolean queries in IRSs.
These evolutionary algorithms are the Nondominated Sorting Genetic Algorithm (NSGA-II
the first version of the Strength Pareto Evolutionary Algorithm
(SPEA the second version of SPEA (SPEA2 and the Multi-Objective Genetic Algorithm (MOGA).

Query expansion (QE) and document expansion (DE) have been proved effective for improving the retrieval performance in language modeling approach.
However, the issue that which expansion technique is more effective in information retrieval (IR has not been well studied and discussed.
To address this issue, this paper performs an empirical study on QE and DE to examine their effects.
Moreover, since QE and DE exploit different corpus structures, we also examine the potential effectiveness of incorporating QE and DE.
Experimental results on several TREC test collections show that both QE and DE significantly outperform the classical language model, but the effectiveness of QE and DE is varied in different settings of retrieval.
In addition, incorporating QE with DE does not always bring about the best performance.

Many practical systems in physics, biology, engineering, and information science exhibit impulsive dynamical behaviors due to abrupt changes at certain instants during the dynamical processes.
In this paper, problems of stability analysis and robust stabilization are investigated for switched stochastic systems which exist impulses at the switching instants.
Multiple Lyapunov techniques are used to derive sufficient conditions for stability in probability of the overall system with arbitrary switching law.
The conditions are in linear matrix inequality form and can be used to solve the problems of stabilization and robust stabilization.

The aim of the Forum for Information Retrieval Evaluation (FIRE) is to create an evaluation framework in the spirit of TREC (Text REtrieval Conference CLEF (Cross-Language Evaluation Forum and NTCIR (NII Test Collection for IR Systems for Indian language Information Retrieval.
The first evaluation exercise conducted by FIRE was completed in 2008.
This article describes the test collections used at FIRE 2008, summarizes the approaches adopted by various participants, discusses the limitations of the datasets, and outlines the tasks planned for the next iteration of FIRE.

This paper describes the work towards Gujarati Ad hoc Monolingual Retrieval task for widely used Information Retrieval (IR) models.
We present an indexing baseline for the Gujarati Language represented by Mean Average Precision (MAP) values.
Our objective is to obtain a relative picture of a better IR model for Gujarati Language.
Results show that Classical IR models like Term Frequency Inverse Document Frequency (TF_IDF) performs better when compared to few recent probabilistic IR models.
The experiments helped to identify the outperforming IR models for Gujarati Language.

Term clustering and syntactic phrase formation are methods for transforming natural language text.
Both have had only mixed success as strategies for improving the quality of text representations for document retrieval.
Since the strengths of these methods are complementary, we have explored combining them to produce superior representations.
In this paper we discuss our implementation of a syntactic phrase generator, as well as our preliminary experiments with producing phrase clusters.
These experiments show small improvements in retrieval effectiveness resulting from the use of phrase clusters, but it is clear that corpora much larger than standard information retrieval test collections will be required to thoroughly evaluate the use of this technique.

Information search and retrieval is one of the most prime fields of importance in today’s computing world.
Information retrieval is the process of obtaining information relevant to the users need, from a huge collection of documents.
Ranking the documents collection is traditionally based on Topic Similarity.
To improve the effectiveness of the retrieval, time can be incorporated into the ranking models.
In this paper, we will carry out a survey of Traditional Ranking Models and how time can be incorporated in those ranking models.
Index terms Information search and retrieval, Topic Similarity, Time Based Ranking Models

We prove new lower bounds for locally decodable codes and private information retrieval.
We show that a 2-query LDC encoding n-bit strings over an l-bit alphabet, where the decoder only uses b bits of each queried position of the codeword, needs code length

Automatic summarization is a topic of common concern in computational linguistics and information science, since a computer system of text summarization is considered to be an effective means of processing information resources.
A method of text summarization based on latent semantic indexing (LSI which uses semantic indexing to calculate the sentence similarity, is proposed in this article.
It improves the accuracy of sentence similarity calculations and subject delineation, and helps the abstracts generated to cover the documents comprehensively as well as reducing redundancies.
The effectiveness of the method is proved by the experimental results.
Compared with the traditional keyword-based vector space model method of automatic text summarization, the quality of the abstracts generated was significantly improved.

Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index DBLP;
GetCited; Google Scholar; HCIBIB; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD PsycINFO SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Special iSSue on Reimagining inteRfaceS foR oldeR adultS

Expert Finding has been a widely studied area of research.
However, most of the work in this area has focused solely on analyzing networks representing people in academia.
In this work, we will present an approach for two types of heterogeneous news sources (i.e Traditional Network Sources (TNS) and Policy Network Sources (PNS for experts on a set of topics.
Our overall objective is to discover who are the expert journalists and policy analysts on specific topics.
This work is based on our intuition that the PNS and TNS could complement each other, thus leveraging information for the learning task.
We propose a probabilistic generative model named Context-based Latent Dirichlet Allocation (CBLDA) that performs the task of co-ranking authors in the heterogeneous networks of TNS and PNS.
We will demonstrate that our proposed approach outperforms baselines in terms of precision, mean average precision, and discounted cumulative gain.

This work proposes a way to integrate an information retrieval (IR) system with an automatic speech recognition (ASR) engine to support natural spoken queries.
A broader interaction between the two modules is achieved by transmitting a lattice of terms to the IR system.
This is in contrast with conventional systems where only the best-path recognition output is transmitted.
Acoustic scores associated with the term-lattice are used to weigh the terms.
A latent semantic indexing (LSI) scheme in which documents and terms are mapped to a single reduced feature-space with 400 semantic components is used.
The conventional LSI method is nevertheless modified to allow the aforementioned broader interaction between acoustic hypothesis and semantic determination.
The results show that the proposed method moderately outperforms the traditional approach for spoken queries formulated as casual phrases.

In the web personalization how to represent user profile is one of the key issues.
The user profile refers to his/her interests which change over time.
This paper, presents a personalized search approach for representation and evolution of the user profile, based on dynamic bayesian network.
The theoretical framework provided by these networks allows to infer and to evolve the user profile from his /her interactions with the search system.
An experimental evaluation was designed to appraise the exploitation impact of the user profile defined by his/her interests on the search results relevance.

This paper introduces a general framework for the use of translation probabilities in cross-language information retrieval based on the notion that information retrieval fundamentally requires matching what the searcher means with what the author of a document meant.
That perspective yields a computational formulation that provides a natural way of combining what have been known as query and document translation.
Two well-recognized techniques are shown to be a special case of this model under restrictive assumptions.
Cross-language search results are reported that are statistically indistinguishable from strong monolingual baselines for both French and Chinese documents.

The ongoing surge in the amount of online information has made the process of accurate retrieval much more difficult.
Providers of information retrieval systems have come under a lot of pressure to improve their techniques to cater for the modern user.
Conventional systems are often limited as they fail to understand the true search intent of the user.
This is usually a result of both poor query formulation by the user and an inability of the search engine to process the query adequately.
In this paper, an approach is presented that attempts to learn a user’s short-term interests through the clustering of their search results.
A profile is maintained for each user to assist in the process of context resolution for a given query.
The details of such an approach and experimental results to evaluate its effectiveness are presented in this paper.

In this paper, we aim to investigate the practical usefulness of the Recall-Fallout Convexity Hypothesis (RFCH) for a number of document score distribution (SD) models.
We compare SD models that do not automatically adhere to the RFCH to modified versions of the same SD models that do adhere to the RFCH.
We compare these models using the inference of average precision as a measure of utility.
For the three models studied in this paper, we conclude that adhering to the RFCH is practically useful for the two-normal model, makes no difference for the two-gamma model, and degrades the performance of the twolognormal model.

A central problem in information retrieval is the automated classification of text documents.
While many existing methods achieve good levels of performance, they generally require levels of computation that prevent them from making sufficiently fast decisions in some applied setting.
Using insights gained from examining the way humans make fast decisions when classifying text documents, two new text classification algorithms are developed based on sequential sampling processes.
These algorithms make extremely fast decisions, because they need to examine only a small number of words in each text document.
Evaluation against the Reuters-21578 collection shows both techniques have levels of performance that approach benchmark methods, and the ability of one of the classifiers to produce realistic measures of confidence in its decisions is shown to be useful for prioritizing relevant documents.

We propose an online access panel to support the evaluation process of Interactive Information Retrieval (IIR) systems called IIRpanel.
By maintaining an online access panel with users of IIR systems we assume that the recurring effort to recruit participants for web-based as well as for lab studies can be minimized.
We target on using the online access panel not only for our own development processes but to open it for other interested researchers in the field of IIR.
In this paper we present the concept of IIRpanel as well as first implementation details.

This paper presents the results of the State University of New York at Buffalo (UB) in the Mono-lingual and Multi-lingual tasks at CLEF 2004.
For these tasks we used an approach based on statistical language modeling.
Our Adhoc retrieval work used the TAPIR toolkit developed in house by M Srikanth.
Our approach focused on the validation and adaptation of the language model system to work in a multilingual environment and in exploring ways to merge results from multiple collections into a single list of results.
We explored the use of a measure of query ambiguity, also known as clarity score, for merging results of the individual collections into a single list of retrieved documents.
Our results indicate that the use of clarity scores normalized across queries gives statistically significant improvements over using a fixed merging order.

monolithic method
There is a trade-off relation between informativeness and robustness of analysis in each processing technique More informative Less informative Less robust
More robust We employ multiple complementary methods in order for our QA system to have a variety of informativeness and robustness Implementation:
Raw score for an answer candidate AC in the i-th retrieved sentence Li with respect to a question sentence Lq.

Name Entity Recognition (NER) is an important process used for several type of applications such as Information Extraction, Information Retrieval, Question Answering, text clustering, etc.
It is intended to identify and classify name entities from a given text.
NER is performed by using a rule-based approach that relies on human intuitive or machine learning methods such as Hidden Markov Model (HMM Maximum Entropy (ME and Decision tree (
In this paper, we describe a model based on the first order HMM to recognize name entity in the Arabic language.
The model is based on stemming process that solves Arabic's inflection problem and ambiguity.
To the best of our knowledge, no work uses this approach for the Arabic language has been reported.

In digital libraries, the traditional retrieval methods cannot efficiently retrieve uncertain concepts, and traditional evaluation methods have disadvantages on algorithm measures.
To overcome above disadvantages, this paper proposes two new methods based on cloud models.
One is to retrieve efficiently uncertain concepts, which can change the granularity of retrieval information.
By this method it remarkably improves the recall and precision of information retrieval.
Another is to evaluate efficiently retrieval algorithm, which can reflect not only average performance of an algorithm but also stability and randomicity.
These two methods set up a transform of qualitative concepts and quantity.
This kind of transform is carried out through strict mathematic means.
Experimental data showed the method is practical.
Results of evaluation will be more accurate and approach to the fact better.

We study the impact of concavity in IR models and propose to use a generalized logarithm function, the <i>n</i>-logarithm to weight words in documents.
We extend the family of information based Information Retrieval (IR) models with this function.
We show that that concavity is indeed an important property of IR models.
Experiments conducted for IR tasks, Latent Semantic Indexing and Text Categorization show improvements.

We participated in the CLEF 2001 monolingual, bilingual, and multilingual tasks.
Our interests in these tasks are to test the utility of applying Chinese word segmentation algorithms to German decompounding, to experiment with techniques for combining translations from diverse resources, and to experiment with different approaches to multilingual retrieval.
This paper describes our retrieval experiments.

The digital revolution is transforming pencil and paper medicine.
Increasingly, health information is being recorded, stored, and analyzed using computerized tools.
This information conversion has significant public health implications.
Public health informatics:
The systematic application of information science, computer science, and technology to public health practice, research, and learning(1
This chapter describes public health informatics and the key role that individual optometrists can play in the care of populations through the use of electronic health records (EHR A case study will be examined illustrating the use of EHRs and the public health implications of the shift to digital health records.

In last few years the Internet has become a true component part of the process of education in many countries in Europe.
This global computer network has brought us many types of applications of computer technology suitable for learning.
However, there are still some methods and procedures which are part of this global network and are still not sufficiently used.
As a result of this opinion, a small group of students of the 4 year of library and information sciences at the Faculty of Philosophy in Zagreb, proposed a project which, they think, would solve the problem of inadequate quantity of printed exam materials at the Faculty necessary for the successful completion of exams by use of a digital collection containing equivalent ready-for-use material in electronic form.

Distributed Information Retrieval (DIR) has been suggested to offer a prospective solution to a number of issues concerning information retrieval in the WWW.
On the other hand, previous studies have indicated that centralized approaches offer the best solution for optimal quality of result (i.e. effectiveness
In this paper, we revisit those claims and investigate if and under which conditions can DIR offer a new paradigm for both efficient and effective information retrieval.

With the exponential increase in the quantity of information circulating on the Internet, an evolution of information-retrieval systems becomes paramount.
Indeed, current approaches for information systems design remain unable to meet the needs of users, either in performance (precision and recall) or response time.
In this paper, we propose a new information-retrieval algorithm based on formal concept analysis.
The proposed algorithm deals with disjunctive and conjunctive queries.
In fact, information retrieval is a direct application of the formal concept analysis (FCA
This makes the adaptation of this theory to this field an easy and intuitive task.
In this context, we exploited the theoretical basis provided by the FCA to design an efficient and flexible approach for information retrieval.

The effectiveness of a help desk system strongly depends on the ability to handle huge amounts of information.
The precision of the information retrieval (IR) phase has a direct impact on both length and quality of the solution process and the improvement of precision is the focus of our research.
Capitalizing on “type 2” fuzzy sets, we propose a keyword-based IR framework where relevance and confidence are explicitly modeled both in the system-user dialog and in the knowledge base management.
Adaptivity features assure a constant evolution of the system knowledge and experiments prove that the proposed techniques actually improve the precision of the IR process.

Rank correlation statistics are useful for determining whether a there is a correspondence between two measurements, particularly when the measures themselves are of less interest than their relative ordering.
Kendall's in particular has found use in Information Retrieval as a "meta-evaluation" measure: it has been used to compare evaluation measures, evaluate system rankings, and evaluate predicted performance.
In the meta-evaluation domain, however, correlations between systems confound relationships between measurements, practically guaranteeing a positive and significant estimate of regardless of any actual correlation between the measurements.
We introduce an alternative measure of distance between rankings that corrects this by explicitly accounting for correlations between systems over a sample of topics, and moreover has a probabilistic interpretation for use in a test of statistical significance.
We validate our measure with theory, simulated data, and experiment.

The vector space model used in Information Retrieval is combined with discriminant analysis to provide an automated WWWenvironment scanning system to detect signals of interest to an organization.
The vector space model converts text-based information to numerical vectors that are then used in discriminant analysis.
We illustrate the methodology using news articles pertaining to a predefined randomly selected set of stocks to test whether they provide predictive signals on whether the stock’s return will increase or decrease relative to the market in the target period following the report or whether the stock’s trading volume will increase or decrease.
D 2005 Elsevier B.V.
All rights reserved.

QUERY-TIME OPTIMIZATION TECHNIQUES FOR STRUCTURED QUERIES IN INFORMATION RETRIEVAL

What is entropy?
Entropy is the quantitative measure of disorder in a closed but changing system, a system in which energy can only be transferred in one direction from an ordered state to a disordered state.
The higher the entropy, the higher the disorder and lower the availability of the system's energy to do useful work.
Although the concept of entropy originated in thermodynamics and statistical mechanics, it has found applications in a lot of subjects such as communications, economics, information science and technology.

As we know about it, the more we try to learn, the more we cannot learn.
The thought of learning is quite different from college to college, and from person to person.
In this paper, I shall discuss some of the major differences in the thoughts of learning (or rather education as from the point of view of artificial neural network.
By the virtues of understanding a neural network learning, we would be able to educate our students more efficiently and effectively.
Particularly with the current rapid-trend of information science and technology, we may have to force ourselves to educate our students differently.
Experimental results, as derived from some of our optical neural network models, as related to education, will be demonstrated and interpreted.

The need of having a topic segmentation system for Arabic text is due essentially to improve the functionalities of Arabic Information Retrieval (AIR Topic segmentation of texts has been used to improve the accuracy of the subsequent processes such as question answering and information retrieval.
In this paper we present the implementation and the evaluation of two algorithms for Arabic text segmentation which are Text-Tilling and C99.
We compare the quality of the outputs of the two algorithms and we evaluate the relative performance of Text Tiling algorithm with respect to another cohesion based segmenter: C99 algorithm using the classical Recall/Precision evaluation metrics and the recently introduced Reader Judgment method.

Prosody is important in spoken language, and especially in dialog, but its utility for search in dialog archives has remained an open question.
Using prosody-based measures of similarity, which also roughly correlate with dialog-activity similarity and topic similarity, we built support for “retrieve more like this” searches.
Performance on the Similar Segments in Social Speech Task at MediaEval 2013 was well above baseline, showing the value of prosody for search.

This paper proposes the integration of KDD, GVis and STDB as a long-term strategy, which will allow users to apply knowledge discovery methods for uncovering spatio-temporal patterns in environmental data.
The main goal is to combine innovative techniques and associated tools for exploring very large environmental data sets in order to arrive at valid, novel, potentially useful, and ultimately understandable spatio-temporal patterns.
The Geolnsight approach is described using the principles and key developments in the research domains of KDD, GVis, and STDB.
The Geolnsight approach aims at the integration of these research domains in order to provide tools for performing information retrieval, exploration, analysis, and visualisation.
The result is a knowledge-based design, which involves visual thinking (perceptual-cognitive process) and automated information processing (computer-analytical process).

This paper proposes a novel document re-ranking approach in information retrieval, which is done by a label propagation-based semi-supervised learning algorithm to utilize the intrinsic structure underlying in the large document data.
Since no labeled relevant or irrelevant documents are generally available in IR, our approach tries to extract some pseudo labeled documents from the ranking list of the initial retrieval.
For pseudo relevant documents, we determine a cluster of documents from the top ones via cluster validation-based k-means clustering; for pseudo irrelevant ones, we pick a set of documents from the bottom ones.
Then the ranking of the documents can be conducted via label propagation.
Evaluation on benchmark corpora shows that the approach can achieve significant improvement over standard baselines and performs better than other related approaches.

In this paper, we present our English-Chinese Cross-Language Information Retrieval (CLIR) system.
We focus our attention on finding effective translation equivalents between English and Chinese, and improving the performance of Chinese IR.
On English-Chinese CLIR, we adopt query translation as the dominant strategy, and utilize English-Chinese bilingual dictionary as the important knowledge resource to acquire correct translations.
On Chinese monolingual retrieval, we investigated the use of different entities as indexes and implement our retrieval system based on the Lucene toolkit.
On system evaluation, we present an effective method to generate the sets of relevant documents for query topics.

Parallel corpus is a valuable resource for cross-language information retrieval and data-driven natural language processing systems, especially for Statistical Machine Translation (SMT
However, most existing parallel corpora to Chinese are subject to in-house use, while others are domain specific and limited in size.
To a certain degree, this limits the SMT research.
This paper describes the acquisition of a large scale and high quality parallel corpora for English and Chinese.
The corpora constructed in this paper contain about 15 million English-Chinese (E-C) parallel sentences, and more than 2 million training data and 5,000 testing sentences are made publicly available.
Different from previous work, the corpus is designed to embrace eight different domains.
Some of them are further categorized into different topics.
The corpus will be released to the research community, which is available at the NLPCT website.

Most existing organizational design processes focus on either the qualitative or domain-independent features of candidate designs.
This paper demonstrates the significance of domain-specific features through an examination of an organizationally-driven information retrieval network.
The behavior of a distributed search process and the consequences of hierarchical control are described.
A model capable of predicting these characteristics is created in the ODML framework using techniques from queuing theory.
This model can be used to guide the search for an appropriate design.

This paper describes the development and testing of the Medical Concept Mapper as an aid to providing synonyms and semantically related concepts to improve searching.
All terms are related to the userquery and fit into the query context.
The system is unique because its five components combine humancreated and computer-generated elements.
The Arizona Noun Phraser extracts phrases from natural language user queries.
WordNet and the UMLS Metathesaurus provide synonyms.
The Arizona Concept Space generates conceptually related terms.
Semantic relationships between queries and concepts are established using the UMLS Semantic Net.
Two user studies conducted to evaluate the system are

Personal Information management (PIM) is a research area that receives interest from a variety of disciplines including human-computer interaction, information retrieval, information systems research and psychology to mention but a few.
The diversity in approaches and the cross-disciplinary nature of PIM have resulted in a fragmented picture of the problems and challenges designers of PIM tools are facing.
In this paper we present a PIM evaluation framework based on a broad literature study of the known challenges within PIM.
Focusing in particular on information fragmentation and the re-finding of information, we built and evaluated a PIM prototype using our framework.
We found that zooming, separation between logical and physical structures, and showing search results in context seem like useful future design ideas.

The key factors for the success of the World Wide Web are its large size and the lack of a centralized control over its contents.
Both issues are also the most important source of problems for locating information.
The Web is a context in which traditional Information Retrieval methods are challenged, and given the volume of the Web and its speed of change, the coverage of modern search engines is relatively small.
Moreover, the distribution of quality is very skewed, and interesting pages are scarce in comparison with the rest of the content.

The purpose of this paper is to investigate the role that MIDI plays in the arena of Multimedia Information Retrieval.
MIDI is an encoding format which encodes and relays performancerelated information about digital music which is used to generate music from a synthesiser.
Current thinking has used this performance information to develop a concept known as contentbased retrieval of audio.
This paper will develop this thinking by analysing the existing research in content-based retrieval of non-speech audio and assessing whether or not MIDI is a good format to support this retrieval.

This report describes metrics for the evaluation of the effectiveness of segment-based retrieval based on existing binary information retrieval metrics.
This metrics are described in the context of a task for the hyperlinking of video segments.
This evaluation approach re-uses existing evaluation measures from the standard Cranfield evaluation paradigm.
Our adaptation approach can in principle be used with any kind of effectiveness measure that uses binary relevance, and for other segment-baed retrieval tasks.
In our video hyperlinking setting, we use precision at a cut-off rank n and mean average precision.

This poster presents an overview of the characteristics of a one-button information retrieval interface with closed captions from TV watching activities, which is intended to lighten the burden of remembering and entering query terms while watching TV.
We investigated this interface with an experimental system named <i>Video Bookmarking Search</i which estimates query terms from closed captions with named-entity recognition and sentence labeling techniques.
According to an empirical evaluation for 1,138 search queries from 206 bookmarks using seven actual TV shows on city life, travel, health, and cuisine, we found wider queries and search results are acceptable through the query-input-free interface, despite the fact that the number of queries and search results that are directly relevant to the users' original intentions is not high.
The main reason is a watching user's interest is wider than what is expressed with query terms.

Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music information retrieval systems.
This paper presents the systematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music.
The evaluations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008.
The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed.

Natural language query formulations exhibit advantages over artificial language statements since they permit the user to approach the retrieval environment without prior training and without using intermediaries.
To obtain adequate retrieval output, it is however necessary to emphasize the good terms and to deemphasize the bad ones.
The usefulness of the terms in a natural language vocabulary is first characterized in terms of their frequency distribution over the documents of a collection.
The construction of "good" natural language vocabularies is then described, and methods are given for improving the vocabulary by transforming terms that operate poorly for retrieval purposes into better ones.

Artificial Intelligence (AAAI) held its 1993 Spring Symposium Series on March 23–25 at Stanford University.
This article contains summaries of the eight symposia that were conducted: AI and Creativity, AI and NP-Hard Problems, Building Lexicons for Machine Translation, CaseBased Reasoning and Information Retrieval, Foundations of Automatic Planning, Innovative Applications of Massive Parallelism, Reasoning about Mental States, and Training Issues in Incremental Learning.
Technical reports of the symposia AI and Creativity, Building Lexicons for Machine Translation, Case-Based Reasoning and Information Retrieval, Foundations of Automatic Planning, Innovative Applications of Massive Parallelism, Reasoning about Mental States, and Training Issues in Incremental Learning are available from AAAI.
Instructions and an order form for purchasing electronic and hardcopy versions can be found elsewhere in this issue.

A lot of digital Farsi content has been produced recently in middle-east.
Local context analysis (LCA) is an automated query expansion method that adds concepts to the original query based on the initial retrieval using the original query.
In our previous works we attempted to tune this method for Farsi language by manipulating three parameters which are number of concepts used for query expansion, number of initially retrieved documents for local feedback and number of passages for concept discovery and weighting.
In this paper we seek to further customize this method for Farsi information retrieval.
To compare our work to the previous attempts we have used Hamshahri collection and 60.
We have experimented with different number of concepts and have also changed the concept weighting algorithm to improve retrieval performance.

Online metrics measured through A/B tests have become the gold standard for many evaluation questions.
But can we get the same results as A/B tests without actually fielding a new system?
And can we train systems to optimize online metrics without subjecting users to an online learning algorithm?
This tutorial summarizes and unifies the emerging body of methods on counterfactual evaluation and learning.
These counterfactual techniques provide a well-founded way to evaluate and optimize online metrics by exploiting logs of past user interactions.
In particular, the tutorial unifies the causal inference, information retrieval, and machine learning view of this problem, providing the basis for future research in this emerging area of great potential impact.
Supplementary material and resources are available online at http www.cs.cornell.edu/~adith/CfactSIGIR2016.

The 27th European Conference on Information Retrieval Research (ECIR 2005 celebrated in Santiago de Compostela (Spain) on 21-23 March 2005, was jointly organized by the University of Santiago de Compostela (Spain) and the University of Granada (Spain under the auspices of the Information Retrieval Specialist Group of the British Computer Society (BCS-IRSG
The conference was co-chaired by David E. Losada (University of Santiago de Compostela) and Juan M. Fern&#225;ndez-Luna (University of Granada) and was celebrated in the Technical School of Engineering of the University of Santiago de Compostela.

This paper describes an on-line interactive student information system designed for graduate level student information at Mississippi State University.
It was designed with three objectives in mind.
The first was to keep graduate students abreast of the requirements to be met for an advanced degree.
The second objective was to aid the Office of Graduate Studies and the Registrar in certifying the completion of requirements for advanced degrees.
Thirdly, the system was designed so that information retrieval for various colleges, departments and federal agencies could be accomplished without a time-consuming manual search of student records.
A description of the software and data files is given, as well as a description of the major report forms produced by the system.
In addition, the operation of the system will be briefly described.

This paper presents the 2005 Miracle’s team approach to Monolingual Information Retrieval.
The goal for the experiments in this year was twofold: continue testing the effect of combination approaches on information retrieval tasks, and improving our basic processing and indexing tools, adapting them to new languages with strange encoding schemes.
The starting point was a set of basic components: stemming, transforming, filtering, proper nouns extracting, paragraph extracting, and pseudo-relevance feedback.
Some of these basic components were used in different combinations and order of application for document indexing and for query processing.
Second order combinations were also tested, by averaging or selective combination of the documents retrieved by different approaches for a particular query.

This paper describes the issues and preliminary work involved in the creation of an information retrieval system that will manage the retrieval from collections composed of both speech recognised and ordinary text documents.
In previous work, it has been shown that because of recognition errors, ordinary documents are generally retrieved in preference to recognised ones.
Means of correcting or eliminating the observed bias is the subject of this paper.
Initial ideas and some preliminary results are presented.
General Terms Measurement, Experimentation.

Dialogue Management components are becoming increasingly useful in Natural Language Information Retrieval systems, to aid users find information they require.
These components need to be evaluated, both intrinsically, to gauge development approaches, and extrinsically to measure subjective performance.
We introduce a Natural Language Dialogue System, the YPA, developed at the University of Essex, that accesses classified directory information such as British Telecom’s Yellow Pages.
These directories contain much useful but hard-to-access information, especially in the free text in semi-display advertisements.
We examine the evaluation methodologies used to assess the performance of the YPA during development, both from users and technology developer perspectives, by using both subjective and objective evaluation approaches.
We finish by drawing some conclusions from these experiments.

The 25 Annual conference on research and development was held in Tampere Hall, Tampere, Finland.
In all, 2 keynote talks were given, 44 papers presented, 41 posters delivered and 12 demonstrations of various software were given.
A vast array of subjects in information retrieval was presented from high-level user studies to theoretical aspects of IR to implementation of IR systems.
We review each of these areas below.
We give a summary of the talks for each session, putting those sessions that are closely linked together.
For each paper presented we provide the main author, the title of the paper and the pages where full paper can be found in the proceedings.

Large amount of unstructured designed information is difficult to deal with.
Obtaining specific information is a hard mission and takes a lot of time.
Information Retrieval System (IR) is a way to solve this kind of problem.
IR is a good mechanism but does not give the perfect solution.
Other techniques have been added to IR to develop the result.
One of the techniques is text classification.
Text classification task is to assign a document to one or more category.
It could be done manually or algorithmically.
Text classification enhances the output of this process by reducing the results.
This study proved that text classification has a positive influence on Information Retrieval Systems.

Information retrieval systems contain large volumes of text, and currently have typical sizes into the gigabyte range.
Inverted indexes are one important method for providing search facilities into these collections, but unless compressed require a great deal of space.
In this paper we introduce a new method for compressing inverted indexes that yields excellent compression, fast decoding, and exploits clustering—the tendency for words to appear relatively frequently in some parts of the collection and infrequently in others.
We also describe two other quite separate applications for the same compression method: representing the MTF list positions generated by the Burrows-Wheeler Block Sorting transformation; and transmitting the codebook for semi-static block-based minimum-redundancy coding.

Eugene Garfield PhD is u piouccr in the field of information science.
He is founder and presiden t oj’the Institute jbr Scientijic Irij&mation (iSI rvhich produces a broad spectrum of‘ inf&-mation services and systems.
The inventor of ‘Current

Ranking solutions is an important issue in Information Retrieval because it greatly influences the quality of results.
In this context, keyword based search approaches use to consider solutions sorting as least step of the overall process.
Ranking and building solutions are completely separate steps running autonomously.
This may penalize the retrieving information process because it binds to order all found matching elements including (possible) irrelevant information.
In this paper we present a theoretical study of YAANII, a novel technique to keyword based search over semantic data.
We demonstrate how effectiveness of the answers depends not so much on the quality of the scoring metrics but on the way such criteria are involved.

Information Retrieval (IR) involves several disciplines to improve the search quality.
In this paper, we focus on terminology extraction which includes critical tasks especially for highly ambiguous languages like Arabic.
Properly retrieving a list of relevant terms for a given domain and enrich it is a persistent problem.
Therefore, and in order to improve the coverage of terminology extraction and enrichment, we must strengthen our research by text-mining technologies based on well-founded methods.
In this article, we present a new Arabic terminology extraction and enrichment approach which exploits the corpus structure as a first step to extract a minimal terminology and uses text-mining techniques to enrich it in a second step.

Multimedia information retrieval poses both technical and design challenges beyond those of established text retrieval.
These issues extend both to the entry of search requests, system interation and the browsing of retrieved content, and the methodologies and techniques for content indexing.
Prototype multimedia information retrieval systems are currently being developed which enable the exploration of both the user interaction and technical issues.
The suitability of the solutions developed within these systems is currently being explored in the annual TRECVID evaluation workshops which enable researchers to test their indexing and retrieval algorithms and complete systems on common tasks

Natural language processing techniques may hold a tremendous potential for overcoming the inadequacies of purely quantitative methods of text information retrieval, but the empirical evidence to support such predictions has thus far been inadequate, and appropriate scale evaluations have been slow to emerge.
In this chapter, we report on the progress of the Natural Language Information Retrieval project, a joint effort of several sites led by GE Research, and its evaluation in a series of Text Retrieval Conferences (TREC) conducted since 1992 under the auspices of the National Institute of Standards and Technology (NIST) and the Defense Advanced Research Projects Agency (DARPA).

This paper introduces a Japanese Named Entity (NE) corpus of various genres.
We annotated 136 documents in the Balanced Corpus of Contemporary Written Japanese (BCCWJ) with the eight types of NE tags defined by Information Retrieval and Extraction Exercise.
The NE corpus consists of six types of genres of documents such as blogs, magazines, white papers, and so on, and the corpus contains 2,464 NE tags in total.
The corpus can be reproduced with BCCWJ corpus and the tagging information obtained from https sites.google.com/ site/projectnextnlpne/en

Domain specific search engines (DSSE) are gaining popularity because of better search relevance and domain specificity.
The growth of IT and internet led to increase of cyber attacks, however, lack of DSSE for Security is making users refer multiple sites for security information.
We demonstrate SIREN, a search engine for ’Information and Cyber Security’ with subdomain coverage, classification and site credibility for ranking search results.
As part of our demonstration, we also automated identification of seed URLs (34,007) and related child URLs (400,726) of the security domain using Artificial Bee Colony algorithm.
We also evaluated functional and non-functional parameters of available open source software stack that can be used for building other DSSEs.

Finding meaningful phrases in a document has been studied in various information retrieval systems in order to improve the performance.
Many previous statistical phrase-finding methods had a different aim such as document classification.
Some are hybridized with statistical and syntactic grammatical methods; others use correlation heuristics between words.
We propose a new phrase-finding algorithm that adds correlated words one by one to the phrases found in the previous stage, maintaining high correlation within a phrase.
Our results indicate that our algorithm finds more meaningful phrases than an existing algorithm.
Furthermore, the previous algorithm could be improved by applying different correlation junctions.

This paper describes Hyper-G, a new hypermedia information system which combines the best of Gopher, WAIS, and World Wide Web.
Hyper-G is speci cally designed as a distributed, large-scale hypermedia information system supporting navigation in a large body of dynamically changing information without becoming \lost in hyperspace Users may choose a hierarchical navigation paradigm, click on hyper-links, go on guided tours, or perform variable-scope searches.
This paper presents Hyper-G from the user's perspective, outlines the basic architecture of the system, and describes its interaction with existing distributed information retrieval tools like Gopher, WAIS, and World Wide Web.

In the field of Music Information Retrieval, there are many tasks that are not only difficult for machines to solve, but that also lack well-defined answers.
In pursuing the automatic recognition of emotions within music, this lack of objectivity makes it difficult to train systems that rely on quantified labels for supervised machine learning.
In recent years, researchers have begun to harness Human Computation for the collection of data spanning an excerpt of music
i>MoodSwings</i> records dynamic (per-second) labels of players' mood ratings of music, in keeping with the unique time-varying nature of musical mood.
Players collaborate to build consensus, ensuring the quality of data collected.
We present an analysis of <i>MoodSwings</i> labels collected to date and propose several modifications for improving both the quality of the gameplay and the collected data as development moves forward.

In Multimedia information retrieval late semantic fusion is used to combine textual pre-filtering with an image reranking.
Three steps are used for retrieval processes.
Visual and textual techniques are combined to help the developed Multimedia Information Retrieval System to minimize the semantic gap for given query.
In the paper, different late semantic fusion approaches i.e. Product, Enrich, MaxMerge and FilterN are used and for experiments publicly available ImageCLEF Wikipedia Collection is used.
Keywords multimedia retrieval, textual-based information retrieval, content-based information retrieval,
multimedia information fusion

Contributive resources, such as Wikipedia, have proved to be valuable to Natural Language Processing or multilingual Information Retrieval applications.
This work focusses on Wiktionary, the dictionary part of the resources sponsored by the Wikimedia foundation.
In this article, we present our effort to extract multilingual lexical data from Wiktionary data and to provide it to the community as a Multilingual Lexical Linked Open Data (MLLOD This lexical resource is structured using the LEMON Model.
This data, called dbnary, is registered at http thedatahub.org/dataset/dbnary.

For the participation of the University of Alicante in the first cross-language Geographic Information Retrieval, we have developed a system made up of three modules.
One of them is an Information Retrieval module and the others are Named Entity Recognition modules based on machine learning and based on knowledge.
We have carried out several runs with different combinations of these modules for resolving the monolingual and bilingual tasks.
The system obtained better result in monolingual task achieving an improvement between 48% and 69% above the average.
The results are shown and discussed in the paper.

XML information retrieval in the INEX workshop series has, for a large part, focused on creating algorithms for testing against the test collections provided by the IEEE and Wikipedia.
The use case track is a new track at INEX and represents an attempt at identifying examples of how XML IR systems can be exploited by end-users for various purposes.
In the present article we present a summary of the discussions and findings of the track.

Evaluation is a systematic determination of a subject's merit, worth and significance, using criteria governed by a set of standards.
It can assist an organization, program, project or any other intervention or initiative to assess any aim, realisable concept/proposal, or any alternative, to help in decisionmaking; or to ascertain the degree of achievement or value in regard to the aim and objectives and results of any such action that has been completed.
The primary purpose of evaluation, in addition to gaining insight into prior or existing initiatives, is to enable reflection and assist in the identification of future change.

International Journal of Geographical Information Science Publication details, including instructions for authors and subscription information: http www.informaworld.com/smpp/title~content=t713599799 Population-density estimation using regression and area-to-point residual kriging X. H. Liu a; P. C. Kyriakidis b; M. F. Goodchild b a Department of Geography Human Environmental Studies, San Francisco State University, HSS 279, San Francisco, CA 94132 b Department of Geography, Ellison Hall, University of California, Santa Barbara, Santa Barbara, CA 94106-4060

IR research is now conducted in multi-media, multi-lingual, and multi-modal environments but largely in a context-free manner (Ingwersen &amp; J&auml 228;rvelin, 2005
However, the retrieval of such information depends on time, place, history of interaction, task in hand, and a range of other factors that are not given explicitly but are implicit in the interaction and ambient environment, namely the context.
Such contextual data can be used effectively to constrain retrieval of information thereby reducing the complexity of the retrieval process.

In this paper, we present an ontology modeling tool for building Chinese domain ontology, which not only offers automatic acquisition knowledge from multiple dictionaries, but also corresponds with a dictionary-based ontology modeling methodology that combines METHONTOLOGY and throwaway prototype.
By using a case study of spatial information science ontology modeling, the theoretical method and implementation tool of dictionary-based ontology modeling are presented in details.
Experience shows that automatic acquisition knowledge from multiple dictionaries for ontology modeling is feasible and merging knowledge between multiple dictionaries can make assistant modeling more effective.

Smoothing document model with word graph is a new and effective method in information retrieval.
Word graph can naturally incorporate the dependency between the words; random walk algorithm based on the graph can be used to estimate the weight of each vertex.
In this paper, we present a new way to construct a local word graph for smoothing document model, which exploits the document's k nearest neighbors: the vertices represent the words in the document and its k nearest neighbors, and the weights of the edges are estimated through word co-occurrence in the local document set.
We argue that word graph is a key factor to the performance in graph-based smoothing method.
By using the local document set, we can obtain a document specific word graph, and achieve better retrieval performance.
Experimental results on three TREC collections show that our proposed approach is effective.

This report presents moreBugs, a new publicly available dataset derived from the AspectJ and JodaTime repositories for the benchmarking of algorithms for retrieval from software repositories.
As a case in point, moreBugs contains all the information required to evaluate a search-based bug localization framework it includes a set of closed/resolved bugs mined from the bug-tracking system, and, for each bug, its patch-file list and the corresponding snapshot of the repository extracted from version history.
moreBugs tracks commit-level changes made to a software repository along with its release information.
In addition to the benchmarking of bug localization algorithms, the other algorithms whose benchmarking moreBugs should prove useful for include: change detection, impact analysis, software evolution, vocabulary evolution, incremental learning, and so on.

Mizar, a proof-checking system, is used to build the Mizar Mathematical Library (MML MML Query is a semantics-based tool for searching, browsing and presentation of the evolving MML content.
The tool is becoming widely used as an aid for Mizar authors and plays an essential role in the ongoing reorganization of MML.
We present new features of MML Query implemented in the third release and describe the possibilities offered by them.

Different users have different needs, even the same user may have different desires in different time, Personalized Information Retrieval makes search results meet different users’ information requirement.
In this paper, a kind of user interest recognition algorithm is proposed, which can analyze the user interest state and identify the user’s Temporary interest; And a state-based user interest model is developed, In this model, user interest is recognized by the algorithm mentioned above, and users’ characteristic is extracted and Dynamic weighted, then gray relational analysis is introduced to for the comprehensive consideration of two aspects above.
The experimental result indicates that the average push accuracy is above 70% and the push service is more accurate for the user who has a long interest cycle.

We investigate the problem of ranking the answers to a database query when many tuples are returned.
In particular, we present methodologies to tackle the problem for conjunctive and range queries, by adapting and applying principles of probabilistic models from information retrieval for structured data.
Our solution is domain independent and leverages data and workload statistics and correlations.
We evaluate the quality of our approach with a user survey on a real database.
Furthermore, we present and experimentally evaluate algorithms to efficiently retrieve the top ranked results, which demonstrate the feasibility of our ranking system.

The Music Instrument Identification research is an important and difficult problem in Music Information Retrieval (
In this paper an algorithm based on flexible harmonic model is proposed to represent the pitch in music by Gaussian mixture structure.
The proposed algorithm models each spectral envelope of underlying harmonic structure to approximate the real music and uses EM algorithm to estimate the parameters.
Not only is it able to estimate the multipitch (F0) but it also takes the attack problem (a kind of inharmonic structure at the beginning of some pitches) into account.
The proposed algorithm makes it possible to envisage the use of timbre features derived from both harmonic part and attack part.
Musical instrument recognition is then carried out by using SVM classifier.
Experiment shows high performance of the proposed algorithm for instrument identification task.

This paper presents the participation of MayoNLPTeam in the 2016 CLEF eHealth Information Retrieval Task (IR Task 1: ad-hoc search We explored a Part-of-Speech (POS) based query term weighting approach which assigns different weights to the query terms according to their POS categories.
The weights are learned by defining an objective function based on the mean average precision.
We applied the proposed approach with the optimal weights obtained from TREC 2011 and 2012 Medical Records Track into the Query Likelihood model (Run 2) and Markov Random Field (MRF) models (
The conventional Query Likelihood model was implemented as the baseline (Run 1).

A follow-up study to further examine issues and future directions of ACA

The requirements imposed on information retrieval systems are increasing steadily.
The vast number of documents in today’s large databases and especially on the World Wide Web causes problems when searching for concrete information.
It is difficult to find satisfactory information that accurately matches user information needs even if it is present in the database.
One of the key elements when searching the web is proper formulation of user queries.
Search effectiveness can be seen as the accuracy of matching user information needs against the retrieved information.
As step towards better search systems represents personalized search based on user profiles.
Personalized search applications can notably contribute to the improvement of web search effectiveness.
This chapter presents design and experiments with an information retrieval system utilizing user profiles, fuzzy information retrieval and genetic algorithms for improvement of web search.

Introduction
The study of geographic information sciences emerges in the last decade as an exciting multidisciplinary domain, spanning such areas as cartography, geography, remote sensing, image processing, and computer sciences.
In recent years many new applications of GIS have appeared in different domain and the technology continues to expand at a spectacular rate.
This advance is the result of the contribution of several disciplines such as computational geometry, spatial statistics, spatial cognition, databases, and computer graphics.

This study presents a new similarity measure based on the geometric-mean averaging operator to handle the similarity measure problems of generalized fuzzy numbers.
Some properties of the proposed similarity measure are demonstrated, and 26 sets of generalized fuzzy numbers are used to compare the proposed method with existing similarity measures.
Comparison results indicate that the proposed similarity measure is better than existing methods.
Finally, the proposed similarity measure is applied to propose an algorithm for handling fuzzy-number information retrieval problems.

This paper presents a new approach to information retrieval from non-structured attributes in databases, which involves the processing of text attributes.
To make retrieval more effective, frequent text sequences are extracted and mathematically represented as intermediate forms which permit a clearer and more precise definition of operations on texts.
These intermediate forms appear to users in the form of tag clouds to facilitate content identification, exploration, and querying.
In this sense, tag cloud visualization is a simple, user-friendly visual interface to data.
This paper proposes a theoretical model for the representation of frequent text sequences and their operations as well as a general procedure for generating tag clouds from text attributes in databases.
The tag clouds thus obtained were compared with conventional tag clouds composed of single terms.
Our study showed that automatically generated multi-term tag clouds provide better results than mono-term tag clouds.

We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations.
There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions.
The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR.

In this paper we propose a method that integrates the notion of understandability, as a factor of document relevance, into the evaluation of information retrieval systems for consumer health search.
We consider the gain-discount evaluation framework (RBP, nDCG, ERR) and propose two understandability-based variants (uRBP) of rank biased precision, characterised by an estimation of understandability based on document readability and by different models of how readability influences user understanding of document content.
The proposed uRBP measures are empirically contrasted to RBP by comparing system rankings obtained with each measure.
The findings suggest that considering understandability along with topicality in the evaluation of information retrieval systems lead to different claims about systems effectiveness than considering topicality alone.

The Natural Language Processing and Information Retrieval Group was formed in 1994 at the National Institute of Standards and Technology (NIST) in recognition of the importance of managing the ever-increasing amount of electronically available text.
The formal objective of the group is "to work with industry, academia and other government agencies to promote the use of more effective and efficient techniques for manipulating (largely) unstructured textual information, especially the browsing, searching, and presentation of that information The group carries on the work in text retrieval started in 1988.

Ambiguity in natural language has an effect on Information Retrieval (IR) system in general and web search system in particular.
Word sense ambiguity is the reason behind the poor performance of IR system.
If the ambiguous words are correctly disambiguated then IR performance can increase.
WSD is defined as the task of identifying the sense of word in textual context.
Word Sense Disambiguation (WSD) is the central problem of language processing.
WSD improves the IR in many ways.
Current IR system do not use explicit WSD and rely on user typing enough content in the query to only retrieve documents relevant to the intended senses.
This paper proposes an algorithm for efficient retrieval of information on the Web according to user’s need.
Results are more accurate and they are according to user’s preference.

This paper describes a flexible information retrieval approach based on conditional preferences networks (CP-Nets Our contribution focuses on document indexing and query evaluation using CP-NET theoretical foundations.
More precisely, we propose first a conceptual document indexing approach using WordNet for identifying concepts and association rules for discovering contextual relations between concepts.
Secondly, we propose an algorithm for query evaluation using graph similartity measures.

What is “intelligent” information retrieval?
Essentially this is asking what is intelligence, hi this article I will attempt to show some of the aspects of human intelligence, as related to information retrieval.
I will do this by the device of a semi imaginary Oracle.
Every Observatory has an oracle, someone who is a distinguished scientist, has great administrative responsibilities, acts as mentor to a number of less senior people, and as trusted advisor to even the most accomplished scientists, and knows essentially everyone in the field.

Documents and queries are rich in temporal features, both at the meta-level and at the content-level.
We exploit this information to define temporal scope similarities between documents and queries in metric spaces.
Our experiments show that the proposed metrics can be very effective for modeling the relevance for different search tasks, and provide insights into an inherent asymmetry in temporal query semantics.
Moreover, we propose a simple ranking model that combines the temporal scope similarity with traditional keyword similarities.
We experimentally show that it is not worse than traditional keywordbased rankings for non-temporal queries, and that it improves the overall effectiveness for time-based queries.

The last decade has witnessed a great interest in e-learning and Web based education areas.
Unfortunately, most of the e-learning environments used in the educational field today are still delivering the same educational resources and services in the same way to different learners.
Hence, observing the increasing need for personalization in e-learning systems, we aim to make these systems deliver the most appropriate content to learners according to their interests and needs.
This paper outlines the use of on-line automatic recommendations in e-learning systems based on learners’ access history.
First we start by mining learner profiles using usage Web mining techniques and content-based profiles using information retrieval techniques.
Then, we use these profiles to compute relevant links to recommend for an active learner by applying a number of recommendation strategies.

ABSTNACT Much is known about depth and breadth tradeoff issues in graphical user interface menu design.
We describe an experiment to see if large breadth and decreased depth is preferable, both subjectively and via performance data, while attempting to design for optimal scent throughout different structures of a website.
A study is reported which modified previous procedures for investigating depth/breadth tradeoffs in content design for the web.
Results showed that, while increased depth did harm search performance on the web, a medium condition of depth and breadth outperformed the broadesf shallow web structure overall.

We present a new context-based information retrieval (IR) system where users could indicate the context in which they are seeking information.
We use a novel way of representing context based on the type of information.
The context defined in this manner is most intuitive to the user and originates from their information need in a problem-solving situation.
This paper presents context categories for documents in computing and information technology domain and offers a new methodology for assigning context to documents in a collection.
This methodology showed promising results and the inter-assigner consistency quite comparable to the results reported in literature.
This new context-based information retrieval system overcomes major limitations of the keyword based IR systems currently in use and provides improved performance.

As a result, there has been a recent shift away from traditional retrieval systems and their statistical approach toward something called Intelligent Information Retrieval Systems IIRS Sparck Jones defined such a system in 1983 as a a user’s probably ill-defined request to a set of relevant documents [3
In other words, an IIRS is a system that carries out intelligent retrieval.
Using stored knowledge about its documents and about the user and his need, an IIRS infers which documents will help the user satisfy his information needs.

In the processing of Chinese documents and queries in information retrieval (IR one has to identify the units that are used as indexes.
Words and n-grams had been used as indexes in several previous studies, which showed that both kinds of indexes lead to comparable IR performances.
In this study, we carried out more experiments to find the better way to index Chinese texts.
First, we investigated the inpacts on IR performance of the accuracy of word segmentation.
Second, fifteen different groups of indexing units, which were the possible combination of words and character n-grams, were discussed detailedly.
Experiments showed that better segmentation results in better IR performances, and a combination of words with uni-grams is the better choice to index Chinese texts for IR.

Nowadays, Web-based information systems, such as web feeds or enterprise microblogs produce a seemingly continuous and endless stream of messages.
Unfortunately, especially information workers currently experience an information overload.
Thus, system support is required that enables a reduction of information load based on an automatic preprocessing of these streams.
This paper presents an innovative approach to author IIR (Internet Information Retrieval) processing chains applicable for these stream-based business information systems.
It is based on a novel system architecture named Spectre for realising highly scalable systems.
Using a dedicated authoring tool, concrete systems can be developed efficiently and adapted to specific requirements.
The solution has been validated using a prototypical implementation within a concrete business information system.

This article presents D TSR, an image content representation describing the spatial layout with triangular relationships of visual entities, which can be symbolic objects or low-level visual features.
A semi-local implementation of D TSR is also proposed, making the description robust to viewpoint changes.
We evaluate D TSR for image retrieval under the query-by-example paradigm, on contents represented with interest points in a bag-of-features model: it improves state-of-the-art techniques, in terms of retrieval quality as well as of execution time, and is scalable.
Finally, its effectiveness is evaluated on a topical scenario dedicated to scene retrieval in datasets of city landmarks 2010 Elsevier Ltd.
All rights reserved.

Information Retrieval (IR) systems try to identify documents relevant to user queries, which are representations of user information needs.
Interaction, context, and document structure are three important and active themes in IR research.
We present how we propose to model the task of Structured IR (SIR) based on a QT inspired framework, with a focus on how to exploit user contextual information and user interaction in the search process.

When processing raw documents in Information Retrieval (IR) System, a <i>term-weighting scheme</i> is used to calculate the importance of each term which occurs in a document.
However, most <i>term-weighting schemes</i> assume that a term is independent of the other terms.
Term dependency is an indispensable consequence of language use [1
Therefore, this assumption can make the information of a document being lost.
In this paper, we propose new approach to refine term weights of documents using term dependencies discovered from a set of documents.
Then, we evaluate our method with two experiments based on the vector space model [2] and the language model [3].

The article presents a method of constructing and applying a cascade consisting of a left-and a right-sequential nite-state transducer, T1 and T2, for part-of-speech disambiguation.
In the process of POS tagging, every word is rst assigned a unique ambiguity class that represents the set of alternative tags that this word can occur with.
The sequence of the ambiguity classes of all words of one sentence is then mapped by T1 to a sequence of reduced ambiguity classes where some of the less likely tags are removed.
That sequence is nally mapped by T2 to a sequence of single tags.
Compared to a Hidden Markov model tagger, this transducer cascade has the advantage of signiicantly higher processing speed, but at the cost of slightly lower accuracy.
Applications such as Information Retrieval, where the speed can be more important than accuracy, could beneet from this approach.

The World Wide Web (WWW) is a powerful way to deliver on-line health information, but one major problem limits its value to consumers: content is highly distributed, while relevant and high quality information is often difficult to find.
To address this issue, we experimented with an approach that utilizes threedimensional anatomic models in conjunction with

Hongfeng Wang Shengxiang Yang, W.H. Ip and Dingwei Wang College of Information Science and Engineering, Northeastern University, Shenyang 110004, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang 110004, China; Department of Information Systems and Computing, Brunel University, Uxbridge, Middlesex UB8 3PH, UK; College of Mathematics and Physics, Nanjing University of Information Science and Technology, Nanjing 210044, China; Department of Industrial and Systems Engineering, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong

The main challenge for effective web Information Retrieval (IR) is to infer the information need from user’s query and retrieve relevant documents.
The precision of search results is low due to vague and imprecise user queries and hence could not retrieve sufficient relevant documents.
Fuzzy set based query expansion deals with imprecise and vague queries for inferring user’s information need.
Trust based web page recommendations retrieve search results according to the user’s information need.
In this paper an algorithm is designed for Intelligent Information Retrieval using hybrid of Fuzzy set and Trust in web query session mining to perform Fuzzy query expansion for inferring user’s information need and trust is used for recommendation of web pages according to the user’s information need.
Experiment was performed on the data set collected in domains Academics, Entertainment and Sports and search results confirm the improvement of precision.

This paper presents a system for retrieving information from a domain specific document collection made up of data rich unnatural language text documents.
Instead of conventional keyword based retrieval, our system makes use of domain ontology to retrieve the information from a collection of documents.
The system addresses the problem of representing unnatural language text documents and constructing a classifier model that helps in the efficient retrieval of relevant information.
Query to this system may be either the key phrases in terms of concepts or a domain specific unnatural language text document.
The classifier used in this system can also be used to assign multiple labels to the previously unseen text document belonging to the same domain.
An empirical evaluation of the system is conducted on the domain of text documents describing the classified matrimonial advertisements to determine its performance.
Ontology, Information Retrieval, Unnatural Language Text Documents

The paper proposes a technical method for the identification of ragas in an Indian classical harmonium recital.
Swaras are musical notes which are produced by pressing any key of the harmonium (an instrument analogous to the piano Ragas are melodic combinations of swaras which capture different moods and emotions.
Raga analysis finds its applications in Music Information Retrieval (MIR) purposes which can help in classification of music pieces from the huge music database.
This paper focuses on the technical ways of identification of a raga applying signal processing approaches.
The aspects which we deal with here are dataset creation of ragas, pitch detection algorithm, steady state detection for note extraction, tonic extraction from drone, octave adjustments, automatic note transcription and matching input with data set.

Tao Fan Changzhong Chen, Fei Tan, Ping He 1 School of Automation and Electronic Information, Sichuan University of Science Engineering, Zigong, Sichuan, 643000
, People’s Republic of China 2 Artificial Intelligence Key Laboratory of Sichuan Province, Sichuan University of Science Engineering, Zigong, Sichuan, 643000
, People’s Republic of China
3 School of Information Science Engineering, Northeastern University, Shenyang, Liaoning, 110819, People’s Republic of China
(Received 24 March 2014, accepted 12 June 2014)

How to assign appropriate weights to terms is one of the critical issues in information retrieval.
Many term weighting schemes are unsupervised.
They are either based on the empirical observation in information retrieval, or based on generative approaches for language modeling.
As a result, the existing term weighting schemes are usually insufficient in distinguishing informative words from the uninformative ones, which is crucial to the performance of information retrieval.
In this paper, we present supervised term weighting schemes that automatically learn term weights based on the correlation between word frequency and category information of documents.
Empirical studies with the ImageCLEF dataset have indicated that the proposed methods perform substantially better than the state-of-the-art approaches for term weighting and other alternatives that exploit category information for information retrieval.

This paper describes the development and computational testing of a model of the information intermediary based on an AI theory of belief revision.
We describe the theoretical foundations of the work in a general account of the way an agent’s beliefs and intentions are formed and modified, and in an analysis of the functional tasks an intermediary has to carry out; we indicate the specific developments required to automate and integrate both aspects of intermediary behaviour, as determinants of interact ive dialogue with the user; and report, with illustrations, on tests and findings.
The research shows that such approaches can be implemented in an essentially principled manner, though there are many large problems still to be overcome, and our experiments are only the first, extremely simple, trials of the basic strategy for intermediary simulation.

The clinical documents stored in a textual and unstructured manner represent a precious source of information that can be gathered by exploiting Information Retrieval techniques.
Classification algorithms, and their composition through Ensemble Methods, can be used for organizing this huge amount of data, but are usually tested on standardized corpora, which significantly differ from actual clinical documents that can be found in a modern hospital.
In this paper we present the results of a large experimental analysis conducted on 36,000 clinical documents, generated by three different medical Departments.
For the sake of this investigation we propose a new classifier, based on the entropy idea, and test four single algorithms and four ensemble methods.
The experimental results show the performance of selected approaches in a real-world environment, and highlights the impact of obsolescence on classification.

The paper describes a named entity recognition system for Amharic, an under-resourced language, using a recurrent neural network, a bi-directional long short term memory model to identify and classify tokens into six predefined classes: Person, Location, Organization, Time, Title, and Other (non-named entity
tokens Word vectors based on semantic information are built for all tokens using an unsupervised learning algorithm, word2vec.
The word vectors were merged with a set of specifically developed language independent features and together fed to the neural network model to predict the classes of the words.
When evaluated by 10-fold cross-validation, the created Amharic named entity recogniser achieved good average precision (77.2 but did worse on recall (63.4 for a 69.7% F1-score.

Recall-oriented information retrieval requires locating as many documents as possible that are relevant to a query.
Traditional information retrieval systems present results only in a ranked list.
Inspired by the cluster hypothesis, we present a novel 3-D visualization tool which aids recalloriented retrieval.
The visualization portrays clustered documents and concepts using a modified spring embedder.
A small user study suggests this approach has merit.

To cite this article: R. Ahas, A. Aasa, Y. Yuan, M. Raubal, Z. Smoreda, Y. Liu, C. Ziemlicki, M. Tiru M. Zook (2015)
Everyday space–time geographies: using mobile phone-based sensor data to monitor urban activity in Harbin, Paris, and Tallinn, International Journal of Geographical Information Science, 29:11, 2017-2039,
10.1080/13658816.2015.1063151
To link to this article: http dx.doi.org/10.1080/13658816.2015.1063151

Agents provide a flexible and scalable method of integrating artificial intelligence techniques on a single cohesive distributed computing system.
We have designed and implemented an agent-based interface for autonomous control, and for providing web-based information retrieval, for a dynamically autonomous mobile robot.
The robot implements and integrates a variety of artificial intelligence techniques including a multimodal interface that allows natural language understanding, gesture interpretation, simultaneous localization and mapbuilding, object identification and spatial reasoning.
The agent-based interface augments these capabilities by providing a method of controlling the robot via the CoABS grid, and by providing the means for the robot/operator to request information available through the grid or through the web.

In this paper we propose features desirable of linear text segmentation algorithms for the Information Retrieval domain, with emphasis on improving high similarity search of heterogeneous texts.
We proceed to describe a robust purely statistical method, based on context overlap exploitation, that exhibits these desired features.
Ways to automatically determine its internal parameter of latent space dimensionality are discussed and evaluated on a data set.

In the information science and technology such as computer science, telecommunications, processing of the signals or images transmission, the field effect components plays a major role.
We are interested in this study in Schottky gate gallium arsenide field-effect transistors commonly called GaAs MESFET.
In this paper, we mainly present the results of calculating the influence of gate length on input and output impedances of GaAs MESFET Transistors, this physical model is based on the analysis of two-dimensional Poisson equation in the active region under the gate.
The theoretical results, based on analytical expressions that we have established, are discussed and compared with those of the simulation.

The phenomena of semantic transitivity flood in many aspects of information science, especially in knowledge organization, measurement of semantic relatedness as well as semantic inference and semantic mining.
However because of the neglect by scholars initially and complex, fuzzy traits of itself, there is little driving, systematic research on semantic transitivity.
For providing a theoretical basis for computer simulation and other applications of semantic transitivity in information science, a theoretical system of semantic transitivity has been established, consisting of the definition, the influencing factors, the judging process and the rules of semantic transitivity.

The Robust-WSD at CLEF 2009 aims at exploring the contribution of Word Sense Disambiguation to monolingual and multilingual Information Retrieval.
The organizers of the task provide documents and topics which have been automatically tagged with Word Senses from WordNet using several state-of-the-art Word Sense Disambiguation systems.
The Robust-WSD exercise follows the same design as in 2008.
It uses two languages often used in previous CLEF campaigns (English, Spanish Documents were in English, and topics in both English and Spanish.
The document collections are based on the widely used LA94 and GH95 news collections.
All instructions and datasets required to replicate the experiment are available from the organizers website (http ixa2.si.ehu.es/clirwsd The results show that some top-scoring systems improve their IR and CLIR results with the use of WSD tags, but the best scoring runs do not use WSD.

Here, formation of continuous attractor dynamics in a nonlinear recurrent neural network is used to achieve a nonlinear speech denoising method, in order to implement robust phoneme recognition and information retrieval.
Formation of attractor dynamics in recurrent neural network is first carried out by training the clean speech subspace as the continuous attractor.
Then, it is used to recognize noisy feedforward network is compared to the same one with a recurrent connection in its hidden layer.
The structure and training of this recurrent connection, is designed in such a way that the network learns to denoise the signal step by step, using properties of attractors it has formed, along with phone recognition.
Using these connections, the recognition accuracy is improved 21% for the stationary signal and 14% for the nonstationary one with 0db SNR, in respect to a reference model which is a feedforward neural network 2011 Elsevier B.V.
All rights reserved.

This is the second participation of Institute of Statistical Studies and Research (ISSR) group in CLEF 2010-Medical image retrieval track.
This paper describes our experiments in monolingual and multilingual tasks.
First, we test Paragraph Extraction (PE) and Sentence Selection (SS) approaches on the classical medical retrieval task (Ad-hoc as well as on Case-based retrieval.
Second, we compare between three Cross Language Information Retrieval (CLIR) methods.
These methods are Machine Translation (MT dictionary translation as well as translating via thesauri.
For indexing and retrieval, we used the Lemur toolkit.
Regarding ad-hoc retrieval task best results obtained when image caption and title used only, and for case-based task, there is no significant difference between adding extra text to the article and using its title and its image captions.
For multilingual task, there is no significant difference between the three methods.

This paper presents a dictionary-based query translation and construction method for CLIR.
The framework focuses on solving morphological and lexical problems in CLIR generally between many language pairs.
An application based on this method, the extendable UTACLIR system, capable of performing query translations between several source and target language pairs, with a relatively easy interface to change and add new external resources like morphological normalisers, dictionaries and stop word lists will be available free to academic users.

The traditional method of presenting a document in an Information Retrieval(IR) system is based on terms.
As the the new words appear dramatically in the Internet era, this kind of method draws back the IR system's performance.
This paper puts forward an approach by using the concepts of the ontology to present the documents.
Constructing the Word-Concept(W-C) model and Concept-Document(C-D) model, we compute the relevance of word-word word-concept and concept-document.
They are used to determine which page is most relevant to the query.
It is proofed to be more effective than prevenient.

One of the key challenges of managing very huge volumes of data in scalable Information retrieval systems is providing fast access through keyword searches.
The major data structure in the information retrieval system is an inverted file, which records the positions of each term in the documents.
When the information set substantially grows, the number of terms and documents are significantly increased as well as the size of the inverted files.
In this research we implement to techniques of inverted file -posting list and Tries treeson Arabic Language, then we will try to state the optimum technique to store the vocabulary according to the speed of retrieving queries from the collection of documents.

Usually the focus of evaluation within Information Retrieval has been placed largely upon the system.
However, the individual user and their submitted queries are typically the greatest source of variation in the search process.
This demonstration paper presents Fu-Finder, a fun and enjoyable game that measures the user's querying abilities (or search-fu
This game provides useful data for the study of user querying behaviour and assesses how well users can find specific web pages using different search engines.

Users of information filtering systems cannot be expected to provide large amounts of information to initialize a profile.
Therefore, term weighting methods for information filtering have somewhat different requirements to those for information retrieval and text categorization.
We present a comparative evaluation of term weighting methods, including a new method, relative document frequency, designed specifically for information filtering.
The best weighting methods appear to be those that favor information provided by the user, over information from a general collection.

Term weighting schemes are central to the study of information retrieval systems.
This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two different aspects of term saliency.
One component of the term frequency is effective for short queries, while the other performs better on long queries.
The final weight is then measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency.
The experimental results also show that the proposed model achieves significantly better precision than the existing models.

A model for optimal information retrieval over a distributed document collection is described and experimentally evaluated.
The fusion of retrieval results corresponding to document subcollections is performed according to the Probability Ranking Principle.
Part of the model is a selection criterion for e ectively limiting the ranking process to a subset of subcollections.

The course of Communication Circuits plays an important role in information science and electronic technology education field.
The course in Tsinghua University has been approved as the national excellent course through many years of continual construction.
The development of the course is introduced and the reforms of the course are explored in this paper.
Utilizing and combining various advanced knowledge resources, the content construction and characteristics of the course is reviewed with new system structure established.

This half-day tutorial on IR evaluation combines an introduction to classical IR evaluation methods with material on more recent user-oriented approaches.
We primarily focus on off-line evaluation, but some material on on-line evaluation is also covered.
The broad goal of the tutorial is to equip researchers with an understanding of modern approaches to IR evaluation, facilitating new research on this topic and improving evaluation methodology for emerging areas.

In the spirit of David Hilbert’s 1900 lecture, it would seem appropriate at the commencement of the 21 century to highlight some research areas, challenges, problems and possible directions that will be taken up by spatial scientists in the coming decades.
The paper covers areas such as spatio-temporal data, data management, data models and representation, intelligent spatial systems, cognition, culture, education, mobile delivery and the world wide web.
The paper does not attempt to describe how these changes will come about, nor does it go into detail with any one idea or challenge.
The purpose here is to highlight the variety and diversity of fields that are part of the domain of spatial information science, and offer some thoughts on some current challenges.

We present nondeterministic hypotheses learned from an ordinal regression task.
They try to predict the true rank for an entry, but when the classification is uncertain the hypotheses predict a set of consecutive ranks (an interval The aim is to keep the set of ranks as small as possible, while still containing the true rank.
The justification for learning such a hypothesis is based on a real world problem arisen in breeding beef cattle.
After defining a family of loss functions inspired in Information Retrieval, we derive an algorithm for minimizing them.
The algorithm is based on posterior probabilities of ranks given an entry.
A couple of implementations are compared: one based on a multiclass SVM and other based on Gaussian processes designed to minimize the linear loss in ordinal regression tasks.

Electronic discharge letters are in many cases insufficient as a recipe for further treatment and recovery.
Through using discharge letters as input to our neural network based information retrieval system Pasent, we have been able to provide relevant medical information.
The Pasent search method uses predefined knowledge about the context, paired with the vocabulary of the input document, to compute a relevance measure for a potential result document.
In the reported experiments, we achieved search results comparable to the tftdf method without building specialised context models for the experiment.

In recent years biology has become an information science, where an avalanche of newly sequenced genomic data has overwhelmed our existing analysis and mining tools.
This paper addresses this challenge by developing a systematic way of speeding up a broad class of bioinformatics algorithms using commodity graphics processing hardware.
Using the example problem of analyzing DNA structural variations, we demonstrate how such computations can be significantly accelerated in various parallel architectures, yielding over two orders of magnitude speedups at low cost and with relatively modest programming effort.
Our implementation of a sliding window -based technique on the GPU and Cell architectures seems promising in its generality and extensibility to other problems and domains.

We examine the use of authorship information in information retrieval for closed communities by extracting expert rankings for queries.
We demonstrate that these rankings can be used to re-rank baseline search results and improve performance significantly.
We also perform experiments in which we base expertise ratings only on first authors or on all except the final authors, and find that these limitations do not further improve our re-ranking method.

Researches in the field of Named Entity recognition and alignment are of strong interest for various applications of natural language processing, such as Cross Lingual Information Retrieval, document management, question-answering systems, data mining etc.
But in the processing of Arabic language, the task is particularly difficult and few resources are available to cope with these

Diversity in a recommendation list has been recognized as one of the key factors to increase user’s satisfaction when interacting with a recommender system.
Analogously to the modelling and exploitation of query intent in Information Retrieval adopted to improve diversity in search results, in this paper we focus on eliciting and using the profile of a user which is in turn exploited to represent her intents.
The model is based on regression trees and is used to improve personalized diversification of the recommendation list in a multi-attribute setting.
We tested the proposed approach and showed its effectiveness in two different domains, i.e. books and movies.

Prior efforts have shown that data fusion techniques can be used to improve retrieval effectiveness under certain situations.
Although the precise conditions necessary for fusion to improve retrieval have not been identified, it is widely believed that as long as component result sets used in fusion have higher relevant overlap than non-relevant overlap, improvements due to fusion can be observed.
We show that this is not the case when systemic differences are held constant and different highly effective document retrieval strategies are fused within the same information retrieval system.
Furthermore, our experiments have shown that the ratio of relevant to non-relevant overlap is a poor indicator of the likelihood of fusion’s effectiveness, and we propose an alternate hypothesis of what needs to happen in order for fusion to improve retrieval when standard voting/merging algorithms such as CombMNZ are employed.

A database which provides information about bacteria and their habitats in a comprehensive and normalized way is crucial for applied microbiology studies.
Having this information spread through textual resources such as scientific articles and web pages leads to a need for automatically detecting bacteria and habitat entities in text, semantically tagging them using ontologies, and finally extracting the events among them.
These are the challenges set forth by the Bacteria Biotopes Task of the BioNLP Shared Task 2016.
This paper describes a system for habitat and bacteria entity normalization through the OntoBiotope ontology and the NCBI taxonomy, respectively.
The system, which obtained promising results on the shared task data set, utilizes basic information retrieval techniques.

MATT2 is a content based music information retrieval system adapted to the characteristics of traditional Irish dance music.
MATT2 compensates for expressive artefacts commonly employed by traditional musicians.
Specifically these are ornamentation the long note reversing and phrasing.
In this paper we describe the main components of MATT2 and present an experiment where we demonstrate that using this higher level knowledge of melodic similarity in traditional Irish dance music results in a significant improvement in annotation accuracy over standard approaches from the MIR literature.

The 10th European Summer School in Information Retrieval (ESSIR 2015) was held in Thessaloniki, Greece between August 31 and September 4, 2015.
The summer school offered high quality lectures on 13 topics in Information Retrieval and related areas, a new edition of the Symposium on Future Directions in Information Access (FDIA group activities, and a rich social programme.
This report provides an overview of a successful international summer school that attracted a total of 51 participants from a wide range of countries in Europe and North America.

The Integrated Information System for Power Planning Studies
-SIIPEP- is an interactive Data Base that automatically Prepares information for Power System Planning models, reducing manual data handling when updating certain Parameters that are representative of a studs case.
Information retrieval from SIIPEP to user application is realized in a simple was by a user interface that avoids modifications to existing models.
This interface is supported by SIIPEP functions.
This Paper describes the results and experiences of the first stage of its design and Presents the theoretical basis for the second stage.

Similarity is a widely used concept, with utilizations in various fields, such as pattern recognition, case-based reasoning, image processing, approximate reasoning, machine learning, information retrieval for instance.
There exist many definitions of similarity or resemblance, or conversely, dissimilarity or distance.
In the case where the objects to be compared are not defined on a metrical universe, distances are clearly not appropriate.
Several types of similarity are nevertheless available [1 which have been connected with seminal works in psychology [6].

Information Retrieval (IR) is presented as the task of retrieving the documents that are relevant to a given query.
In the context of a Terminological Logic (TL) based approach to IR, this amounts to embodying a notion of relevance in the logical implication relation of the chosen TL.
Among the many possible readings of the term “relevance the one captured by relevance logic, and in particular by first-order tautological entailment, can be viewed as a promising source of inspiration, to the end of incorporating a logic-based form of relevance in the inference mechanism of TLs.
The aim of this paper is to present a Relevant Terminological Logic, while maintaining the desired “relevance” flavour of relevance logics, which could be considered as a base towards a suitable document description logic for IR purposes.

This paper enhances some of the main aspects of a system which converses in Portuguese to provide a library service covering the field of Artificial Intelligence.
Its central feature is the use of logic as a single uniform language for knowledge and data representation, deductive information retrieval and linguistic analysis.
The objective of designing this system was the development of a feasible method for consulting and creating data bases in natural Portuguese.
The system is implemented in Prolog, a programming language essentially identical in syntax and semantics to a subset of predicate calculus in clausal form.

Disclaimer/Complaints regulations
If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons.
In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website.
Please Ask the Library: http uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands.
You will be contacted as soon as possible.

Cross Lingual Word Semantic (CLWS) similarity is defined as a task to find the semantic similarity between two words across languages.
Semantic similarity has been very popular in computing the similarity between two words in same language.
CLWS similarity will prove to be very effective in the area of Cross Lingual Information Retrieval, Machine Translation, Cross Lingual Word Sense Disambiguation, etc.
In this paper, we discuss a system that is developed to compute CLWS similarity of words between two languages, where one language is treated as resourceful and other is resource scarce.
The system is developed using WordNet.
The intuition behind this system is that, two words are semantically similar if their senses are similar to each other.
The system is tested for English and Hindi with the accuracy 60.5% precision@1 and 72.91% precision@3.

The current grid information retrieval commonly uses the method based on keywords, and the retrieval method depends on the keywords that match their participation in the external form of characters, rather than the concept expressed by their neglect of the semantic information inherent in the word.
It brings about varieties of problems, including too much information or false drop, etc.
Thus, the prevalence of low searching efficiency existed and accuracy of the phenomenon is not high.
Ontology technology which is combined with Semantic Grid platform would provide a new implement method to the intelligent information retrieval.
Based on this, this paper constructs the grid information retrieval model to achieve a higher degree of matching between user's requests and the retrieval content, thereby, increasing the recall ratio and precision ratio.

Ontology provides a shared and reusable piece of knowledge about a specific domain, and has been applied in many fields, such as semantic Web, e-commerce and information retrieval, etc.
However, building ontology by hand is a very hard and error-prone task.
Learning ontology from existing resources is a good solution.
Because relational database is widely used for storing data and OWL is the latest standard recommended by W3C, this paper proposes an approach of learning OWL ontology from data in relational database.
Compared with existing methods, the approach can acquire ontology from relational database automatically by using a group of learning rules instead of using a middle model.
In addition, it can obtain OWL ontology, including the classes, properties, properties characteristics, cardinality and instances, while none of existing methods can acquire all of them.
The proposed learning rules have been proven to be correct by practice.

DEPENDENCE OF THE ROLL ANGULAR VESTIBULO-OCULAR REFLEX (aVOR) ON 2 GRAVITY 3 4 Sergei.
B. Yakushin, Yongqing Xiang, Bernard Cohen and Theodore Raphan 5 6 From the Departments of Neurology, Mount Sinai School of Medicine, New York, New York,
7 10029, USA, the Department of Computer and Information Science, Brooklyn College of the City 8 University of New York, Brooklyn, New York 11210
9 10 Running Title:
DEPENDENCE OF THE ROLL aVOR ON GRAVITY 11 12 13
Corresponding Author:
14 15 Sergei B. Yakushin, PhD 16 Department of Neurology, Box 1135
17 Mount Sinai School of Medicine 18 1 East 100th Street 19 New York, NY 10029-6574 20 Phone:
212 241 7068 21 Fax: 212 831 1610 22 Email: sergei.yakushin@mssm.edu 23 Articles in PresS. J Neurophysiol (August 19, 2009 doi:10.1152/jn.00245.2009

This paper describes the oÆcial runs of the Twenty-One group for TREC-8.
The Twenty-One group participated in the Ad-hoc, CLIR, Adaptive Filtering and SDR tracks.
The main focus of our experiments is the development and evaluation of retrieval methods that are motivated by natural language processing techniques.
The following new techniques are introduced in this paper.
In the Ad-Hoc and CLIR tasks we experimented with automatic sense disambiguation followed by query expansion or translation.
We used a combination of thesaurial and corpus information for the disambiguation process.
We continued research on CLIR techniques which exploit the target corpus for an implicit disambiguation, by importing the translation probabilities into the probabilistic term-weighting framework.
In ltering we extended the the use of language models for document ranking with a relevance feedback algorithm for query term reweighting.

Performance prediction is an appealing problem in Recommender Systems, as it enables an array of strategies for deciding when to deliver or hold back recommendations based on their foreseen accuracy.
The problem, however, has been barely addressed explicitly in the area.
In this paper, we propose adaptations of query clarity techniques from ad-hoc Information Retrieval to define performance predictors in the context of Recommender Systems, which we refer to as user clarity.
Our experiments show positive results with different user clarity models in terms of the correlation with single recommender‟s performance.
Empiric results show significant dependency between this correlation and the recommendation method at hand, as well as competitive results in terms of average correlation.

This paper describes work in progress on a workbench for symbolic music information retrieval (MIR Drawn from three broad techniques used to perform symbolic retrieval—state-based matching, dynamic programming, and text IR
the workbench implements several variations based on these techniques.
To perform an experiment, the user can either interact with the workbench though command line options, or develop scripts that batch process the commands.
The latter is preferable when running a series of experiments.
The workbench is also designed so it can be embedded into a digital music library to provide content-based querying.
Development of a graphical user interface is planned for the near future.

In this paper, we focus on one of the most challenging tasks in temporal information retrieval: detection of a web page <i>publication date i>
The natural approach to this problem is to find the publication date in the HTML body of a page.
However, there are two fundamental problems with this approach.
First, not all web pages contain the publication dates in their texts.
Second, it is hard to distinguish the publication date among all the dates found in the page's text.
The approach we suggest in this paper supplements methods of date extraction from the page's text with novel link-based methods of dating.
Some of our link-based methods are based on a probabilistic model of the Web graph structure evolution, which relies on the publication dates of web pages as on its parameters.
We use this model to estimate the publication dates of web pages: based only on the link structure currently observed, we perform a reverse engineering to reveal the whole process of the Web's evolution.

Today&#8217;s Web provides many different functionalities, including communication, entertainment, social networking, and information retrieval.
In this article, we analyze traces of HTTP activity from a large enterprise and from a large university to identify and characterize Web-based service usage.
Our work provides an initial methodology for the analysis of Web-based services.
While it is nontrivial to identify the classes, instances, and providers for each transaction, our results show that most of the traffic comes from a small subset of providers, which can be classified manually.
Furthermore, we assess both qualitatively and quantitatively how the Web has evolved over the past decade, and discuss the implications of these changes.

The quantity of biomedical publications is growing at an exponential rate.
With such explosive growth of the content, it is more and more difficult to locate, retrieve and manage the resulting information.
This is why text mining has become a necessity.
The main goal of biomedical research is to put knowledge to practical use in the form of diagnoses, prevention, and treatment.
It is important to pool the resources between the different individuals researching results.
The objective of this paper is to discuss the variety of issues and challenges surrounding the perspectives regarding the use of Information Retrieval and Text Mining methods in biomedicine.
The article will first look at the directions in biomedical TM and then describe the work done for the BIAM project, the French on-line Medical Data Base.

Information Retrieval (IR) aims at solving a ranking problem: given a query q and a corpus C, the documents of C should be ranked such that the documents relevant to q appear above the others.
This task is generally performed by ranking the documents d C according to their similarity with respect to q, sim(q, d The identification of an effective function a, b sim(a, b) could be performed using a large set of queries with their corresponding relevance assessments.
However, such data are especially expensive to label, thus, as an alternative, we propose to rely on hyperlink data which convey analogous semantic relationships.
We then empirically show that a measure sim inferred from hyperlinked documents can actually outperform the state-of-the-art Okapi approach, when applied over a non-hyperlinked retrieval corpus.

There is a renewed interest in word sense disambiguation (WSD) as it contributes to various applications in natural language processing.
Applications for which WSD is potentially an issue are: Machine Translation, Information Retrieval (IR QA systems, Dialogue systems,etc.
In this paper we survey vector-based methods for WSD in machine learning approache.

This paper presents and discusses the results produced by the KEPLER system for the 2017 Ontology Alignment Evaluation Initiative (OAEI 2017
This method is based on the exploitation of three different strategy levels.
The proposed alignment method KEPLER is enhanced by the integration of powerful treatments inherited from other related domains, such as Information Retrieval (IR 1 For scaling, the method is equipped with a partitioning module.
For the management of multilingualism, the KEPLER method develops a well-defined strategy based on the use of a translator, and this provides very encouraging re-

A Grid Information Retrieval (GIR) simulation was used to process the TREC Million Query Track queries.
The GOV2 collection was partitioned by hostname and the aggregate performance of each host, as measured by qrel counts from the past TREC Terabyte Tracks, was used to rank the hosts in order of quality.
Only the 100 highest quality hosts were included in the Grid IR simulation, representing less than 20% of all GOV2 documents.
The IR performance of the GIR simulation, as measured by the topic-averaged AP, b-pref, and Rel@10 over the TREC Terabyte-Track topics is within one standard deviation of the respective topic-averaged TREC Million Query participant median scores.
Estimated AP of the Million Query topic results is comparable to the topic-averaged AP of the Terabyte topic results.

Any IR system is trying to a n s w e r or aims at answering m a user's request by way of retrieval, which is implemented using algorithms based on some mathematical model of the main entities involved
(objects to be searched, user's request, relevance to query, and so on The mathematical methods used so far in retrieval modelling rely on vector space theory, probability theory, classical set theory, Boolean logic, fuzzy set theory, topology, algebra, matroid theory, recursion theory, rough sets, decision theory.
Beside the mathematical methods, different alternative ideas, methods and techniques (e.g. knowledge base, artificial neural networks, etc help improve retrieval.

The fast development of Web technologies has introduced a world of big data.
How efficiently and effectively to retrieve the information from the ocean of data that the users really want is an important topic.
Recommendation systems have become a popular approach to personalized information retrieval.
On the other hand, social media have quickly entered into your life.
The information from social networks can be an effective indicator for recommender systems.
In this paper we present a recommendation mechanism which calculates similarity among users and users' trustability and analyzes information collected from social networks.
To validate our method an information system for tourist attractions built on this recommender system has been presented.
We further evaluate our system by experiments.
The results show our method is feasible and effective.

For bridging the gap between information retrieval (IR) and databases (DB this article focuses on the logical view.
We claim that IR should adopt three major concepts from DB, namely inference, vague predicates and expressive query languages.
By regarding IR as uncertain inference, probabilistic versions of relational algebra and Datalog yield very powerful inference mechanisms for IR as well as allowing for more flexible systems.
For dealing with various media and data types, vague predicates form a natural extension of text retrieval methods to attribute values, thus switching from propositional to predicate logic.
A more expressive IR query language should support joins, be able to compute aggregated results, and allow for restructuring of the result objects.

This paper discusses research on distinguishing word meanings in the context of information retrieval systems.
We conducted experiments with three sources of evidence for making these distinctions: morphology, part-of-speech, and phrases.
We have focused on the distinction between homonymy and polysemy (unrelated vs. related meanings Our results support the need to distinguish homonymy and p o l y semy.
We found: 1) grouping morphological variants makes a significant improvement in retrieval performance, 2) that more than half of all words in a dictionary that differ in part-of-speech are related in meaning, and 3) that it is crucial to assign credit to the component words of a phrase.
These experiments provide a better understanding of word-based methods, and suggest where natural language processing can provide further improvements in retrieval perform-

This paper presents a vector space model approach, for representing documents and queries, using concepts instead of terms and WordNet as a light ontology.
This way, information overlap is reduced with respect to the classic semantic expansion techniques.
Experiments undertaken on MuchMore benchmark showed the effectiveness of the approach.

We discuss ways in which EuroWordNet (EWN) can be used in multilingual information retrieval activities, focusing on two approaches to Cross-Language Text Retrieval that use the EWN database as a large-scale multilingual semantic resource.
The first approach indexes documents and queries in terms of the EuroWordNet Inter-Lingual-Index, thus turning term weighting and query/document matching into language-independent tasks.
The second describes how the information in the EWN database could be integrated with a corpus-based technique, thus allowing retrieval of domain-specific terms that may not be present in our multilingual database.
Our objective is to show the potential of EuroWordNet as a promising alternative to existing approaches to Cross-Language Text Retrieval.
Abbreviations:
CLTR Cross-Language Text Retrieval; EWN EuroWordNet; ILI Inter-LingualIndex; IR Information Retrieval; NLP Natural Language Processing; POS Part of Speech; WSD Word Sense Disambiguation

Retrieving information from heterogeneous database systems involves a complex process and remains a challenging research area.
We propose a cognitively-guided approach for developing an information retrieval agent that takes the user’s information request, identifies relevant information sources, and generates a multidatabase access plan.
Our work is distinctive in that agent design is based on an empirical study of how human experts retrieve information from multiple, heterogeneous database systems.
To improve on empirically observed information retrieval capabilities, the design incorporates mathematical models and algorithmic components.
These components optimize the set of information sources that need to be considered to respond to a user query and are used to develop efficient multidatabase access plans.
This agent design which integrates cognitive and mathematical models has been implemented using the Soar architecture.

Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics.
The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data.
In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings.
Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel.
The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings.
Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka.

This paper describes our work at the fifth NTCIR workshop on the subtasks of monolingual information retrieval (IR Query expansions using automatically acquired related term groups were explored.
Unlike traditional query expansion methods, the related term groups extracted from web-based corpuses and the related terms extracted from document set are used in combination to improve the effectiveness of query expansion in our method.
Experiments show that our method achieves an average 13.1% improvement compare to the traditional relevance feedback technique.

A fuzzy Bayesian approach helping the Internet user to filter Web pages is discussed.
In the proposed approach, one page can be classified as having the continuous quality of being interesting, this means that a certain grade of membership can be associated with each page relative to a category of selection.
Filtering is based on the evidences of the content of the page title, abstract or complete document.
An example comparing crisp and fuzzy classifiers implemented as a part of multi-agent system to support information filtering and retrieval in the Web is discussed illustrating the proposed approach.

This paper presents the results of task 3 of the ShARe/CLEF eHealth Evaluation Lab 2014.
This evaluation lab focuses on improving access to medical information on the web.
The task objective was to investigate the effect of using additional information such as a related discharge summary and external resources such as medical ontologies on the effectiveness of information retrieval systems, in a monolingual (Task 3a) and in a multilingual (Task 3b) context.
The participants were allowed to submit up to seven runs for each language (English, Czech, French, German one mandatory run using no additional information or external resources, and three each using or not using discharge summaries.

Digital libraries are huge and complex information systems.
Digital library users are from various backgrounds with diversified information requirements.
Digital library users are fed up with the information overload problem due its unsophisticated search features.
Digital libraries can be effectively implemented by addressing the information overload problem.
Hence, this paper presents newly designed personalized information retrieval services architecture and its implementation details to enhance the digital library users&apos; search experience.

The growth of digital information increases the need to build better techniques for automatically storing, organizing and retrieving it.
Much of this information is textual in nature and existing representation models struggle to deal with the high dimensionality of the resulting feature space.
Techniques like latent semantic indexing address, to some degree, the problem of high dimensionality in information retrieval.
However, promising alternatives, like random mapping (RM have yet to be completely studied in this context.
In this paper, we show that despite the attention RM has received in other applications, in the case of text retrieval it is outperformed not only by principal component analysis (PCA) and independent component analysis (ICA) but also by a simple noise reduction algorithm.

Web services are accessed via query interfaces which hide databases containing thousands of relevant information.
User’s side, distant database is a black box which accepts query and returns results, there is no way to access database schema which reflect data and query meanings.
Hence, web services are very autonomous.
Users view this autonomy as a major drawback because they need often to combine query capabilities of many web services at the same time.
In this work, we will present a new approach which allows users to benefit of query capabilities of many web services while respecting autonomy of each service.
This solution is a new contribution in Information Retrieval research axe and has proven good performances on two standard datasets.
KeywordsInformation Retrieval Model of Query Representation; Query Extraction

This paper reports the work of NTU in the bilingual-retrieval task at CLEF 2001.
In this experiment, we compared the effectiveness of several approaches in Chinese-English cross-language information retrieval.
Five models were proposed.
Model 1 used co-occurrence information in the target language to disambiguate translation equivalents; Model 2 augmented restriction terms to the original queries to restrict the use of query terms in the target language; Model 3 used a Chinese-English WordNet to translate queries;
Model 4 combined Model 3 with Model 2; Model 5 merged the queries constructed by Model 2 and 3.

This report describes the 2011 Workshop on Human-Computer Interaction and Information Retrieval.
Now in its fifth year, the workshop was held in October, in Mountain View, CA.
The event brings together researchers from academia, industry, and government and a range of disciplines for in-depth discussions in an informal atmosphere.
The workshop continues to grow, with around 100 attendees this year.
We continued the HCIR Challenge, this time focusing on the problem of information availability, with four in-depth system demonstrations, and audience selection of a challenge winner.

Automatic resolution of Crossword Puzzles (CPs) heavily depends on the quality of the answer candidate lists produced by a retrieval system for each clue of the puzzle grid.
Previous work has shown that such lists can be generated using Information Retrieval (IR) search algorithms applied to the databases containing previously solved CPs and reranked with tree kernels (TKs) applied to a syntactic tree representation of the clues.
In this paper, we create a labelled dataset of 2 million clues on which we apply an innovative Distributional Neural Network (DNN) for reranking clue pairs.
Our DNN is computationally efficient and can thus take advantage of such large datasets showing a large improvement over the TK approach, when the latter uses small training data.
In contrast, when data is scarce, TKs outperform DNNs.

Automatically constructing multirelationship fuzzy concept networks for document retrieval Yih-JEN Horng a Shyi-Ming Chen
b Chia-Hoang Lee a a Department of Computer and Information Science
National Chiao Tung University Hsinchu, Taiwan, R. O. C. b Department of Computer Science and Information Engineering National Taiwan University of Science and Technology Taipei, Taiwan, R. O. C. Published online: 30 Nov 2010.

This paper explores the role of information retrieval in answering “relationship” questions, a new class complex information needs formally introduced in TREC 2005.
Since information retrieval is often an integral component of many question answering strategies, it is important to understand the impact of different termbased techniques.
Within a framework of sentence retrieval, we examine three factors that contribute to question answering performance: the use of different retrieval engines, relevance (both at the document and sentence level and redundancy.
Results point out the limitations of purely term-based methods to this challenging task.
Nevertheless, IR-based techniques provide a strong baseline on top of which more sophisticated language processing techniques can be deployed.

Music genre classification is one of the most important element of the music information retrieval (MIR) community.
In this paper, we present a music genre classification system using field programmable gate arrays (FPGA) and dedicated DSP processors.
The proposed system uses FPGA based acoustic feature extraction of mel frequency cepstral coefficients (MFCC) and dynamic time warping (DTW) based classifier using TMS320C6713 floating point processor.
We successfully implemented MFCC extraction algorithm on Spartan 6 FPGA clocked at 150 MHz with support from TMS320C6713 floating point processor followed by DTW based matching engine.
The paper attempts to implement music genre classification algorithm in hardware, yielding competitive performance in music information retrieval applications.

We aim at detecting salient objects in unconstrained images.
In unconstrained images, the number of salient objects (if any) varies from image to image, and is not given.
We present a salient object detection system that directly outputs a compact set of detection windows, if any, for an input image.
Our system leverages a Convolutional-Neural-Network model to generate location proposals of salient objects.
Location proposals tend to be highly overlapping and noisy.
Based on the Maximum a Posteriori principle, we propose a novel subset optimization framework to generate a compact set of detection windows out of noisy proposals.
In experiments, we show that our subset optimization formulation greatly enhances the performance of our system, and our system attains 16-34% relative improvement in Average Precision compared with the state-of-the-art on three challenging salient object datasets.

Spend your few moment to read a book even only few pages.
Reading book is not obligation and force for everybody.
When you don't want to read, you can get punishment from the publisher.
Read a book becomes a choice of your different characteristics.
Many people with reading habit will always be enjoyable to read, or on the contrary.
For some reasons, this critical theory for library and information science exploring the social from across the disciplines library and information science text tends to be the representative book in this website.

A new model named Boolean Latent Semantic Indexing model based on the Singular Value Decomposition and Boolean query formulation is introduced.
While the Singular Value Decomposition alleviates the problems of lexical matching in the traditional information retrieval model, Boolean query formulation can help users to make precise representation of their information search needs.
Retrieval experiments on a number of test collections seem to show that the proposed model achieves substantial performance gains over the Latent Semantic Indexing model.

This report provides an overview of the field of Information Retrieval (IR) in healthcare.
It does not aim to introduce general concepts and theories of IR but to present and describe specific aspects of Health Information Retrieval (HIR
After a brief introduction to the more broader field of IR, the significance of HIR at current times is discussed.
Specific characteristics of Health Information, its classification and the main existing representations for health concepts are described together with the main products and services in the area (e.g databases of health bibliographic content, health specific search engines and others Recent research work is discussed and the most active researchers, projects and research groups are also presented.
Main organizations and journals are also identified.

Duplicated web pages responded by search engines not only waste valuable storage, but also aggravate burdens of users&#x02019; browse.
Web page de-duplication can effectively improve the information retrieval.
This paper proposes pretreatment of web pages to improve the effectiveness and efficiency of web page de-duplication based on feature code according to the principle of data clearing.
This paper features that ranking feature code to reduce the comparison times of the system and space and time complexity.
Experiments show that this method has a promising prospect in eliminating large-scale duplicated web pages.

Crowdsourcing is a new tool for businesses, academics, and now intelligence analysts.
Enabled by recent technology, crowdsourcing allows researchers to harness the wisdom of crowds and provide recommendations and insight into complex problems.
This paper examines the potential benefits and limitations of crowdsourcing for intelligence analysis and the intelligence community beyond its primary use: anticipatory intelligence.
The author constructs a model and compares it to existing crowdsourcing theories in business, information science, and public policy.
Finally, he offers advice for intelligence analysis and public policy.
This article is available in Journal of Strategic Security: http scholarcommons.usf.edu/jss/ vol8/iss5/2

Index terms—Ontology, multilingual, cross language information retrieval.

A Business Intelligence Process to support Information Retrieval in an Ontology-Based Environment

Structured queries have proven to be an effective technique for crosslanguage information retrieval when evidence about translation probability is not available.
Query execution time is adversely impacted, however, because the full postings list for each translation is used in the computation.
This paper describes an alternative approach, translation-based indexing, that improves query-time efficiency by integrating the translation and indexing processes.
Experiment results demonstrate that similar effectiveness can be achieved at a cost in indexing time that is roughly linear in the average number of known translations for each term.

In order to assist managers in implementing total quality management (TQM) programs in dairy companies, an integrated system has been developed.
Divided in three modules, it combines the technologies of multimedia and artificial intelligence to provide support in the form of hypertext tutorials, dynamic information retrieval and expert diagnosis.
The first module presents tutorials on TQM concepts and implementation processes.
The second supports the identification of control items for a variety of dairy product processing lines and assists managers in the diagnosis of cleaning and sanitation problems, also referring to the applicable legislation, when warranted.
A third module still under development analyses current quality control practices followed by users.
Preliminary evaluations with dairy company managers suggest a strong potential for such systems in increasing TQM awareness and adoption in the sector.

As the World Wide Web infiltrates more and more countries, banalizing network, interface, and computer system differences which have impeded information access, it becomes more common for non-native speakers to explore multilingual text collections.
Beyond merely accepting 8-bit accented characters, information retrieval systems should provide help in searching for information across language boundaries.
This situation has given rise to a new research area called Cross Language Information Retrieval, at the intersection of Machine Translation and Information Retrieval.
Though sharing some problems in both of these areas, Cross Language Information Retrieval poses specific problems, three of which are described in this chapter.

Information Retrieval (IR) System is very complex in nature due to the complex interactions between documents and queries, which means that the matching of document representations and query representations is not straightforward.
The Genetic Algorithm (GA) is widely used in IR systems to improve the effectiveness such systems.
This study uses the Vector Space Model (VSM) and the Extended Boolean Model (EBM) to compute the similarities between queries and documents.
Two fitness functions are proposed in this paper:
One as fitness function and the other as adaptive mutation.
Then comparing each of these functions with a number of ratio mutations that have been introduced to get better results.
The experimental results reveal that the proposed cosine function outperformed other fitness models.

As the volume of data and computational requirements of modern information retrieval systems continue to expand, it is inevitable that parallel systems will be necessary to meet these demands.
In this research, we provide a conceptual model of a reconfigurable parallel information retrieval system in a multicomputer environment.
We develop strategies for scheduling queries in such systems, provide simulation results for implementing these strategies under various different theoretical situations, and present an analytical model of the system behavior.

As more and more e-commerce companies are mushrooming on the Internet, the competition becomes very high for those sites to be appeared on the top of Search Engine Results Pages (SERPs With the massive growth of Internet, the dependability of Search Engines for Information Retrieval (IR) becomes a mandatory.
This paper provides an introduction to link structure base search engine ranking algorithms and efficient methodologies to optimize Websites for link structure based ranking algorithms.

Since the 60’s, evaluation has been a key problem for Information Retrieval (IR) systems and has been extensively discussed in the IR community.
New IR paradigms, like Structured Information Retrieval (SIR make classical evaluation measures inappropriate.
A few tentative extensions to these measures have been proposed but are also inadequate.
We propose in this paper a new measure which is a generalisation of recall.
This measure takes into account the specificity of SIR, when elements to be retrieved are linked by structural relationships.
We show an instantiation of this measure on the INEX database and present experiments to show how well it is adapted to SIR evaluation.

The Center for Intelligent Information Retrieval (CIIR) was started at the University of Massachusetts two years ago to do research and technology transfer in the area of distributed, text-based information systems.
The CIIR is part of the NSF State/Industry University Collaborative Research Centers program and relies heavily on government and industrial partners, in addition to federal funding.
There are currently 24 members from a variety of industries and government agencies, and many of these are involved in both research and development projects.

We describe the use of quantum-mechanically entangled photons for sensing intrusions across a physical perimeter.
Our approach to intrusion detection uses the no-cloning principle of quantum information science as protection against an intruder’s ability to spoof a sensor receiver using a ‘classical’ interceptresend attack.
We explore the bounds on detection using quantum detection and estimation theory, and we experimentally demonstrate the underlying principle of entanglement-based detection using the visibility derived from polarization-correlation measurements.

We propose a new approach to querying hypermedia documents on the Web based on information retrieval (IR browsing, and database techniques so as to provide maximum exibility to the user.
We present a model based on object representation where an identity does not correspond to a source HTML page but to a fragment of it.
A fragment is identi ed using the explicit structure provided by the HTML tags as well as the implicit structure extracted using IR techniques.
Our fragmentation provides access to di erent heterogeneous components (text, image, audio, video, etc of a given document, and to their relationships (implicit or explicit through hyperlinks Our language expresses browsing and restructuring based on IR techniques in a uni ed framework.
All these are integral components of the AKIRA system, currently under development.

With the large increase of collections of structured documents (e.g. XML, HTML the need to retrieve different granules (fragments, sub-units of such documents, instead of the whole structure, becomes obvious.
Nowadays, all existing Formal Concept Analysis-based Information Retrieval approaches address exclusively unstructured documents.
They rely on the use of dyadic formal contexts (i.e. binary Documents x00D7; Terms relations In this paper an original approach which consists of enlarging FCA-based IR paradigm to structured documents is proposed.
Our approach stands from the idea of modeling structured documents by means of triadic formal contexts (i.e. ternary Documents x00D7; Terms x00D7; Structure relation
This allows to retrieve sub-units or fragments of structured documents.
In structured information retrieval, queries may be of different types.
This paper deals with content-only queries and gives a theoretical framework for both conjunctive as well as disjunctive forms.

The objective of this research is to examine the interaction of institutions, based on their citation and collaboration networks.
The domain of Library and Information Science (LIS) is examined, using data from 1965-2010.
A linear model is formulated to explore the factors that are associated with institutional citation behaviors, using the number of citations as the dependent variable, and the number of collaborations, physical distance, and topical distance as independent variables.
It is found that the institutional citation behaviors are associated with social, topical, and geographical factors.
Dynamically, the number of citations is becoming more associated with collaboration intensity and less dependent on the country boundary and/or physical distance.
This research is informative for scientometricians and policy makers.

Previous research has shown that using term associations could improve the effectiveness of information retrieval (IR) systems.
However, most of the existing approaches focus on query reformulation.
Document reformulation has just begun to be studied recently.
In this paper, we study how to utilize term association measures to do document modeling, and what types of measures are effective in document language models.
We propose a probabilistic term association measure, compare it to some traditional methods, such as the similarity co-efficient and window-based methods, in the language modeling (LM) framework, and show that significant improvements over query likelihood (QL) retrieval can be obtained.
We also compare the method with state-of-the-art document modeling techniques based on latent mixture models.

This is a report on the NTCIR-12 conference held in June 2016, in Tokyo, Japan.
NTCIR-12 is the twelfth sesquiannual research project for evaluating information access technologies that organizes a diverse set of tasks related to information retrieval, question answering, and natural language processing.
The NTCIR-12 conference is a venue in which task organizers and task participants presented their effort on their participating tasks, and attracted 236 participants from 21 countries/regions in this round.
This report introduces the highlights of the conference, describes the scope and task designs of nine tasks organized at NTCIR-12, and provides a brief introduction to NTCIR-13, which started from June 2016 and will be closed in December 2017.

Information retrieval techniques play a critical role in the development of the information systems.
Different searches have focused on the way of improving the retrieval effectiveness.
Query expansion via relevance feedback is a good technique that proved to be a good way to improve the retrieval performance.
In this paper, we investigate new methods to improve the query reformulation process.
A two step process is employed to reformulate query.
In a preliminary step, a local set of documents is built from the retrieved result.
In a second step, a co-occurrence analysis is performed on the local document set to deduce the terms to be used for the query expansion.
To build the local set we use firstly a content-based analysis.
It is a similarity study between the retrieved documents and the query.
The second method combines content and hypertext analyses to achieve the local set construction.
The TREC frame is used to evaluate the proposed processes.

In the context of music, a cover version is a remake of a song, often with significant stylistic variation.
In this paper we describe a distance measure between sampled audio files that is designed to be insensitive to instrumentation, time shift, temporal scaling and transpositions.
The algorithm was submitted to the Music Information Retrieval eXchange (MIREX) 2007 audio cover song identification task, where it came fourth of the eight submitted algorithms.

This research is aimed at comparing techniques of indexing that exist in the current information retrieval processes.
The techniques being inverted files, suffix trees, and signature files will be critically described and discussed.
The differences that occur in their use will be discussed.
The performance and stability of each indexing technique will be critically studied and compared with the rest of the techniques.
The paper also aims at showing by the end the role that indexing plays in the process of retrieving information.
It is a comparison of the three indexing techniques that will be introduced in this paper.
However, the details arising from the detailed comparison will also enhance more understanding of the indexing techniques.
Keywords—Information Retrieval; Indexing Techniques; Inverted Files; Suffix Trees; Signature Files

Since WWW encourages hypertext and hypermedia document authoring (e.g. HTML or XML Web authors tend to create documents that are composed of multiple pages connected with hyperlinks.
A Web document may be authored in multiple ways, such as (1) all information in one physical page, or (2) a main page and the related information in separate linked pages.
Existing Web search engines, however, return only physical pages containing keywords.
In this paper, we introduce the concept of information unit, which can be viewed as a logical Web document consisting of multiple physical pages as one atomic retrieval unit.
We present an algorithm to efficiently retrieve information units.
Our algorithm can perform progressive query processing.
These functionalities are essential for information retrieval on the Web and a large XML database.
We also present experimental results on synthetic graphs and real Web data.

In this paper we introduce a novel approach to manifold alignment, based on Procrustes analysis.
Our approach differs from "semi-supervised alignment" in that it results in a mapping that is defined everywhere when used with a suitable dimensionality reduction method rather than just on the training data points.
We describe and evaluate our approach both theoretically and experimentally, providing results showing useful knowledge transfer from one domain to another.
Novel applications of our method including cross-lingual information retrieval and transfer learning in Markov decision processes are presented.

Having in mind today's growth of information sources, both in terms of their number and of their size, whether we are referring to the Internet, a corporate intranet, or a library information retrieval system, we can say that manipulating information is not a trivial task.
The user is not often being catered for in distributed information systems.
He/ she seems to be interacting with systems that do not recognize his/her uniqueness and thus do not offer an individualized treatment.
As a result, User Modeling is a core, essential factor in achieving personalization.
We present here an intelligent way of inferring user related information that is not available, a situation that is very likely to occur due to sparseness of relevant data.
This method can be very useful in recommender systems and this is illustrated with an example.

As retrieval systems become more oriented towards endusers, there is an increasing need for improved methods to evaluate their effectiveness.
We performed a task-oriented assessment of two MEDLINE searching systems, one which promotes traditional Boolean searching on human-indexed thesaurus terms and the other natural language searching on words in the title, abstract, and indexing terms.
Medical students were randomized to one of the two systems and given clinical questions to answer.
The students were able to use each system successfully, with no significant differences in questions correctly answered, time taken, relevant articles retrieved, or user satisfaction between the systems.
This approach to evaluation was successful in measuring effectiveness of system use and demonstrates that both types of systems can be used equally well with minimal training.

Implementing scalable information retrieval systems requires the design and development of efficient methods to ingest data from multiple sources, search and retrieve results from both English and foreign language document collections and from collections comprising of multiple data types, harness high performance computer technology, and accurately answer user questions.
Some recent efforts related to the development of scalable information retrieval systems are described.
Particular emphasis is placed on those efforts that were adopted into commercial use.

In an information retrieval system, the query is the bridge between the collection database and the user.
With query operators to define terms that should or should not appear in result documents, users could actually access their desired documents.
For the web IR systems, no matter the underlying database or the user group is changed a lot comparing to the traditional environment, but the set of query operators is still unchanged.

Ranking authors is vital for identifying a researcher’s impact and his standing within a scientific field.
There are many different ranking methods (e.g citations, publications, h-index, PageRank, and weighted PageRank but most of them are topic-independent.
This paper proposes topic-dependent ranks based on the combination of a topic model and a weighted PageRank algorithm.
The Author-Conference-Topic (ACT) model was used to extract topic distribution of individual authors.
Two ways for combining the ACT model with the PageRank algorithm are proposed: simple combination (I_PR) or using a topic distribution as a weighted vector for PageRank
(PR_t Information retrieval was chosen as the test field and representative authors for different topics at different time phases were identified.
Principal Component Analysis (PCA) was applied to analyze the ranking difference between I_PR and PR_t.

The majority of existing work in music information retrieval for audio signals has followed the content-based query-by-example paradigm.
In this paradigm a musical piece is used as a query and the result is a list of other musical pieces ranked by their content similarity.
In this paper we describe algorithms and graphical user interfaces that enable novel alternative ways for querying and browsing large audio collections.
Computer audition algorithms are used to extract content information from audio signals.
This automatically extracted information is used to configure the graphical user interfaces and to genereate new query audio signals for browsing and retrieval.

Advances in web technology have given rise to new information retrieval applications.
In this paper, we present a model for geographical region search and call this class of query similar region query.
Given a spatial map and a query region, a similar region search aims to find the top-k most similar regions to the query region on the spatial map.
We design a quadtree based algorithm to access the spatial map at different resolution levels.
The proposed search technique utilizes a filter-and-refine manner to prune regions that are not likely to be part of the top-k results, and refine the remaining regions.
Experimental study based on a real world dataset verifies the effectiveness of the proposed region similarity measure and the efficiency of the algorithm.

This paper proposes a model for Information Retrieval (IR) based on possibilistic directed networks.
Relations documents-terms and query-terms are modeled through possibility and necessity measures rather than a probability measure.
The relevance value for the document given the query is measured by two degrees: the necessity and the possibility.
More precisely, the user’s query triggers a propagation process to retrieve necessarily or at least possibly relevant documents.
The possibility degree is convenient to filter documents out from the response (retrieved documents) and the necessity degree is useful for document relevance confirmation.
Separating these notions may account for the imprecision pervading the retrieval process.
Experiments carried out on a sub-collection of CLEF, namely LeMonde 1994, a French newspapers collection, showed the effectiveness of the model.
MOTS-CLÉS Modèle de recherche d’information, Réseaux Possibilistes, Pertinence,Théorie des

For NTCIR Workshop 7 UC Berkeley participated in IR4QA (Information Retrieval for Question Answering) as well as the Patent Mining tracks.
For IR4QA we only did Japanese monolingual search.
Our focus was thus upon Japanese topic search against the Japanese News document collection as in past NTCIR participations.
We preprocessed the text using the ChaSen morphological analyzer for term segmentation.
We utilized a timetested logistic regression algorithm for document ranking coupled with blind feedback.
The results were satisfactory, ranking second among IR4QA overall submissions..

This paper describes HCCL lab’s submission to the Queryby-Singing/Humming(QBSH) task of Music Information Retrieval eXchange(MIREX) 2006.
As we do not participate in the second sub-task, this paper will only deal with the Known-Item Retrieval sub-task.
In the submitted system, we apply a novel algorithm called Recursive Alignment(RA) to compute the similarity score between query and candidates.
We also employ the multilevel filter strategy to reduce the running time.
Finally, we give the evaluation results of the presented system.

This paper demonstrates that a user of multilingual search has different interests depending on the language used, and that the user model should reflect this.
To demonstrate this phenomenon, the paper proposes and evaluates a set of result re-ranking algorithms based on various user model representations.

This paper introduces the attributes of emotional evaluation in the Grammatical Knowledge-base of Contemporary Chinese.
Lexical emotion tagging is studied by means of both qualitative and quantitative approaches.
Based on the statistical results from the People’s Daily tagging corpus, lexical emotional trends are described and formulated in our Knowledge-base.
Lastly, we also discuss potential applications of emotion tagging in related tasks such as text filtering, information retrieval and web page evaluation.
1 相關研究得到中國國家 973 項目(2004CB318102)和國家 863 計劃(2001AA114210,2002AA117010)
的支持 北京大學計算語言學研究所,100871
中國 Institute of Computational Linguistics, Peking University, 100871 China E-mail wangzm, yusw}@pku.edu.cn

Music genre classification is a challenging task in the field of music information retrieval.
Existing approaches usually attempt to extract features only from acoustic aspect.
However, spectrogram also provides useful information because it describes the temporal change of energy distribution over frequency bins.
In this paper, we propose the use of Gabor filters to generate effective visual features that can capture the characteristics of a spectrogram&#x0A1 x0A6;s texture patterns.
On the other hand, acoustic features are extracted using universal background model and maximum a posteriori adaptation.
Based on these two types of features, we then employ SVM to perform the final classification task.
Experimental results demonstrate that combining visual and acoustic features can achieve satisfactory classification accuracy on two widely used datasets.

The cycle of abstraction-reconstruction, which occurs as a fundamental principle in the development of culture and in cognitive processes, is described and analyzed.
This approach leads to recognition of boundary conditions for and directions of probable development of cognitive tools.
It is shown how the transition from a conventional Japanese-English character dictionary to a multi-dimensional language database is an instance of such an abstraction-reconstruction cycle.
The individual phases in the design of a multi-dimensional language database based upon diierent computer software technologies are investigated in regard to the underlying cycle.
The methods used in the design of a multi-dimensional language database include the use of unix software tools, classical database methods as well as the use of search engines based upon full text search.
Several directions of application and extension for multi-dimensional language databases are discussed.

For information retrieval, users hope to acquire more relevant information from the top N ranking documents.
In this paper, a hybrid Chinese language model is presented, which is defined as a combination of ontology with statistical method, to improve the precision of top N ranking documents by reordering the initial retrieval documents.
The experiment with NTCIR-3 formal Chinese test collection shows the proposed method improved the precision at top N ranking documents level

Representing and fusing multimedia information is a key issue to discover semantics in multimedia.
In this paper we address more specifically the problem of multimedia content retrieval by first defining a novel preference-based representation particularly adapted to the fusion problem, and then, by investigating the RankBoost algorithm to combine those preferences and a learn multimodal retrieval model.
The approach has been tested on annotated images and on the complete TRECVID 2005 corpus and compared with SVM-based fusion strategies.
The results show that our approach equals SVM performance but, contrary to SVM, is parameter free and faster.

Web spam has become one of the most exciting challenges and threats to Web search engines.
The relationship between the search systems and those who try to manipulate them came up with the field of adversarial information retrieval.
In this paper, we have set up several experiments to compare HostRank and TrustRank to show how effective it is for TrustRank to combat Web spam and we have also reported a comparison on different link based Web spam detection algorithms.

Text mining is a new and exciting research area that tries to solve the information overload problem by using techniques from machine learning, natural language processing (NLP data mining, information retrieval (IR and knowledge management.
Text mining involves the pre-processing of document collections such as information extraction, term extraction, text categorization, and storage of intermediate representations.
The techniques that are used to analyse these intermediate representations such as clustering, distribution analysis, association rules and visualisation of the results.

The Laboratory for Information and Decision Systems (LIDS) is an interdepartmental laboratory for research and education in systems, communication, networks, optimization, control, and statistical signal processing.
LIDS emphasizes pursuit of basic knowledge as the foundation for innovation.
While maintaining roots in fundamental research related to information science, LIDS has also initiated work on system architectures and joined with computer scientists and hardware engineers to broaden perspectives in the research, design, prototyping, and tests of systems such as networks and unmanned air-vehicles.

The Fifteenth IAPR International Conference on Machine Vision Applications will be held at Nagoya University, Japan, from May 8 through 12, 2017.
The conference is co-sponsored by the MVA Organization, IAPR, and Graduate School of Information Science, Nagoya University.
In conjunction with the MVA 2017, we plan to publish the special section that consists of the papers published in the MVA 2017 as well as new contributions with related topics.
The submitted papers are expected to be related to Machine Vision and its Applications that include the following topics:

This project addresses extraction of medical concepts relationship in scientific documents, medical records and general information on the Internet, in several languages by using advanced Natural Language Processing and Information Retrieval techniques and tools.
The project aims to show, through two use cases, the benefits of the application of language technology in the health sector.

Name extraction is indispensable for both natural language understanding and information retrieval.
However, proper names are major unknown words in natural language texts, and unknown word identification is still a challenge problem in natural language processing.
This paper deals with identification of person names, organization names and location names from Chinese texts.
Different types of information from different levels of text are employed, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech–act and locative verbs, cache and n–gram model.
We also clarify which strategies can be used in which cases, i.e queries and/or documents.
In our experiments, the recall rates and the precision rates for the extraction of person names, organization names, and location names under MET data are (87.33 82.33 76.67 79.33 and (77.00 82.00 respectively.

This paper presents one part of a broad research project entitled 'Activity-Based Information Retrieval AIR) which is being carried out at EuroPARC.
The basic hypothesis of this project is that if contextual data about human activities can be automatically captured and later presented as recognisable descriptions of past episodes, then human memory of those past episodes can be improved.
This paper describes an application called Pepys, designed to yield descriptions of episodes based on automatically collected location data.
The program pays particular attention to meetings and other episodes involving two or more people.
The episodes are presented to the user as a diary generated at the end of each day and distributed by electronic mail.
The paper also discusses the methods used to assess the accuracy of the descriptions generated by the recogniser.

Information retrieval concerns the problem of retrieving those documents from a given document-base that are likely to be relevant to a certain information need.
In 1971, Cooper introduced an objective part of the relevance relation termed logical relevance [5 We call this relation “aboutness
Underlying every IR model is a theory that defines the notion of aboutness.
Such a theory could be implicit or explicit in the model.
In this article, we will present a meta-theory for studying information retrieval.
A meta-theory should offer the possibility to axiomatise the aboutness relation of every IR model.

The MPEG Query Format (MPQF) which MPEG Group is developing is to standardize query interface for XML-based multimedia retrieval systems.
MPQF provides query types for Information Retrieval and Data Retrieval over XML Database in the current draft.
As more and more multimedia metadata modeled with semantic ontology like RDFS and OWL are stored in the Knowledge Base, there is a requirement for querying multimedia information over Knowledge-Based system.
In this paper we propose the necessary extension (QueryByKQML type) which will allow MPQF to support Knowledge-Based Multimedia Information Retrieval based on Knowledge Query and Manipulation Language (KQML).

Technological advances are driving statistics beyond mathematical philosophy and beyond computer-aided empiricism towards experimentally supported information science.
Interplay between falsifiable theory and reproducible experiment is the essence of experimental statistics.
A distinguishing goal of statistics is quantification of uncertainty in data-analyses through risk estimation and confidence sets.
Numerical experiments with superefficient estimators for the mean vector in the one-way layout
—estimators based on adaptive monotone shrinkage and on adaptive soft thresholding—illustrate how experimental statistics conditions minimax risk comparisons from theoretical statistics.

Data Mining is the process of identifying patterns in large sets of data.
Text mining is the discovery of interesting knowledge in text documents.
Many data mining techniques have been proposed for mining useful patterns in text documents.
It is a challenging issue to find accurate knowledge (or features) in text documents to help users to find what they want.
In existing, Information Retrieval (IR) provided many term-based methods to solve this challenge.
The term-based methods suffer from the problems of polysemy and synonymy.
The polysemy means a word has multiple meanings, and synonymy is multiple words having the same meaning.
In proposed system we want to use pattern (or phrase)-based approaches should perform better than the termbased ones.
The proposed approach can improve the accuracy of evaluating term weights because discovered patterns are more specific than whole documents.

Content-based information retrieval (IR) involves low-level feature extraction and utilize similarity search methods applied either in the feature space or in derived higher-level semantic spaces.
These methods assume that similarity is measured by accounting only the degree in which two entities are related, ignoring the hesitancy introduced by the degree in which they are unrelated.
Aiming at semantically relevant IR from cultural databases, this paper proposes a novel intuitionistic fuzzy clustering scheme based on intuitive features and a similarity defined over higher-level patterns.

We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms.
We propose an efficient algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem.
We prove a finite time expected regret upper bound of order O K ln(K)T for this algorithm and a general lower bound of order Ω KT
At the end, we provide experimental results using real data from information retrieval applications.

Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks.
However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data.
We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains.
Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.

This paper addresses current chances and challenges in computational musicology.
Computational musicology is a genuinely interdisciplinary research area that requires the contribution of questions, methods and insights from both musicology and computer science.
This paper demonstrates how computational approaches to musicological questions generate new perspectives for musicology.
In turn, computational musicology has the potential to become an indispensible partner for computer science in Music Information Retrieval.
We argue that, for unfolding the potential of computational musicology, the full interdisciplinary enterprise has yet to be realized and we discuss examples of promising collaborative

Based on rough sets, fuzzy set theory gives a model of information retrieval Contains" relationship reflects a match between the set of documents and user queries using fuzzy set theory, and its inclusion degree to achieve the sort of document sets of search results.
The use of rough set equivalence relation reflects the correlation between keywords, achieve synonyms retrieve.
Compared with the traditional rough set model, the model represented the right weight of the document sets.
Through user query mode, it gives the interest of each keyword.
It improved information recall and precision.

We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer.
We grounded our work on the empirical analysis of elicited user queries.
We found that the majority of elicited queries (around 60 pertain to argumentative processes and outcomes.
Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.

Wednesday evening, July 10th, 8:30-10:30 is the time scheduled for a panel discussion Government Publications in Machine-Readable Form:
A New Tool for the Reference Librarian A part of the American Library Association's 1974 New York Conference, the meeting is co-sponsored by the Government Documents Round Table's (GODORT) Machine-Readable Data Files Committee, the Federal Librarians Round Table (FLIRT the RASD Information Retrieval Committee and the RASD/RTSD/ASLA Public Documents Committee.

The rapid growth of scientific data shows no sign of abating.
This growth has led to a new problem: with so much scientific data at hand, stored in thousands of datasets, how can scientists find the datasets most relevant to their research interests?
We have addressed this problem by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data.
We propose an approach that uses a blend of automated and x201C;semi-curated&#x201D; methods to extract metadata from large archives of scientific data, then evaluates ranked searches over this metadata.
We describe a challenge identified during an implementation of our approach: the large and expanding list of environmental variables captured by the archive do not match the list of environmental variables in the minds of the scientists.
We briefly characterize the problem and describe our initial thoughts on resolving it.

We propose and analyze a scheme by which a many-particle system can be prepared in highly entangled wave-packet states.
One of the particles is prepared initially in a quantum superposition of multiple coherent states and then coupled via a quadratic interaction Hamiltonian to a number of other particles.
The system evolves into a highly entangled wave-packet state.
An appropriate measure of this time-dependent entanglement is given.
This scheme is applicable to a number of systems of interest in quantum-information science.

This article summarizes the approach developed for TREC 2016 Clinical Decision Support Track.
In order to address the daunting challenge of retrieval of biomedical articles for answering clinical questions, an information retrieval methodology was developed that combines pseudo-relevance feedback, semantic query expansion and document similarity measures based on unsupervised word embeddings.
The individual relevance metrics were combined through a supervised learning -to-rank model based on gradient boosting to maximize the normalized discounted cumulative gain (nDCG Experimental results show that document distance measures derived from unsupervised word embeddings contribute to significant ranking improvements when combined with traditional document retrieval approaches.

TheWEBSOM is a method developed originally at Helsinki University of Technology for analyzing and visualizing large document collections [1, 4
In the WEBSOM method, the self-organizing map algorithm [3] is used to automatically organize collections of documents on a map to enable easy exploration and search of the collection.
Map regions that are close to each other contain similar documents.
The main objectives in developing the WEBSOM method has been to o er a method for exploring text collections that is di erent from the queryresult approach, enabling the user to get an overall view to the document collection.
Moreover, there are no principled limits on the type of text material that the method can handle.

Visualization of search results is an essential step in the Information Retrieval process.
Indeed Information User Interfaces are used as a link between users and Information Retrieval Systems, and so they enable to give sense to the results for users.
Due to the importance that takes visualization of search results, many interfaces (which can be textual, in 2D or in 3D) have been proposed in the last decade.
However, although some evaluations of these interfaces were proposed, they are too rare and not really interpretable (and comparable) due to the big heterogeneity of systems and due to the absence of an evaluation framework.
So, in this paper, we propose to define a set of criteria which take users, tasks and system into account.
MOTS-CLÉS interface utilisateur d’information, interface de restitution de résultats de recherche, systèmes de recherche d’information, critères d’évaluation et de comparaison.

This paper present the details of participation of DEMIR (Dokuz Eylul University Multimedia Information Retrieval) research team to the context of our participation to the ImageCLEF 2011 Medical Retrieval task.
This year, we evaluated fusion and re-ranking method which is based on the best low level feature of images with best text retrieval result.
We improved results by examination of different weighting models for retrieved text data and low level features.
We tested multi–modality image retrieval in ImageCLEF 2011 medical retrieval task and obtained the best seven ranks in mixed retrieval, which includes textual and visual modalities.
The results clearly show that proper fusion of different modalities improve the overall retrieval performance.

Text retrieval using Latent Semantic Indexing (LSI) with truncated Singular Value Decomposition (SVD) has been intensively studied in recent years.
However, the expensive complexity involved in computing truncated SVD constitutes a major drawback of the LSI method.
In this paper, we demonstrate how matrix rank approximation can influence the effectiveness of information retrieval systems.
Besides, we present an implementation of the LSI method based on an eigenvalue analysis for rank approximation without computing truncated SVD, along with its computational details.
Significant improvements in computational time while maintaining retrieval accuracy are observed over the tested document collections.

Word sense disambiguation (WSD) is described as the job of searching the sense of a word in a situation.
WSD is a core problem in many tasks related to language processing.
It is aggravated by make use of in several critical utilization like Part-of-Speech tagging, Machine Translation, Information retrieval, etc.
Different topics such as ambiguity, evaluation, scalability and diversity cause challenges to results of WSD.
In this paper we have discussed about some issues related to WSD and some WSD methods like knowledge-based, supervised, unsupervised and semi supervised.

We introduce and validate bootstrap techniques to compute confidence intervals that quantify the effect of test-collection variability on average precision (AP) and mean average precision (MAP) IR effectiveness measures.
We consider the test collection in IR evaluation to be a representative of a population of materially similar collections, whose documents are drawn from an infinite pool with similar characteristics.
Our model accurately predicts the degree of concordance between system results on randomly selected halves of the TREC-6 ad hoc corpus.
We advance a framework for statistical evaluation that uses the same general framework to model other sources of chance variation as a source of input for meta-analysis techniques.

Elham Khabiri joined IBM T.J. Watson Research Center as a Postdoc researcher on January 2013.
Her research is on exploring data mining and machine learning techniques to analyze large scale data contributed by people using social media.
She received her Ph.D. in Computer Science from Texas A&M University (2013 M.S. in Computer Information Systems (2007) from University of Houston-Clear Lake with her thesis focused on Bioinformatics, and her B.S. in Computer Engineering from University of Tehran in Iran (2005
Her interest includes information retrieval, text mining, machine learning and their applications on the large scale information management and social computing.

We propose an approach to Distributed Information Retrieval based on the periodic and incremental centralisation of full-text indices of widely dispersed and autonomously managed content sources.
Inspired by the success of the Open Archive Initiative’s protocol for metadata harvesting, the approach occupies middle ground between i) the crawling of content, and (ii) the distribution of retrieval.
As in crawling, some data moves towards the retrieval process, but it is statistics about the content rather than content itself.
As in distributed retrieval, some processing is distributed along with the data, but it is indexing rather than retrieval itself.
We show that the approach retains the good properties of centralised retrieval without renouncing to cost-effective resource pooling.
We discuss the requirements associated with the approach and identify two strategies to deploy it on top of the OAI infrastructure.

We consider the task of retrieving online information in mobile environments.
We propose question answering as a more appropriate interface than page browsing for small displays.
We assess different modalities for communicating using a mobile device with question-answering systems, focusing on speech.
We then survey existing research in spoken information retrieval, present some new findings, and assess the feasibility of the endeavor.

Geographic information science increasingly meets cognitive science, as comprehending (i.e. perceiving and conceptualising) spatial characteristics induces spatial cognition and behaviour.
However, the widely deployed models of space fail in accounting for those conceptualisation mechanisms.
In this context, the present work aims at relating human spatial perception of land use to the landscape characteristics in order to build an ontology from human experience of geographical space.
This experience is to be captured using a questionnaire.
Though Land use information is readily available from various commercial sources, the methods with which such information is created are time and resource-consuming.
Here, the retained solution is to infer as much land use information from topology as possible.
MOTS-CLÉS Information géographique, sciences cognitives, OS MasterMap, foncier, ontologies spatiales.

Diversity as a relevant dimension of retrieval quality is receiving increasing attention in the Information Retrieval and Recommender Systems (RS) fields.
The problem has nonetheless been approached under different views and formulations in IR and RS respectively, giving rise to different models, methodologies, and metrics, with little convergence between both fields.
In this poster we explore the adaptation of diversity metrics, techniques, and principles from ad-hoc IR to the recommendation task, by introducing the notion of user profile aspect as an analogue of query intent.
As a particular approach, user aspects are automatically extracted from latent item features.
Empirical results support the proposed approach and provide further insights.

This is our first participation with TREC.
Our team researches natural language processing, and we have developed English-Japanese and Japanese-English machine translation system
The code name of machine translation system is VENUS
We are now researching a new natural language processing environment, including information retrieval and text understanding The environment name is VIRTUE: VENUS for Information Retrieval and Text Understanding
Last year, our team participated with MUC5, and we got promising results[1 This year,

Sentiment analysis research has acquired a growing importance due to its applications in several different fields.
A large number of companies have included the analysis of opinions and sentiments of costumers as a part of their mission.
Therefore, the analysis and automatic classification of large corpora of documents in natural language, based on the conveyed feelings and emotions, has become a crucial issue for text mining purposes.
This chapter aims to relate the sentimentbased characterization inferred from books with the distribution of emotions within the same texts.
The main result consists in a method to compare and classify texts based on the feelings expressed within the narrative trend.

In Latent Semantic Indexing (LSI a collection of documents is often pre-processed to form a sparse term-document matrix, followed by a computation of a low-rank approximation to the data matrix.
A multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term-document matrix representing the data.
The main goal is to reduce the cost of the matrix approximation without sacrificing accuracy.
Because coarsening by multilevel hypergraph techniques is a form of clustering, the proposed approach can be regarded as a hybrid of factorization-based LSI and clustering-based LSI.
Experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost

Most current large-scale information retrieval systems are impersonal, allowing uniform access to a non-modifiable corpus of information.
The Haystack project is a personal information repository, which employs artificial intelligence techniques to adapt to the needs of its user.
This thesis outlines a new overall design for Haystack.
The design includes an RDFbased data model, a transaction-based storage system, services which are triggered by patterns in the data, a kernel with the machinery to invoke and execute these services, and a query processing system.
In addition, the thesis serves as a valuable resource to future Haystack researchers by explaining and documenting the fundamental ideas of Haystack, as well as the reasoning for all design decisions.
Thesis Supervisor: David R. Karger
Title: Associate Professor

In one form or another language translation is a necessary part of cross-lingual information retrieval systems.
Often times this is accomplished using machine translation systems.
However, machine translation systems offer low quality for their high costs.
This paper proposes a machine translation method that is low cost while improving translation quality.
This is done by utilizing multiple web based translation services to negate the high cost of translation.
A best translation is chosen from the candidates using either consensus translation selection or statistical analysis.
Which to use is determined by a heuristic rule that takes into account that most web based translation services are of similar quality and that machine translation still produces relatively poor results.
By choosing the best translation the method is able to increase translation quality over the base systems, which is verified by the experimentation.

Semantic similarity between words is fundamental to various fields such as Cognitive Science, Artificial Intelligence, Natural Language Processing and Information Retrieval.
According to Baeza-Yates and Neto [2] an Information Retrieval system “should provide the user with easy access to the information in which he is interested
Therefore, in this domain, relying on a robust semantic similarity measure is crucial for automatic query suggestion and expansion process.
In this same context, we propose a method that uses on one hand, an online English dictionary provided by the Semantic Atlas project of the French National Centre for Scientific Research (CNRS) and on the other hand, a page counts based metric returned by a social website.

The following paper presents a scientific contribution that explores the clinicians' use of online information retrieval systems for their clinical decision making.
Particularly, the research focuses on the ability of doctors and nurses in seeking information through MEDLINE and ScienceDirect.
The research process took place by an electronic form consisted of five clinical scenarios and an evaluation sheet.
The results testify that only a small percent of clinicians use the recommended electronic bibliographic databasesand take the right clinical decision to the scenarios.
Health professionals have to be educated in information searching and take advantage from the provided literature taking more useful and reliable answers on their clinical questions.

Information retrieval mechanisms from the web are a great need of the hour as the amount of the content is growing dynamically every day.
There are many algorithms which have been proposed in literature mainly relying on the output of the search engines.
These algorithms are either content based or snippet based and perform a clustered outcome re-ranking of the content for the user.
This work proposes a hybrid approach to content clustering that combines the best of the web information retrieval methods and also uses the personal preference information of the users modeling a wide range of contexts.
This work introduces a context mechanism of the users in the overall process and presents taxonomy of the methods to organize the output of the search engines.
Experimental results are promising and show that this approach has great promise for a wide range of queries.

One of the biggest problems facing Web-based Information Systems (WIS) is the complexity of the information searching/retrieval processes, especially the information overload, to distinguish between relevant and irrelevant content.
In an attempt to solve this problem, a wide range of techniques based on different areas has been developed and applied to WIS.
One of these techniques is the information retrieval.
In this paper we described an information retrieval mechanism (only for structured data) with a client/server implementation based on the Query-Searching/Recovering-Response (QS/RR) model by means of a trading model, guided and managed by ontologies.
This mechanism is part of SOLERES system, an Environmental Management Information System (EMIS).

The need for training engineers in the use of information resources and retrieval techniques, and the advantages to be gained from such a training, are stressed.
The possibility of replacing the foreign language proficiency requirement by such a training in doctoral degree programs is examined.
Some of the anticipated arguments against this training are answered.
Possible course contents and some operational problems are discussed.

New application domains cause today's database sizes to grow rapidly, posing great demands on technology.
Data fragmentation facilitates techniques (like distribution, parallelization. and main-memory computing) meeting these demands.
Also, fragmentation might help to improve efficient processing of query types such as top N. Database design and query optimization require a good notion of the costs resulting from a certain fragmentation.
Our mathematically derived selectivity model facilitates this.
Once its two parameters have been computed based on the fragmentation, after each (though usually infrequent) update, our model can forget the data distribution, resulting in fast and quite good selectivity estimation.
We show experimental verification for Zipfian distributed IR databases.

The last few years have seen an explosion in the amount of text be oming available on the World Wide Web as online ommunities of users in diverse domains emerge to share do uments and other digital resour es.
In this paper we explore the issue of how to provide a low-level information extra tion tool based on hidden Markov models that an identify and lassify terminology based on previously marked-up examples.
Su h a tool should provide the basis for a domain portable information extra tion system, that when ombined with sear h te hnology an help users to a ess information more e e tively within their do ument olle tions than today's information retrieval engines alone.
We present results of applying the model in two diverse domains: news and mole ular biology and dis uss the model and term markup issues that this investigation reveals.

Dictionary methods for cross-language information retrieval give performance below that for mono-lingual retrieval.
Failure to translate multi-term phrases has been shown to be one of the factors responsible for the errors associated with dictionary methods.
First, we study the importance of phrasal translation for this approach.
Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with automatic dictionary translation.

In this study, we present a systematic evaluation of machine translation methods applied to the image annotation problem.
We used the well-studied Corel data set and the broadcast news videos used by TRECVID 2003 as our dataset.
We experimented with different models of machine translation with different parameters.
The results showed that the simplest model produces the best performance.
Based on this experience, we also proposed a new method, based on cross-lingual information retrieval techniques, and obtained a better retrieval performance.

This paper presents a new approach in guiding users to formulate unambiguous queries based on their common nature of asking for information.
The approach known as the "front-end approach" gives users an overview about the system data through a "virtual data component" which stores the merged metadata of data storage sources.
Based on this component, users are aware of their stored data while generating requests.
This approach reduces the ambiguities in users' requests at very early stage; and this makes the query refinement process easily fulfill users' demands

This paper present the details of participation of DEMIR (Dokuz Eylul University Multimedia Information Retrieval) research team to TREC 2011 Medical Records track.
In this study, our aim is to index and retrieve medical terms and term phrases in medical text archives.
We searched medical terms and term phrases with using UMLS which is a metathesaurus about medical.
We evaluated the effects of terms and term phrases on retrieval system in TREC 2011 Medical Records track, considering terms and term phrases as medical entities.
We improved results by examination of different weighting schemes for retrieved data.

The article at hand analyses an often disregarded aspect of design science research that is how design knowledge is actually built or, more precisely, how new design knowledge is discovered.
In the article we distinguish abductive and inductive forms of discovery.
We describe how inductive and abductive discoveries are dealt with in traditional science and how these two forms of discovery have been discussed in Information Systems Design Science Research literature.
By means of a case study we specifically illustrate the impact of a chosen mode of discovery on validity, utility, generality, and innovativeness of a problem solution.
We find that the strength of inductively discovered design knowledge is that its validity, utility, and generality can be proven more easily than that of abductive discoveries.
However, inductively discovered design knowledge often suffers from a smaller degree of innovative-

Understanding how people interact when searching is central to the study of Interactive Information Retrieval (
IIR Most of the prior work has either been conceptual, observational or empirical.
While this has led to numerous insights and findings regarding the interaction between users and systems, the theory has lagged behind.
In this paper, we extend the recently proposed search economic theory to make the model more realistic.
We then derive eight interaction based hypotheses regarding search behaviour.
To validate the model, we explore whether the search behaviour of thirty-six participants from a lab based study is consistent with the theory.
Our analysis shows that observed search behaviours are in line with predicted search behaviours and that it is possible to provide credible explanations for such behaviours.
This work describes a concise and compact representation of search behaviour providing a strong theoretical basis for future IIR research.

The realm of knowledge discovery extends across several allied spheres today.
It encompasses database management areas such as data warehousing and schema versioning; information retrieval areas such as Web semantics and topic detection; and core data mining areas, e.g knowledge based systems, uncertainty management, and time-series mining.
This becomes particularly evident in the topics that Ph.D. students choose for their dissertation.
As the grass roots of research, Ph.D. dissertations point out new avenues of research, and provide fresh viewpoints on combinations of known fields.
In this article we overview some recently proposed developments in the domain of knowledge discovery and its related spheres.
Our article is based on the topics presented at the doctoral workshop of the ACM Conference on Information and Knowledge Management, CIKM 2011.

In this paper we show the need to see the Music Information Retrieval world from different points of view in order to make any progress.
To help the interaction of different languages (engineers, musicians, psychologists, etc we present a tool that tries to link all those backgrounds.

Users who downloaded this article also downloaded: PETER INGWERSEN 1996 COGNITIVE PERSPECTIVES OF INFORMATION RETRIEVAL INTERACTION: ELEMENTS OF A COGNITIVE IR THEORY Journal of Documentation, Vol.
52 Iss 1 pp.
3-50 http dx.doi.org/10.1108/eb026960 JOHN F. FARROW 1991
A COGNITIVE PROCESS MODEL OF DOCUMENT
INDEXING Journal of Documentation, Vol.
47 Iss 2 pp.
149-166 http dx.doi.org/10.1108/eb026875 BIRGER HJØRLAND 1992 THE CONCEPT OF ‘SUBJECT’ IN INFORMATION SCIENCE Journal of Documentation, Vol.
48 Iss 2 pp.
172-200 http dx.doi.org/10.1108/eb026895

One of the difficulties faced in implementing information management and retrieval systems is that each case seems to present its own special complexities.
As a result information retrieval systems typically fall behind their programming schedule and have many bugs when delivered.
In this paper a set of basic operations on types of files are defined.
These operations are intended to fulfill the same role for information retrieval systems programmers that functions such as LOG(X) fill for mathematical applications programmers they should make the job very much easier.
The file operations have been implemented as a run-time package written in FORTRAN IV and Burroughs Extended Algol.
The approach has been used to develop three different information management systems; an APL interactive computing system, a generalized information retrieval system, and a specialized information retrieval system for map oriented data.
These systems are described.

E-Collaboration in Modern Organizations:
Initiating and Managing Distributed Projects combines comprehensive research related to e-collaboration in modern organizations, emphasizing topics relevant to those involved in initiating and managing distributed projects.
Providing authoritative content to scholars, researchers, and practitioners, this book specifically describes conceptual and theoretical issues that have implications for distributed project management, implications surrounding the use of e-collaborative environments for distributed projects, and emerging issues and debate related directly and indirectly to e-collaboration support for distributed project management.
Information Science Reference Illustration:
N Language:
ENG Title: E-Collaboration in Modern Organizations:
Initiating and Managing
Distributed Projects Pages: 00320 (Encrypted PDF)
On Sale: 2008-01-01 SKU-13/ISBN: 9781599048277 Category: Business Economics Project Management

This paper examines the effectiveness of different phrase identification and weighting methods for Japanese text retrieval in an operational information retrieval (IR) system, called <i>NACSIS-IR</i Based on our previous experiments, we used character-based indexing with positional information and word-or phrase-based query processing, which allowed us to implement sophisticated linguistic analysis on large-scale databases while maintaining adequate efficiency.
The results of retrieval experiments on a large-scale Japanese test collection showed that the combination of enhanced phrase identification using patterns defined over part-of-speech tags and our algorithms <i>
Phrase2</i> and <i>Phrase5</i> made a significant positive contribution to retrieval effectiveness.
The paper also discusses indexing and phrase processing of Japanese or East Asian languages.

Tracking a variety of traceability links between artifacts assists software developers in comprehension, efficient development, and effective management of a system.
Traceability systems to date based on various Information Retrieval (IR) techniques have been faced with a major open research challenge: how to extract these links with both high precision and high recall.
In this paper we describe an experimental approach that combines Regular Expression, Key Phrases, and Clustering with IR techniques to enhance the performance of IR for traceability link recovery between documents and source code.
Our preliminary experimental results show that our combination technique improves the performance of IR, increases the precision of retrieved links, and recovers more true links than IR alone.

In any information retrieval system, retrieval is based on some formal model.
Apart from a few non-classical models (which are relatively newer every other model ultimately relies on two basic models: vector space, and probabilistic (two early and classical models
Hence the vector space and probabilistic models are of a fundamental importance, and thus a unified formal definition for them would allow for working out a unified, coherent and consistent formal framework (mathematical theory as a foundation, for IR.
The paper shows that such foundations can be elaborated.

PARIS (Personal Archiving and Retrieving Image System) is an experiment personal photograph library, which includes more than 80,000 of consumer photographs accumulated within a duration of approximately five years, metadata based on our proposed MPEG-7 annotation architecture, Dozen Dimensional Digital Content (DDDC and a relational database structure.
The DDDC architecture is specially designed for facilitating the managing, browsing and retrieving of personal digital photograph collections.
In annotating process, we also utilize a proposed Spatial and Temporal Ontology (STO) designed based on the general characteristic of personal photograph collections.
This paper explains PRAIS system.
Keywords— Ontology, Databases and Information Retrieval, MPEG-7, Spatial-Temporal, Digital Library Designs
l, metadata, Semantic Web, semi-automatic annotation

We consider information retrieval in a wireless sensor network deployed to monitor a spatially correlated random field.
We address sensor scheduling in each data collection under the performance measure of network lifetime.
We formulate this problem as an energy constrained coverage problem and proposal a scheduling algorithm based on a greedy approach.
In the proposed algorithm, we consider the impact of both the network geometry and the energy consumption by sensors to the network lifetime.
Numerical examples are carried out to demonstrate the performance of the proposed algorithm.

Informatics, the applied use of information science, is undergoing definition and development in the health professions with the encouragement of the National Library of Medicine and other professional organizations.
Medical informatics and disciplinary subsets such as nursing and dental informatics look to optimize a growing range of applications ranging from video-disks to expert systems.
Recommendations call for the integration of informatics into the education of health care professionals.
Academic health sciences centers are developing informatics programs; the University of Maryland Campus for the Professions has convened a task force and a national advisory board on informatics.

In this paper, we present a new approach based on knowledge medium using associative representation as a framework of information representation to gather raw information from vast information sources and to integrate it into information bases cost-e ectively.
We then present a knowledge media information base system called CM-2 which provides users with a means of accumulating, sharing, exploring and re ning conceptually diverse information gathered from vast information sources.
We describe the system's four major facilities a) an information capture facility b) an information integration facility
d) an information retrieval facility and (d)
an information re nement facility.
We discuss the strength and weakness of our approach by analyzing results of experiments.
keywords: associative representation, knowledge media, knowledge media system, CM-2, information base

Some cases of Thai language Co-reference analysis can be solved by using methods used in other languages, but some cases cannot.
This paper will thus suggest a Centering method, which will increase efficiency in analyzing the Thai language.
This method has been tested out with various sample documents and has been proved to be successful for the purpose stated.

Information retrieval systems have traditionally been evaluated over absolute judgments of relevance: each document is judged for relevance on its own, independent of other documents that may be on topic.
We hypothesize that preference judgments of the form “
document A is more relevant than document B” are easier for assessors to make than absolute judgments, and provide evidence for our hypothesis through a study with assessors.
We then investigate methods to evaluate search engines using preference judgments.
Furthermore, we show that by using inferences and clever selection of pairs to judge, we need not compare all pairs of documents in order to apply evaluation methods.

The evaluation of an implication by Imaging is a logical technique developed in the framework of the modal logic.
Its interpretation in the context of a \possible worlds" semantics is very appealing for IR.
In 1989, Van Rijsbergen suggested its use for solving one of the fundamental problems of IR logical models: the evaluation of the implication d!
(where d and q are respectively a document and a query representation Since then, others have tried to follow that suggestion proposing models and applications, though without much success.
Most of these approaches had as their basic assumption the consideration that \a document is a possible world We propose instead an approach based on a completely di erent assumption a term is a possible world
This approach enables the exploitation of term{term relationships which are estimated using an information theoretic measure.

We propose a search method for detecting a query audio signal fragment in long audio recordings.
The query signal is assumed to be captured by a portable terminal, such as a cellular phone, in the real world.
A major problem in this kind of search is that the features of the query sound may include distortions due to terminal characteristics or environment noise.
The method proposed here comprises local time-frequency-region normalization and robust subspace spanning.
The former is used to make features invariant to additive noise and frequency characteristics, and the latter to choose frequency bands that minimize the effect of feature distortions.
Experiments using cellular phones in the real world show the proposed method is effective.

Collaboratively created online encyclopedias have become increasingly popular.
Especially in terms of completeness they have begun to surpass their printed counterparts.
Two German publishers of traditional encyclopedias have reacted to this challenge and decided to merge their corpora to create a single more complete encyclopedia.
The crucial step in this merge process is the alignment of articles.
We have developed a system to identify corresponding entries from different encyclopedic corpora.
The base of our system is the alignment algorithm which incorporates various techniques developed in the field of information retrieval.
We have evaluated the system on four real-world encyclopedias with a ground truth provided by domain experts.
A combination of weighting and ranking techniques has been found to deliver a satisfying performance.

Current e-Book browsers provide minimal support for comprehending the organization, narrative structure, and themes, of large complex books.
In order to build an understanding of such books, readers should be provided with user interfaces that present, and relate, the organizational, narrative and thematic structures.
We propose adapting information retrieval techniques for the purpose of discovering these structures, and sketch three distinctive visualizations for presenting these structures to the e-Book reader.
These visualizations are presented within an initial design for an e-Book browser.

Natural Language Generation (NLG) focuses on the generation of written texts in natural language from some underlying semantic representation of information.
A new semantic representation called Rich Semantic Graph (RSG) has been proposed to be used as an intermediate representation during recent research for Natural Language processing applications.
In this paper, a new model to generate an English text from RSG is proposed.
The proposed model can be exploited in Text Summarization, Machine Translation and Information Retrieval applications.
In this model, WordNet ontology is used to generate multiple texts according to the word synonyms.
Also, the model enables users to determine the output text style by selecting one of two writing styles (Cause-Effect and Description-Narration
Finally, the model evaluates the generated texts to rank them based on two criteria: most frequently used words and discourse sentence relations.

We present and analyze the results of the MATHWEBSEARCH (MWS) system in the Math-2 task in the NTCIR-11 Information Retrieval challenge.
MWS is a content-based full-text search engine that focuses on low-latency query answering for interactive applications.
It combines a powerful exact formula unification/matching with the fulltext search capabilities of ElasticSearch to achieve simultaneous full-text search for mathematical/technical documents.
MWS 1.0 focuses on scalability (memory footprint, index persistence integration of keywordand formula search, and hit presentation issues.
It forms a stable basis for future research into extended query languages and user-interaction issues.
The system has been integrated into high-profile information systems like Zentralblatt Math.
In this paper we describe MWS 1.0, evaluate the submission and results for the NTCIR-11 Math-2 Task and conclude with future work suggested by the task results.

The Web pages nowadays were written in various languages including English, Chinese, Spanish, etc.
There are increasing needs in searching Web pages of different languages using single query.
This task is called multilingual information retrieval (MLIR
However, MLIR is difficult to achieve since we need some kind of method to find the associations between linguistic elements of different languages.
In this work, we provide a method based on GHSOM to discover the associations between different languages and apply this method on MLIR task.
The experiments show that our method provide a promising approach to tackle MLIR task.

Much research in information retrieval (IR) focuses on optimization of the rank of relevant retrieval results for single shot ad hoc IR tasks.
Relatively little research has been carried out on user engagement to support more complex search tasks.
We seek to improve user engagement for IR tasks by providing richer representation of retrieved information.
It is our expectation that this strategy will promote implicit learning within search activities.
Specifically, we plan to explore methods of finding semantic concepts within retrieved documents, with the objective of creating improved document surrogates.
Further, we would like to study search effectiveness in terms of different facets such as the user's search experience, satisfaction, engagement and learning.
We intend to investigate this in an experimental study, where our richer document representations are compared with the traditional document surrogates for the same user queries.

This paper introduces the first NTCIR Workshop, Aug.30 Sept.1, 1999, which is the first evaluation workshop designed to enhance research in Japanese text retrieval and cross-lingual information retrieval.
The test collection used in the Workshop consists of more than 330,000 documents of English and Japanese.
Twentythree groups from four countries have conducted IR tasks and submitted the search results.
Various approaches were tested and reported at the Workshop.
Finally some thoughts on the future directions are suggested.

With the advent of the Web along with the unprecedented amount of information coming from sources of heterogeneous data, Formal Concept Analysis (FCA) is more useful and practical than ever, because this technology addresses important limitations of the systems that currently support users in their quest for information.
In this paper, we will focus on the unique features of FCA for searching in distributed heterogeneous information.
The development of FCA-based applications for distributed heterogeneous information returns a major gain.

We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.
We demonstrate the application of statistical machine translation techniques to “translate
the phonemic representation of an English name, obtained by using an automatic text-to-speech system, to a sequence of initials and finals, commonly used subword units of pronunciation for Chinese.
We then use another statistical translation model to map the initial/final sequence to Chinese characters.
We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries.

Many search applications involve documents with structure or fields.
Since query terms often are related to specific structural components, mapping queries to fields and assigning weights to those fields is critical for retrieval effectiveness.
Although several field-based retrieval models have been developed, there has not been a formal justification of field weighting.
In this work, we aim to improve the field weighting for structured document retrieval.
We first introduce the notion of field relevance as the generalization of field weights, and discuss how it can be estimated using relevant documents, which effectively implements relevance feedback for field weighting.
We then propose a framework for estimating field relevance based on the combination of several sources.
Evaluation on several structured document collections show that field weighting based on the suggested framework improves retrieval effectiveness significantly.

The Internet and corporate intranets have brought a lot of information.
People usually resort to search engines to find required information.
However, these systems tend to use only one fixed ranking strategy regardless of the contexts.
This poses serious performance problems when characteristics of different users, queries, and text collections are taken into account.
We argue that the ranking strategy should be context specific and we propose a new systematic method that can automatically generate ranking strategies for different contexts based on genetic programming (GP The new method was tested on TREC data and the results are very promising.

Both the Human Computer Interaction and Information Retrieval fields have developed techniques to allow a searcher to find the information they seek quickly.
However, these techniques are designed to augment one's direct-recall memory, where the searcher is actively trying to find information.
Associative memory, in contrast, happens automatically and continuously, triggering memories that relate to the observed world.
This paper presents design techniques and heuristics for building 8220;remembrance agents 8221; applications that watch a user's context and proactively suggest information that may be of use.
General design issues are discussed and illuminated by a description of Margin Notes, an automatic just-in-time information system for the Web.

One of the issues that researchers are interested in electronic markets is how to describe and delivery of goods.
To complete these issues there are so many offers each with specific benefits and defects.
One of these methods is that besides the properties of goods usually available in goods' description, there is another group of properties as descriptive properties and other properties which have not been resided in any other property categories will be set in this category beside each other, and will be searched like a text using available information retrieval algorithms.
In this paper an appropriate method for searching these properties will be offered based on ngram algorithms and search results will be compare users scoring results.

A search system that allows the users to search and find the most interesting software artifacts based on the current context of the user is highly desirable.
This paper sets forth the requirements of contextual search for software engineering.
A context for software engineering is defined by four dimensions in.
A contextual search system is presented to address the requirements.
We conclude that an approach combining NoSQL database, tagging artifacts with special labels and categories, traditional information retrieval, and intelligently designed domain specific ontologies achieves the requirements of contextual and semantic search.
The learning capabilities resulting from such a system are outlined.

We propose a framework for ranking information based on quality, relevance and importance, and argue that a socio-semantic contextual approach that extends topicality can lead to enhanced precision in information retrieval.
We use Topic Maps to implement our framework, and discuss procedures for collecting the pertinent metadata and for calculating the resource ranking.
A fuzzy neural network approach is envisioned to complement the process of manual metadata creation.

The Smart project in automatic text retrieval was started in 1961.
It is the oldest, continuously running research project in information retrieval.
The panel members are all major contributors to the Smart system work.
The discussion covers aspects of the Smart system design and examines the past and future significance of some of the research conducted in the Smart environment.

Clustering techniques have more importance in data mining especially when the data size is very large.
It is widely used in the fields including pattern recognition system, machine learning algorithms, analysis of images, information retrieval and bio-informatics.
Different clustering algorithms are available such as Expectation Maximization (EM Cobweb, FarthestFirst, OPTICS, SimpleKMeans etc.
SimpleKMeans clustering is a simple clustering algorithm.
It partitions n data tuples into k groups such that each entity in the cluster has nearest mean.
This paper is about the implementation of the clustering techniques using WEKA interface.
This paper includes a detailed analysis of various clustering techniques with the different standard online data sets.
Analysis is based on the multiple dimensions which include time to build the model, number of attributes, number of iterations, number of clusters and error rate.

Patrik Waldmann, Gábor Mészáros Birgit Gredler, Christian Fuerst and Johann Sölkner 1 Division of Livestock Sciences, Department of Sustainable Agricultural Systems, University of Natural Resources and Life Sciences, Vienna, Austria 2 Division of Statistics, Department of Computer and Information Science, Linköping University, Linköping, Sweden 3 Qualitas AG, Zug, Switzerland 4 ZuchtData
Dienstleistungen GmbH, Vienna,
Austria *Correspondence: gabor.meszaros@boku.ac.at

A crucial piece of semantic web development is the creation of viable ontology matching approaches to ensure interoperability in a wide range of applications such as information integration and semantic multimedia.
In this paper, a new approach for ontology matching called IROM (Information Retrieval-based Ontology Matching) is presented.
This approach derives the different components of an information retrieval (IR) framework based on the information provided by the input ontologies and supported by ontology similarity measures.
Subsequently, a retrieval algorithm is applied to determine the correspondences between the matched ontologies.
IROM was tested with ontology pairs taken from two resources for reference ontologies, OAEI and FOAM.
The evaluation shows that IROM is competitive with top-ranked matchers on the benchmark test at OAEI campaign of 2009.

The Dagstuhl Seminar 10461 “Schematization in Cartography, Visualization, and Computational Geometry” was held November 14–19, 2010 in Schloss Dagstuhl Leibniz Center for Informatics.
The seminar brought together experts from the areas graph drawing, information visualization, geographic information science, computational geometry, very-large-scale integrated circuit (VLSI) layout, and underground mining.
The aim was to discuss problems that arise when computing the layout of complex networks under angular restrictions (that govern the way in which the network edges are drawn This collection consists of abstracts of three different types of contributions that reflect the different stages of the seminar a) survey talks about the role of schematization in the various communities represented at the seminar b) talks in the open problem and open mic sessions, and (c) introductory talks.

Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity.
However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations.
In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed.
In particular, we focus on the arrangement and aggregation of the relevant results, and show that it is not possible to ensure lists completely consistent.
We develop a measure of consistency based on Average Dynamic Recall and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method.
The results of the MIREX 2005 evaluation are revisited using these alternative ground truths.

The use of MPI in implementing algorithms for Parallel Information Retrieval Systems is outlined.
We include descriptions on methods for Indexing, Search and Update of Inverted Indexes as well as a method for Information Filtering.
In Indexing we describe both local build and distributed build methods.
Our description of Document Search includes that for Term Weighting, Boolean, Proximity and Passage Retrieval Operations.
Document Update issues are centred on how partitioning methods are supported.
We describe the implementation of term selection algorithms for Information Filtering and finally work in progress is outlined.

DTDs have proved important in a variety of areas: transformations between XML and databases, XML storage, XML publishing, consistency analysis of XML specifications, typechecking, and optimization of XML queries.
Much of this work depends on certain assumptions about DTDs, e.g the absence of recursion and non-determinism.
With this comes the need to justify these assumptions against DTDs in the real world.
This paper surveys a number of DTDs collected from the Web, and provides statistics with respect to a variety of criteria commonly discussed in XML resesarch.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report
. MSCIS-02-05.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/17

Salem Chakhar is with the Portsmouth Business School in the Operations Systems Management subject group.
Salem Chakhar has a PhD in Computer Science from the University of Paris-Dauphine (Paris, France an MPhil degree in Computer Science and Modelling from the High School of Management of Tunis (Tunis, Tunisia and a degree in Computer Science from the Faculty of Economics and Management, University of Sfax (Sfax, Tunisia He has published in journals such as International Journal of Geographical Information Science; Computers, Environment and Urban Systems; Information Sciences; Information and Software Technology; European Journal of Operational Research; Environment and Planning B: Planning and Design; and Decision Support Systems.
Salem Chakhar is a member of the International Cartographic Association (ICA) commission on geospatial analysis and modelling.

This paper presents in detail a linguistics study of a journalistic corpus of Malay describing Indonesian terrorism.
The initial raw text was manually annotated for its parts-of-speech.
It is the first corpus of its nature ever established in Malaysia.
The objective of this research is to conduct an empirical analysis of the actual patterns of use in journalistic texts.
This paper presents the characteristics of Malay terrorism corpus which include the properties, word classes, named entities and word occurrences.
The results of this work are given purely in terms of the characteristics of a Malay terrorism corpus.
The results are highly useful for solving larger tasks in the Natural Language Processing area, such as Information Retrieval and Information Extraction, in the area of terrorism.

In previous work, we have shown that using terms from around citations in citing papers to index the cited paper, in addition to the cited paper's own terms, can improve retrieval effectiveness.
Now, we investigate how to select text from around the citations in order to extract good index terms.
We compare the retrieval effectiveness that results from a range of contexts around the citations, including no context, the entire citing paper, some fixed windows and several variations with linguistic motivations.
We conclude with an analysis of the benefits of more complex, linguistically motivated methods for extracting citation index terms, over using a fixed window of terms.
We speculate that there might be some advantage to using computational linguistic techniques for this task.

Optimization is an important and critical step in the data mining process and it has a huge impact on the success of a data mining process.
Selecting a set of feature which is optimal for a given task is a problem which plays an important role in a wide variety of context including pattern recognition, adaptive control and machine learning Clusters are formed of the reduced dataset using Swarm Intelligence Technique algorithms i.e. Particle Swarm Optimization(PSO),Ant Colony Optimization(ACO),Cluster Hypothesis is verified which is the intra cluster distance should be minimum and inter cluster distance should be maximum.
Most relevant documents are stored i+n the clusters An Information Retrieval System is used for retrieval of data from the clusters.
When user enters a query from a Graphical User Interface, using Information Retrieval algorithm the document is searched and retrieved from the clusters.
It is then given as an output to the user

The string matching algorithms have broad applications in many areas of computer sciences.
These areas include operating systems, information retrieval, editors, Internet searching engines, security applications and biological applications.
Two important factors used to evaluate the performance of the sequential string matching algorithms are number of attempts and total number of character comparisons during the matching process.
This research proposes to integrate the good properties of three single string matching algorithms, Quick-Search, Zuh-Takaoka and Horspool, to produce hybrid string matching algorithm called Maximum-Shift algorithm.
Three datasets are used to test the proposed algorithm, which are, DNA, Protein sequence and English text.
The hybrid algorithm, Maximum-Shift, shows efficient results compared to four string matching algorithms, Quick-Search, Horspool, Smith and Berry-Ravindran, in terms of the number of attempts and the total number of character comparisons.

Five different classification models, namely RFR_SUM, CRFs, Maximum Entropy, SVM and Semantic Similarity Model, are employed for polyphonic disambiguation.
Based on observation of the experiment outcome of these models, an additional ensemble method based on majority voting is proposed.
The ensemble method obtains an average precision of 96.78&#x025 which is much better than the results obtained in previous literatures.

s on Human Factors in Computing Systems (pp.
4585–4590 New York: ACM Press.
Buchanan, T Paine, C Joinson, A.N Reips, U 2007 Development of measures of online privacy concern and protection for use on the Internet.
Journal of the American Society for Information Science and Technology,

We describe a principled method for representing documents by phrases abstracted into Head/Modifier pairs.
First the notion of aboutness and the characterization of full-text documents by HM pairs is didcussed.
Based on linguistic arguments, a taxonomy of HM pairs is derived.
We briefly describe the EP4IR parser/transducer of English and present some statistics of the distribution of HM pairs in newspaper text.
Based on the HM pairs generated, a new technique to measure the accuracy of a parser is introduced, and applied to the EP4IR grammar of English.
Finally we discuss the merits of HM pairs and HM trees as a document representation.

The melodic similarity is an important concept to consider in music information retrieval.
Among the possible applications a number of content-based systems may be developed for copyright management, plagiarism detection, computer-aided composition, etc and the intervallic analysis is an essential tool for these applications.
There exist several techniques exposed in melodic similarity that use diverse statistical and probabilistic analysis.
The objective in this work is to establish a text words equivalent to musical notation using three representations based on simple intervallic relationships, rhythmics and durations, and to evaluate a textual information retrieval technique applied to these representations, as well as to propose changes to improve the system precision and recall

The CLEF 2010 Conference on Multilingual and Multimodal Information Access Evaluation was held at the University of Padua, Italy, September 20–23, 2010.
CLEF 2010 was organized by the Information Management Systems (IMS) research group of the Department of Information Engineering (DEI) of the University of Padua, Italy.
Since 2000, Cross-Language Evaluation Forum (CLEF) has played a leading role in stimulating research into a wide range of key areas in the information retrieval domain, including cross-language question answering, image and video search, and interactive retrieval.
It has also promoted the study and implementation of evaluation methodologies for diverse IR tasks and media.
As a result, CLEF has been extremely successful in building a strong, multidisciplinary

In the last years, Learning to Rank (LtR) had a significant influence on several tasks in the Information Retrieval field, with large research efforts coming both from the academia and the industry.
Indeed, efficiency requirements must be fulfilled in order to make an effective research product deployable within an industrial environment.
The evaluation of a model can be too expensive due to its size, the features used and several other factors.
This tutorial discusses the recent solutions that allow to build an effective ranking model that satisfies temporal budget constrains at evaluation time.

Chinese word segmentation ambiguity can be divided into two categories: overlapped ambiguity and combinational ambiguity.
This paper only focuses on the resolution to combinational ambiguity of Chinese word segmentation.
We select 36 typical combinational ambiguity strings, and make use of transformation-based learning methods to learn the rules of combinational ambiguity.
Using these rules to test "People's Daily" Corpus of 1996, we find that the average precision rate is improved from 79.08&#x025; to 94.35&#x025;.

This article reports on the efforts to establish a research project on a geospatial search engine for the Latin American country of Honduras during the author’s research stay at a local university.
Honduras is an interesting example of the challenges for information and knowledge management in developing countries as it combines many of the issues that are otherwise encountered more isolated or with less impact.
These include low Web coverage in a low-resource country with limited Web infrastructure, and generally, work in challenging circumstances.
The specific focus on geospatial information uncovers further issues that need to be addressed, such as informal addressing schemes or landmark-oriented location references, broad or incorrect locations for places, or insufficient ground truth in databases.
While the tangible results of the project stay behind the original goals, several interesting results were achieved, which are condensed here as an experience report.

Almost the entire life of society (and of an individual too) is determined by needs and seeking methods to satisfy them.
As has long been clear, civilization has witnessed perpetual development of individual needs that become ever more diversified and complicated.
The character and structure of human demands are fundamentally connected with the nature of information space for the social world and with the progress of informatics.
Prospects for the sustainable development of society are internally dependent on the development of reasonable needs for the social world and the establishment of the information society.

A private information retrieval (PIR) scheme on coded storage systems with colluding, byzantine, and nonresponsive servers is presented.
Furthermore, the scheme can also be used for symmetric PIR in the same setting.
An explicit scheme using an [n, k] generalized Reed-Solomon storage code is designed, protecting against t-collusion and handling up to b byzantine and r non-responsive servers, when n n ν +1)k+ t+2b+ r− 1, for some integer ν 1.
This scheme achieves a PIR rate of 1− k+2b+t+r−1
In the case where the capacity is known, namely when k 1, it is asymptotically capacity achieving as the number of files grows.

In this paper we describe our participation in the INEX 2012 Tweet Contextualization track and present our contributions.
We combined Information Retrieval, Automatic Summarization and Topic Modeling techniques to provide the context of each tweet.
We first formulate a specific query using hashtags and important words in the Tweets to retrieve the most relevant Wikipedia articles.
Then, we segment the articles into sentences and compute several measures for each sentence, in order to estimate their contextual relevance to the topics expressed by the Tweets.
Finally, the best scored sentences are used to form the context.
Official results suggest that our methods performed very well compared to other participants.

Over the past decade, information retrieval has emerged as an active research area in the application of fuzzy set theory.
Fuzzy information retrieval utilizes fuzzy sets to represent documents, membership degrees for query term relevance, fuzzy logical operators to define queries, and fuzzy compatibility measures to assess the retrieval status value of a document.
This paper presents an overview of fuzzy relational databases and fuzzy information retrieval.
A general description of the main components of fuzzy information retrieval are given: document representation, query representation, computer-aided query formulation, document retrieval status, and performance measures.
Examples of areas currently being researched are provided.
The relation between fuzzy information retrieval and fuzzy relational databases is examined.

Cross-domain recommendation is an emerging research topic.
In the last few years an increasing amount of work has been published in various areas related to the Recommender System field, namely User Modeling, Information Retrieval, Knowledge Management, and Machine Learning.
The problem has thus been addressed from distinct perspectives.
Hence there are even conflicting definitions of the cross-domain recommendation task, and there is no rigorous comparison of existing approaches.
In this paper we provide a formal statement of the problem, and present a review of the state of the art.
We also establish a general taxonomy that let us to better characterize, categorize and compare the revised work.
Finally, we conclude this review with a survey of interesting research topics on cross-domain recommendation.

Evaluation of information retrieval systems should be based on measures of the information provided by the retrieval process 8220;informativeness&#8221; measures which take into account the interactive and full-text nature of present-day systems and the different types of questions which are asked of them.
Desirable properties for an informativeness measure are developed, including context sensitivity, user centrality, and logarithmic response.
A hypergraph-based framework for measuring the informativeness of a retrieval process is presented and a measure developed which satisfies the desired properties.
The measure is compared to previously developed information measures and illustrated via an application.

The paper presents our design of a next generation information retrieval system based on tag co-occurrences and subsequent clustering.
We help users getting access to digital data through information visualization in the form of tag clusters.
Current problems like the absence of interactivity and semantics between tags or the difficulty of adding additional search arguments are solved.
In the evaluation, based upon SERVQUAL and IT systems quality indicators, we found out that tag clusters are perceived as more useful than tag clouds, are much more trustworthy, and are more enjoyable to use.

In this paper, we present an approach to the incorporation of object versioning into a distributed full-text information retrieval system.
We propose an implementation based on 8220;partially versioned&#8221; index sets, arguing that its space overhead and query-time performance make it suitable for full-text IR, with its heavy dependence on inverted indexing.
We develop algorithms for computing both historical queries and time range queries and show how these algorithms can be applied to a number of problems in distributed information management, such as data replication, caching, transactional consistency, and hybrid media repositories.

The formalism of quantum physics is said to provide a sound basis for building a principled information retrieval framework.
Such a framework is based on the notion of information need vector spaces, where events, such as document relevance, correspond to subspaces, and user information needs are represented as weighted sets of vectors.
In this paper, we look at possible ways to build, using an algebra defined over weighted sets, sophisticated query representations and report on corresponding experiments on TREC.

Nowadays we enter the Web 2.0 era where people’s participation is a key principle.
In this context, collective annotations enable to share and discuss readers’ feedback with regard to digital documents.
The results of this activity are going to be used in the Information Retrieval context, which already tends to harness similar collective contributions.
In this paper, we propose a collective annotation model supporting feedback exchange through discussion threads.
Considering this model, we associate annotations with a measure of the sparked consensus degree (social validation this allows to provide a synthesized view of associated discussions.
Finally, we investigate how Information Retrieval systems may benefit from the proposed model, thus taking advantage of human-contributed highly value-added information, namely collective annotations.

Internet has become a ubiquitous access to information.
Mobile smart phones are becoming popular today as well in the area of interactive information retrieval.
Many applications are developed on these mobile intelligent machines.
This paper presents the design and implementation of a seamless distributed product access information system that can be accessed through Internet and/or Google Android smart phones.
The system uses a hybrid approach of master-slave and peer-to-peer communication models.
This system may help to increase business activities in an area because consumers can access to more information about products that they are interested to purchase anywhere with Internet connection and any time.
In addition, these customers are able to know about shops that sell a particular product, including the price and the shops addresses.
This might help to save their shops hunting time.
The main purpose of the administration system is to help the administrator manage the system.

10 On Processing Nested Queries in Distributed Object-Oriented Database Systems Wang-Chien Lee Department of Computer and Information Science
The Ohio State University Columbus, Ohio 43210-1277, USA wlee@cis.ohio-state.edu, FAX: 614-292-2911
Dik Lun Lee* Department of Computer Science Hong Kong University of Science and Technology Clear Water Bay, Hong Kong dlee@cis.ohio-state.edu, FAX: 614-292-2911 Abstract In this paper, we discuss nested query processing in a distributed object-oriented database system.
We present three query processing strategies to exploit parallelism in a distributed environment.
Then, we review three access methods designed for centralized systems and discuss how they can be applied to the distributed environment.
Heuristics for selecting attributes for indexing and determining where to store the indexes are also presented.
Finally, a replication strategy is proposed for the path dictionary method.

SRS is a widely used system for integrating biological databases.
Currently, SRS relies only on locally provided copies of these databases.
In this paper we propose a mechanism that also allows the seamless integration of remote databases.
To this end, our proposed mechanism splits the existing SRS functionality into two components and adds a third component that enables us to employ peer-to-peer computing techniques to create optimized overlay-networks within which database queries can efficiently be routed.
As an additional benefit, this mechanism also reduces the administration effort that would be needed with a conventional approach using replicated databases.
Index terms Biological Information Retrieval, SRS, Overlay-network Formation, Peer-to-peer Computing

The structural aspects of the unified XML view are rigid enough to support data retrieval (DR) queries as known from database systems.
Over the past few years, increasingly powerful query language, most notably the recent XQuery standard [2 have exploited this fact to provide expressive DR query capabilities for XML.
On the other hand, XML’s structural aspects are transparent enough to treat arbitrary parts of the XML-represented data as documents.
Document or information retrieval (IR) providing one of the most important capabilities for querying text-rich documents, however, is not supported by XQuery or any of the earlier XML query languages so far.

We describe our approach and results towards the genre tagging task of MediaEval 2011.
We approached this as an Information Retrieval task and applied a pseudo relevance feedback (PRF) approach for query expansion.
Query expansion was also done using WordNet and Wikipedia

Information Retrieval Systems (IRSs) based on an ordinal fuzzy linguistic approach present some problems of loss of precision and information when working with discrete linguistic expression domains or when applying approximation operations in the symbolic aggregation methods.
In this paper, we present an IRS based on a 2tuple fuzzy linguistic approach which allows us to overcome the problems of ordinal fuzzy linguistic IRSs and improve the performance.

We have developed computer technologies for a system that extracts domain speci c knowledge from human written biological papers.
This system consists of two components, Information Retrieval (IR) and Information Extraction (IE
We propose a query modi cation method using automatically constructed thesaurus for IR and a statistical keyword prediction method for IE.
Although by a purely statistical model with no heuristics, the experimental result has shown the good performance.

can be found at: Progress in Physical Geography Additional services and information for http ppg.sagepub.com/cgi/alerts  Email Alerts: http ppg.sagepub.com/subscriptions  Subscriptions: http
www.sagepub.com/journalsReprints.nav
Reprints: http www.sagepub.com/journalsPermissions.nav Permissions: http ppg.sagepub.com/cgi/content/refs/25/1/111 SAGE Journals Online and HighWire Press platforms
this article cites 11 articles hosted on the Citations

In recent years, case-based reasoning researchers have started to address tasks that have traditionally been coped with by the Information Retrieval community, namely the handling of textual documents.
When considering the roots of CBR, this development is not surprising at all:
As discussed already in Chapter 1, the fundamental idea of this problem solving paradigm is to collect experiences from earlier problem solving episodes and to explicitly reuse these for dealing with new tasks.
In real life, many of the most valuable experiences are stored as textual documents, such as:

The ability of a user to understand a document would seem to be an critical aspect of that document’s relevance, and yet a document’s reading difficulty is a factor that has typically been ignored in information retrieval systems.
In this position paper we advocate for incorporating estimates of reading proficiency of users, and reading difficulty of documents, into retrieval models, representations for learning algorithms, and large-scale analyses of information retrieval systems and users, particularly for Web search.
We describe key research problems such as estimating user proficiency, estimating document difficulty, and re-ranking, and summarize some potential future extensions that could exploit this new type of meta-data.

In the Web 2.0, where everyone is the creator of content, information spreads and evolves rapidly through unpredictable paths of rebounds between news sources and Social Media.
In this context, modeling, analyzing and tracking the information evolution through time offers unprecedented opportunities to diverse research fields, including Information Retrieval.
In this paper we propose a synthetic analysis of the state-of-art on Information Evolution on the Web, and we summarize the interesting opportunities it offers to Information Retrieval.

The potentials of formal concept analysis (FCA) for information retrieval (IR) have been highlighted by a number of research studies since its inception.
The growth of the web has favoured the emergence of new search applications.
In this paper, we will focus on the unique features of FCA for searching in distributed information and for reducing the size of the set information.
The development of a FCA-based applications for distributed information returns a major gain and the obtained results are promising.
This study has several perspectives for real and fuzzy data.

Beim Information Retrieval ist in Anbetracht der Informationsflut entscheidend, relevante Informationen zu finden.
Ein vielversprechender Ansatz liegt
im semantischen Web, wobei dem System die Bedeutung von Informationen ontologiebasiert beigebracht wird.
Sucht der Benutzer nach Stichworten, werden ihm anhand der Ontologie verwandte Begriffe angezeigt, und er kann mittels Mensch-Maschine-Interaktion seine relevanten Informationen extrahieren.
Um eine solche Interaktion zu fördern, werden die Ergebnisse visuell aufbereitet.
Dabei liegt der Mehrwert darin, dass der Benutzer anstelle von Tausenden von Suchresultaten in einer fast endlosen Liste ein kartografisch visualisiertes Suchresultat geliefert bekommt.
Dabei hilft die Visualisierung, unvorhergesehene Beziehungen zu entdecken und zu erforschen.

In information retrieval, how to analyze correctly and express exactly user&#x02019;s information need is the key to improve the precision of information retrieval.
This paper proposes a model called bound of information need based on an approximation space.
The model can measure user&#x02019;s information need domain.
The paper also introduces two kinds of bound models: bound with query and bound with no query.
The experiments show that the models have good results.

Ranking is an essential component for a number of tasks, such as information retrieval and collaborative filtering.
It is often the case that the underlying task attempts to maximize some evaluation metric, such as mean average precision, over rankings.
Most past work on learning how to rank has focused on likelihoodor margin-based approaches.
In this work we explore directly maximizing rank-based metrics, which are a family of metrics that only depend on the order of ranked items.
This allows us to maximize different metrics for the same training data.
We show how the parameter space of linear scoring functions can be reduced to a multinomial manifold.
Parameter estimation is accomplished by optimizing the evaluation metric over the manifold.
Results from ad hoc information retrieval are given that show our model yields significant improvements in effectiveness over other approaches.

Latent semantic indexing (LSI) is a well-known unsupervised approach for dimensionality reduction in information retrieval.
However if the output information (i.e. category labels) is available, it is often beneficial to derive the indexing not only based on the inputs but also on the target values in the training data set.
This is of particular importance in applications with <i>multiple labels</i in which each document can belong to several categories simultaneously.
In this paper we introduce the multi-label informed latent semantic indexing (MLSI) algorithm which preserves the information of inputs and meanwhile captures the correlations between the multiple outputs.
The recovered "latent semantics" thus incorporate the human-annotated category information and can be used to greatly improve the prediction accuracy.
Empirical study based on two data sets, Reuters-21578 and RCV1, demonstrates very encouraging results.

The growth of the Web and other Big Data sources lead to important performance problems for large-scale and distributed information retrieval systems.
The scalability and efficiency of such information retrieval systems have an impact on their effectiveness, eventually affecting the experience of their users and monetization as well.
The LSDS-IR'15 workshop will provide space for researchers to discuss the existing performance problems in the context of large-scale and distributed information retrieval systems and define new research directions in the modern Big Data era.
The workshop expects to bring together information retrieval practitioners from the industry, as well as academic researchers concerned with any aspect of large-scale and distributed information retrieval systems.

Web crawls provide valuable snapshots of the Web which enable a wide variety of research, be it distributional analysis to characterize Web properties or use of language, content analysis in social science, or Information Retrieval (IR) research to develop and evaluate effective search algorithms.
While many English-centric Web crawls exist, existing public Arabic Web crawls are quite limited, limiting research and development.
To remedy this, we present ArabicWeb16, a new public Web crawl of roughly 150M Arabic Web pages with significant coverage of dialectal Arabic as well as Modern Standard Arabic.
For IR researchers, we expect ArabicWeb16 to support various research areas: ad-hoc search, question answering, filtering, cross-dialect search, dialect detection, entity search, blog search, and spam detection.
Combined use with a separate Arabic Twitter dataset we are also collecting may provide further value.

This paper investigates factors influencing user satisfaction in information retrieval.
It is evident from this study that user satisfaction is a subjective variable which can be influenced by several factors such as system effectiveness, user effectiveness, user effort and user characteristics and expectations.
Therefore, information retrieval evaluators should consider all these factors in obtaining user satisfaction and in using it as a criterion of system effectiveness.
Previous studies have conflicting conclusion on the relationship between user satisfaction and system effectiveness, this study has substantiated this relationship and supports using user satisfaction as a criterion of system effectiveness.

We review some recent research topics by Laboratory for Cognitive Modeling (LKM) at Faculty of Computer and Information Science, University of Ljubljana, Slovenia.
Classification and regression models, either automatically generated from data by machine learning algorithms, or manually encoded with the help of domain experts, are daily used to predict the labels of new instances.
Each such individual prediction, in order to be accepted/trusted by users, should be accompanied by an explanation of the prediction as well as by an estimate of its reliability.
In LKM we have recently developed a general methodology for explaining individual predictions as well as for estimating their reliability.
Both, explanation and reliability estimation are general techniques, independent of the underlying model and provide on-line (effective and efficient) support to the users of prediction models.

The development of both musicologically based and efficient music information retrieval metrics to query large music database is crucial in modern music information retrieval, knowledge management and database research.
Graph spectral representation of pitch class sequences has proved to outperform other pitch class based melodic similarity methods
Here we compare different spectral approaches to structural queries in databases of symbolic music, which exploits mathematical music theory results to improve the descriptive power of representative graphs
In particular, we explore graph representation of other relevant music features like intervals.
The experiments have been conducted on a subset of the RISM collection, and results have been evaluated against a ground truth for the same collection developed for the MIREX competition.

Audio key finding is an integral step in content-based music indexing and retrieval.
In this paper, we present a system that combines ensemble learning with an existing model-based key finding algorithm: the Fuzzy Analysis Center of Effect Generator algorithm.
We demonstrate the manner in which AdaBoost improves the accuracy of FACEG using a dataset containing 2785 audio excerpts of real performances composed by Bach and Mozart.
Two sets of experiments were conducted: intra-system comparison examining the effect of different settings in FACEG/AdaBoost, and inter-system comparison comparing FACEG/AdaBoost with the key finding implementation in Music Information Retrieval (MIR) toolbox.
When FACEG is executed to generate keys at multiple stopping points of the excerpt, AdaBoost with multi-views of tonal information improves key detection accuracy up to 35% on the challenging dataset and up to 21% on the entire dataset.

Looking for ontology in a search engine, one can find so many different approaches that it can be difficult to understand which field of research the subject belongs to and how it can be useful.
The term ontology is employed within philosophy, computer science, and information science with different meanings.
To take advantage of what ontology theories have to offer, one should understand what they address and where they come from.
In information science, except for a few papers, there is no initiative toward clarifying what ontology really is and the connections that it fosters among different research fields.
This article provides such a clarification.
We begin by revisiting the meaning of the term in its original field, philosophy, to reach its current use in other research fields.
We advocate that ontology is a genuine and relevant subject of research in information science.
Finally, we conclude by offering our view of the opportunities for interdisciplinary research.

Relevance judgments are often the most expensive part of information retrieval evaluation, and techniques for comparing retrieval systems using fewer relevance judgments have received significant attention in recent years.
This paper proposes a novel system comparison method using an expectationmaximization algorithm.
In the expectation step, real-valued pseudo-judgments are estimated from a set of system results.
In the maximization step, new system weights are learned from a combination of a limited number of actual human judgments and system pseudo-judgments for the other documents.
The method can work without any human judgments, and is able to improve its accuracy by incrementally adding human judgments.
Experiments using TREC Ad Hoc collections demonstrate strong correlations with system rankings using pooled human judgments, and comparison with existing baselines indicates that the new method achieves the same comparison reliability with fewer human judgments.

There are two important research topics in the field of Music Information Retrieval (MIR One is how to improve the robustness of features and the other is how to speed up the retrieval process.
This paper improved the algorithms which proposed by Shazam company in these two aspects.
We improve the robustness of the system by a new audio finger-printing extraction using computer graphics, and the system can recognize the recordings which get in complex environment accurately.
On the other hand, we propose a recursive search algorithm based on the confidence measure to improve the retrieval speed.
Quantitative analysis of the opposite experiment verifies the improvement in the retrieval speed and accuracy.

Hybrid information retrieval (IR) schemes combine di erent normalization techniques and similarity functions.
Hybrid schemes provide an eÆcient technique to improve precision and recall (see e.g 4 This paper reports a hybrid clustering scheme that applies a singular value decomposition (SVD) based algorithm followed by a k{means type clustering algorithm.
The output of the rst algorithm becomes the input of the next one.
The second algorithm generates the nal partition of the data set.
We report results of numerical experiments performed with three k{means type clustering algorithms.
Those are: the classical k{means (see e.g 9 the spherical k{means (see [7 and the information{ theoretical clustering algorithm introduced recently by [8 and [1 A comparison with the results reported by [7] is provided.

In TREC 2007, we participate in four tasks of the Blog and Enterprise tracks.
We continue experiments using Terrier [14 our modular and scalable Information Retrieval (IR) platform, and the Divergence From Randomness (DFR) framework.
In particular, for the Blog track opinion finding task, we propose a statistical term weighting approach to identify opinionated documents.
An alternative approach based on an opinion identification tool is also utilised.
Overall, a 15% improvement over a non-opinionated baseline is observed in applying the statistical term weighting approach.
In the Expert Search task of the Enterprise track, we investigate the use of proximity between query terms and candidate name occurrences in documents.

This paper deals with Chinese, English and Japanese multilingual information retrieval (MLIR Merging problem in distributed MLIR is studied.
The prediction of retrieval effectiveness is used to determine the merging weight of each intermediate run.
The translation penalty and collection weight are considered to improve merging performance.
Several merging strategies are experimented.
Experimental results show that the performance of normalized-by-top-k merging with translation penalty and collection weight is similar to that of raw-score merging and better than that of the other merging strategies.

To increase retrieval e ectiveness, information retrieval systems must o er better supports to users in their information seeking activities.
To achieve this, one major concern is to obtain a better understanding of the nature of the interaction between a user and an information retrieval system.
For this, we need a means to analyse the interaction in information retrieval, so as to compare the interaction processes within and across information retrieval systems.
We present a framework for investigating the interaction between users and information retrieval systems.
The framework is based on channel theory, a theory of information and its ow, which provides an explicit ontology that can be used to represent any aspect of the interaction process.
The developed framework allows for the investigation of the interaction in information retrieval at the desired level of abstraction.
We use the framework to investigate the interaction in relevance feedback and standard web search.

In this paper we discuss our efforts in Soundscape Information Retrieval (SIR Computational soundscape analysis is a key research component in the Citygram Project which is built on a cyber-physical system that includes a scalable robust sensor network, remote sensing devices (RSD spatio-acoustic visualization formats, as well as software tools for composition and sonification.
By combining our research in soundscape studies, which includes the capture, collection, analysis, visualization and musical applications of spatio-temporal sound, we discuss our current research efforts that aim to contribute towards the development of soundscape information retrieval (SIR This includes discussion of soundscape descriptors, soundscape taxonomy, annotation, and data analytics.
In particular, we discuss one of our focal research agendas in measuring and quantifying urban noise pollution.

In Modern Mongolian, a content word can be inflected when concatenated with suffixes.
Identifying the original forms of content words is crucial for natural language processing and information retrieval.
We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval.
We use technical abstracts to show the effectiveness of our method experimentally.

The World Wide Web is an unregulated communication medium which exhibits very limited means of quality control.
Quality assurance has become a key issue for many information retrieval services on the Internet, e.g. web search engines.
This paper introduces some quality evaluation and assessment methods to assess the quality of web pages.
The proposed quality evaluation mechanisms are based on a set of quality criteria which were extracted from a targeted user survey.
A weighted algorithmic interpretation of the most significant user quoted quality criteria is proposed.
In addition, the paper utilizes machine learning methods to produce a prediction of quality for web pages before they are downloaded.
The set of quality criteria allows us to implement a web search engine with quality ranking schemes, leading to web crawlers which can crawl directly quality web pages.
The proposed approaches produce some very promising results on a sizeable web repository.

Search engines have become the most popular tools for finding information on the Internet.
A real-world Semantic Web application can benefit from this by combining its features with some features from search engines.
In this paper, we describe methods for indexing and searching a populated ontology by using an information retrieval tool; its results are enriched with inference.
For visualization purposes, all of the retrieved ontology instances are clustered based on their classes; and the clusters are linked using instance properties.
The approach is illustrated using our SWHi (Semantic Web for History) prototype as a case study.

This abstract describes the tempo extraction algorithm used for the University of Victoria submission to the MIREX (Music Information Retrieval Exchange) 2005.
The algorithm is mostly based on self-similarity rather than onset detection.
However, an onset detection component is used to calculate the phase of the dominant periodicities.
Multiple frequency bands are calculated using a Discrete Wavelet Transform.
Subsequently the envelope of each band is extracted and autocorrelation is used to find the dominant periodicities of the audio signal.
These dominant periodicities are accumulated into a Beat Histogram which is used to detect the primary and secondary tempo and their relative strength.

In this paper we present a new classification system called ECHO.
This system is based on a principle of echo and applied to document classification.
It computes the score of a document for a class by combining a bottom-up and a top-down propagation of activation in a very simple neural network.
This system bridges a gap between Machine Learning methods and Information Retrieval since the bottom-up and the top-down propagations can be seen as the measures of the specificity and exhaustivity which underlie the models of relevance used in Information Retrieval.
The system has been tested on the Reuters 21578 collection and in the context of an international challenge on large scale hierarchical text classification with corpus extracted from Dmoz and Wikipedia.
Its comparison with other classification systems has shown its efficiency.

In this paper, we explain a novel technique for using vector quantization (VQ) for large scale content based information retrieval of any type of images, not restricted to the compressed domain.
The main problem with VQ is the size of the source vectors that is used to generate the global codebook, which represents all images in the database.
We have proposed a technique, where the local codebooks are generated and the locally global codebook for the images is computed from all the local codebooks.
It gives better image quality than the global codebook.
This incremental codebook generation process makes the index scalable, as new image codebooks can be used to generate the locally global codebook easily.

<i>We present a method of searching text collections that takes advantage of hierarchrical information within documents and integrates searches of structured and unstructured data.
We show that Multidimensional databases (MDB designed for accessing data along hierarchical dimensions, are effective for information retrieval.
We demonstrate a method of using On-Line Analytic Processing (OLAP) techniques on a text collection.
This combines traditional information retrieval and the slicing, dicing, drill-down, and roll-up of OLAP.
We demonstrate use of a prototype for searching documents from the TREC collection

We propose a novel dimensional analysis approach to employing meta information in order to find the relationships within the unstructured or semi-structured document/passages for improving genomics information retrieval performance.
First, we make use of the auxiliary information as three basic dimensions, namely "temporal journal and "author The reference section is treated as a commensurable quantity of the three basic dimensions.
Then, the sample space and subspaces are built up and a set of events are defined to meet the basic requirement of dimensional homogeneity to be commensurable quantities.
After that, the classic graph analysis algorithm in the Web environments is applied on each dimension respectively to calculate the importance of each dimension.
Finally, we integrate all the dimension networks and re-rank the outputs for evaluation.
Our experimental results show the proposed approach is superior and promising.

The authors have developed an information retrieval system named AIR (Augmented Information Retrieval system which might be one of the most efficient systems for very large document databases.
AIR can store the document data compactly and retrieve them quickly.
The techniques bringing AIR to the high efficiency, the data compression, the quick keyword index, and the automatic keyword selection, are discussed.
These techniques, which are based on the statistical properties of word occurrence, are fairly simple, so that the information retrieval systems employing them can be implemented with ease.
The data compression technique reduces English text by a factor of 4.
The quick keyword index decreases the average number of disk accesses to retrieve a keyword to about 0.3.
The automatic keyword selection technique roughly halves both the number of different keywords and the size of the inverted file with only 2% loss of retrieval power.

Continuous space word embedding have been shown to be highly effective in many information retrieval tasks.
Embedding representation models make use of local information available in immediately surrounding words to project nearby context words closer in the embedding space.
With rising multi-tasking nature of web search sessions, users often try to accomplish different tasks in a single search session.
Consequently, the search context gets polluted with queries from different unrelated tasks which renders the context heterogeneous.
In this work, we hypothesize that task information provides better context for IR systems to learn from.
We propose a novel task context embedding architecture to learn representation of queries in low-dimensional space by leveraging their task context information from historical search logs using neural embedding models.
In addition to qualitative analysis, we empirically demonstrate the benefit of leveraging task context to learn query representations.

Muugle (Musical Utrecht University Global Lookup Engine) is a modular framework that allows the comparison of different MIR techniques and usability studies.
A system overview and a discussion of a pilot usability experiment are given.
A demo version of the framework can be found on http give-lab.cs.uu.nl/muugle.

There is an abundance of systems today to search for relevant patents, ranging from free ones like Google Patents (google.com/patents) to subscription ones like Delphion (delphion.com
After studying many existing systems, we found that they all apply general-purpose Information Retrieval (IR) techniques to rank patents.
We argue that the quality of search can be significantly improved by exploiting the domain semantics: e.g patents are organized into classes and subclasses, and have links to external publication and to other patents.
Also patents' text is organized into various sections and uses specific legal wording We present the <i>PatentsSearcher</i> system, available at PatentsSearcher.com, whose key contribution is to leverage the domain semantics to improve the quality of discovery and ranking.
PatentsSearcher also offers other novel functionalities to help users locate and navigate relevant and important patents or applications.

We describe the University of Amsterdam’s participation in the Cross-Lingual Information Retrieval task at NTCIR-5.
We focused on Chinese monolingual retrieval, and aimed to study the effectiveness of language models and different tokenization methods for Chinese.
Our main findings are the following.
First, where the vector space model excels on a bigram index, the language model performs poorly.
Second, on a unigram index, the language model is very effective, and even exceeds the performance of the vector space model on the bigram index.
Third, and at a more technical level, in comparison to word-based langauges such as English we found that language models for Chinese require less smoothing, due to the different indexing unit.

Several studies have pointed out the need for accurate mid-level representations of music signals for information retrieval and signal processing purposes.
In this paper, we propose a new mid-level representation based on the decomposition of a signal into a small number of sound atoms or molecules bearing explicit musical instrument labels.
Each atom is a sum of windowed harmonic sinusoidal partials whose relative amplitudes are specific to one instrument, and each molecule consists of several atoms from the same instrument spanning successive time windows.
We design efficient algorithms to extract the most prominent atoms or molecules and investigate several applications of this representation, including polyphonic instrument recognition and music visualization.

In 1979, the dean of Harvard Medical School, in an address to the Council of Deans of the American Association of Medical Colleges, stated that there were two key areas which needed to be added to the medical school curriculum: medical decision analysis and information science.
It has even been suggested that computer sc science, li should be ii medical schc the past several years, at dozen special programs desi provide physicians and health professionals with advanced computer knowl all over the examine the increased int and computer physicians and school s.
Today' s cannot avoid t office practice a c may be a necessi competitive produ respect to billin insurance claims~ e physician's offlce.
regarding the typl service to use versus in-house) an system to choose is confusing.

Often the best way to adumbrate a dark and dense assemblage of material is to describe the background in contrast to which the edges of the nebulosity may be clearly discerned.
Hence, perhaps the most appropriate way to introduce this paper is to describe what it is not.
It is not a comprehensive study of stochastic processes, nor an in-depth treatment of convergence.
In fact, on the surface, the material covered in this paper is nothing more than a compendium of seemingly loosely-connected and barely-miscible theorems, methods and conclusions from the three main papers surveyed VC71 Pol89] and [DL91 Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-92-30.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/470 Convergence of Stochastic Processes MS-CIS-92-30 GRASP LAB 311

In traditional Information Retrieval (IR users often express their needs using simple keywords, because this is the most simple way to query systems.
Queries composed of simple keyword terms are also very used in structured IR (i.e. IR on structured documents like XML documents since they do not require any knowledge of the documents structure.
However, such queries are often not sufficient to describe precisely users’ needs.
In this paper, we propose a new approach of Structured Relevance Feedback on queries composed of simple keywords terms.
This approach allows on the one hand to enrich the initial query according to relevance judgements, and on the other hand, to express a degree of preference for each keyword, by weighting them.
Our approach is based on a combination of term features in relevant elements.
Our proposition is evaluated thanks to the INEX evaluation campaign and results show the interest of our methods.
MOTS-CLÉS reformulation, document XML, contexte, contenu, RI.

Incorporating digital tools in the business and scientific research workflows is at the moment an on-going process, challenging and demanding as every domain has its own needs in terms of data models and information retrieval methods.
The information in some domains involves entity evolution, a characteristic that introduces additional tasks, such as finding all evolution stages of an entity, and poses additional requirements for the information retrieval process.
In this paper we present a user study aiming to investigate the effectiveness of current ontology browsing and visualization methods for supporting users in tasks involving research on entity evolution.

Learning preferences is a useful tool in application fields like information retrieval, or system configuration.
In this paper we show a new application of this Machine Learning tool, the analysis of sensory data provided by consumer panels.
These data sets collect the ratings given by a set of consumers to the quality or the acceptability of market products that are principally appreciated through sensory impressions.
The aim is to improve the production processes of food industries.
We show how these data sets can not be processed in a useful way by regression methods, since these methods can not deal with some subtleties implicit in the available knowledge.
Using a collection of real world data sets, we illustrate the benefits of our approach, showing that it is possible to obtain useful models to explain the behavior of consumers where regression methods only predict a constant reaction in all consumers, what is useless and unacceptable.

In this work we propose new utility models for the structured information retrieval system Garnata, and expose the results of our participation at INEX’08 in the AdHoc track using this system.

This paper introduces a new approach of query reuse in order to help the user to retrieve relevant information.
Past search experiences are a source of information that can be useful for a user trying to find information answering his information need.
For example, a user searching about a new subject can benefit from past search experiences carried out by previous users about the same subject.
The approach presented in this paper is based on collecting the different search attempts submitted to a search engine by a user trying to fulfil an information need.
This approach takes mainly advantage of implicit links that exist between the different search attempts that try to satisfy a single information need.
Search experiences are modelled according to the concepts defined in the domain of version management.
This modelling provides multiple possibilities to reuse past experiences notably to recommend terms for query reformulation or documents judged relevant by other users.

This paper examines the application and implementation of a computer-based information system designed to record, store, retrieve and analyse archaeological site data for the Brighton area based upon techniques currently being developed in the field of Geographic Information Systems
(GIS The paper discusses the need for spatial data handling capabilities in archaeology at the regional level to satisfy archival, educational and research purposes, and importantly, as a decision-making tool in the management of the historic environment.

The classical Probability Ranking Principle (PRP) forms the theoretical basis for probabilistic Information Retrieval (IR) models, which are dominating IR theory since about 20 years.
However, the assumptions underlying the PRP often do not hold, and its view is too narrow for interactive information retrieval (IIR
In this article, a new theoretical framework for interactive retrieval is proposed:
The basic idea is that during IIR, a user moves between situations.
In each situation, the system presents to the user a list of choices, about which s/he has to decide, and the first positive decision moves the user to a new situation.
Each choice is associated with a number of cost and probability parameters.
Based on these parameters, an optimum ordering of the choices can the derived—the PRP for IIR.
The relationship of this rule to the classical PRP is described, and issues of further research are pointed out.

[1] Mathieu Barthet, David Marston, Chris Baume, György Fazekas, and Mark Sandler.
Design and Evaluation of Semantic Mood Models for Music Recommendation.
International Society for Music Information Retrieval Conference, 2013.
A selected set of 1760 tracks was evaluated, using one-third for testing and 2-fold cross-validation.
Every pair of features were tested before taking the top 12, testing those with every other feature and repeating.
The results show that 32 spectral, harmonic, rhythmic and temporal features are needed for optimum performance, but as the error converges quickly, good performance can be achieved with much fewer.

Spatial data integration and fusion is an important research field in Geographical Information Science.
The key of integration and fusion of spatial data lies in automated recognition and matching of homonymous features.
This article concludes the spatial location matching relationships among the homonymous linear entities.
Linear entity matching algorithm based on multi-steps buffer analysis is presented and accomplished in this article.
Sequence matching strategy is adopted to recognize homonymous linear entities step by step.
At each step, a part of non-matching linear entities are eliminated rapidly, which can increase the match speed effectively.
In the end, a test and analysis have been done.
Two different kinds of navigation data of a same region are used for the test.
This matching method can reach satisfactory matching speed and efficiency.

In this paper, we investigate the consumers’ perception of on-line product search using a questionnaire-based survey.
We identify that the information retrieval activity of the purchase process can be performed with three Web applications: a search engine, a price comparison service, and a Web shop.
The study underlines the need for linked product data as proposed by the Semantic Web.
We argue that linked data will result in easier product search on the Web for the consumer.

Human-Agent Interaction as a speci c area of Human-Computer Interaction is of primary importance for the development of systems that should cooperate with humans.
The ability to learn, i.e to adapt to preferences, abilities and behaviour of a user and to peculiarities of the task at hand, should provide for both a wider range of application and a higher degree of acceptance of agent technology.
In this paper, we discuss the role of Machine Learning as a basic technology for human-agent interaction and motivate the need for interdisciplinary approaches to solve problems related to communication with arti cial agents for task speci cation, teaching, or information retrieval purposes.

Image processing and pattern recognition has become a powerful technique in many areas.
This includes Engineering, Computer Science, Statistics, Information Science, Physics, Chemistry and Medicine.
Anyone who wants to extract data from image or visual project, image processing is required.
Most of the image processing software’s are able to process two dimensional images alone.
This paper implements three basic image processing operations namely enhancement, blur and segmentation.
Each of the operation can be implemented using a variety of algorithms.
The algorithms are implemented using and their performance are compared in this paper.
The proposed work produces better result of 3D images also.

Tnis paper is concernd with information retrieval based upon secondary keys; that is, keys which cannot in general uniquely identify a record, but can indicate certain attributes of the associated record.
Partial-match retrieval deals with accessing and reading those records of a data base which match the user's query albeit the query is only partially specified.
For example, suppose that 'the data base consists of the binary words 1010, 1110, 0011, 1101, 0010, 1111.
The response to query 1**0 where is a don't know symbol is the set of records with keys 1010 or 1110 while the response to query 1101 is the set of records with key 1101.

In this paper, I go through the evolution of the learning environments to justify the need for Virtual Learning Places (VLP I also describe, briefly, the design principle that are inspiring the development of a concrete realization of a VLP LIFE and the open challenge on which we are currently working on: a) the ecological monitoring of the experience and of the experience styles; b) the promotion of a Design literacy.
Categories and Subject description
H.5 INFORMATION INTERFACES AND PRESENTATION.
[User Interfaces H.5.m
[Miscellaneous K.3.1 [Computer and Information Science Education Distance learning; K.3.2 [Computer and Information Science Education Computer science education, Information systems education.

International communication and multitude of information in several languages require information retrieval systems that can cross language borders.
Cross-language information retrieval (CLIR) is defined as the retrieval of documents in another language than the language of the request or query in Anurag Seetha, et al
The language of request is the source language and the language of documents is the target language.
Among several approaches to CLIR, we opted for the method of translating queries for English-Hindi language pair based on a bilingual dictionary.
This paper describes our first phase experiments with the CLIR system.

In this paper, we build a hybrid Web-based metric for computing semantic relatedness between words.
The method exploits page counts, titles, snippets and URLs
returned by a Web search engine.
Our technique uses traditional information retrieval methods and is enhanced by page-count-based similarity scores which are integrated with automatically extracted lexico-synantic patterns from titles, snippets and URLs for all kinds of semantically related words provided by WordNet (synonyms, hypernyms, meronyms, antonyms A support vector machine is used to solve the arising regression problem of word relatedness and the proposed method is evaluated on standard benchmark datasets.
The method achieves an overall correlation of 0.88, which is the highest among other metrics up to date.

Information pertaining to the patent system is scattered not only across the patent domain, but also across other scientific and regulatory domains.
In recent years, there has been an explosive growth in scientific and regulatory documents related to the patent system.
In this project, the use of networks and network analyses is applied to an Information Retrieval (IR) problem in the patent domain.
First a patent citation network is developed and tested.
Second, a text-based semantic network of patents is developed based on a use case “erythropoietin An algorithm is proposed which attempts to identify the important patents in the network.
Keywords-patent, citation, semantic, network, information retrieval, search, similarity.

In the framework of data imputation, this paper provides a non-parametric approach to missing data imputation based on Information Retrieval.
In particular, an incremental procedure based on the iterative use of a tree-based method is proposed and a suitable Incremental Imputation Algorithm is introduced.
The key idea is to define a lexicographic ordering of cases and variables so that conditional mean imputation via binary trees can be performed incrementally.
A simulation study and real world applications are shown to describe the advantages and the good performance with respect to standard approaches1

The automated software system “Black Square Version 1.2 is described.
The system is intended for the automation of image processing, analysis, and recognition.
It is an open system for generating new knowledge: objects, algorithms of image processing, recognition procedures originally not intended for image processing, and methods for solving applied problems.
The system combines the features of information retrieval, reference, training, and computing systems.

A method is presented to filter the output of a word recognition algorithm, which may contain errors, to locate decisions that should be correct with a high degree of certainty.
The algorithm uses the output of a word recognition system and a vector space model for information retrieval to locate a set of documents that have topics which are similar to that of the input document.
The vocabulary from these similar documents is then used to locate the correct word recognition decisions.
Experimental results show that a subset of the word recognitionder;i.
sions for an input document can be ,located that are between 90 and 99 percent correct.
This peljormance was obtained on word recognition results for a sample of text into which a 13, 24, and 30 percent word recognition error rate had been introduced The subset located by this method can be used to drive other recognition processes applied to the rest of the text.

In volume 19(10) of the International Journal of Geographical Information Science, Valavanis et al 2005) proposed a new method sink method’ for identifying oceanic thermal fronts.
Although it is encouraging to see the authors apply geographic information system (GIS) in an oceanographic study, we argue that this proposed method for front detection in the oceans is flawed as it is based on an erroneous definition of an oceanic thermal front.

We present a framework in which probabilistic models for textual and visual information retrieval can be integrated seamlessly.
The framework facilitates searching for imagery using textual descriptions and visual examples simultaneously.
The underlying Language Models for text and Gaussian Mixture Models for images have proven successful in various retrieval tasks.

In this paper, we introduce the technique of co-citation analysis from the field of Library and Information Science.
In addition to describing how this analytic method has been employed in other fields, we explain how document co-citation analysis differs from author co-citation analysis in terms of precision.
We pose three questions for our empirical study of HIT research and describe our document co-citation analysis of citations to HIT research appearing in 20 leading IS journals and eight leading general medicine journals from 2000 to 2010.
We performed co-citation analysis separately for published research in IS and medicine, identifying nine and eight subfields of HIT research, respectively.
We describe the specific subfields in each domain and list sample papers corresponding to them.
Finally, we identify the common attributes of older studies that cause them to be co-cited often, including how such attributes differ between IS vs. medicine.

This paper describes some experiments which use meta-learning to combine families of information retrieval (IR) algorithms obtained by varying the normalizations and similarity functions.
By meta-learning, we mean the following simple idea: a family of IR algorithms is applied to a corpus of documents in which relevance is known to produce a learning set.
A machine learning algorithm is then applied to this data set to produce a classifier which combines the different IR algorithms.
In experiments with TREC-3 data, we could significantly improve precision at the same level of recall with this technique.
Most prior work in this area has focused on combining different IR algorithms with various averaging schemes or has used a fixed combining function.
The combining function in meta-learning is a statistical model itself which in general depends on the document, the query, and the various scores produced by the different component IR algorithms.

This paper describes a new test collection for passage retrieval from multilingual, informal text.
The task being modeled is that of a monolingual English-speaking user who wishes to search discussion forum text in a foreign language.
The system retrieves relevant short passages of text and presents them to the user, translated into English.
The test collection contains more than 2 billion words of discussion thread text, 250 queries representing complex informational search needs, and manual relevance judgments of forum post passages, pooled from real systems.
This information retrieval test collection is the first to combine multilingual search, passage retrieval, and informal online genre text.

Using current systems together and integrate them with new ones is necessary for rapid and optimum development of web based systems.
Increasing the volume and growth rate of Web information, beside of the current developments in Information Retrieval media shows the importance of developing and improvement of these systems.
In this paper, a cheap, simple and quick method for web system integration in order to develop and improve the current systems is presented.
This method is based on the x0022;state machine&#x0022; diagram.
We have developed a search scenario as a sample of web systems integration in information retrieval applications.
This method has been evaluated through several tests such as integration test, replacement test and cut off service test, to approve its performance and reliability to be used for creating other scenarios in Information Retrieval Systems Integration.

This paper describes the World Wide Web Index and Search Engine (WISE) for resource discovery.
The system is designed around a resource database containing meta-information about WWW resources and is automatically built using an indexer robot, a special WWW client agent.
The resource database allows users to search for resources based on keywords, and to learn about potentially relevant resources without having to directly access them.
Such capabilities can signiicantly reduce the amount of time that a user needs to spend in order to nd the information of his/her interest.
The main strength of WISE is its use of advanced information retrieval techniques.
The paper discusses WISE's main components: the resource database, the indexer robot, the search engine, and the user interface, and discusses the issues involved in the design, implementation and evaluation of the system.

—Locating and distilling the valuable relevant information continued to be the major challenges of Information Retrieval (IR) Systems owing to the explosive growth of online web information.
These challenges can be considered the XML Information Retrieval challenges as XML has become a de facto standard over the Web.
The research on XML IR starts with the classical IR strategies customized to XML IR.
Later novel IR strategies specific to XML IR are evolved.
Meanwhile literatures reveal development of the rapid and intelligent IR systems.
Despite their success in their specified constrained domains, they have additional limitations in the complex information space.
The effectiveness of IR systems is thus unsolved in satisfying the most.
This article attemptsan overview of earlier efforts and the gaps in XML IR.

The increasing use of the World Wide Web (WWW or Web) for information retrieval from information systems has raised a demand for Web to database interface building tools.
The requirements for such a building tool is analysed and a framework (WebinTool) for rapid building and easy maintenance of Web interfaces is presented.
The key component of the WebinTool is an interface tool language which mainly consists of the HTML statements and dedicated SQL-based data manipulation statements.
Using this language, a variety of user-customized Web interfaces can be created.

We describe a system intended to help report writers produce summaries of important activities based on weekly interviews with members of a project.
A key element of this system is to learn different user and audience preferences in order to produce tailored summaries.
The system learns desired qualities of summaries based on observation of user selection behavior, and builds a regression-based model using item features as parameters.
The system’s assistance consists of presenting the writer with a successively better ordered list of items from which to choose.
Our evaluation study indicates a significant improvement in average precision (and other metrics) by the end of the learning period as compared to baseline of no learning.
We also describe our ongoing work on automatic feature extraction to make this approach domain independent.

Multiple objective genetic algorithms (MOGA) are heuristics that have seen relatively little use in geographic information science and spatial decision support in comparison to other geocomputational methods, despite their potential to generate robust alternative options for decision making.
MOGA based decision analysis relieves the burden of a priori preference specification by an analyst, but the resulting solution set is potentially large and one must sift through many feasible alternatives in an intelligent manner.
Since the task of evaluating the alternatives can be cognitively difficult there is a need for visual and analytic approach to aid the decision option selection process.
One solution based on such an approach and proposed in this paper is the integration of MOGA with geovisual analytic tools.

Information retrieval is an empirical science; the field cannot move forward unless there are means of evaluating the innovations devised by researchers.
However the methodologies conceived in the early years of IR and used in the campaigns of today are starting to show their age and new research is emerging to understand how to overcome the twin challenges of scale and diversity.
With such challenges in mind it was decided to hold the first Workshop on Novel Methodologies for Evaluation in Information Retrieval.
The workshop was composed of two invited talks as well as long and short papers covering a range of important evaluation methods and tools.
The workshop was chaired by Mark Sanderson; with co-organization from Julio Gonzalo, Nicola Ferro and Martin Braschler.

User modeling is the key element in assisting intelligence analysts to meet the challenge of gathering relevant information from the massive amounts of available data.
We have developed a dynamic user model to predict the analyst’s intent and help the information retrieval application better serve the analyst’s information needs.
In order to justify the effectiveness of our user modeling approach, we have conducted a user evaluation study with actual end user, three working intelligence analysts, and compared our user model enhanced information retrieval system with a commercial off-the-shelf system, the Verity Query Language.
We describe our experimental setup and the specific metrics essential to evaluate user modeling for information retrieval.
The results show that our user modeling approach tracked individual’s interests, adapted to their individual searching strategies, and helped retrieve more relevant documents than the Verity Query Language system.

This paper describes how the bootstrap approach to statistics can be applied to the evaluation of IR effectiveness metrics.
More specifically, we describe straightforward methods for comparing the discriminative power of IR metrics based on Bootstrap Hypothesis Tests.
Unlike the somewhat ad hoc Swap Method proposed by Voorhees and Buckley, our Bootstrap Sensitivity Methods estimate the overall performance difference required to achieve a given confidence level directly from Bootstrap Hypothesis Test results.
We demonstrate the usefulness of our methods using four different data sets (i.e test collections and submitted runs) from the NTCIR CLIR track series for comparing seven IR metrics, including those that can handle graded relevance and those based on the Geometric Mean.
We also show that the Bootstrap Sensitivity results are generally consistent with those based on the more ad hoc methods.

A large volume of visual content is inaccessible until effective and efficient indexing and retrieval of such data is achieved.
In this paper, we introduce the DREAM system, which is a knowledge-assisted semantic-driven context-aware visual information retrieval system applied in the film post production domain.
We mainly focus on the automatic labelling and topic map related aspects of the framework.
The use of the context-related collateral knowledge, represented by a novel probabilistic based visual keyword co-occurrence matrix, had been proven effective via the experiments conducted during system evaluation.
The automatically generated semantic labels were fed into the Topic Map Engine which can automatically construct ontological networks using Topic Maps technology, which dramatically enhances the indexing and retrieval performance of the system towards an even higher semantic level.

This paper presents Russian information retrieval evaluation initiative and results obtained during first year.
In particular, we describe first ROMIP seminar, used Cyrillic Web collection and search tasks as well as ongoing efforts on ROMIP’2004.

THIS BIBLIOGRAPHY was searched in January 1982, and is focused on eight topical areas in selected intersection believed to be of primary interest to membership of the Industrial Electronics Society.
The search was made using the DIALOG Information Retrieval Service on the COMPENDEX database covering the period 1970 through 1981 and the INSPEC database covering the period 1969 through 1981.

Grid infrastructures are in operation around the world, federating an impressive collection of computational resources and a wide variety of application software.
In this context, it is important to establish advanced software discovery services that could help end-users locate software components suitable to their needs.
In this paper, we present the design, architecture and implementation of an open-source keyword-based paradigm for the search of software resources in Grid infrastructures, called Minersoft.
A key goal of Minersoft is to annotate automatically all the software resources with keyword-rich metadata.
Using advanced Information Retrieval techniques, we locate software resources with respect to users queries.
Experiments were conducted in EGEE, one of the largest Grid production services currently in operation.
Results showed that Minersoft successfully crawled 12.3 million valid files (620 GB size) and sustained, in most sites, high crawling rates.

One of the most significant problems which inhibits further developments in the areas of Knowledge Representation and Artificial Intelligence is a problem of semantic alignment or knowledge mapping.
The progress in its solution will be greatly beneficial for further advances of information retrieval, ontology alignment, relevance calculation, text mining, natural language processing etc.
In the paper the concept of multidimensional global knowledge map, elaborated through unsupervised extraction of dependencies from large documents corpus, is proposed.
In addition, the problem of direct Human Knowledge Representation System interface is addressed and a concept of adaptive decoder proposed for the purpose of interaction with previously described unified mapping model.
In combination these two approaches are suggested as basis for a development of a new generation of knowledge representation systems.

A spoken dialogue system of information retrieval on academic documents has been developed with a special attention to reply speech generation.
In order to realize speech reply with its prosodic features properly controlled to express dialogue focuses, a scheme was developed to directly generating speech reply from reply content.
When developing the system firstly, a priority was placed on the automatic processing, and prosodic focus was controlled by rather simple rules (original rules Based on the listening test for the reply speech generated using original rules, new rules were then developed.
Through the further listening test, the rules were revised and called the revised rules.
The validity of the revised rules was verified through an evaluation experiment.
It was also indicated that there existed users’ preferences on the intonation of the reply speech.

In this paper, it is assumed that a user interface is a collection of agents that assist the user in performing information retrieval (IR) tasks.
The design of such agents requires a better understanding of IR human capabilities and tasks.
User queries are usually vague and incrementally refined.
Librarian agents help users to better define and refine their queries.
They transform human-based fuzzy queries into system-manageable queries.
In addition, users can refine queries according to appropriate feedback.
A first attempt at the design of interface agents in IR is proposed, as well as examples of such agents.

In our first participation in the NTCIR Workshop, we focused on the evaluation of the relative effectiveness of different indexing approaches (word-based, N-gram-based, and yomior pronunciation-based) for Japanese IR and the benefits of their fusion.
Our MIMOR multiple indexing for method-object relations in IR system has already proved very effective in CLEF.
The results show that our approach is also promising for Japanese IR.

Christian Majenz,1,2
Tameem Albash,2,3 Heinz-Peter Breuer,1 and Daniel A. Lidar2,4 1Physikalisches Institut, Universität Freiburg, Freiburg, Germany 2Center for Quantum Information Science Technology, University of Southern California, Los Angeles, California 90089, USA 3Department of Physics and Astronomy, University of Southern California, Los Angeles, California 90089, USA 4Department of Electrical Engineering, Department of Chemistry, and Department of Physics and Astronomy, University of Southern California, Los Angeles, California 90089,
USA (Received 4 April 2013; published 8 July 2013)

Query Expansion (QE) is one of the most important mechanisms in the Information Retrieval field.
A typical short Internet query will go through a process of query refinement to improve its retrieval power.
Most of the existing QE techniques suffer from retrieval performance degradation due to imprecise choice of query
’s additive terms in the QE process.
This paper, introduce QE mechanism involve new expansion process that guided by conceptual representation approach using Concept Mapping tool for expanding the original query, in the context of the utilized corpus.
Experimental results, using the MEDLINE test collection data, show the effect of using conceptual representation approach via linguistic and domain based on the recall of retrieval results to enhance its performance.
Index Term— Concept Mapping, Information Retrieval, Query Expansion, Relevance Feedback.

GorUp is an Information Retrieval system that provides information about the contents of audio broadcast news in Basque, Spanish, and French.
Since the resources available for Basque in general, and for this task in particular, were very few, data optimization methodologies had to be applied in various phases of the development.
Moreover, the agglutinative nature of Basque required the use of morphemes and other sub-word units.
Additionally, some keyword spotting and semantic methods have been also applied in the system in order to retrieve information properly.
In most of the cases, the methods employed during this project could suit the requirements of many under-resourced languages, and one of these techniques could be the ontology-based approach.
This paper presents the system in general for Basque and emphasizes the techniques employed in order to enhance the system using a semantic ontology.

In this paper, we propose a framework for personalized access to location-aware services.
The system relies on the integration of a Geographic Information Retrieval module with a User Modeling component.
We describe an application into a specific field, but the platform is easily usable in different contexts.
Currently, the framework has been extending to the mobile domain.

Digital Library; Australian Business Deans Council (ABDC Bacon’s Media Directory; Burrelle’s Media Directory; Cabell’s
Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; Gale Directory of Publications Broadcast Media; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory; Web of Science Emerging Sources Citation Index (ESCI) Research Articles

We study a new task, proactive information retrieval by combining implicit relevance feedback and collaborative filtering.
We have constructed a controlled experimental setting, a prototype application, in which the users try to find interesting scientific articles by browsing their titles.
Implicit feedback is inferred from eye movement signals, with discriminative hidden Markov models estimated from existing data in which explicit relevance feedback is available.
Collaborative filtering is carried out using the User Rating Profile model, a state-of-the-art probabilistic latent variable model, computed using Markov Chain Monte Carlo techniques.
For new document titles the prediction accuracy with eye movements, collaborative filtering, and their combination was significantly better than by chance.
The best prediction accuracy still leaves room for improvement but shows that proactive information retrieval and combination of many sources of relevance feedback is feasible.

Department of Mathematics, City University of Hong Kong, 83 Tat Chee Avenue, Kowloon, Hong Kong.
E-mail: madaniel@cityu.edu.hk School of Information Science Engineering, East China University of Science Technology, Shanghai, 200237, P.R.C. E-mail:acniuyg@ecust.edu.cn Department of Mathematics, City University of Hong Kong, 83 Tat Chee Avenue, Kowloon, Hong Kong.
E-mail: macwli@cityu.edu.hk

We present an approach to using ontologies as interlingua in cross-language information retrieval in the medical domain.
Our approach is based on using the Unified Medical Language System (UMLS) as the primary ontology.
Documents and queries are annotated with multiple layers of linguistic information (part-of-speech tags, lemmas, phrase chunks Based on this we identify medical terms and semantic relations between them and map them to their position in the ontology.
The paper describes experiments in monolingual and cross-language document retrieval, performed on a corpus of medical abstracts.
Results show that semantic information, specifically the combined use of concepts and relations, increases the precision in monolingual retrieval.
In cross-language retrieval the semantic annotation outperforms machine translation of the queries, but the best results are achieved by combining a similarity thesaurus with the semantic codes.

Learning to rank is becoming more and more popular in machine learning and information retrieval field.
However, like many other supervised approaches, one of the main problems with learning to rank is lack of labeled data.
Recently, there have been attempts to address the challenges in active sampling for learning to rank.
But none of these methods take into consideration the differences between queries In this paper, we propose a novel active ranking framework on query-level which aims to employ different ranking models for different queries.
Then, we used Rank SVM as a base ranker, realized a query-level active ranking algorithm and applied it to document retrieval.
Experimental results on real-world data set show that our approach can reduce the labeling cost greatly without decreasing the ranking accuracy.

This study summarizes various linguistic approaches proposed for document analysis in information retrieval environments.
Included are standard syntactic methods to generate complex content identifiers, and the use of semantic know-how obtained from machine-readable dictionaries and from specially constructed knowledge bases.
A particular syntactic analysis methodology is also outlined and its usefulness for the automatic construction of book indexes is examined.

We study the impact of introducing syntax information into a stochastic component for natural language understanding that is based on a purely semantic case grammar formalism.
The parser operates in an application for train travel information retrieval, the French ARISE (Automatic Railway Information Systems for Europe) task.
This application supports the development of schedule inquiry services by telephone.
The semantic case grammar has been chosen in order to enhance robustness facing spontaneous speech effects.
However, this robustness is likely to turn into a drawback, if the semantic analysis ignores information that is propagated by syntactic relations.
Introducing additional syntax information, whose complexity is well adapted to the size of the stochastic model, may disambiguate and therefore improve the decoding.

High precision at the top ranks has become a new focus of research in information retrieval.
This paper presents the multiple nested ranker approach that improves the accuracy at the top ranks by iteratively re-ranking the top scoring documents.
At each iteration, this approach uses the RankNet learning algorithm to re-rank a subset of the results.
This splits the problem into smaller and easier tasks and generates a new distribution of the results to be learned by the algorithm.
We evaluate this approach using different settings on a data set labeled with several degrees of relevance.
We use the normalized discounted cumulative gain (NDCG) to measure the performance because it depends not only on the position but also on the relevance score of the document in the ranked list.
Our experiments show that making the learning algorithm concentrate on the top scoring results improves precision at the top ten documents in terms of the NDCG score.

A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.
In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.
We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.
We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.
Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.

Formal concept analysis (FCA) is an effective tool for data analysis and knowledge discovery.
Concept lattice, which is derived from mathematical order theory and lattice theory, is the core of FCA.
Many research works of various areas show that concept lattices structures is an effective platform for data mining, machine learning, information retrieval, software engineer, etc.
This paper offers a brief overview of FCA and proposes to apply FCA as a tool for analysis and visualization of data in digital ecosystem, and also discusses the applications of data mining for digital ecosystem

In this paper, the most recent version of the system developed by the SCHEMA NoE, termed SCHEMA Reference System, is presented.
The Reference System adopts a module-based, expandable architecture, with well defined interfaces between different modules, facilitating the ongoing expansion of the system based on researchers’ contributions.
The proposed system employs the MPEG-7 XM (MPEG-7 eXperimentation Model) along with extensions developed specifically for the system to improve functionality and efficiency.
In addition, the system supports high level descriptors and content-based indexing and retrieval using other modalities (e.g. pre-existing keyword annotations, text generated via automatic speech recognition (ASR
In this paper, the TRECVID 2004 test corpus is used as a common data set for demonstrating the functionalities and the efficiency of the proposed system.

This study presents an N-gram adaptation technique when additional text data for the adaptation do not exist.
We use a language modeling approach to the information retrieval (IR) technique to collect the appropriate adaptation corpus from baseline text data.
We propose to use a dynamic interpolation coefficient to merge the N-gram, where the interpolation coefficient is estimated from the word hypotheses obtained by segmenting the input speech.
Experimental results show that the proposed adapted N-gram always has better performance than the background Ngram.
key words: language model adaptation, adaptation corpus, dynamic interpolation coefficient, speech recognition

Information retrieval is concerned with documents relevant to a user’s information needs from a collection of documents.
The user describes information needs with a query which consists of a number of words.
Finding weight of a query is important to determine importance of a query.
Calculating term importance is fundamental aspect of most information retrieval approaches and it is commonly determined through Term FrequencyInverse Document Frequency (TF-IDF
This paper proposed Concept-based Term Weighting (CBW) technique to determine the term importance by finding the weight of a query.
WordNet ontology is used to find the conceptual information of each word in the query.
General Terms Term frequency
(TF Inverse Document Frequency (IDF Vector Space Model, Extraction Algorithm.

—Information Retrieval (IR) is concerned with searching and retrieving information within the documents and also searching the online databases and internet.
Genetic Algorithms (GA) are robust and efficient search and optimization techniques inspired by the Darwin's theory of natural evolution.
In this paper, a novel frame work of information retrieval system is proposed.
The applicability of Genetic algorithms in the field of web search and a review on how a GA is applied to different problem domains in web search is discussed.

This paper concerns the role of ontology in Information Retrieval (IR) and Artificial Intelligence
(AI First, it discusses the relation between IR and AI in a general way.
It also gives an introduction of ontology, which could bridge the gap between IR and AI in a certain sense.
This paper provides some case studies on either using IR techniques (mainly co-occurrence theory) to semi-automatically generate lightweight ontology or using already existing ontology to strengthen the retrieval.
Related works has been exploited and future research has been proposed.

Ontology is a conceptualization of a knowledge domain being shared by a group of users and represented as a machine-readable format.
Embedding ontology into information retrieval is a natural way to improve the effectiveness of searching results.
In this paper, an ontology-based information retrieval framework is proposed to combine the documents retrieval technique with ontology.
The ontology used in the system includes a general semantic ontology, domain knowledge and automatic generated fuzzy concept hierarchy.
The experimental results show that combination of applying different ontology has a great influence on the retrieval effectiveness for different query domains.

This research gives the impending supremacy of P2P Botnet to educated guess its belongings on the networks and hosts once it perform malicious operations.
Uninflected the Botnet traffic from regular traffic makes sharing of the info doable.
It uses Address anonymization will be done by Apriori conserving Anonymization algorithmic program to regulate the attack of P2P Botnet.
The on top of a technique for anonymizing has several edges.
Apriori Anonymization algorithm could be a cryptography sanitation algorithmic program for network trace house owners to anonymize the information science addresses of P2P Botnet and their traces in an exceedingly prefix preserving manner as a Software Define Type (SDT).

Similarity search is important in information-retrieval applications where objects are usually represented as vectors of high dimensionality.
This paper proposes a new dimensionality-reduction technique and an indexing mechanism for high-dimensional datasets.
The proposed technique reduces the dimensions for which coordinates are less than a critical value with respect to each data vector.
This flexible datawise dimensionality reduction contributes to improving indexing mechanisms for high-dimensional datasets that are in skewed distributions in all coordinates.
To apply the proposed technique to information retrieval, a CVA file (compact VA file which is a revised version of the VA file is developed.
By using a CVA file, the size of index files is reduced further, while the tightness of the index bounds is held maximally.
The effectiveness is confirmed by synthetic and real data.

I review and expand the model of quantum associative memory that I have recently proposed.
In this model binary patterns of n bits are stored in the quantum superposition of the appropriate subset of the computational basis of n qbits.
Information can be retrieved by performing an inputdependent rotation of the memory quantum state within this subset and measuring the resulting state.
The amplitudes of this rotated memory state are peaked on those stored patterns which are closest in Hamming distance to the input, resulting in a high probability of measuring a memory pattern very similar to it.
The accuracy of pattern recall can be tuned by adjusting a parameter playing the role of an effective temperature.
This model solves the well-known capacity shortage problem of classical associative memories, providing an exponential improvement in capacity.
The price to pay is the probabilistic nature of information retrieval, a feature that, however, this model shares with our own brain.

Information Retrieval is concerned with the identification of documents in the collection that are relevant to users information needs.
Queries formed by user are generally short and vague which makes it difficult to estimate the exact user need.
Information retrieval may improve their effectiveness by using process of query expansion, which automatically adds new terms to the original query posed by the user.
In this paper, a new technique is proposed based on WordNet for N-layer vector space approach.
WordNet is an online lexical dictionary which describes word semantic relationships in terms of Synset.
New query expansion approach is proposed to use semantic relationship while adding new terms.
The concept of term association is used to mine out the word from semantic relation.
The result shows that N-layer vector space model with proposed query expansion approach improves precision by approximately 5% and recall is improved by approximately 20%.

The Hindi morpheme x02018; vaalaa&#x02019; is very widely used as a suffix and also as a separate word.
The common usage of this suffix is to denote an activity or profession of a person.
This form of the usage has been borrowed in English with the spelling of x02018; wallah&#x02019
However, it has a large number of other interpretations depending upon the context in which it is used.
This paper presents an account of different senses in which this morpheme is used in Hindi and presents a strategy for learning their disambiguation based on contextual features with sparse data using a semi-supervised method.
We present a new technique of unifying learned instances using supervised training with limited data and computing matching index and bootstrapping the training set to deal with corpus sparseness.
This study finds application in machine translation, information retrieval, text understanding and text summarization.

It sounds good when knowing the knowledge and knowing in library and information science a philosophical framework in this website.
This is one of the books that many people looking for.
In the past, many people ask about this book as their favourite book to read and collect.
And now, we present hat you need quickly.
It seems to be so happy to offer you this famous book.
It will not become a unity of the way for you to get amazing benefits at all.
But, it will serve something that will let you get the best time and moment to spend for reading the book.

This paper assesses the retrieval effectiveness of automatically constructed inter-document hypertext links in Information Retrieval (IR
The objectives of the experiments described are to obtain evidence concerning the usefulness of querying and browsing automatically constructed IR hypertexts.
Links are built by using IR techniques, as these enable rapid, automatic production of hypertexts from a document collection for accessing the collection itself.
These tests are carried out in a laboratory environment and through simulation of link browsing.
Results of experiments show that browsing has little impact on the retrieval of relevant documents if used in place of querying or relevance feedback methods, though may be practical if used in combination with them.

Many approaches to Music Information Retrieval tasks rely on correctly determining if two segments of a given musical recording are repeats of each other.
Repetitions in recordings are rarely exact, and identifying the appropriate threshold for these pairwise decisions is crucial for tuning MIR algorithms.
However, current approaches for determining and reporting this threshold parameter are devoid of contextual meaning and interpretations, which makes comparing previous results difficult and which requires access to specific datasets.
This paper highlights weaknesses in current approaches to choosing similarity thresholds, provides a framework using the proportion of orthogonal musical change to tie thresholds back to feature spaces with the cosine dissimilarity measure, and introduces new research possibilities given a music-centered approach for selecting similarity thresholds.

We study an explicit parametric model of documents, queries, and relevancy assessment for Information Retrieval (IR Mean-field methods are applied to analyze the model and derive efficient practical algorithms to estimate the parameters in the problem.
The hyperparameters are estimated by a fast approximate leave-one-out cross-validation procedure based on the cavity method.
The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.

for QCRYPT 2013 contributed talk Ämin Baumeler and Anne Broadbent Faculty of Informatics, Università della Svizzera italiana, Lugano, Switzerland Department of Combinatorics and Optimization Institute for Quantum Computing University of Waterloo, Waterloo, Canada (
Dated: June 19, 2013)
Our contribution [2] is twofold.
On the one hand, we show that information-theoretic single-server Quantum Private Information Retrieval requires a linear amount of communication to be secure against specious adversaries, which are the quantum analog of honest-but-curious adversaries.
On the other hand, we stress the importance of adequate comparison between classical and quantum adversaries—unfair comparisons might lead to an unjustified advantage for the quantum case.

A Machine Translation (MT) system is an automatic process that translates from one human language to another language by using context information.
We evaluate the use of an MT-based approach for query translation in an Arabic-English Cross-Language Information Retrieval (CLIR) system.
We empirically evaluate the use of an MT-based approach for query translation in an Arabic-English CLIR system using the TREC-7 and TREC-9 topics and collections.
The effect of query length on the performance of the machine translation is also investigated to explore how much context is actually required for successful MT processing.

This paper describes an extensible and scalable approach to indexing documents that is utilized within the Highly Organised Team of Agents for Information Retrieval (HOTAIR) architecture.

This report was originally written to fulfill in part the requirements of the author's WPE examinations, part of the qualifying examinations for the University of Pennsylvania'a Computer Science Ph.D program.
The report first introduces CCS and uses it to illustrate various features of established methods of modelling concurrent, communicating systems.
The report then goes on to describe and investigate two new models for such systems: The Chemical Abstract Machine, a simple yet predominant in most models for such systems; and the π-calculus, a calculus similar in many respects to CCS, but able to model mobile processes and other, more difficult phenomena.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-91-34.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/369 Formal Models for Concurrent Communicating Systems MS-CIS-91-34 LOGIC COMPUTATION 33

The contemporary scenario predominated by information and knowledge perspectives indicate the pressing need to educate and train the library and information manpower towards a sustainable professional competence.
The manpower of today will meet in the near future the new challenges and the onslaught of the impact of Information Technology on LIS envisages to make substantial contribution to the ever perpetuating Information Society.
They need to be equipped in this context with necessary skills and competency to satisfy the high level, complex and evergrowing multifarious information needs of the user.
This paper discusses the pros and cons of LIS Education Scenario in the developing countries and stresses the need for model curriculum.

Just-In-Time Information Retrieval agents proactively retrieve information based on queries that are implicit in, and formulated from, the user’s current context, such as the blogpost she is writing.
This paper compares five heuristics by which queries can be extracted from a user
’s blogpost or other document.
Four of the heuristics use shallow Natural Language Processing techniques, such as tagging and chunking.
An experimental evaluation reveals that most of them perform as well as a heuristic based on term weighting.
In particular, extracting noun phrases after chunking is one of the more successful heuristics and can have lower costs than term weighting.
In a trial with real users, we find that relevant results have higher rank when we use implicit queries produced by this chunking heuristic than when we use explicit user-formulated queries.

In this study, we propose a double-phase neural-network-based metalearning approach to perform distributed text information retrieval.
In the first phase, a single neural network model is deployed in different text collections distributed in different physical sites to retrieve some relevant text documents.
In the second phase, a neural-network-based metalearning approach is proposed to integrate the relevance results for text documents with a specific query.
For illustration purpose, a simulated web text information retrieval experiment is performed to verify the effectiveness and efficiency of the proposed neural-network-based metalearning approach.

Blogs have become an important medium for people to express opinions and share information on the web.
Predicting the interests of bloggers can be beneficial for information retrieval and knowledge discovery in the blogosphere.
In this paper, we propose a two-layer classification model to categorize the interests of bloggers based on a set of short snippets collected from their blog posts.
Experiments were conducted on a list of bloggers collected from blog directories, with their snippets collected from Google Blog Search.
The results show that the proposed method is robust to errors in the lower level and achieve satisfactory performance in categorizing blogger's interests.

We propose research to investigate a new paradigm for Interactive Information Retrieval (IIR) where all input and output is mediated via speech.
Our aim is to develop a new framework for effective and efficient IIR over a speech-only channel: a <i>Spoken Conversational Search System (
This SCSS will provide an interactive conversational approach to determine user information needs, presenting results and enabling search reformulations.
We have thus far investigated the format of results summaries for both audio and text, features such as summary length and summaries documents (noisy document or clean document) generated from (noisy) speech-recognition output from spoken document.
In this paper we discuss future directions regarding a novel spoken interface targeted at search result presentation, query intent detection, and interaction patterns for audio search.

In this report a novel approach concerned with the general framework of Information Management, is presented.
We use a Multi-Agent System to cope with the problem of Distributed Information Retrieval.
The Distributed Information Retrieval task deals with the collection of information from multiple and usually heterogeneous information sources that exist in a distributed environment.

A 4D effect video played at cinema or other designated places is a video annotated with physical effects such as motion, vibration, wind, flashlight, water spray, and scent.
In order to automate the time-consuming and labor-intensive process of creating such videos, we propose a new method to classify videos into 4D effect types with shot-aware frame selection and deep neural networks (DNNs Shot-aware frame selection is a process of selecting video frames across multiple shots based on the shot length ratios to subsample every video down to a fixed number of frames for classification.
For empirical evaluation, we collect a new dataset of 4D effect videos where most of the videos consist of multiple shots.
Our extensive experiments show that the proposed method consistently outperforms DNNs without considering multi-shot aspect by up to 8.8% in terms of mean average precision.

We believe the techniques for evaluating clone detectors can be improved, and that the improvements can lead the way to better clone detector research and research results.
Current techniques are based on simple performance measures borrowed from information retrieval (IR) research.
Here we argue that additional IR evaluation measures can be usefully imported.
Limitations of current research Automated methods for finding software clones have, in the past, been primarily evaluated according to simple measures adapted from prior work in the field of information retrieval (IR The main stated purpose of an IR system is to enable users to find documents which are relevant to the user based on some approximation of their needs in the form of a query.
One classic way of evaluating IR systems is to examine their scores on precision and recall.
These measures are usually neatly defined and related by way of a table (see e.g van Rijsbergen

is fundamental to the conduct of contemporary science, there has been little research into this topic in information studies.
This article reports on a study in which, using ethnographic methods, the author studied record-keeping as it is practiced in a basic research science laboratory.
The process by which the record is created to reflect both personal need and professional norms is framed as a series of acts of selection, synthesis, and standardization.
The article concludes with reflections on the role of deep understanding of scientific record-keeping for other disciplines and the design of digital laboratory technologies.

Creating descriptors for trajectories has many applications in robotics/human motion analysis and video copy detection.
Here, we propose a novel descriptor for 2D trajectories: Histogram of Oriented Displacements (HOD Each displacement in the trajectory votes with its length in a histogram of orientation angles.
3D trajectories are described by the HOD of their three projections.
We use HOD to describe the 3D trajectories of body joints to recognize human actions, which is a challenging machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance.
The descriptor is fixedlength, scale-invariant and speed-invariant.
Experiments on MSR-Action3D and HDM05 datasets show that the descriptor outperforms the state-ofthe-art when using off-the-shelf classification tools.

In this paper we will present a system that is able to perform cooperative information retrieval actions over a text knowledge base.
The knowledge base is composed by four levels: Interaction, Domain, Information Retrieval and Text.
The interaction level is responsible for the dialogue management, including the inference of attitudes.
The domain level is composed by rules encoding knowledge about the text domain.
The information retrieval level includes knowledge about IR actions over sets of documents.
The text level has knowledge about the words in each text.
Cooperation is achieved through two main strategies: 1) clustering the answer sets of documents accordingly with the domain and IR-level knowledge; 2) keeping the context of the interaction and inferring the user intentions.

Query translation is an important task in cross-language information retrieval (CLIR) aiming to translate queries into languages used in documents.
The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another.
Some untranslated terms cause irreparable performance drop while others do not.
We propose an approach to estimate the translation probability of a query term, which helps decide if it should be translated or not.
The approach learns regression and classification models based on a rich set of linguistic and statistical properties of the term.
Experiments on NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks demonstrate that the proposed approach can significantly improve CLIR performance.
An in-depth analysis is also provided for discussing the impact of untranslated out-of-vocabulary (OOV) query terms and translation quality of non-OOV query terms on CLIR performance.

Internet advertising, a form of advertising that utilizes the Internet to deliver marketing messages and attract customers, has seen exponential growth since its inception around twenty years ago; it has been pivotal to the success of the World Wide Web.
The dramatic growth of internet advertising poses great challenges to information retrieval, machine learning, data mining and game theory, and it calls for novel technologies to be developed.
The main purpose of this workshop is to bring together researchers and practitioners in the area of Internet Advertising and enable them to share their latest research results, to express their opinions, and to discuss future directions.

I investigated the interactive searching behavior of two groups of subjeets using a novel best-match, ranked-output information retrieval (IR) engine to search a large, full-text document collection.
The research focuses on the use of relevance feedback, a query reformulation tool.
Ten searchers who had a background in IR were observed in the first study; 64 complete novices took part in a second experiment that systematically varied the user knowledge and user control of the feedback meehanism.
Behavioral and performance data suggest that user control over relevance feedback benefits retrieval performance and user satisfaction.

Two widely used criteria for evaluating the effectiveness of information retrieval systems are, respectively, the recall and the precision.
Since the determination of these measures is dependent on a distinction between documents which are relevant on the one hand, and documents which are not relevant on the other to a given query set, it has sometimes been claimed that an accurate, generally valid evaluation cannot be based on recall and precision.
A study was made to determine the effect of variations in relevance assessments on the average recall and precision values used to measure retrieval effectiveness.
Using a collection of 1200 documents in information science for test purposes, it is found that large scale differences in the relevance assessments do not produce significant variations in average recall and precision.
It thus appears that properly computed recall and precision data may represent effectiveness indicators which are generally valid for many distinct user classes.

This survey paper aims to provide the multimedia researcher with the different fusion approaches.
The different fusion approaches are used to have multiple modalities are used to accomplish various multimedia related analysis tasks.
Fusion method are describe by using different perspective .The
problem of metadata based image retrieval system and a very rapid growth in the quantity and availability of digital image involves the researches into the multimedia information system.
Finally in last section we are discussing on the multimedia information retrieval based on late semantic fusion approaches.

In this paper we report on the first Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab.
Our main goal with the lab is to provide a benchmarking platform for researchers to evaluate their ranking systems in a live setting with real users in their natural task environments.
For this first edition of the challenge we focused on two specific use-cases: product search and web search.
Ranking systems submitted by participants were experimentally compared using interleaved comparisons to the production system from the corresponding use-case.
In this paper we describe how these experiments were performed, what the resulting outcomes are, and conclude with some lessons learned.

High dimensionality can pose severe difficulties, widely recognized as different aspects of the curse of dimensionality.
In this paper we study a new aspect of the curse pertaining to the distribution of <i>k</i>-occurrences, i.e the number of times a point appears among the <i>k</i> nearest neighbors of other points in a data set.
We show that, as dimensionality increases, this distribution becomes considerably skewed and hub points emerge (points with very high <i>k</i>-occurrences We examine the origin of this phenomenon, showing that it is an inherent property of high-dimensional vector space, and explore its influence on applications based on measuring distances in vector spaces, notably classification, clustering, and information retrieval.

This paper reviews the development of statistically-based retrieval.
Since the 1950s statistical techniques have clearly demonstrated their practical worth and statistical theories their staying power, for document or text retrieval.
In the last decade the TREC programme, and the Web, have offered new retrieval challenges to which these methods have successfully risen.
They are now one element in the much wider and very productive spread of statistical methods to all areas of information and language processing, in which innovative approaches to modelling their data and tasks are being applied.

This special issue brings together eight papers from experts of communities which often have been perceived as different once: bibliometrics, scientometrics and informetrics on the one side and information retrieval on the other.
The idea of this special issue started at the workshop
Combining Bibliometrics and Information Retrieval held at the 14th International Conference of Scientometrics and Informetrics, Vienna, July 14–19, 2013.
Our motivation as guest editors started from the observation that main discourses in both fields are different, that communities are only partly overlapping and from the belief that a knowledge transfer would be profitable for both sides.
Hereby, we were inspired by the bibliometric analysis of the broader field of Library and Information Science done by White and McCain (1998 The discussions during the ISSI workshop in Vienna and the papers in this special issue highlighted the following features of distinction:

In Information Retrieval (IR the semantic gap is the difference between what computers store and what users expect via their queries.
There are several reasons for the existence of those gaps such as homonymy and synonymy in text retrieval, or the typical difference between low-level representations and keyword-based queries in image retrieval.
The objective of this work is to close these gaps by effective, scalable and not-so-expensive solutions.
The main idea is to exploit available unstructured data and hidden topic models to infer surrounding contexts for better information retrieval (in both text retrieval and image retrieval Early results obtained on two problems, namely Web search clustering and image annotation, show the effectiveness of the proposed approaches.

The U.S. Agency for Healthcare Research and Quality has created a public website to disseminate critical information regarding its health information technology initiative.
The website is maintained by AHRQ's Natiomal Resource Center (NRC) for Health Information Technology.
In the latest continuous quality improvement project, the NRC used the site's search logs to extract user-generated search phrases.
The phrases were then compared to the site's controlled vocabulary with respect to language, grammar, and search precision.
Results of the comparison demonstrate that search log data can be a cost-effective way to improve controlled vocabularies as well as information retrieval.
User-entered search phrases were found to also share many similarities with folksonomy tags.

In professional environments which are characterized by a domain (Medicine, Law, etc information retrieval systems must be able to process precise queries, mostly because of the use of a specific domain terminology, but also because the retrieved information is meant to be part of the professional task (a diagnosis, writing a law text, etc
In this paper we address the problem of solving domain-specific precise queries.
We present an information retrieval model based on description logics to represent external knowledge resources and provide expressive document indexing and querying.

Recording university lectures through lecture capture systems is increasingly common, generating large amounts of audio and video data.
Transcribing recordings greatly enhances their usefulness by making them easy to search.
However, the number of recordings accumulates rapidly, rendering manual transcription impractical.
Automatic transcription, on the other hand, suffers from low levels of accuracy, partly due to the special language of academic disciplines, which standard language models do not cover.
This paper looks into the use of Wikipedia to dynamically adapt language models for scholarly speech.
We propose Ranked Word Correct Rate as a new metric better aligned with the goals of improving transcript search ability and specialist word recognition.
The study shows that, while overall transcription accuracy may remain low, targeted language modelling can substantially improve search ability, an important goal in its own right.

This paper presents a study of three statistical query translation models that use different units of translation.
We begin with a review of a word-based translation model that uses co-occurrence statistics for resolving translation ambiguities.
The translation selection problem is then formulated under the framework of graphic model resorting to which the modeling assumptions and limitations of the co-occurrence model are discussed, and the research of finding better translation units is motivated.
Then, two other models that use larger, linguistically motivated translation units (i.e noun phrase and dependency triple) are presented.
For each model, the modeling and training methods are described in detail.
All query translation models are evaluated using TREC collections.
Results show that larger translation units lead to more specific models that usually achieve better translation and cross-language information retrieval results.

PhD Dissertation: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer and Information Science and Engineering

Half-cycle pulses (HCP'S) can manipulate quantum information in Rydberg wavepacket data registers.
This paper summarizes recent experiments to control and measure wavepacket structure and quantum coherence using HCP interactions.

The SIGIR 2013 Workshop on Modeling User Behavior of Information Retrieval Evaluation brought together researchers interested in improving Cranfield-style evaluation of information retrieval through the modeling of user behavior.
The workshop included two invited talks, ten short paper presentations, and breakout groups.
Workshop participants brainstormed research questions of interest and formed breakout groups to explore these questions in greater depth.
In addition to summarizing the invited talks and presentations, this report details the results of the breakout groups, which provide a set of research directions for the improvement of information retrieval evaluation.

This paper describes use of ecommerce for easy crawling of information on an ecommerce website.
As ecommerce has become an essential part in people's day-to-day activities of this modernized world.
Searching &amp; surfing data for ecommerce activity becomes tedious as lot many options &amp; websites for single entity of ecommerce is presented before user.
Internet is a huge set of database were many irrelevant data of no choice to particular task of user exist.
In this paper we have designed a basic ontology of ecommerce along with its superclass &amp; subclass as well as its siblings.
This crawler design named ECOMMTOLOGY is an Ontology based Ecommerce application.
Hence, we have classified ecommerce into hierarchy of business-to-business and business-to-consumer modules.
The Ecommerce based ontology specifies each product details by searching, storing and retrieving user log details.

We present a new scheme for active, or programmable, packets based upon a new packet language, SNAP (Safe Networking with Active Packets SNAP's semantics permit us to prove that all SNAP programs are safe with respect to network resource usage and evaluation isolation.
Furthermore, we describe an implementation of a SNAP interpreter, snapd, which achieves high performance for standard networking tasks.
This work represents the first active packet system that is demonstrated to be both safe and efficient.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-99-24.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/181 Safe and Efficient Active Packets

This paper presents a new approach to information retrieval (IR) based on run-time selection of the best set of techniques to respond to a given query.
A technique is selected based on its projected effectiveness with respect to the specific query, the load on the system, and a time-dependent utility function.
The paper examines two fundamental questions 1) can the selection of the best IR techniques be performed at run-time with minimal computational overhead?
and (2) is it possible to construct a reliable probabilistic model of the performance of an IR technique that is conditioned on the characteristics of the query?
We show that both of these questions can be answered positively.
These results suggest a new system design that carries a great potential to improve the quality of service of future IR systems.

Information is characterized as the basic notion of informatics (information science Different views of the nature of information are presented.
An analysis of information definitions, including the Stanford definition, is given.
On the basis of analyzing and generalizing the presented definitions the authors present their own definition of information.
Information is studied as a sign using the concept of the semiotic triangle.
The problem of the existence of information in animate and inanimate nature is considered and concrete examples of this view are given.

There are enormous amounts of information widely available in the Intranets.
This information is only useful if data can be retrieved in an accurate and timely manner.
Currently, Intranet search engine has become a necessity due to lack of an efficient way to disseminate useful information to its members.
In this paper, we examine the importance of Intranet search engines and propose D’Galaxy as the search engine in Multimedia University
This intranet search engine will be of content-based using the full-text indexing.

Building on a notion of keys for XML, we propose a novel indexing scheme for hierarchical data that is based not only on the structure but also the content of the data.
The index can be used to check the validity of data with respect to a set of key specifications, as well as for efficiently evaluating queries and updates on key paths.
We develop algorithms for the construction and incremental maintenance of the indexing structure, and study the complexity of these algorithms.
Finally, we discuss how our indexing techniques can be used for more general queries involving key paths.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-01-30.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/46 Indexing Keys in Hierarchical Data Yi Chen, Susan B. Davidson and Yifeng Zheng

MuTaTeD'II started in November 1999, building on the results of the MuTaTeD project.
Its aim is to design and implement a music information retrieval system with delivery/access services for encoded music.
The prototype service will provide a user friendly, web-based search/browse/query interface to access musical content.

Hierarchic document clustering has been widely applied to Information Retrieval (IR) on the grounds of its potential improved effectiveness over inverted file search.
However, previous research has been inconclusive as to whether clustering does bring improvements.
In this paper we take the view that if hierarchic clustering is applied to search results (query-specific clustering then it has the potential to increase the retrieval effectiveness compared both to that of static clustering and of conventional inverted file search.
We conducted a number of experiments using five document collections and four hierarchic clustering methods.
Our results show that the effectiveness of query-specific clustering is indeed higher, and suggest that there is scope for its application to IR.

The aim of the AnHitz project, whose participants are research groups with very different backgrounds, is to carry out research on language, speech and visual technologies for Basque.
Several resources, tools and applications have been developed in AnHitz, but we have also integrated many of these into a prototype of a 3D virtual expert on science and technology.
It includes Question Answering and Cross Lingual Information Retrieval systems in those areas.
The interaction with the system is carried out in Basque (the results of the CLIR module that are not in Basque are translated through Machine Translation) and is speech-based (using Speech Synthesis and Automatic Speech Recognition
The prototype has received ample media coverage and has been greatly welcomed by Basque society.
The system has been evaluated by 50 users who have completed a total of 300 tests, showing good performance and acceptance.

For the TREC 2009, we exhaustively classified every document in each corpus, using machine learning methods that had previously been shown to work well for email spam [9, 3 We treated each document as a sequence of bytes, with no tokenization or parsing of tags or meta-information.
This approach was used exclusively for the adhoc web, diversity and relevance feedback tasks, as well as to the batch legal task: the ClueWeb09 and Tobacco collections were processed end-to-end and never indexed.
We did the interactive legal task in two phases: first, we used interactive search and judging to find a large and diverse set of training examples; then we used active learning process, similar to what we used for the other tasks, to find find more relevant documents.
Finally, we fitted a censored (i.e. truncated) mixed normal distribution to estimate recall and the cutoff to optimize F1, the principal effectiveness measure.

s and Bibliographies Adverse Reactions Titles.
Excerpta<lb>
Bulletin of the National Clearinghouse for Poison<lb>Control Centers.
Maryland: US Department of<lb>Health, Education and Welfare.
Clin-Alert.
Louisville, Kentucky: Science Editors Inc. Drug Literature Index.
Amsterdam: Excerpta Medica.
Index Medicus.
Bethesda, Maryland: National Library<lb>of Medicine.
Pharmacology and Toxicology.
Excerpta<lb>Medica.
Sydney: ADIS Press.
Toxicology Abstracts.
London: Information Retrieval"<lb>Ltd.
Microfiche Infonnation Drugdex.
Colorado: Micromedex Inc.
Iowa Drug Information Service, Univer-<lb>sity of Iowa.
Colorado: Micromedex Inc.

Querying for information is a commonality between databases and information retrieval systems.
For both areas, there are a variety of issues relative to representation of uncertainty and its retrieval.
This paper reviews these issues for both types of systems and discusses potential future directions 1997 Elsevier Science B.V.

We describe indexing and retrieval techniques that are suited to perform terabyte-scale information retrieval tasks on a standard desktop PC.
Starting from an Okapi-BM25-based default baseline retrieval function, we explore both sides of the effectiveness spectrum.
On one side, we show how term proximity can be integrated into the scoring function in order to improve the search results.
On the other side, we show how index pruning can be employed to increase retrieval efficiency at the cost of reduced retrieval effectiveness.
We show that, although index pruning can harm the quality of the search results considerably, according to standard evaluation measures, the actual loss of precision, according to other measures that are more realistic for the given task, is rather small and is in most cases outweighed by the immense efficiency gains that come along with it.

With the explosion of on-line non-English documents, cross language information retrieval (CLIR) systems have become increasingly important in recent years.
Moreover, roles of individual entities in an information retrieval system, specially a cross language one, can be implemented as distinct agents.
Many methodologies for modeling the multi-agent system have been proposed.
In this paper, we use Tropos methodology for modeling a Cross Language Information Retrieval System.
The methodology leads in defining four different classes of agents: Personal-Agent, User-Agent, Translator, and Retrieval.

Information retrieval evaluation based on the pooling method is inherently biased against systems that did not contribute to the pool of judged documents.
This may distort the results obtained about the relative quality of the systems evaluated and thus lead to incorrect conclusions about the performance of a particular ranking technique We examine the magnitude of this effect and explore how it can be countered by automatically building an unbiased set of judgements from the original, biased judgements obtained through pooling.
We compare the performance of this method with other approaches to the problem of incomplete judgements, such as bpref, and show that the proposed method leads to higher evaluation accuracy, especially if the set of manual judgements is rich in documents, but highly biased against some systems.

The second workshop on Gamification for Information Retrieval took place at ECIR 2015 in Vienna, Austria on the 29th of March.
The workshop program included two invited keynote presentations, seven oral presentations of refereed papers, lots of mini discussion sessions and a fishbowl session.
The presentations covered diverse topics from playing around with an eye tracker to a game with IR papers and even a game of scientific hangman, generating lively and fun discussions.
The workshop was a crowdpinion experiment itself, gathering participants’ momentary opinions via an Android app.
One of the main themes of the day was the interplay of gamification aspects and incentives, where the key challenge is to align player motivations with the goal of the task.
Any misalignment may lead to gamification as a tool being more damaging than useful with users’ focus shifting from the task to gaming the system.

Automatic learning of geospatial intelligence is challenging due to the complexity of articulating knowledge from visual patterns and to the ever-increasing quantities of image data generated on a daily basis.
In this setting, human inspection and annotation is subjective and, more importantly, impractical.
In this letter, we propose a knowledge-discovery algorithm that uses content-based methods to link low-level image features with high-level visual semantics in an effort to automate the process of retrieving semantically similar images.
Our algorithm represents geospatial images by using a high-dimensional feature vector and generates a set of association rules that correlate semantic terms with visual patterns represented by discrete feature intervals.
We also provide a mathematical model to customize the relevance of feature measurements to semantic assignments as well as methods of querying by semantics and by example.

Information retrieval techniques play vital role in the era of information technology.
Inverted index is one of the technique to retrieve the information/data related with certain keyword.
This technique gives faster results to retrieve relevant document from billions of documents, which contains specified keyword.
In order to support wrongly spelled keyword, many techniques have been proposed including edit distance, wild-card and n-gram.
The n-gram index has language-neutral and errortolerant advantage.
However, it has a drawback of large size and less performance.
In this paper, we have proposed NOVEL technique to search fuzzy keyword.
We have implemented and tested the proposed technique on two datasets.
The result shows that NOVEL technique supports not only wrongly spelled keywords, but also reduced gram size by 40-50% than K/n-gram technique.
Therefore, the proposed technique is the most efficient technique to support fuzzy keyword search.

Give us 5 minutes and we will show you the best book to read today.
This is it, the symbolic projection for image information retrieval and spatial reasoning that will be your best choice for better reading book.
Your five times will not spend wasted by reading this website.
You can take the book as a source to make better concept.
Referring the books that can be situated with your needs is sometime difficult.
But here, this is so easy.
You can find the best thing of book that you can read.

The enormous data sharing and data availability on the Internet provides opportunities for new services tailored to extract, search, aggregate, and mine data in meaningful ways, At the same time, it poses challenges with regards to data privacy.
This paper offers insight into this problem, focusing on current relevant research and potential areas of synergy between the information retrieval community and the privacy community.
We then analyze example publicly shared data and discuss the types of data that can be extracted, the methods used for extracting them, and the implications for individuals who share personal information.

This paper presents an improved framework for voice retrieval of Mandarin broadcast news speech.
First, several unsupervised and data-driven approaches for broadcast news transcription were proposed to improve the speech recognition accuracy and efficiency.
Then, a multiscale indexing paradigm for broadcast news retrieval was exploited to alleviate the problems caused by the speech recognition errors and the flexible wording structure of the Chinese language.
Finally, we used the PDA as the platform and broadcast radio programs collected in Taiwan as the document collection to establish a speech-based multimedia information retrieval prototype system.
Very encouraging results were obtained.

Most of the Information Retrieval techniques are based on representing the documents using the traditional vector space model i.e. bag-of-words model.
In this paper, associations among words in the documents are assessed and it is expressed in term graph model to represent the document content and the relationship among the keywords.
Most modern web search engines typically employ two-level ranking strategy.
Firstly, an initial list of documents is prepared using a low-quality ranking function with consumes less computation.
Secondly, initial list is then re-ranked by machine learning algorithms which involve expensive computation.
This paper experiments the second level of ranking strategy which exploits term graph data structure to assess the importance of a document for the user query and thus documents are re-ranked according to the association and similarity exists among the documents.
The proposed algorithms achieve promising results within the top 10 search results.

In this paper, we address both standard and focused retrieval tasks based on comprehensible language models and interactive query expansion (IQE Query topics are expanded using an initial set of Multi Word Terms (MWTs) selected from top n ranked documents.
MWTs are special text units that represent domain concepts and objects.
As such, they can better represent query topics than ordinary phrases or n-grams.
We tested different query representations: bag-of-words, phrases, flat list of MWTs, subsets of MWTs.
We also combined the initial set of MWTs obtained in an IQE process with automatic query expansion (AQE) using language models and smoothing mechanism.
We chose as baseline the Indri IR engine based on the language model using Dirichlet smoothing.
The experiment is carried out on two benchmarks: TREC Enterprise track (TRECent) 2007 and 2008 collections; INEX 2008 Ad-hoc track using the Wikipedia collection.

Collaboration has been identified as an important aspect in information seeking.
People meet to discuss and share ideas and through this interaction an information need is quite often identified.
However the process of resolving this information need, through interacting with a search engine and performing a search task, is still an individual activity.
We propose an environment which allows users to collaborate to satisfy a shared information need.
We discuss ways to divide the search task amongst collaborators and propose the use of relevance feedback, a common information retrieval process, to enable the transfer of knowledge across collaborators during a search session.
We describe the process by which co-searchers can collaborate effectively with little redundancy and how we can combine relevance judgements from multiple searchers into a coherent model for synchronous collaborative information retrieval

The aim is to build a speech based query interface to a text retrieval system.
The interface will be independent of the underlying Information Retrieval engine.
First steps in this project will be presented in this paper.
A study of the properties of typical Information Retrieval queries has shown that the speech interface must effectively address requirements like speaker independence, fluently spoken speech, and uncontrolled vocabulary.
These requirements can only be approximated with the currently available speech recognition techniques.
Furthermore, such requirements demand a multidisciplinary approach from areas like signal processing, pattern recognition, and linguistics.
In order to coordinate these approaches and to be open for further developments in these areas, a speech recognition architecture has been introduced and this will also be presented.

The University of California, Berkeley and the University of Liverpool are developing a Information Retrieval and Digital Library system (Cheshire3) that operates in both single-processor and "Grid" distributed computing environments.
This paper discusses the architecture of the system and how it performs Digital Library tasks in a Grid computing environment.

Rankboost has been shown to be an effective algorithm for combining ranks.
However, its ability to generalize well and not overfit is directly related to the choice of weak learner, in the sense that regularization of the rank function is due to the regularization properties of its weak learners.
We present a regularization property called consistency in preference and confidence that mathematically translates into monotonic concavity, and describe a new weak ranking learner (MWGR) that generates ranking functions with this property.
In experiments combining ranks from multiple face recognition algorithms and an experiment combining text information retrieval systems, rank functions using MWGR proved superior to binary weak learners.

An interesting issue “whether the factual search result is better than the satisfactory one is discussed in this paper, and all effort is to illustrate the publicly recognized conception
the supreme satisfactory result is optimal” could lead Information Retrieval astray.
By contrast, we propose a new hypothesis “the factually result should be the optimal although sometimes unsatisfactory To implement the pilot study on this, we developed a search-engine (Google) based labeling platform and designed a new evaluation method involving user experiences.
The results on the platform show the existence of bias on determining the optimal search results (factual or satisfactory And the very different performances of NDCG and our evaluation metric demonstrate the weakness of current ranking algorithms in mining and recommending the long-term effective information.

The purpose of this paper is to examine empirically factors having effects on performance of cross-lingual information retrieval.
In order to obtain experimental data, at the NTCIR-4 CLIR task, we submitted search results of Japanese monolingual run and three bilingual runs retrieving the Japanese document collection (i.e Chinese-Japanese, Korean-Japanese and English-Japanese runs It turns out that a regression model of which independent variables are
“quality” of query translation and “difficulty” of the search in itself explains well variations of values of average precision by CLIR runs.
The “quality” of translations was measured as a score assigned by a human assessor based on the degree to which each translation is coincident with the corresponding term in the Japanese topic that the task organizers provided, and the “difficulty” of the search was represented as a value of average precision by a run using the Japanese topic (i.e monolingual run).

I feel honored in inviting you, the learned participants, to take part in the 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2016) sponsored by the IEEE Computer Society and the International Association for Computer and Information Science (
The conference is organized by Shanghai University and co-organized by Shanghai Key Laboratory of Computer Software Testing Evaluating.
SNPD is a forum that brings together researchers, scientists, engineers, industry practitioners, and students to present their original work, discuss and exchange new ideas, research results, and experiences on all aspects of Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing.

In this paper, we would like to represent our research in Natural Language Information Retrieval (NLIR) system using grammar analysis.
In our system, the queries are small texts of description in natural language.
The system tries to get searching conditions from these queries by grammar analyzing and searches for the expected documents.
In many NLIR system, the descriptions are analyzed by either annotations[5] or predefined patterns[8 These techniques can lead to miss-understanding the user's request and find inappropriate results.
Therefore, the research in NLIR system focuses on understanding the user's descriptions in natural language by grammar analysis technique should be conducted to make found results more accurate.

Vector Space Model (VSM) is used frequently in Text Classification (TC
However, it is usually produces a high dimensional feature space which leads to huge cost of computation and storage.
Recently, statistic topic model plays an important role in the field of Information Retrieval (IR TC and Document Clustering.
In this chapter, we try to use a kind of statistic model—Latent Dirichlet Allocation (LDA) Model to represent texts and construct corresponding classifier.
We expect that this model could include more complicated semantic information and simultaneously reduce dimensionality greatly.

This paper explores a dual score system that simultaneously evaluates the relative importance of researchers and their works.
It is a modification of the CITEX algorithm recently described in Pal and Ruj (2015 Using available publication data for m author keywords (as a proxy for researchers) and n papers it is possible to construct a m n author-paper feature matrix.
This is further combined with citation data to construct a HITS-like algorithm that iteratively satisfies two criteria: first, a good author is cited by good authors, and second, a good paper is cited by good authors.
Following Pal and Ruj, the resulting algorithm produces an author eigenscore and a paper eigenscore.
The algorithm is tested on 213,530 citable publications listed under Thomson ISI’s “
Information Science Library Science
JCR category from 1980–2012.

OSVIRA (Ontology-based System for Semantic Visioconference Information Retrieval and Annotation) is a system devoted to the development of help to the annotation and to the semantic research of multimedia videoconferencing resources.
It is founded on the use of dense ontology associated with a thesaurus.
OSVIRA allows to describe semantically the content of a pedagogic multimedia resources on the basis of an intuitive model of annotation based on the triplet {Object, Relation, Object It formally represents this content with the aid of

Herein we introduce a system for managing the content of information systems in order to improve retrieval effectiveness over a large, constantly growing collection of structured and unstructured data.
Specifically, we target the problems relating to the submission of new content from various sites, the seamless integration of said content into the main document corpus, and effective methods of searching the available information.
In a unique interdisciplinary effort with the Institute of Design at IIT we have implemented a prototype for a document management system, which acts as a unifying abstraction layer that the various components of our architecture use to coordinate their efforts.

A weakly-supervised extraction method identifies concepts within conceptual hierarchies, at the appropriate level of specificity (e.g i>Bank</i>
vs i>Institution</i to which attributes
(e.g i>routing number</i extracted from unstructured text best apply.
The extraction exploits labeled classes of instances acquired from a combination of Web documents and query logs, and inserted into existing conceptual hierarchies.
The correct concept is identified within the top three positions on average over gold-standard attributes, which corresponds to higher accuracy than in alternative experiments.

The Information Retrieval and Web Intelligent (IR-WI) research group is a research team at the Faculty of Information Technology, QUT, Brisbane, Australia.
The IR-WI group participated in the NTCIR 5 for the first time.
This paper focuses on our participation in the CLIR task.
For this track, we experiment our XML search engine within the NTCIR English document collection.
Our results indicate that in general, our XML search engine is not suitable for non-structured document-level information retrieval due to the difference of document structure.

The Center for Intelligent Information Retrieval in the Computer Science Department is an NSFsponsored center that is supported by the State of Massachusetts, the Federal government, and industrial members.
More than 50 students, professionals, and faculty are involved in a broad range of basic research and technology transfer programs.
The faculty involved are Jamie Callan, Wendy Lehnert, Victor Lesser, Kathryn McKinley, Elliot Moss, and Edwina Rissland.

To support more precise query translation for English-Chinese Bi-Directional Cross-Language Information Retrieval (CLIR we have developed a novel framework by integrating a semantic network to characterize the correlations between multiple inter-related text terms of interest and learn their inter-related statistical query translation models.
First, a semantic network is automatically generated from large-scale English-Chinese bilingual parallel corpora to characterize the correlations between a large number of text terms of interest.
Second, the semantic network is exploited to learn the statistical query translation models for such text terms of interest.
Finally, these inter-related query translation models are used to translate the queries more precisely and achieve more effective CLIR.
Our experiments on a large number of official public data have obtained very positive results.

This paper reports on one of the first steps in building a very large annotated database of American English.
We present and discuss the results of an experiment comparing manual part-of-speech tagging with manual verification and correction of automatic stochastic tagging.
The experiment shows that correcting is superior to tagging with respect to speed, consistency and accuracy.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-90-46.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/569 First Steps Towards
An Annotated Database of American English MS-CIS-90-46 LINC LAB 175 Mitchell P. Marcus Beatrice Santorini David Magerman Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19 104

Information retrieval is great technology behind web search services.
In information retrieval, it is common to model index terms and documents as vectors in a suitably defined vector space.
The vector space model is one of the classical and widely applied retrieval models to evaluate relevance of web page.
The retrieval operation consists of computing the cosine similarity function between a given query vector and the set of documents vector and then ranking documents accordingly.
In this paper, we present different approaches of vector space model to compute similarity score of hits from search engine and more importantly, it is felt that this investigation will lead to a clearer understanding of the issues and problems in using the vector space model in information retrieval and our work intends to discuss the main aspects of Vector space models and provide a comprehensive comparison for TermCount model, Tf-Idf model and Vector space model based on normalization.

In this paper, we propose an approach based on a topic generative model called Latent Dirichlet Allocation (LDA) to promoting ranking diversity for biomedical information retrieval.
Different from other approaches or models which consider aspects on word level, our approach assumes that aspects should be identified by the topics of retrieved documents.
We present LDA model to discover topic distribution of retrieval passages and word distribution of each topic dimension, and then re-rank retrieval results with topic distribution similarity between passages based on N-size slide window.
Experiments on TREC 2007 Genomics collection and two distinctive IR baseline runs demonstrate the effectiveness of our method in promoting ranking diversity for biomedical information retrieval.
Evaluation results show that our approach can achieve 8&#x025; improvement over the highest Aspect MAP reported in TREC 2007 Genomics track.

The observations regarding computational linguistics in this paper are limited to developments in the United States, and further, to work which employs some measure of linguistic (rather than purely statistical) analysis.
The paper is not a survey, which would be redundant in view of the comprehensive reports on natural language processing provided by Walker (1973) and Damerau (1976 and in Sparck Jones and Kay (1973
Rather, it is organized around problems and solutions to problems, and it attempts to document the view that techniques of computational linguistics have reached the stage where applications in information science are now possible.*

Type systems commonly used in practice today fail to capture essential aspects of program behavior: The effects and dependencies of the programs.
In this paper, we examine a prototypical effect type system in the style of Gifford et al and a canonical example of a dependency type system based upon the work of Zdancewic.
Finally, we show how these two type systems can be embedded in a more general framework, a monadic type system as developed by Pfenning and Davies.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
. MSCIS-05-05.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/129 Cause and Effect: Type Systems for Effects and Dependencies Geoffrey Washburn
geoffw@cis.upenn.edu Technical Report MS-CIS-05-05 (Revision Department of Computer and Information Science University of Pennsylvania November th

We give a unified account ofthe probabilistic semantics underlying the language modeling approach and the traditional probabilistic model for information re-<lb>trieval, showing that the two approaches can be viewed as being equivalent probabilistically, since they are based on different factorizations of the same generative relevance model.
We also discuss how the two approaches lead to different<lb>retrieval frameworks in practice, since they involve component models that are estimated quite differently.

The rapidly growing data on the web has created a big challenge for directing the user to the web pages in their areas of interest.
Sentiment analysis or Opinion mining plays an important role in finding the area of interest based on user’s previous actions.
Social networking portals have been widely used for expressing opinions in the public domain.
Text based sentiment classifiers often prove inefficient.
Semantic web is the solution for Searching relevant information from huge repository of unstructured web data.
Semantic web leads the idea of ontology as background knowledge represents the concepts and the relationship in specialized domain.
The basic idea behind this survey is to take domain ontology for providing more elaborate sentiment scores.
We discuss an approach where information retrieved from web and ontology is created before sentiment classification and focuses on how to classify the semantic orientation of text.

A model based two-dimensional object recognition system capable of performing under occlusion and geometric transformation is described in this paper.
The system is based on the concept of associative search using overlapping local features.
During the training phase, the local features are hashed to set up the associations between the features and models.
In the recognition phase, the same hashing procedure is used to retrieve associations that participate in a voting process to determine the identity of the shape.
Two associative retrieval techniques for discrete and continuous features, respectively, are described in the paper.
The performance of the system is studied using a test set of 1,000 shapes that are corrupted versions of 100 models in the shape database.
It is shown that the incorporation of a verification phase to confirm the retrieved associations can provide zero error performance with a small reject rate.

We present Magnet Mail (MM a visualization system to retrieve information from email archives via a zoomable interface.
MM allows users to infer relationships among their email documents based on searching keywords.
The prototype interacts with a mass-market email system and uses a magnet metaphor to simulate user interaction with emails.
The graphical implementation relies mostly on the Piccolo toolkit.

In the context of the European Union (EU) funded research project PROMISE, a winter school was organized in the small ski resort of Zinal, Valais, Switzerland from January 23-27, 2012.
The title of the winter school was From Information Retrieval to Information Visualization and the goal was to bring together these two research domains that are currently quite separated but have an important potential to help each other in advancing the fields.
Indeed, the school has been attended by participants who came from one domain or the other and offered them the possibility of starting to acquire cross-disciplinary competencies.
Interestingly enough, the school turned out to be a brainstorming and discussion opportunity also for the lecturers, since they had the occasion of meeting colleagues from a quite different field with

To realize more efficient information retrieval it is critical to improve the user’s original query, because novice users can not be expected to formulate precise and effective queries.
Queries can often be improved by adding extra terms that appear in relevant documents but which were not included in the original query.
This is called query expansion.
Query refinement, a variant of query expansion, interactively recommends new terms related to the original query.
Because previous research did not offer any criterion to guarantee optimality, this paper proposes an optimal algorithm for query refinement with reference to the Bayes criterion.

The ubiquitous, diverse and growing impact of digital living creates a massive amount of short text a search query, a twit or a caption.
Short text frequently presents itself as an arbitrary combination of semantically unconnected words.
Using machine learning to classify the corpora of such texts is a challenging task.
A large body of research exists in this area, but in this paper we will focus on Background Knowledge (BK) and its role in machine learning for shorttext and non-topical classification.
More specifically, we present an effort to create a short text classification framework based on Background Knowledge.
We propose novel Information Retrieval techniques to construct BK and demonstrate the advantages of Automatic Query Expansion (AQE) vs. basic search.
We discuss other results of this research and its implications on the advancement of short text classification.

Information Gathering is becoming a tool more and more important for network information process as well as Information Retrieval (IR) and Information Filtering (IF In order to enhance the ratio of relevant information to the whole information gathered from network, relevance estimation must be carried out in the course of gathering.
Information Gathering Systems must estimate the relevance of document content to be gathered to the query users have put forward just before actual gathering actions.
We skim over TFIDF algorithm and in order to make up for its drawback, We introduce naive Bayes+KL algorithm based on the combination of naive Bayes model in Probabilistic Theory and Kullback Leibler algorithm in Information Theory.
Experimental results show that this kind of probabilistic algorithm could find out more relevant documents for it comes more close to semantic comprehension.

Polysemous words have more than one possible meaning, thus word ambiguity is a key issue for the systems which access textual information.
Computational linguistics proposes two main methods to cope with word ambiguity: sense disambiguation and sense discrimination Word)
Sense Disambiguation is the task of selecting a sense for a word from a set of predefined possibilities, while (Word)
Sense Discrimination is the task of dividing the usages of a word into different meanings, discriminating among word meanings based on information found in unannotated corpora.
This paper proposes a strategy to compare disambiguation and discrimination systems by adopting an "in vivo" evaluation in an Information Retrieval scenario.
The goal of the evaluation is to establish how disambiguation and discrimination bias the retrieval performance.

The amount of structured content published on the Web has been growing rapidly, making it possible to address increasingly complex information access tasks.
Recent years have witnessed the emergence of large scale human-curated knowledge bases as well as a growing array of techniques that identify or extract information automatically from unstructured and semi-structured sources.
The ESAIR workshop series aims to advance the general research agenda on the problem of creating and exploiting semantic annotations.
The eighth edition of ESAIR took place at CIKM 2015 in Melbourne, Australia, on the 23rd of October.
Having a special focus on applications, we dedicated an “annotations in action” track to demonstrations that showcase innovative prototype systems, in addition to the regular research and position paper contributions.
The workshop also featured invited talks from leaders in the field.
This report presents an overview of the event and its major outcomes.

3 Information Retrieval 8 3.1 Basic Issues and Problems 9 3.2 Document Space Granulations 10 3.3 Query
(User) Space Granulations 11
3.4 A Unified Probabilistic Model 12
3.5 Term Space Granulations 13 3.6 Retrieval Results Granulations 14
3.7 Structured and XML Documents 14

The terminology used in Biomedicine shows lexical peculiarities that have required the elaboration of terminological resources and information retrieval systems with specific functionalities.
The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the proliferation of polysemic acronyms and their interaction with common language.
Information retrieval systems in the biomedical domain use techniques oriented to the treatment of these lexical peculiarities.
In this paper we review some of the techniques used in this domain, such as the application of Natural Language Processing (BioNLP the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources.

Agency’s evaluation of offerors
quality” and “delivery
” past performance was reasonable, where solicitation advised offerors that past performance would be evaluated based on information listed in Past Performance Information Retrieval System--Statistical Reporting (PPIRS offerors were given an opportunity to correct inaccuracies in PPIRS records, and the agency confirmed the validity of negative past performance; agency’s decision not to select protester’s
lower-priced proposal was reasonable given its poor record of delivery performance and the agency’s rational decision that awardee’s superior performance record was worth the additional cost.

The link prediction problem has received extensive attention in fields such as sociology, anthropology, information science, and computer science.
In many practical applications, we only need to predict the potential links between the vertices of interest, instead of predicting all of the links in a complex network.
In this paper, we propose a fast similarity based approach for predicting the links related to a given node.
We construct a path set connected to the given node by a random walk.
The similarity score is computed within a small sub-graph formed by the path set connected to the given node, which significantly reduces the computation time.
By choosing the appropriate number of sampled paths, we can restrict the error of the estimated similarities within a given threshold.
Our experimental results on a number of real networks indicate that the algorithm proposed in this paper can obtain accurate results in less time than existing methods.

This paper proposes a standard testing platform for Information Retrieval (IR) that can be used for testing different IR statistical models in a controlled environment or in the Web.
The development of a standard system avoids the effort of developing a specific testing system to validate each method or model on the IR field of activity, working as a common platform.
Examples of applications on filtering, classification and retrieval of information are presented.

Controlled generation of single photons is one of the current interests in quantum optics and quantum information science.
Unlike classical light sources, the single photon sources can generate a sequence of single photons with a regular time interval exhibiting the so-called photon anti-bunching, which is an important subject in quantum optics.
Moreover, a stream of regular-spaced single photons can serve as an ideal carrier of quantum information with the quantum information encoded in a superposition of photon numbers or polarizations.

This work presents two studies that aim to discover whether age can be used as a suitable metric for distinguishing performance between individuals or if other factors can provide greater insight.
Information retrieval tasks are used to test the performance of these factors.
First, a study is introduced that examines the effect that fluid intelligence and Internet usage has on individuals.
Second, a larger study is reported on that examines a collection of Internet and cognitive factors in order to determine to what extent each of these metrics can account for disorientation in users This work adds to growing evidence showing that age is not a suitable metric to distinguish between individuals within the field of human-computer interaction.
It shows that factors such as previous Internet experience and fluid-based cognitive abilities can be used to gain better insight into users&#8217; reported browsing experience during information retrieval tasks.

This paper describes a new technique for the direct translation of character n-grams for use in Cross-Language Information Retrieval systems.
This solution avoids the need for word normalization during indexing or translation, and it can also deal with out-of-vocabulary words.
This knowledge-light approach does not rely on language-specific processing, and it can be used with languages of very different natures even when linguistic information and resources are scarce or unavailable.
Our proposal also tries to achieve a higher speed during the n-gram alignment process with respect to previous approaches.

Designing good user interfaces to information retrieval systems is a complex activity.
The design space is large and evaluation methodologies that go beyond the classical precision and recall figures are not well established.
In this paper we present an evaluation of an intelligent interface that covers also the user-system interaction and measures user's satisfaction.
More specifically, we describe an experiment that evaluates i) the added value of the semiautomatic query reformulation implemented in a prototype system ii) the importance of technical, terminological, and strategic supports and (iii) the best way to provide them.
The interpretation of results leads to guidelines for the design of user interfaces to information retrieval systems and to some observations on the evaluation issue.

In this paper, results of an empirical comparison of objects-first vs. objects-later are presented and discussed.
The study was carefully designed to align the two approaches so that the comparison is focused on the main difference between the two approaches; that is the different sequence in which topics are taught: object oriented topics from the beginning or not.
The study with duration of one year was carried out in a secondary school.
In the end, both groups showed the same increase in learning gain, but perceived the difficulty of topics differently.
We discuss study design, results and pedagogical implications.
Categories' and' Su34ect' Descri7tors K3.2 [Com7uters Education Computer and Information Science Education 0
1 General'Terms:'Experimentation, Human Factors.'

Many applications compute aggregate functions over an attribute (or set of attributes) to find aggregate values above some specified threshold.
We call such queries iceberg queries, because the number of abovethreshold results is often very small (the tip of an iceberg relative to the large amount of input data (the iceberg Such iceberg queries are common in many applications, including data warehousing, information-retrieval, market basket analysis in data mining, clustering and copy detection.
We propose efficient algorithms to evaluate iceberg queries using very little memory and significantly fewer passes over data, when compared to current techniques that use sorting or hashing.
We present an experimental case study using over three gigabytes of Web data to illustrate the savings obtained by our algorithms.

1 Research Group on Natural Computing Department of Computer Science and Artificial Intelligence University of Seville Avda.
Reina Mercedes s/n, 41012 Sevilla, Spain E-mail: mdelamor@us.es, marper@us.es 2 High Performance/Heterogeneous and Parallel Computing Lab Department of Computer and Information Science Norwegian University of Science and Technology Sem Sælands vei 9, NO-7491, Trondheim, Norway E-mail
: Ian.Karlin@colorado.edu, runeerle@idi.ntnu.no, elster@idi.ntnu.no

Information retrieval research, at least as conceived by the SIGIR community, is fundamentally experimental in nature.
As such, the presentation of results from controlled, reproducible experiments lies at the core of our work.
Many reports follow the same general format: authors propose a new retrieval method, whose performance on some well-defined task is compared against a baseline.
Authors also report results from alternative configurations, e.g variations in parameters, turning off (ablation) of different components, etc.
The presentation of experimental results forms an integral part of the conferences and journals that comprise the medium in which knowledge is disseminated.

After being a pilot track in 2005, GeoCLEF advanced to be a regular track within CLEF 2006.
The purpose of GeoCLEF is to test and evaluate cross-language geographic information retrieval (GIR retrieval for topics with a geographic specification.
For GeoCLEF 2006, twenty-five search topics were defined by the organizing groups for searching English, German, Portuguese and Spanish document collections.
Topics were translated into English, German, Portuguese, Spanish and Japanese.
Several topics in 2006 were significantly more geographically challenging than in 2005.
Seventeen groups submitted 149 runs (up from eleven groups and 117 runs in GeoCLEF 2005
The groups used a variety of approaches, including geographic bounding boxes, named entity extraction and external knowledge bases (geographic thesauri and ontologies and gazetteers).

The field “
Intelligent Fuzzy Theory
” has been expanding its topic and boundary in the past decades in order to reflect the complex requirements of the information society.
Consequently, many academic studies have tried to address the complex information science and technology issues in wider perspectives.
Thus, this special issue has a set of selected extended articles of the 4th World Conference on Information Systems and Technologies held at Recife, PE, Brazil, 22 24 March 2016, which aims to

With the rapid development of the Internet and great capacity of online documents, information retrieval has become an active research topic.
This paper proposes a novel information retrieval algorithm based on query expansion and classification.
The algorithm is induced by the observation that very short queries with the traditional information retrieval methods often have low precision, although they can get high recall.
Our approach attempts to catch more relevant documents by query expansion and text classification.
The results of the experiments show that the algorithm we proposed is more precise and efficient than the traditional query expansion methods.

Hypertext links are a powerful extension of standard information retrieval techniques based on query languages.
However, the generation of links is often impractical due to large manual and/or computational effort.
In this paper, we analyze the effects of two main approaches that aim at a restriction of the necessary efforts:
The direct use of OCR-processed documents instead of manually post-processed—
e. corrected—documents and the use of shorter excerpts of documents instead of complete documents.
For our tests, similarity links were computed based on the vector-space model; the links that are generated based on unmodified OCR documents and excerpts of documents are then compared to those links that are generated based on complete documents without OCR errors.

In this paper we introduce the intuitive notion of trivergence of probability distributions (
This notion allow us to calculate the similarity among triplets of objects.
For this computation, we can use the well known measures of probability divergences like Kullback-Leibler and Jensen-Shannon.
Divergence measures may be used in Information Retrieval tasks as Automatic Text Summarization, Text Classification, among many others.

In recent years Bayesian methods have become widespread in many domains including computer vision, signal processing, information retrieval and genome data analysis.
The availability of fast computers allows the required computations to be performed in reasonable time, and thereby makes the benefits of a Bayesian treatment accessible to an ever broadening range of applications.
In this tutorial we give an overview of the Bayesian approach to pattern recognition in the context of simple regression and classification problems.
We then describe in detail a specific Bayesian model for regression and classification called the Relevance Vector Machine.
This overcomes many of the limitations of the widely used Support Vector Machine, while retaining the highly desirable property of sparseness.

Collaborative Information Retrieval (CIR) is one of the popular social-based IR approaches.
A CIR system registers the previous user interactions to response to the subsequent user queries more efficiently.
But CIR suffers from the personalization problem because the goals and the characteristics of two users may be different; so when they send the same query to a CIR system, they may be interested in two different lists of documents.
We have developed a personalized CIR system, called PERCIRS, to solve this problem.
Selecting an efficient method to calculate the similarity between user profiles is a key factor for enhancing PERCIRS's efficiency.
In this paper, we propose a new graph-based method for user profile similarity calculation.
Finally, by introducing an evaluation method, we will show that this new method is more efficient than the previous methods.

In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task.
The formulas for combining the models are tuned on training data.
Then the system is evaluated on test data.
The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many recognition errors.
Also, the topics are real information needs, difficult to satisfy.
Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included.

Arabic, a highly inflected language, requires good stemming for effective information retrieval, yet no standard approach to stem¿ming has emerged.
We developed several light stemmers based on heuristics and a statistical stemmer based on co-occurrence for Arabic retrieval.
We compared the retrieval effectiveness of our stemmers and of a morphological analyzer on the TREC-2001 data.
The best light stemmer was more effective for cross-lan¿guage retrieval than a morphological stemmer which tried to find the root for each word.
A repartitioning process consisting of vowel removal followed by clustering using co-occurrence analy¿sis pro¿duced stem classes which were better than no stemming or very light stemming, but still inferior to good light stemming or mor¿phological analysis.

We present an introductory review of the latest advancements cold Rydberg atom research.
First, we briefly summarize the exaggerated properties of Rydberg atoms, and we discuss the new perspectives of Rydberg atom research that has been enabled by laser cooling and trapping technique.
We then highlight the latest developments and achievements in the newly emerged research fields for Rydberg molecules and cold neutral plasmas.
Various applications of the Rydberg blockade effect for quantum optics and quantum information science are also reviewed.

The Clef Ip track ran for the rst time within Clef 2009.
The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of cross lingual information access.
The track focused on the task of prior art search.
The 15 European teams who participated in the track deployed a rich range of Information Retrieval techniques adapting them to this new speci c domain and task.
A large-scale test collection for evaluation purposes was created by exploiting patent citations.

The information explosion across the Internet and elsewhere offers access to an increasing number of document collections.
Retrieval system evaluation plays an important role in judging the efficiency and effectiveness of the retrieval process.
Validating the performance of information retrieval systems provides assurance that a system as a whole is likely to meet its quantitative performance goals.
In this paper, we describe procedure to calculate the response time early in the life cycle and simulate the performance metrics for an information retrieval system in different architectures namely, centralized and distributed.
We consider different characterizations for the hardware resources' configurations and simulate the results for two different architectures of Information retrieval systems.

In this paper, we apply the concept of k-core on the graphof-words representation of text for single-document keyword extraction, retaining only the nodes from the main core as representative terms.
This approach takes better into account proximity between keywords and variability in the number of extracted keywords through the selection of more cohesive subsets of nodes than with existing graphbased approaches solely based on centrality.
Experiments on two standard datasets show statistically significant improvements in F1-score and AUC of precision/recall curve compared to baseline results, in particular when weighting the edges of the graph with the number of co-occurrences.
To the best of our knowledge, this is the first application of graph degeneracy to natural language processing and information retrieval.

This talk is in response to two Awards: the Association for Computing Machinery’s Athena Award, given by the ACM’s Committee on Women, on the nomination of the ACM Special Interest Group on Information Retrieval; and the British Computer Society’s Lovelace Medal.
It is a very great honour to have been given these awards.
I would like to say how much I appreciate this recognition.
Thank you, ACM and BCS.
I would particularly like to say, and I hope the ACM will not take this amiss, how I appreciate being the first woman to be awarded the BCS Lovelace Medal.
The awards carry the opportunity to give a lecture with them.
I deeply regret not being able to do this live and in a way to suit each specifically.
But I hope the single video based on this talk will go a little way as a substitute for two proper lectures.

Thesaurus has been widely used in many applications, including information retrieval, natural language processing, and question answering.
In this paper, we propose a novel approach to automatically constructing a domain-specific thesaurus from the Web using link structure information.
The proposed approach is able to identify new terms and reflect the latest relationship between terms as the Web evolves.
First, a set of high quality and representative websites of a specific domain is selected.
After filtering out navigational links, link analysis is applied to each website to obtain its content structure.
Finally, the thesaurus is constructed by merging the content structures of the selected websites.
The experimental results on automatic query expansion based on our constructed thesaurus show 20% improvement in search precision compared to the baseline.

A system for authentication or authorization maintains log data which records when someone uses some service, in addition to personal information, and therefore privacy protection for such a system is mandatory.
In this paper, we develop an infrastructure system which provides authentication and authorization functions to multiple services, protecting privacy of users of the services.
After deriving requirements for the privacy protection, we develop the system satisfying the requirements.
In the proposed system, because information is divided into different subsystems, it is difficult for such a subsystem to identify an individual.
The proposed system also utilizes Private Information Retrieval to prevent a subsystem identifying an individual who request an authentication process.

Geographical Information Retrieval is one of the top progressive research field in computer science since last decennium.
Many web queries have geographical references and now-a-days users are many times interested to find information related to not only particular location but also in certain range of particular direction.
Spatial search engines can handle these types of queries.
It searches those documents which are more relevant according to geographical references in the user's query.
In this paper, we present the research progress in the geographical searching in last ten years.
It is observed that still there are issues and challenges for searching in geographical web documents.
So, we proposed a model by which we can remove some issues and challenges and it will improve searching and relevance ranking.

This paper introduces the Chinese Semantic Retrieval model and the key technology.
It initially points out the core idea of mapping natural language to RDF triples.
Based on the Language Technology Platform dependency relationship, it classifies the Chinese question.
Then, the article outlines the concrete method of mapping Chinese question to SPARQL query with examples.
At last, it implements the system and gives the retrieval result, which decreases the irrelevant searching return compared with the tradition information retrieval method based on keyword matching.

The standard deviation of scores in the top <i>k</i> documents of a ranked list has been shown to be significantly correlated with average precision and has been the basis of a number of query performance predictors.
In this paper, we outline two hypotheses that aid in understanding this correlation.
Using score distribution (SD) models with known parameters, we create a large number of document rankings using Monte Carlo simulation to test the validity of these hypotheses.

Information Retrieval calls for accurate web page data extraction.
To enhance retrieval precision, irrelevant data such as navigational bar and advertisement should be identified and removed prior to indexing.
We propose a novel approach that identifies the web page templates and extracts the unstructured data.
Our experimental results on several different web sites demonstrate the feasibility of our approach.

Ratio 0.023 114.373
Title Ratio 0.021 132.651

In most Information Retrieval (IR) applications, Euclidean distance is used for similarity measurement.
It is adequate in many cases but this distance metric is not very accurate when there exist some different local data distributions in the database.
We propose a Gaussian mixture distance for performing accurate nearest-neighbor search for Information Retrieval
Under an established Gaussian finite mixture model for the distribution of the data in the database, the Gaussian mixture distance is formulated based on minimizing the Kullback-Leibler (KL) divergence between the distribution of the retrieval data and the data in database.
We compared the performance of the Gaussian mixture distance with the well-known Euclidean and Mahalanobis distance based on a precision performance measurement.
Experimental results demonstrate that the Gaussian mixture distance function is superior in the others for different types of testing data.

Extracting the users expected information from a large text collection based on some query is the aim of a Information Retrieval (IR) system.
Now a days Assamese Digital documents are increasing at a huge rate and to collect the information efficiently from them we are in need of an Assamese IR system for retrieving documents.
Comparing query and document term on lexical level and the shorter length query implies the problem like word mismatch.
Adding additional term with user's query means expanding the query can improve the IR system's performance.
The electronic lexical database, WordNet can help identifying the synonymous expressions and linguistic entities that are semantically similar with the input query.
Here we present an Assamese IR system based on vector space model and show our result by considering the query vector as the original user's query and the query is extended using the Assamese WordNet synsets.

Language resources such as machine dictionaries and lexical databases, aligned parallel corpora or even complete machine translation systems are essential in Cross-Language Text Retrieval (CLTR although not standard tools for the Information Retrieval task in general.
We outline the current use and adequacy for CLTR of such resources, focusing on the participants and experiments performed in the CLEF 2000 evaluation.
Our discussion is based on a survey conducted on the CLEF participants, as well as the descriptions of their systems that can be found in the present volume.
We also discuss how the usefulness of the CLEF evaluation campaign could be enhanced by including additional tasks which would make it possible to distinguish between the effect on the results of the resources used by the participating systems, on the one hand, and the retrieval strategies employed, on the other.

Therefore, information scientists are required to regularly review—and if necessary—redefine its fundamental building blocks.
This article is one of a group of four articles, which resulted from a Critical Delphi study conducted in 2003–2005.
The study Knowledge Map of Information Science was aimed at exploring the foundations of information science.
The international panel was composed of 57 leading scholars from 16 countries, who represent (almost) all the major subfields and important aspects of the field.
This particular article documents 130 definitions of data, information, and knowledge formulated by 45 scholars, and maps the major conceptual approaches for defining these three key concepts.

Application of the hypertext paradigm to information retrieval requires 1) an automatic generation of hypertext links, 2) a compact graphical representation of the data.
After a brief review of the family of neural algorithms required for deriving a compact and relevant representation of a documentary database, as well as links between synthetic “topics” and documents, we present a user interfaee based on these grounds.
This representation is two-step 1) a global topics map, 2) local topic axes, ranking both terms and documents according to the values of their “centrality index A prototype, running in a Macintosh environment and implementing a basic version of this browser, is then described and commented.

The ambition of this paper is to resume briefly the challenges that Arabic offers in CrossLingual Information Retrieval, to show the potential of MIMOR, a retrieval system, that has proved to be succesful for cross-lingual retrieval tasks, and to propose string matching techniques for feature unification instead of stemming techniques.

Representing and fusing multimedia information is a key issue to discover semantics in multimedia.
In this paper we address more specifically the problem of multimedia content retrieval through the joint design of an original multimodal information representation and of a machine learning-based fusion algorithm.
We first define a novel preference-based representation particularly adapted to the retrieval problem, and then, we investigate the RankBoost algorithm to combine those preferences to fullfill a user’s query.
Interestingly, it ends up being a flexible retrieval model that only manipulates ranking information and is blind to the intrinsic properties of the multimodal information input.
The approach is tested on annotated images and on the complete TRECVID 2005 corpus and compared with SVM-based fusion strategies.
The results show that our approach equals SVM performance but, contrary to SVM, is parameter free and faster.

This paper introduces mass estimation—a base modelling mechanism that can be employed to solve various tasks in machine learning.
We present the theoretical basis of mass and efficient methods to estimate mass.
We show that mass estimation solves problems effectively in tasks such as information retrieval, regression and anomaly detection.
The models, which use mass in these three tasks, perform at least as well as and often better than eight state-of-the-art methods in terms of task-specific performance measures.
In addition, mass estimation has constant time and space complexities.

We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations.
A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances.
Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.

The Public Sector produces and disseminates huge amounts of government information considered to be essential not only for its proper inner function (i.e. decision making processes but also for citizen transactions and the internal market.
However, the access to government information is limited and complicated due to the lack of advanced information systems based on international standards and formats.
This paper presents a knowledge management framework for the functional improvement of Government Information Centres, which efficiently controls the flow and disposal of all types of government information in Regional Administration.
In particular the implemented system: a) facilitates the flow of information within multifaceted administrative channels, b) ensures thematic homogeneity and interoperability for accurate and quality data/metadata exchange, and c) secures efficient information retrieval.

the paper proposes RecoProd a recommender system which uses sentiment analysis techniques to provide the best products for the customers.
The system uses the existing product reviews upon which sentiment classification is carried out.
RecoProd consists of an Information Retrieval component which extracts the reviews from the ecommerce websites using the product names as queries.
Sentiment Analysis algorithms like Naive Bayes and SVM are used to categorize the reviews and opinion scores are assigned to the reviews.
A comparative study on the accuracy of the sentiment analysis algorithms used is also carried out.
Aspect based summary of opinions for each product is carried out and visually compared.
The products are then clustered and the optimal product along with the recommended products is displayed to the user.

The problem of asymmetric information in electricity market is due to uncertainty in determining individual cost functions in assessing the participants' bids.
Individual loads or demand functions are assumed to be known to all participants.
Based on the information asymmetry, it is proposed that individual generators may attempt to optimize their operating profits by making assumptions about the uncertain information.
Each generator selects a supply bid curve as the best response to all simulated bidding scenarios.
After observing the actual outcomes and performing sensitivity analysis from the most similar bidding scenario, the bids are adjusted for subsequent use.
The optimality of the decisions may need to be altered by the power system constraints.
The objective of the paper is to provide a possible way of extracting information from the bidding process rather than analyzing the strategic behavior of the generators.

Information about the social networks of people is now becoming available through APIs offered by websites such as Facebook and Orkut.
Our insight is that this social network information can be used to infer the social context in which people are embedded in real life, and to develop mathematical models that can incorporate context-sensitivity in information retrieval algorithms.
We focus on participatory messages such as blog posts and forum discussions, and develop models considering the social network of message authors and recipients to predict the usefulness of messages to different recipients.
We validate our claims by surveying real users and correlating their responses with measurements on a web-crawl of a social networking website.
The precise mathematical definitions offered by us can be incorporated in personalized document ranking metrics, and demonstrate a novel approach that integrates elements of information science and sociology for analysis of participatory messages.

*Northeastern University, Shenyang, School of Information Science Engineering Liaoning 110004, P.R. China (e-mail:
lgliuyi @163.com SS Cyril Methodius University, Skopje, R. Macedonia, and Dogus University, Acibadem, Istanbul, 34722 Turkey (Tel 90-216-5445555; e-mail: gdimirovski @dogus.edu.tr
Australian National University, Research School of Information Sciences Engineering, Canberra ACT 0200, Australia (e-mail:
jun.zhao @anu.ac.au)

This study explores the benefits of integrating knowledge representations in prior art patent retrieval.
Key to the introduced approach is the utilization of human judgment available in the form of classifications assigned to patent documents.
The paper first outlines in detail how a methodology for the extraction of knowledge from such an hierarchical classification system can be established.
Further potential ways of integrating this knowledge with existing Information Retrieval paradigms in a scalable and flexible manner are investigated.
Finally based on these integration strategies the effectiveness in terms of recall and precision is evaluated in the context of a prior art search task for European patents.
As a result of this evaluation it can be established that in general the proposed knowledge expansion techniques are particularly beneficial to recall and, with respect to optimizing field retrieval settings, further result in significant precision gains.

Bug localization is the process of identifying the elements of source code that require modification to fix the bug.
By automating the task of bug localization efficiently, the cost of software can also be reduced.
For performing bug localization, many Information Retrieval models have been used in past.
In this paper, bug localization has been performed using Pachinko Allocation Model (PAM PAM is also an IR model, which falls under the category of topic models and has not been used for locating bugs yet.
This paper describes the proposed PAM based approach for bug localization at file level.
The PAM based approach is compared with LDA based approach and it has been shown that PAM based bug localization performs better as compared to LDA based bug localization.
For evaluating the performance of PAM and LDA based approaches, the datasets downloaded from two open source projects, i.e. Rhino and ModeShape, have been used.

Semantic similarity models are a series of mathematical models for computing semantic similarity values among nodes in a semantic net.
In this paper we reveal the paradox in the applications of these semantic similarity models in the field of information retrieval, which is that these models rely on a common prerequisite the words of a user query must correspond to the nodes of a semantic net.
In certain situations, this sort of correspondence can not be carried out, which invalidates the further working of these semantic similarity models.
By means of two case studies, we analyze these issues.
In addition, we discuss some possible solutions in order to address these issues.
Conclusion and future works are drawn in the final section.

andrei@derpi.tuwien.ac.at ABSTRACT
In this paper we describe the functional requirements for research information systems and problems which arise in the development of such a system.
Here is shown which problems could be solved by using knowledge markup technologies.
In this article one DAML OIL ontology for Research Information System is offered.
The already developed ontologies for research analyzed and compared.
The architecture based on knowledge markup for collecting research data and providing access to it is described.
It is shown how RDF Query Facilities can be used for information retrieval about research data.

This paper presents LIMSI’s participation in the User-Centered Health Information Retrieval task (task 2) at the CLEF eHealth 2015
In our contribution we explored two different strategies to query expansion, i.e. one based on entity recognition using MetaMap[1] and the UMLS[3 and a second strategy based on disease hypothesis generation using self-constructed external resources such a corpus of Wikipedia pages describing diseases and conditions, and webpages from the MedlinePlus health portal.
Our best-scoring run was a weighed UMLS-based run which put emphasis on incorporating signs and symptoms recognized in the topic text by MetaMap.
This run achieved a P@10 score of 0.262 and nDCG@10 of 0.196, respectively.

As a multidisciplinary field, medical informatics draws on a range of disciplines, such as computer science, information science, and the social and cognitive sciences.
The cognitive sciences can provide important insights into the nature of the processes involved in human- computer interaction and help improve the design of medical information systems by providing insight into the roles that knowledge, memory, and strategies play in a variety of cognitive activities.
In this paper, the authors survey literature on aspects of medical cognition and provide a set of claims that they consider to be important in medical informatics.

YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence.
In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning.
The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model.
We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.

We present a new approach based on neural networks to solve the merging strategy problem for Cross-Lingual Information Retrieval (CLIR
In addition to language barrier issues in CLIR systems, how to merge a ranked list that contains documents in different languages from several text collections is also critical.
We propose a merging strategy based on competitive learning to obtain a single ranking of documents merging the individual lists from the separate retrieved documents.
The main contribution of the paper is to show the effectiveness of the Learning Vector Quantization (LVQ) algorithm in solving the merging problem.
In order to investigate the effects of varying the number of codebook vectors, we have carried out several experiments with different values for this parameter.
The results demonstrate that the LVQ algorithm is a good alternative merging strategy.

We suggest partial logarithmic binning as the method of choice for uncovering the nature of many distributions encountered in information science (IS Logarithmic binning retrieves information and trends “not visible” in noisy power-law tails.
We also argue that obtaining the exponent from logarithmically binned data using a simple least square method is in some cases warranted in addition to methods such as the maximum likelihood.
We also show why often used cumulative distributions can make it difficult to distinguish noise from genuine features, and make it difficult to obtain an accurate power-law exponent of the underlying distribution.
The treatment is non-technical, aimed at IS researchers with little or no background in mathematics.

Classical retrieval models support content-oriented searching for documents using a set of words as data model.
However, in hypertext and database applications we want to consider the link structure and attribute values of documents in addition to the pure content.
In this paper, we present a framework based on probabilistic logical retrieval for describing the retrieval function for a query which refers to the content of documents, to the hypertext structure of documents, and to the database attribute values of documents.
The challenge is to find a retrieval function which yields welldefined retrieval weights for ranking the documents with respect to a combination of the query criteria.
We demonstrate the implementation and evaluation of our approach using HySpirit, a prototypical system of a probabilistic deductive database.

Finding opinionated blog posts is still an open problem in information retrieval, as exemplified by the recent TREC blog tracks.
Most of the current solutions involve the use of external resources and manual efforts in identifying subjective features.
In this paper, we propose a novel and effective dictionary-based statistical approach, which automatically derives evidence for subjectivity from the blog collection itself, without requiring any manual effort.
Our experiments show that the proposed approach is capable of achieving remarkable and statistically significant improvements over robust baselines, including the best TREC baseline run.
In addition, with relatively little computational costs, our proposed approach provides an effective performance in retrieving opinionated blog posts, which is as good as a computationally expensive approach using Natural Language Processing techniques.

We investigated extensions to the lexical association strategy.
The extensions include using conceptual association and acquiring the association information from different kind of lexical relations not limited to relations in VOPN structures.
We refer to this approach as DeepAttach.
Thus, it is possible to take information from all kinds of syntactical structures as long as they are alternations of a common deep structure [PU93] related to that implied by the intended attachment.

The 7<sup>th</sup>
Dutch-Belgian Information Retrieval Workshop on March 28 and 29, 2007 was hosted at the Katholieke Universiteit Leuven, Belgium.
The DIR workshop was held for the second time at this university.
Compared to its 2002 edition at the Katholieke Universiteit Leuven the DIR workshop has gained in importance over the years and is now run in a two-day format.
During the two days of DIR 2007 we have welcomed 94 participants.

With the enormous growth of internet and its global use, users find it difficult to extract meaningful information from it.
To overcome this problem, semantics methods for retrieval of required data are being studied.
This research paper explores the possibility of extracting semantic based retrieval of information.
First ontology was built which was published on the web.
Then the ontology was loaded to MySQL data store.
Using PHP language information from this ontology is retrieved using SPARQL.
WAMP server which is a package comprising of Apache, MySQL and PHP is used for the process.
This paper demonstrates the dominant nature of information retrieval from the web using semantic technologies.

Content-oriented XML retrieval approaches aim at a more focused retrieval strategy: Instead of retrieving whole documents, document components that are exhaustive to the information need while at the same time being as specific as possible should be retrieved.
In this article, we show that the evaluation methods developed for standard retrieval must be modified in order to deal with the structure of XML documents.
More precisely, the size and overlap of document components must be taken into account.
For this purpose, we propose a new effectiveness metric based on the definition of a concept space defined upon the notions of exhaustiveness and specificity of a search result.
We compare the results of this new metric by the results obtained with the official metric used in INEX, the evaluation initiative for content-oriented XML retrieval.

This paper describes the MUMIS project, which applies ontology based Information Extraction to improve the results of Information Retrieval in multimedia archives.
The domain specific ontology, the multilingual lexicons and the information passed between the different processing modules are all encoded in XML.
The innovative aspect is the use of a cross document merging algorithm that uses the information extracted from textual sources to produce an integrated, more complete, annotation of the material.
Ontology based reasoning and scenarios are used to merge and unify the separate annotations.
The techniques presented here have been implemented in a working demonstration prototype and have been tested on material from the European Championships Soccer 2000.

Over the last several decades, there have been numerous proposals for systems which can preserve the anonymity of the recipient of some data.
Some have involved trusted third-parties or trusted hardware; others have been constructed on top of link-layer anonymity systems or mix-nets.
In this paper, we evaluate a pseudonymous message system which takes the different approach of using Private Information Retrieval (PIR) as its basis.
We expose a flaw in the system as presented: it fails to identify Byzantine servers.
We provide suggestions on correcting the flaw, while observing the security and performance trade-offs our suggestions require.

Automatic Speech Recognition is an area which requires a large amount of training data.
Collecting such quantities of data involves significant time and cost owing to the tedious nature of collecting speech recordings and manual nature of transcribing it.
For a low resourced language such as Sinhala, collecting a sufficient data set is a major problem.
To address this issue we used the Active Learning technique from the Machine Learning paradigm which is applied to many tasks such as information retrieval.
Our experiment using a simple Sinhala speech corpus shows that through the use of Active Learning, the amount of utterances that need to be transcribed can be reduced by some 42% to achieve the same accuracy as using the whole data set without such a strategy.
This suggests that Active Learning techniques can be successfully applied to make optimal use of scarce resources for speech recognition for new languages.

In this paper we analyze and de ne the introduction of st or der logic in Formal Concept Analysis FCA the aims are both theoretical as a complete model is needed and applied so as to improve expression power of FCA as a knowledge mining tool and the relevance of its results
Our contribution consists in i the implementation of clas sical FCA in logic programming and the analysis of real cases ii the design of a complete st order FCA model iii the imple mentation of this st order FCA

The results of experiments comparing the relative performance of natural language and Boolean query formulations are presented.
The experiments show that on average a current generation natural language system provides better retrieval performance than expert searchers using a Boolean retrieval system when searching full-text legal materials.
Methodological issues are reviewed and the effect of database size on query formulation strategy is discussed.

Mobile users have the capability of accessing information anywhere at anytime with the introduction of mobile web search.
However, they still need to devote time and effort in order to retrieve relevant information in mobile devices.
Conversely, context is proposed to reduce users time and effort.
The recognition of context is supported by the availability of embedded sensors in mobile devices.
In this study, the context acquisition and utilization for mobile information retrieval are proposed.
The x201C;just-in-time&#x201D; approach is exploited in which the relevant information is retrieved without the user requesting it.
This will reduce the mobile user's effort, time and interaction and present the relevant information to the user in the right time and at the right place.

The use of structured documents following XML representation allows us to create content and structure (CAS) queries which are more specific for the user’s needs.
In this paper we are going to study how to enrich this kind of queries with the user feedback in order to get results closer to their needs.
More formally, we are considering how to perform Relevance Feedback (RF) for CAS queries in XML Information Retrieval.
Our approach maintains the same structural constraints but expands the content of the queries by adding new keywords to the original CAS query.
These new terms are selected by considering their presence/absence in the jugded units.
This RF method is integrated in a XML-based search engine and evaluated with the INEX 2006 and INEX 2007 collections.

Information retrieval is the selection of documents relevant to a query.
Inverted index is the conventional way to store the index of the collection.
Because of the large amounts of data, compression techniques are commonly used in information retrieval systems to reduce the size of the inverted index.
We experimentally evaluate the result of the mapping of such techniques on the Compressed Sparse Row (CSR) information retrieval (IR Our experimental results, using some of these compression techniques such Elais Gamma, Golomb, Interpolative, and fixed length Byte-Aligned, demonstrate that such techniques can easily be applied to compress the index in CSR IR.

In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query.
Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism.
This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion.
The embeddings are trained with convolutional neural networks or the word2vec model.
We demonstrate the performance of this model with image retrieval and text querying data sets.

We approached the problem as learning how to order documents by estimated relevance with respect to a user query.
Our support vector machines based classifier learns from the relevance judgments available with the standard test collections and generalizes to new, previously unseen queries.
For this, we have designed a representation scheme, which is based on the discrete representation of the local (lw) and global (gw) weighting functions, thus is capable of reproducing and enhancing the properties of such popular ranking functions as tf.idf, BM25 or those based on language models.
Our tests with the standard test collections have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely from the labeled examples and without taking advantage of knowing those functions or their important properties or parameters.

We present an approach to using ontologies as interlingua in cross-language information retrieval in the medical domain.
Our approach is based on using the Unified Medical Language System (UMLS) as the primary ontology.
Documents and queries are annotated with multiple layers of linguistic information (part-of-speech tags, lemmas, phrase chunks Based on this we identify medical terms and semantic relations between them and map them to their position in the ontology.
The paper describes experiments in monolingual and cross-language document retrieval, performed on a corpus of medical abstracts.
Results show that semantic information, specifically the combined use of concepts and relations, increases the precision in monolingual retrieval.
In cross-language retrieval the semantic annotation outperforms machine translation of the queries, but the best results are achieved by combining a similarity thesaurus with the semantic codes.

This paper presents UOWD-Sharif team’s approach for XML information retrieval.
This approach is an extension of PLIR which is an experimental knowledge-based information retrieval system.
This system like PLIR utilizes plausible inferences to first infer the relevance of sentences in XML documents and then propagates the relevance to the other textual units in the document tree.
Two approaches have been used for propagation of confidence.
The first approach labeled “propagate-DS” first propagates the confidence from sentences to upper elements and then combines these evidences by applying Dempster-Shafer theory of evidence to estimate the confidence in that element.
The second approach
DS-propagate” first applies the Dempster-Shafer theory of evidence to combine the evidences and then propagates the combined confidence to the parent element.
The second approach performs relatively better than the first approach.

Dealing with unstructured information is currently a hot research topic since most documents exist in an unstructured form.
The effective exploitation of unstructured document, although intricate, is of paramount importance to Information Retrieval
(IR The key to using unstructured data set is to identify the hidden structures within the data set.
In this paper, we present an approach to recognize the semantic structure of documents in Arabic legal data.
Several main concepts of a document are expressed in this structure, which includes title, the headings of the chapters, sections, subsections, etc.
This structural information is employed to obtain a richer and more fine-grained annotation of documents forming a useful and coherent infrastructure ready for IR.
Some experiments were conducted in order to evaluate our approach.
The initial results seem promising.

The development of Web2.0 not only update the network industry, but also vastly impact on the traditional retrieval methods of networked information and put forward more new demands.
The paper analyses the development of Web2.0 and the new demand of networked information retrieval, describes and evaluates the current mode of networked information retrieval.
At last, the author put forwards a new conceptual mode which based on JXTA and P2P of networked information retrieval.

Question retrieval aims to increase the accessibility of the community Question Answer (cQA) archives and has attracted increasing research interests recently.
In this paper, we present a novel method for improving the question retrieval performance by investigating the question term selection and weighting as well as reranking results.
Different from previous work, we propose a hierarchical question classification method with a sparse regularization to mimc user's question labeling in cQAs.
Based on the hierarchical classification, we explore the local context of the question for term selection and reranking results and then integrating them into our proposed general question retrieval framework.
The experimental results on a Yahoo!
Answers dataset show the effectiveness of our method as compared to existing general question retrieval models and some state-of-the-art methods of utilizing category information for question retrieval.

Topic models are a useful and ubiquitous tool for understanding large corpora.
However, topic models are not perfect, and for many users in computational social science, digital humanities, and information studies—who are not machine learning experts
—existing models and frameworks are often a “take it or leave it
” proposition.
This paper presents a mechanism for giving users a voice by encoding users’ feedback to topic models as correlations between words into a topic model.
This framework, interactive topic modeling (itm allows untrained users to encode their feedback easily and iteratively into the topic models.
Because latency in interactive systems is crucial, we develop more efficient inference algorithms for tree-based topic models.
We validate the framework both with simulated and real users.

We present results of text data mining experiments for music retrieval, analyzing microblogs gathered from November 2011 to September 2012 to infer music listening patterns all around the world.
We assess relationships between particular music preferences and spatial properties, such as month, weekday, and country, and the temporal stability of listening activities.
The findings of our study will help improve music retrieval and recommendation systems in that it will allow to incorporate geospatial and cultural information into models for music retrieval, which has not been looked into before.

In this paper, we propose a query-based pre-retrieval approach to the model selection problem, which automatically selects the best-performing retrieval model before the retrieval process takes place.
In this approach, the queries are clustered according to their statistics and the bestperforming retrieval model is associated to each cluster.
For a given new query, we assign the closest cluster to the query, and then we apply the model associated to the cluster.
We evaluate the model selection approach on the disk1&2 of the TREC collections.
The results show that our model selection approach achieves stable performance, which could outperform the use of the optimal retrieval model indifferently for each query.
The results also show that, interestingly, a retrieval model provides consistent performance for queries belonging to the same cluster.

The Rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval.
Here, a probabilistic analysis of this algorithm is presented in a text categorization framework.
The analysis gives theoretical insight into the heuristics used in the Rocchio algorithm, particularly the word weighting scheme and the similarity metric.
It also suggests improvements which lead to a probabilistic variant of the Rocchio classi er.
The Rocchio classi er, its probabilistic variant, and a naive Bayes classi
er are compared on six text categorization tasks.
The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classi er not only because they are more well-founded, but also because they achieve better performance.

In this chapter, we describe our question answering system, which was the winning system at the Human–Computer Question Answering (HCQA) Competition at the Thirty-first Annual Conference on Neural Information Processing Systems (NIPS The competition requires participants to address a factoid question answering task referred to as quiz bowl.
To address this task, we use two novel neural network models and combine these models with conventional information retrieval models using a supervised machine learning model.
Our system achieved the best performance among the systems submitted in the competition and won a match against six top human quiz experts by a wide margin.

The notion of interpoint-distance-based graphs has in the past, guided the extension of distributional order-measures to multivariate observations.
The concept of minimal spanning tree (MST) was introduced as the key pattern for generalizing the univariate two-sample problem to multivariate observations.
Here, the multivariate Wald-Wolfowitz test is further quantified using the enhanced representations of orthogonal MSTs.
Their advantages are investigated by comparing the similarity between color distributions in the feature space, using a standard feature extraction technique borrowed computer vision.
To demonstrate the performance of the proposed scheme, the application on a diverse collection of images has been systematically studied in a query-by-example visual information retrieval task.
Experimental results show that a powerful measure of similarity can emerge from the statistical comparison of their efficiently drawn pattern representations

When dealing with large scale applications, data sets are huge and very often not obvious to tackle with traditional approaches.
In web information retrieval, the greater the number of documents to be searched, the more powerful approach required.
In this work, we develop document search processes based on particle swarm optimization and show that they improve the performance of information retrieval in the web context.
Two novel PSO algorithms namely PSO1-IR and PSO2-IR are designed for this purpose.
Extensive experiments were performed on CACM and RCV1 collections.
The achieved results exhibit the superiority of PSO2-IR on all the others in terms of scalability while yielding comparable quality.

Database technology offers design methodologies to rapidly develop and deploy applications that are easy to understand, document and teach.
It can be argued that information retrieval (IR) lacks equivalent methodologies.
This poster discusses a generic data model, the Probabilistic Object-Oriented Content Model, that facilitates solving complex IR tasks.
The model guides how data and queries are represented and how retrieval strategies are built and customised.
Application/task-specific schemas can also be derived from the generic model.
This eases the process of tailoring search to a specific task by offering a layered architecture and well-defined schema mappings.
Different types of knowledge (facts and content) from varying data sources can also be consolidated into the proposed modelling framework.
Ultimately, the data model paves the way for discussing IR-tailored design methodologies.

Automatic citation recommendation based on citation context, together with consideration of users’ preference and writing patterns is an emerging research topic.
In this paper, we propose a novel personalized convolutional neural networks (p-CNN) discriminatively trained by maximizing the conditional likelihood of the cited documents given a citation context.
The proposed model not only nicely represents the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also includes authorship information.
It includes each paper’s author into our neural network’s input layer and thus can generate semantic content features and representative author features simultaneously.
The results show that the proposed model can effectively captures salient representations and hence significantly outperforms several baseline methods in citation recommendation task in terms of recall and Mean Average Precision rates.

Quality of our living environment matters.
As in book called Place Matter is said Where we live makes a big difference in the quality of our lives Dreier et al.
Quality of the living environment has been in the interest of different disciplines.
Environmental psychology, geography, sociology, communicative and collaborative urban planning are mainly those fields, as Manzo et al 2006) says, that gives us a more deeper understanding not only how planning impacts our experience of place, but also how community-focused emotions, cognitions, and behaviours can impact community planning and development To these fields the geographical information science (GIS) can offer a substantial addition by giving a possibility to study the quality of the living environment extensively.

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.
General Terms Natural Languages Processing Arabic Language Processing, Information Retrieval.

Source code has become a data source of interest in the recent years.
In the software industry is common the extraction of source code metrics, mainly for quality assurance purposes.
In this paper source code metrics are used to consolidate programmers profiles with the purpose to identify different personality traits using machine learning algorithms.
This work was done as part of the Personality Recognition in SOurce COde (PR-SOCO) shared task in the Forum for Information Retrieval Evaluation 2016
(FIRE 2016 CCS Concepts •Information systems
Content analysis and feature selection Computing methodologies
Supervised learning by regression; Cluster analysis General and reference Metrics Software and its engineering Parsers;

In this paper, we describe VIRLab, a novel web-based virtual laboratory for Information Retrieval (IR Unlike existing command line based IR toolkits, the VIRLab system provides a more interactive tool that enables easy implementation of retrieval functions with only a few lines of codes, simplified evaluation process over multiple data sets and parameter settings and straightforward result analysis interface through operational search engines and pair-wise comparisons.
These features make VIRLab a unique and novel tool that can help teaching IR models, improving the productivity for doing IR model research, as well as promoting controlled experimental study of IR models.

The Open-i project combines research in text processing, image analysis and machine learning to create a system (also called Open-i) that enables about 10,000 users a day to retrieve relevant images and expanded citations from the open-access biomedical literature, as well as from clinical and historic image collections.
Searching may be done by text as well as image queries.
Images include a wide range of clinical imaging modalities, graphs, charts, photographs and other illustrations.
The images are indexed by text in captions and mentions in the article, as well as by image features.
This report presents the underlying research in natural language processing, biomedical image analysis, and informatics leading to the design, development and practical implementation of this system.

Consider the setting where judges are repeatedly asked to (partially) rank sets of objects, and assume that each judge tries to reproduce some true underlying ranking to the best of their ability.
Rank aggregation aims to combine the rankings of such experts to produce a better joint ranking, and is a ubiquitous problem in Information Retrieval (IR) and Natural Language Processing (NLP
In IR, for instance, meta-search aims to combine the outputs of multiple search engines.
In machine translation (MT aggregation of multiple systems built on different underlying principles has received considerable attention recently (e.g 5

In this paper the Unicode Cross-Language Information Retrieval system (UCLIR) is described.
UCLIR accepts a query in one language and then retrieves relevant documents in any of several languages.
The core process involves a suite of technologies including machine translation and standard monolingual information retrieval.

Due to the cold-start problem, measuring the similarity between two pieces of audio music based on their low-level acoustic features is critical to many Music Information Retrieval (MIR) systems.
In this paper, we apply the bag-offrames (BOF) approach to represent low-level acoustic features of a song and exploit music tags to help improve the performance of the audio-based music similarity computation.
We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we propose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM.
The results of audio-based query-by-example MIR experiments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ

We address the problem of developing a flexible framework for information retrieval (IR) in structured documents, such as XML.
The framework is able to support a wide range of structured IR queries, transparent instantiations of different retrieval models, and different physical implementations.
It is based on so-called score region algebra (SRA) that can express the following four essential ranked retrieval aspects for structured IR: term and element selection, element relevance score computation, element score propagation, and element score combination.
Our preliminary research shows that different instantiations of each aspect, as well as different combinations of these instantiations, yield significantly different results.
Our goal is to better understand structured IR by studying these aspects alone and their combination in the framework of SRA, and to use this knowledge to improve our structured IR system.

Peer-to-peer information systems have gained prominence of late with applications such as file sharing systems and grid computing.
However, the information retrieval component of these systems is still limited because traditional techniques for searching and ranking are not directly applicable.
This work compares search in peer-to-peer information systems to that in metasearch engines, and describes how they are unique.
Many works describing advances in peer-to-peer information retrieval are cited.

If you really want to be smarter, reading can be one of the lots ways to evoke and realize.
Many people who like reading will have more knowledge and experiences.
Reading can be a way to gain information from economics, politics, science, fiction, literature, religion, and many others.
As one of the part of book categories, applied informetrics for information retrieval research always becomes the most wanted book.
Many people are absolutely searching for this book.
It means that many love to read this kind of book.

We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation.
We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed.
After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation.
We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases.
As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.

Applications of ubiquitous computing are increasingly leveraging contextual information from several sources to provide users with behavior appropriate to their environment.
The method of information retrieval is one of the most fundamental research issues in ubiquitous computing.
Applications where users contexts change continuously over time require prompt retrieval of relevant information.
This paper proposes a new ubiquitous retrieval method that enables users to obtain relevant information efficiently using the multi level characteristics of the contexts.
This paper describes the ubiquitous information retrieval process.
Several experiments are performed and the results verify that proposed method has better retrieval performance.

This paper first proposes a new efficient algorithm for extracting similar sections between two time sequence data sets.
The algorithm is called Relay Continuous Dynamic Programming: Relay CDP, which realizes fast matching between arbitrary sections in the reference pattern and the input speech and enables extracting similar sections frame-synchronously.
We extend Relay CDP to extract repeated utterances in a presentation speech because the repeated utterances, such as the same words, same phrases or same sentences, are assumed to be important phrases in the speech.
Those repeated utterances can be regarded as some labels for information retrieval.
This paper describes the detail of Relay CDP and the performance, which is evaluated for extracting the similar sections between two speech data sets and for identifying the repeated utterances by extracting similar sections in a presentation speech.

While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in infrared (IR) videos is significantly less explored.
Our objective is to exploit imaging data in this modality for the action recognition task.
In this work, we propose a novel two-stream 3D convolutional neural network architecture by introducing the discriminative code layer and the corresponding discriminative code loss function.
The proposed network processes IR images and the IR-based optical flow field sequences.
We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset.
We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs.
Experimental results demonstrate that our approach can achieve state-of-the-art average precision performances on the InfAR dataset.

A space of fuzzy multisets is considered and applied to clustering of documents/terms for information retrieval.
Fuzzy c-means clustering algorithms with kernel functions in support vector machines are studied.
A numerical example is given.

Personalization of web search is used for effective Information Retrieval in order to better satisfy the information need of the user on the web.
The web usage mining has been used widely in Personalization of Web Search(PWS
The effectiveness of the Personalization of Web Search based on clustered web usage data depends on the quality of clusters.
It is found in research that there exist no clustering algorithms that produce clusters of 100% quality.
In this paper the Genetic Algorithm(GA) is used for clusters optimization in order to improve the quality of clusters for effective Personalized web search.
Experiment was conducted on the data set of query sessions captured on the web in Academics, Entertainment and Sports Domain.
The search results confirm the improvement in the average precision of the PWS(with cluster optimization) in comparison to PWS( without cluster optimization).

This classic panel session will review recent trends and issues in the study of the history of information science and technology and present findings from the first awards given by the ASIST History Fund.
It will consist of three presentations 1) an overview, by Robert V. Williams, of recent trends and issues and identify some of the major gaps that need to be addressed in future work 2) a presentation by Charles Meadow, winner of the 2009 ASIST History Fund Research Grant award, of the results of his study of the history of the digital divide 3) a presentation by Rachel Plotnick, winner of the 2009 ASIST History Fund Best Paper award, on her study of the history of a total hospital medical information system developed in the 1960's and 1970's.

This paper proposes a method to overcome the drawbacks of WordNet when applied to information retrieval by complementing it with Roget 's thesaurus and corpus-derived thesauri.
Words and relations which are not included in WordNet can be found in the corpus-derived thesauri.
Effects of polysemy can be minimized with weighting method considering all query terms and all of the thesauri.
Experimental results show that our method enhances information retrieval performance significantly.
Department of Computer Science Tokyo Institute of Technology 2-12-1 Oookayama Meguro-Ku Tokyo 152-8522 Japan {rila,take,tanaka}@cs.titech.ac.jp expansion (Voorhees, 1994; Smeaton and Berrut, 1995 computing lexical cohesion (Stairmand, 1997 word sense disambiguation (Voorhees, 1993 and so on, but
the results have not been very successful.
Previously, we conducted query expansion experiments using WordNet (Mandala et al to appear 1999) and found limitations, which can be summarized as follows :

This thesis examines the architectural issues in the design of a video capture board intended for use in multimedia videoconferencing.
The major issues examined are: Control of reception and transmission of multimedia video streams i)
Quality of service and service provision (ii) Compression requirements and solutions (iii) Data buffering and card connection strategies (iv) Handling multiple video streams Results of measurements for prototype boards designed and constructed at Penn are also given.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-93-31.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/321 Experimental Evaluation Of A Video Capture Board For Networked Workstations MS-CIS-93-31 DISTRIBUTED SYSTEMS LAB 29

This workshop report discusses the collaborative work of UT, EMC and TNO on the TREC Genomics Track 2007.
The biomedical information retrieval task is approached using cross language methods, in which biomedical concept detection is combined with effective IR based on unigram language models.
Furthermore, a co-occurrence method is used to select and filter candidate answers.
On its own, the cross lingual approach and the filtering do not strongly improve retrieval results.
However, the combination of approaches does show a strong improvement over the monolingual baseline.

Dictionary methods for cross-language information retrieval have shown performance below that for mono-lingual retrieval.
Failure to translate multi-term phrases has been shown to be one of the factors responsible for the errors associated with dictionary methods.
First, we study the importance of phrasal translation for this approach.
Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to signiicantly reduce the error associated with automatic dictionary translation.

We demonstrate a parallel implementation of a sparse matrix information retrieval engine.
We use a shared nothing PC cluster.
We perform our experiments with TREC disk 4 and 5 data, a NIST 2 Gigabytes standard benchmark text collection on 2, 4, 6, 8, 10, 12 and 14 processing nodes with different queries.
We compare the results with the results of sequential inverted index, a conventional and common indexing and query processing method.
The experimental results are promising and show a significant speedup.

Audio data is one of typical multimedia data and it contains plenty of information.
Audio retrieval is becoming important content in multimedia information retrieval.
In multimedia retrieval researches, it becomes more and more important research part how to construct better classifiers for audio classification and retrieval.
Support Vector Machines, a novel method of the Pattern Recognition, presents excellent performance in solving the problems with small sample, nonlinear and local minima.
But audio classification is a multi-class classification problem and its just one of problems to be solved in SVM researches.
In this paper, it compares several common Support Vector Machines and proposes a hierarchical Support Vector Machines based on audio features cluster method, combining audio features and hierarchical SVMS.
It uses hierarchical classification method to classify audio data and its proved better performance by experiments.

Short and ambiguous queries are the major problems in search engines which lead to irrelevant information retrieval for the users’ input.
The increasing nature of the information on the web also makes various difficulties for the search engine to provide the users needed results.
The web search engine experience the ill effects of ambiguity, since the queries are looked at on a rational level rather than the semantic level.
In this paper, for improving the performance of search engine as of the users’ interest, personalization is based on the users’ clicks and bookmarking is proposed.
Modified agglomerative clustering is used in this work for clustering the results.
The experimental results prove that the proposed work scores better precision, recall and F-score.

Geo-spatial ontologies provide knowledge about places in the world and spatial relations between them.
They are fundamental in order to build semantic information retrieval systems and to achieve semantic interoperability in geo-spatial applications.
In this paper we present GeoWordNet, a semantic resource we created from the full integration of GeoNames, other high quality resources and WordNet.
The methodology we followed was largely automatic, with manual checks when needed.
This allowed us accomplishing at the same time a never reached before accuracy level and a very satisfactory quantitative result, both in terms of concepts and geographical entities.

The paper provides an introduction to and survey of probabilistic approaches to modelling Information Retrieval.
The basic concepts of probabilistic approaches to Information Retrieval are outlined, and the principles and assumptions upon which the approaches are based are presented.
The various models that have been proposed in the development of IR are described, classi ed, and compared.
The models are classi ed and compared using a common formalism.
New approaches that constitute the basis of future research are described.

Various feature schemes have been proposed through acoustic study and pattern recognition research.
In this paper our main intention is to investigate the performance of different rhythmic feature schemes as well as find a good rhythmic feature combination for a robust musical instrument classifier.
Lots of work has been done on speech and speaker recognition.
Musical instrument recognition is an important aspect of music information retrieval system.
In this paper we have discussed different rhythmic features namely beat histogram, dynamic range, spectral crest facture, mean, variance.
kurtosis etc.[1].

Information Retrieval in large digital document repositories is at the same time a hard and crucial task.
While the primary type of information available in documents is usually text, images play a very important role because they pictorially describe concepts that are dealt with in the document.
Unfortunately, the semantic gap separating such a visual content from the underlying meaning is very wide.
Additionally image processing techniques are usually very demanding in computational resources.
Hence, only recently the area of Content-Based Image Retrieval has gained more attention.
In this paper we describe a new technique to identify known objects in a picture based on a comparison of the shapes to known models.
The comparison works by progressive approximations to save computational resources, and relies on novel algorithmic and representational solutions to improve preliminary shape extraction.

Augmented reality is one of the new applications to archaeology that gives to user the sense of “being there” and allows to observe virtually reconstructed archaeological landscapes with historical buildings.
This paper presents a system that aims to representation of a virtual environment in stereo, using 3 virtual cameras.
It allows user to put in scene one or more models that are ready to use and to navigate in them.
In addition to this, it gives the opportunity to interact with the virtual environment in real time, to rotate the objects presented in scene and observe their detail.
It also establishes links between the objects of the scene and any database and allows user to obtain additional information about the places or objects.
The system can be used by researchers as a tool that will help them in developing and testing new techniques on 3D representation (e.g. multiple LOD meshes, etc) of any 3D data.

This paper presents an architecture for information retrieval agents in which each agent declaratively describes its domain, input, output, and user interface.
A mediating piece of software can then assemble software agents for a given information retrieval task, and produce a single, unified user interface for that task from the individual agents’ descriptions.

A retrodirective transponder based on a novel compact phaseconjugating mixer with conversion gain has been developed.
The active circuit uses one port for both incoming and outgoing signals, enabling a reduction of circuit size, and the balanced structure provides suppression of undesired signals.
By using a modulated local oscillator, the circuit can modulate the received signal in order to retransmit local information to the remote site.
A microstrip antenna is integrated with the phase conjugator and the whole system was fabricated on a single substrate, enabling a one-card system.
A four-element prototype array with 0 5 array spacing demonstrated excellent measured retrodirectivity.
Additionally, a simplified binary-phase-shift-keying signal transmitted by the array is recovered successfully at the source location, demonstrating great potential for remote tagging and wireless sensor applications.

Text Categorization (TC also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set.
If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task.
TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers.
In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents.

1. EXTENDED ABSTRACT
This demonstration presents the Do-It-Yourself (DIY) web service of the Music Information Retrieval Evaluation eXchange (MIREX As TREC does for text retrieval, MIREX provides standardized datasets and evaluation frameworks to evaluate Music Information Retrieval (MIR) systems and algorithms [1
However, unlike TREC where participants are given the datasets and execute their code locally, MIREX data sets cannot be distributed due to copyright restrictions.
In previous years, MIREX participants submitted systems to the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL where they were manually executed, and evaluated.

Introduction Since its origin in the late 1980’s, the development of geographical information science and of geographical information systems (GIS the toolset enabling to conduct this type of research, has now reached the necessary maturity to be considered a main stream application:
GIS evolved from the status of ‘a promising tool’ to the status of ‘a tool achieving its promises To maintain this status the entire chain of events from data collection to data analysis must be adapted to the specific needs and requirements of spatial analysis.

Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition.
The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model.
A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness.
In this paper, we study the problem of language model smoothing and its influence on retrieval performance.
We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collection.

There is need for more foundational research in the development of interactive information retrieval systems.
The results of a week long discussion by a group of multi­disciplinary researchers have reported here.
A broief description of main activities and major recommendations of the workshop are reported here.

Integrated photonics is required to fully exploit the capabilities of Optical Quantum Information science.
We demonstrate new components that take full advantage of the integrated architecture; we show quantum interference in MMI couplers and two-particle quantum walks in coupled waveguides.

The retrieval performance of an information system usually increases when it uses the relationships among the terms in a given document collection.
This paper presents an improved co-occurrences frequency method to mine relationship among document index terms, then gives an extended model by adding one term layer in belief network model, shows its topology, probability estimating and information retrieval process.
Experiment shows that this model can realize the semantic retrieval to some extent

This paper summarizes the state of the art in Multilingual Information Retrieval, paying special attention to the linguistic resources used and to the interactive aspects of searching documents in unknown languages.

Today’s Web, large intranets and even the documents collected by a single user are enormous sources of distributed, heterogeneous information that cannot be easily mastered.
Syntactical and semantical differences as well as missing semantic annotations make effective query evaluation on such corpora a hard task.
The Semantic Web aims at providing a standard for semantic annotations, but has not yet made large progress in the real world.
This paper presents a light-weight version of the Semantic Web.
We advocate the use of Information Extraction tools to automatically detect and annotate important classes of information that are frequently used in queries, like locations and dates.
We propose a query language that can exploit the extra annotations and allows novel range and join conditions.

Information extraction is one of the ways to convert unstructured text into structured records.
Most of the previous work in this field are devoted to add semantic tags to specific textual content, so their structures are often plain which cannot illustrate relationships among semantic features.
A novel approach, Structure Information Extraction System based on Hidden Markov Model (SIEHMM for the task of extracting structure from plain texts is proposed in these papers, which utilizes path information for HMM training and automatically generate XML.
Experiments on a real life dataset show SIEHMM has a high precision and recall ratio and can not only help solve problems of structural storage and text information retrieval, but also take advantages of XML to meet the future trends.

We describe a new approach to information retrieval: algorithmic mediation for intentional, synchronous collaborative exploratory search.
Using our system, two or more users with a common information need search together, simultaneously.
The collaborative system provides tools, user interfaces and, most importantly, algorithmically-mediated retrieval to focus, enhance and augment the team's search and communication activities.
Collaborative search outperformed <i>post hoc</i> merging of similarly instrumented single user runs.
Algorithmic mediation improved both collaborative search (allowing a team of searchers to find relevant information more efficiently and effectively and exploratory search (allowing the searchers to find relevant information that cannot be found while working individually).

Search effort is an important aspect of Interactive Information Retrieval (IIR Prior findings show that higher cognitive ability searchers tend to perform more actions than lower ability searchers.
In an eye-tracking lab study we investigated the effects of working memory (WM) on search effort.
The findings show that higher WM searchers perform more actions and that most significant differences are in time spent on reading results pages.
We also show that behavior of high and low WM searchers changes differently in the course of a search task performance.

Document clustering methods which have been proposed by R. E. Bonner and J. J. Rocchio are compared.
Bonner's method is found to give higher precision than Rocchio T s method, while the recall for the two methods is about the same.
Bonner's method necessitates about twice as many comparisons against a query vector as Rocchio's method; this is to be expected since Rocchio controls the cluster size in order to maximize search efficiency.
Manual relevance judgments are used as well as relevance judgments determined by query document cosines.
The results are found to be invariant under the two measures.

In this paper we describe a novel approach to applying text-based information retrieval techniques to music collections.
We represent tracks with a joint vocabulary consisting of both conventional words, drawn from social tags, and audio <i>muswords</i representing characteristics of automatically-identified regions of interest within the signal.
We build vector space and latent aspect models indexing words and muswords for a collection of tracks, and show experimentally that retrieval with these models is extremely well-behaved.
We find in particular that retrieval performance remains good for tracks by artists unseen by our models in training, and even if tags for their tracks are extremely sparse.

To assist a user in an information search on the web, an agent needs a model of the user’s interests and mechanisms to interpret queries and evaluate the relevance of documents.
This paper presents a system where a user profile is built from a personal document archive managed by the user and a task context is computed when a user submits a query, from her/his profile and an action log built by tracking the user’s activity.
A query is then interpreted through a vocabulary learning step, where terms in the context are selected, and a query learning step, where new queries are built by the conjunction of terms of the initial query with terms of the vocabulary.
First results are presented and future directions, concerning the relative evaluation of this system, are considered.

The aim of this research is twofold.
On the one hand, high accuracy retrieval has been a concern of the information retrieval community for some time.
We aim to investigate this issue via data fusion.
On the other hand, correlation among component results has been proven to be harmful to data fusion, but has not been taken into account in data fusion algorithms.
In the hope of achieving better performance, we propose a group of algorithms to eliminate the effect of uneven correlation among component results by assigning different weights to all component results or their combinations.
Then the linear combination method or a variation

This paper reports on recent work in the field of information retrieval that attempts to go beyond the overly simplified approach of representing documents and queries as bags of words.
Simple models make it difficult to accurately model a user’s information need.
The model presented in the paper is based on Markov random fields and allows almost arbitrary features to be encoded.
This provides a powerful mechanism for modeling many of the implicit constraints a user has in mind when formulating a query.
Simple instantiations of the model that consider dependencies between the terms in a query have shown to significantly outperform bag of words models.
Further extensions of the model are possible to incorporate even more complex constraints based other domain knowledge.
Finally, we describe what place our model has within the broader realm of artificial intelligence and propose several open questions that may be of general interest to the field.

We perform importance sampling for a randomized matrix multiplication algorithm by Drineas, Kannan, and Mahoney and derive probabilities that minimize the expected value (with regard to the distributions of the matrix elements) of the variance.
We compare these optimized probabilities with uniform probabilities and derive conditions under which the actual variance of the optimized probabilities is lower.
Numerical experiments with query matching in information retrieval applications illustrate that the optimized probabilities produce more accurate matchings than the uniform probabilities and that they can also be computed efficiently.

In cooperative peer-to-peer information retrieval systems, each node can be considered an intelligent agent and these agents work collectively to provide an information retrieval service.
In order to effectively support multiple and concurrent search sessions in the network, we propose two traffic engineering techniques that minimize processing and communication bottlenecks.
One is a novel agent control mechanism whose elements include resource selection, local search scheduling, and feedback-based load control.
The other is a new two-phase query routing algorithm based on organizational knowledge.
Experimental results show that this framework can reduce congestion situations, increase system throughput, and improve considerably the overall system utility.

In this paper, the interconnection and integration problem of disparate Information sources including multilingual information related to the Unemployed and Business is analyzed.
A possible solution based on the use of the European curriculum vitae and the creation of Data Marts is briefly described.
The approach is also influenced by well-known Cross-Lingual Information Retrieval (CLIR) techniques.
We also focus on the creation of a pilot Information System for the Institute of Labour (INE) of the Greek General Confederation of Labour
Eventually, our experience and a first evaluation of the system are discussed.

This paper presents the findings of a study that re-uses (with permission) thirty interviews gathered by the charity Dipex (Youthhealthtalk to explore the relationship between information and coping where young people are coping with long-term illnesses.
The study uses the analytical approach Situational Analysis, an elaboration of Grounded Theory developed by Clarke (2005
As the approach is not widely used in Library and Information Science research and will therefore be explained, before illustrating its application in this research context.

This paper designed an information retrieval system which can take the effective classification and unified planning to all kinds of information of enterprises, and offer a variety of search mechanism to organize information and following the permissions to show to users easily, so that sharing the different levels of information and raise the utilization of information in maximum.
After the actual use, this retrieval system can meet the design requirements basically.
Keywords-information warehouse, information retrieval

We study whether it is possible to infer from eye movements measured during reading what is relevant for the user in an information retrieval task.
Inference is made using hidden Markov and discriminative hidden Markov models.
The result of this feasibility study is that prediction of relevance is possible to a certain extent, and models benefit from taking into account the time series nature of the data.

The procedure with advancement of information surge has made it hard to get significant information on the web.
In this proposed system, the necessity for practical Information Retrieval (IR) strategy has been extended.
Document data contains huge information user can easily get the information by using only title and keywords of document or information.
We propose a fast and effective content-based document information retrieval system that retrieves the information from the actual content of a document.
In proposed system we use model of Latent Dirichlet Allocation that is used to extract major keywords for a given document.
To improve the performance of system we use MongoDB database for the effective documents indexing.
B-tree based indexing of MongoDB makes our system flexible, effective and fast than the previous system.

To cite this article: Nan Mi, Jingwei Hou, Wenbao Mi Naiping Song (2015)
Optimal spatial land-use allocation for limited development ecological zones based on the geographic information system and a genetic ant colony algorithm, International Journal of Geographical Information Science,
29:12, 2174-2193, DOI:
10.1080/13658816.2015.1070411
To link to this article: http dx.doi.org/10.1080/13658816.2015.1070411

In this article, we suggest some recommendations for helping the search tools to decrease noise and silence.
Taking account different contextual spaces (user, search tool, document and interactions between the user and the search tool we explain how a search tool may provide customized responses within a “specific user context Keyword: Context, Information Retrieval on the Web

Ontology, which derives from philosophy, is a discipline of Philosophy that deals with existence.
In recent years, it has been widely used in artificial intelligence and computer science, mainly representing, sharing and reusing knowledge.
So it's significant to introduce ontology into Geographic Information Science for the implementation of the geographical information representation, sharing and reuse.
Based on analyses of geographical information taxonomy, after explaining the concept of ontology, this paper expatiates a method of building Geographical Information Ontology and a process of classifying geographical information concepts.
In classification, it places emphasis on using ontology for method, starting from property of concepts, analysing the property of the concepts sufficiently, working out the priorities of ontological properties, and carries out the classification of concepts based on ontological properties.

This paper describes a software agent developed specifically for integration with existing information retrieval interfaces and search engines.
The software agent assists the user with query reformulation.
The agent assistance is based on characteristics of the user population, user actions during the search process, information from retrieved documents, and historical information from past queries.
With minor modification, the software agent can be integrated with a variety of interfaces and search engines.

As an important data structure model, ontology has become one of the core contents in information science.
Multi-dividing ontology algorithm combines the advantages of graph structure and learning algorithms proved to have high efficiency.
In this paper, in terms of multi-dividing proper loss functions, we propose new multi-dividing ontology learning algorithms for similarity measure and ontology mapping construction.
Several theoretical statistical characteristics supporting the new learning model are given.
Finally, four experiments on different scientific fields verify that our multi-dividing ontology algorithm has high accuracy and efficiency in application implements.

The quantum mechanics is one of two backbones of the modern physics, and it is the most important required major course in the physics specialty.
It is deeply applied not only in many branches of physics, but also quickly and extensively in many important domains such as chemistry, biology, material science, and information science.
By the studying of this course, students could masterly grasp the basic concepts and theories of the quantum mechanics, and have the basic theories of quantum mechanics to analyze the solve problems.
The course of “quantum mechanics” of the department of physics of Dezhou University was appraised as the Shandong Provincial provincial-level quality course, and by virtue of this chance, the teaching content and method of quantum mechanics are reformed, and primary results have been obtained.

This paper presents ongoing research into user requirements for 3-dimensional «virtual worlds» to be used as a means of information retrieval.
There is a brief review of the literature in the fields in which design of 3-dimensional virtual spaces has been carried out, and the conclusion is reached that this design has usually been done without regard to user preferences.
This study has used grounded theory to establish user preferences, initially amongst a group of about fifty postgraduate information management students.
Initial results of open coding of these interviews are described, and the next steps in the study are laid out.
The Virtual Reality Modelling Language (VRML) will be used as a world design tool.

QuestionCube is a framework for Question Answering (QA) that combines several techniques to retrieve passages containing the exact answers for natural language questions.
It exploits a) Natural Language Processing algorithms for question and candidate answers analysis both in English and Italian b) Information Retrieval probabilistic models for candidate answers retrieval and (c) Machine Learning methods for question classification.
The data source for the answer is an unstructured text document collection stored in search indices.
In this paper an overview of the QuestionCube framework architecture is provided, together with a description of Wikiedi, a QA system for Wikipedia which exploits the proposed framework.

Named Entity Recognition (NER) is the subtask of Natural Language Processing (NLP) which is the branch of artificial intelligence.
It has many applications mainly in machine translation, text to speech synthesis, natural language understanding, Information Extraction, Information retrieval, question answering etc.
The aim of NER is to classify words into some predefined categories like location name, person name, organization name, date, time etc.
In this paper we describe the Hidden Markov Model (HMM) based approach of machine learning in detail to identify the named entities.
The main idea behind the use of HMM model for building NER system is that it is language independent and we can apply this system for any language domain.
In our NER system the states are not fixed means it is of dynamic in nature one can use it according to their interest.
The corpus used by our NER system is also not domain specific.

GeoFEM is a parallel nite element analysis system intended for multiphysics/multi-scale problems and is being developed at RIST.
Within \Earth Simulator" project, the GeoFEM group will deal with the modeling and simulation of solid earth eld phenomena, and the development of large-scale parallel software for the \Earth Simulator Since there are models which are not completely established in the solid earth eld, the simulation must be carried out on a trial and error basis.
Therefore, a joint venture with the geoscience modeling research group is crucial for the development of a targeted simulation software system.
This project is expected to be a breakthrough in bridging the geoscience and information science elds.
When complete, this software system will be able to solve problems in the scale of 100 million degree of freedoms.

We propose a topic based approach lo language modelling for ad-hoc Information Retrieval (IR Many smoothed estimators used for the multinomial query model in IR rely upon the estimated background collection probabilities.
In this paper, we propose a topic based language modelling approach, that uses a more informative prior based on the topical content of a document.
In our experiments, the proposed model provides comparable IR performance to the standard models, but when combined in a two stage language model, it outperforms all other estimated models.

This article examines and extends the logical models of information retrieval in the context of probability theory.
The fundamental notions of term weights and relevance are given probabilistic interpretations.
A unified framework is developed for modeling the retrieval process with probabilistic inference.
This new approach provides a common conceptual and mathematical basis for many retrieval models, such as the Boolean, fuzzy set, vector space, and conventional probabilistic models.
Within this framework, the underlying assumptions employed by each model are identified, and the inherent relationships between these models are analyzed.
Although this article is mainly a theoretical analysis of probabilistic inference for information retrieval, practical methods for estimating the required probabilities are provided by simple examples.

The majority of information in a company is often useless or not equally interesting to everybody.
The set of techniques called Analytical Business Intelligence provides a valid way to tackle the needs of information classification and of knowledge extraction that naturally arises inside a company.
This paper proposes a suitable solution to make the digital resources useful, identifiable and accessible, through the creation of a Knowledge Base, i.e. a company-wide Intranet where techniques of Information Retrieval used by the Web are applied in order to reach a higher level of efficiency.

Informationen und Wissen sind die Grundlage für betriebliche Entscheidungen.
Sie müssen oft in kurzer Zeit verfügbar sein.
Unternehmen verfügen über eine beeindruckende Sammlung von Informationen und eine Vielfalt von Informationsquellen, so dass die Suche bereits bei der Auswahl der geeigneten Suchmaschine bzw.
Quelle beginnt.
Im Rahmen dieses Tracks werden daher Ansätze zur Beherrschung und Nutzung einer Sammlung von Informations-Ressourcen adressiert, wie sie beispielsweise in Unternehmen durch die getrennten Silos unterschiedlicher Informationssysteme existieren.
Problematisch für die Suchmaschinen sind dabei z.
B. die unterschiedlichen Strukturen und Zugriffsrechte der Inhalte, unterschiedliche Arten der Suchanfrage hinsichtlich Syntax und semantischer Ausdrucksstärke (z.
B. abfragbare Merkmale die
Art der Ergebnispräsentation und die
Möglichkeiten der Interaktion mit der Ergebnisliste.

Document representation is a crucial step in any Information Retrieval (IR) system.
Since most of the traditional methods do not consider much of semantic or syntactic information, the representation becomes insufficiently informative for an IR task.
We describe a novel approach to incorporating Natural Language Processing (NLP) in document representation for addressing this problem.
Use is made of additional information about the sentences, viz i) syntactic links among the words found by the Link Parser and (ii) heuristically determined semantic attributes of the words.
After mapping this information to the document level using Self-Organizing Map (SOM we use it for embellishing the document vectors constructed by the TFIDF method.
The efficacy of the proposed method is established by showing that the document vectors (i) have higher mutual information content and (ii) achieve better class separation.

Information Retrieval methods have been largely adopted to identify traceability links based on the textual similarity of software artifacts.
However, noise due to word usage in software artifacts might negatively affect the recovery accuracy.
We propose the use of smoothing filters to reduce the effect of noise in software artifacts and improve the performances of traceability recovery methods.
An empirical evaluation performed on two repositories indicates that the usage of a smoothing filter is able to significantly improve the performances of Vector Space Model and Latent Semantic Indexing.
Such a result suggests that other than being used for traceability recovery the proposed filter can be used to improve performances of various other software engineering approaches based on textual analysis.

Yulin Fang is an assistant Professor in the Department of Information Systems, city university of Hong kong.
He earned his Ph.D. at richard Ivey School of Business, university of Western Ontario, london, Ontario.
His current research is focused on knowledge management, virtual teams, and open source software projects.
He has published papers in journals such as Strategic Management Journal, Journal of Management Information Systems, Journal of Management Studies, Journal of the American Society for Information Science and Technology, Information Management, Communications of the AIS, and others.

In this paper, we describe our experiments carried out for the robust word sense disambiguation (WSD) track of the CLEF 2009 campaign.
This track consists of a monolingual and bilingual task and addresses information retrieval utilizing word sense annotations.
We took part in the monolingual task only.
Our objective was twofold.
On the one hand, we intended to increase the precision of WSD by a heuristic-based combination of the annotations of the two WSD systems.
For this, we provide an extrinsic evaluation on different levels of word sense accuracy.
On the other hand, we aimed at combining an often used probabilistic model, namely the Divergence From Randomness BM25 model
(DFR BM25 with a monolingual translation-based model.
Our best performing system with and without utilizing word senses ranked 1st overall in the monolingual task.
However, we could not observe any improvement by applying the sense annotations compared to the retrieval settings based on tokens or lemmas only.

With advent of internet, there has been tremendous surge in search for videos.
Research suggests that users spend a lot of time browsing: viewing part of one video after another while watching only a few of the videos to its completion.
This is termed as seek, a special form of browsing repeating partial viewing of the same video.
This impacts the network bandwidth resulting in streaming issues with the browser.
We propose to devise a system where the user would be able to view pre selected video sequence which would contain parts required by the user.
This method of video browsing would be significant improvement over the traditional video browsing in terms of efficiency and user friendliness.
Keywords—video processing, frames, visual recognition, image indexing, visual content tags

Information retrieval systems are being challenged to managelarger and larger document collections.
In an effort to providebetter retrieval performance on large collections, moresophisticated retrieval techniques have been developed that supportrich, structured queries.
Structured queries are not amenable topreviously proposed optimization techniques.
Optimizing execution,however, is even more important in the context of large documentcollections.
We present a new structured query optimizationtechnique which we have implemented in an inference network-basedinformation retrieval system.
Experimental results show that queryevaluation time can be reduced by more than half with little impacton retrieval effectiveness.

Cross-modal retrieval is a classic research topic in multimedia information retrieval.
The traditional approaches study the problem as a pairwise similarity function problem.
In this paper, we consider this problem from a new perspective as a listwise ranking problem and propose a general cross-modal ranking algorithm to optimize the listwise ranking loss with a low rank embedding, which we call Latent Semantic Cross-Modal Ranking (LSCMR
The latent low-rank embedding space is discriminatively learned by structural large margin learning to optimize for certain ranking criteria directly.
We evaluate LSCMR on the Wikipedia and NUS-WIDE dataset.
Experimental results show that this method obtains significant improvements over the state-of-the-art methods.

We present a new framework for visualization and retrieval of knowledge organized as a personal semantic web.
This framework is an extension of other semantic web technologies and aims to achieve a fast, elegant solution to carrying your knowledge with you.
The architecture described has been developed on the Google Android platform and embedded OpenGL.
We call this a Personal Knowledge Advantage Machine (pKaM) as it is designed to access a personalized semantic network using a user-specific ontology.
This also includes a graphical browser which enables a user visualize the portion of the semantic network that is relevant to the specific user context.

The English and Mäori word translator ngä aho whakamäori-ä-tuhi was designed to provide single head-word translations to on-line web users.
There are over 13,000 words all based on traditional text sources, derived because of their high frequency used within each of the respective languages.
The translator has been operational for well over a year now, and it has had the highest web traffic usage in the Department of Information Science.
Two log files were generated to record domain hits and language translations, both provided the up-to-date data for analysis contained in this paper.

The development and testing of systems to support users engaged in exploratory search activities (i.e searches where the target may be undefined) is an challenge for the online search community.
In this article we report on a workshop on exploratory search issues organized in conjunction with the University of Maryland Human-Computer Interaction Laboratory's Annual Symposium and Open House in June 2005.
This workshop brought together researchers from the fields of Information Seeking (IS Information Retrieval (IR Human-Computer Interaction (HCI) and Information Visualization (IV) for a cross-disciplinary exploration of these and other issues.
Although originally intended to focus on interfaces to support exploratory search the workshop blossomed into a rich discussion of not only interface issues, but also evaluation, the cognitive processes that underlie information exploration and research methods.

Power consumption in data centres is a growing issue as the cost of the power for computation and cooling has become dominant.
An emerging challenge is the development of ldquoenvironmentally friendlyrdquo systems.
In this paper we present a novel application of FPGAs for the acceleration of information retrieval algorithms, specifically, filtering streams/collections of documents against topic profiles.
Our results show that FPGA acceleration can result in speed-ups of up to a factor 20 for large profiles.

We describe our efforts to analyze network intrusion detection data using information retrieval and visualization tools.
By regarding Telnet sessions as documents, which may or may not include attacks, a session that contains a certain type of attack can be used as a query, allowing us to search the data for other instances of that same type of attack.
The use of information visualization techniques allows us to quickly and clearly find the attacks and also find similar, potentially new types of attacks.

This paper reports on the fourth Information Interaction in Context (IIiX) Symposium held in Nijmegen, the Netherlands in August 2012.
It featured a lively program with 3 keynotes, 25 long papers with oral presentation, 20 short papers with poster presentation, a doctoral consortium, a workshop on human-computer information retrieval, and was followed by a summer school on information foraging.
IIiX'12 is an ACM and ACM SIGIR in cooperation conference with its proceedings published by the ACM.

In the last few years Artificial Immune Systems (AISs) have proved their usefulness in several domains, ranging from Intrusion Detection to Information Retrieval.
Although their use in several applications, no formal representation has been given yet to AIS concepts and framework.
In this paper we propose an architecture for constructing ontologies of artificial immune systems applied to several domains.
The architecture can be used as simple framework to create the AIS ontology necessary in the domain of application.

This paper introduces a system for cross language information retrieval where selective term technique and query expansion will be combined to improve the retrieval of more relevant documents.
The document and query processing is done using the techniques of tf-idf and cosine similarity for retrieval.
Threshold value is decided to retrieve the documents relevant to query.
Later a pseudo Relevance technique will be applied to further expand the query and improve relevance in future.

Graphical models have been applied to various information retrieval and natural language processing tasks in the recent literature.
In this paper, we apply a probabilistic graphical model for answer ranking in question answering.
This model estimates the joint probability of correctness of all answer candidates, from which the probability of correctness of an individual candidate can be inferred.
The joint prediction model can estimate both the correctness of individual answers as well as their correlations, which enables a list of accurate and comprehensive answers.
This model was compared with a logistic regression model which directly estimates the probability of correctness of each individual answer candidate.
An extensive set of empirical results based on TREC questions demonstrates the effectiveness of the joint model for answer ranking.
Furthermore, we combine the joint model with the logistic regression model to improve the efficiency and accuracy of answer ranking.

Das diesem Bericht zugrundeliegende Forschungsvorhaben wurde mit Mitteln des Bundesministers
f ur Forschung und Technologie unter dem F orderkenn-zeichen 01 IV 102 H/0 und 01 IV 102 C 6 gef ordert.
Die Verantwortung f
ur den Inhalt dieser Arbeit liegt bei dem Autor.
During any kind of information retrieval dialog, the repetition of parts of information just given by the dialog partner can often be observed.
As these repetitions are usually ellip-tic, the intonation is very important for determining the speakers intention.
In this paper prototypically the times of day repeated by the customer in train table inquiry dialogs are investigated.
A scheme is developed for the oocers reactions depending on the intonation of these repetitions; it has been integrated into our speech understanding and dialog system EVAR (cf.
6 Gaussian classiiers were trained for distinguishing the dialog guiding signals connrmation, question and feedback; recognition rates of up to 87.5% were obtained.

Over the last four years, the Information Interaction Laboratory at Rutgers' School of Communication, Information and Library Studies has carried out a series of investigations concerned with various aspects of people's interactions with advanced information retrieval systems.
We have been especially concerned with understanding not just what people do, and why, and with what effect, but also with what they would like to do, and how they attempt to accomplish it, and with what difficulties.
These investigations have led to some quite interesting conclusions about the nature and structure of people's interactions with information, about support for cooperative human-computer interaction in query reformulation, and about the value Research supported in part by a DARPA Equipment Grant, by NIST Cooperative Agreement
No. 70NANB5H0050, and by TIPSTER Phase III
Contract No
. MDA904-96-C-1297

A multimedia report is a multimedia presentation which integrates data returned by one or more queries to a multimedia database, thus extending the concept of report familiar in traditional structured databases.
In such a scenario information retrieval consists in building a continuous presentation in which the retrieved data are located, connected, synchronized and coherently presented to a user.
We discuss modelling of multimedia reports in terms of data co-ordination and synchronization, based on a synchronization model we have defined for specifying complex multimedia presentations.
As in a report the user can browse the returned data without loosing consistency, in a multimedia report moving along the presentation time requires appropriate synchronization to be guaranteed.

Nicholas J. Belkin is currently a professor of information science at the Department of Library and Information Science, Rutgers University.
His major research interests are in the area of information retrieval (IR) theory, cognitive theory in information retrieval, human interaction with information, human-computer interaction in information systems, and interactive information retrieval.
He is one of the prominent researchers in information retrieval area and publishes numerous important papers that made significant contribution to the field.

This paper reports improvement of indirect matching, which is a fast CBMIR (content-based music information retrieval) framework proposed in our previous study.
Indirect matching achieves fast retrieval by combining offline search with representative queries and online quick similarity estimation based on the results of the offline search.
We have found that the retrieval accuracy of indirect matching decreases when representative queries have little variation.
This paper proposes a method for selecting representative queries having wide variation.
To ensure wide variation between representative queries, the proposed method combines MDS (multi-dimensional scaling) and Ward's clustering.
Experimental results have shown that the retrieval accuracy of indirect matching can be stabilized by the proposed method.

This paper focuses on the problem of ranking documents for Geographic Information Retrieval.
It aims to demonstrate that by using some query-related example texts it is possible to improve the final ranking of the retrieved documents.
Experimental results indicated that our approach could improve the MAP of some sets of retrieved documents using only two example texts.

In probabilistic approaches to information retrieval, the occurrence of a query term in a document contributes to the probability that the document will be judged relevant.
It is typically assumed that the weight assigned to a query term should be based on the expected value of that contribution.
In this paper we show that the degree to which observable document features such as term frequencies are expected to <i>vary</i> is also important.
By means of stochastic simulation, we show that increased variance results in degraded retrieval performance.
We further show that by decreasing term weights in the presence of variance, this degradation can be reduced.
Hence, probabilistic models of information retrieval must take into account not only the expected value of a query term's contribution but also the variance of document features.

Identifying the most influential documents in a corpus is an important problem in many fields, from information science and historiography to text summarization and news aggregation.
Unfortunately, traditional bibliometrics such as citations are often not available.
We propose using changes in the thematic content of documents over time to measure the importance of individual documents within the collection.
We describe a dynamic topic model for both quantifying and qualifying the impact of these documents.
We validate the model by analyzing three large corpora of scientific articles.
Our measurement of a document’s impact correlates significantly with its number of citations.

This paper describes the forensic and intelligence analysis capabilities of the Email Mining Toolkit (EMT) under development at the Columbia Intrusion Detection (IDS) Lab.
EMT provides the means of loading, parsing and analyzing email logs, including content, in a wide range of formats.
Many tools and techniques have been available from the fields of Information Retrieval (IR) and Natural Language Processing (NLP) for analyzing documents of various sorts, including emails.
EMT, however, extends these kinds of analyses with an entirely new set of analyses that model ”user behavior EMT thus models the behavior of individual user email accounts, or groups of accounts, including the ”social cliques” revealed by a user’s email behavior.

User Friendly Online Searching is examined in the context of Natural Language Processing in Information Retrieval and Artificial Intelligence.
Opportunities for synergetic R &amp; D are identified as the basis for Intelligent Information Retrieval and Artificial Retrieval Intelligence.

This paper overviews soft clustering algorithms applied in the context of information retrieval (IR First, a motivation of the utility of soft clustering approaches in IR is discussed.
Then, an outline of the two main flat soft approaches, namely probabilistic clustering and fuzzy clustering, is described.
Specifically, the expectation maximization and fuzzy c-means algorithms are introduced, and some of their extensions defined to overcome their main drawbacks when applied for organizing large document collections.
Finally, soft hierarchical clustering algorithms designed for generating taxonomies of documents are introduced.
C 2011 John Wiley Sons, Inc.
WIREs Data Mining Knowl Discov
2011 1 138–146 DOI: 10.1002/widm.3

The number of users and the amount of information available has exploded since the advent of the World Wide Web (WWW Most of Web users use various search engines to get specific information.
A key factor in the success of Web search engines are their ability to rapidly find good quality results to the queries that are based on specific terms.
This paper aims at retrieving more relevant documents from a huge corpus based on the required information.
We propose a text mining framework that consists of four distinct stages: 1.
Text preprocessing 2.
Dimensionality reduction using latent semantic indexing 3.
Clustering based on hybrid combination of particle swarm optimization (PSO) and k-means algorithm 4.
Information retrieval process using simulated annealing (SA This framework provides more relevant documents to the user and reduces the irrelevant documents.

Search tools are essential for an information system.
Being syntax based, the existing search engines have a number of limitations in particular their difficulty in returning relevant results.
Typically these limitations are partly mitigated through use of various customised techniques in order to provide the best possible user experience.
Unlike conventional search engines, a semantic web search engine attaches meanings to key words.
One of the biggest challenges for such a semantic search engine is to maintain the standard of user experience while serving its purpose of finding relevant data with meanings.
A semantic search tool needs to both link meanings to search keywords and be able to efficiently retrieve information.
The purpose of this work is to investigate the techniques that can be used for efficient information retrieval and effective user experience within the context of a semantic search engine.
We propose two algorithms for query optimisation and ontology crawler.

There is a great demand for highly accurate and timely Information Retrieval and Information Extraction in medicine and health care.
To meet this need, we have developed a novel system, Intelligent Clinical Notes System (ICNS) to assist doctors in retrieving clinical notes based on concept searching.
This has required dealing with the both the software engineering and natural language processing aspects of the task This system has been installed and integrated into the existing clinical information system in the Intensive Care Unit, Royal Prince Alfred Hospital, Sydney.

This paper is in two parts, following the suggestion that I first comment on my own past experience in information retrieval, and then present my views on the present and future.

Open-set recognition, a challenging problem in computer vision, is concerned with identification or verification tasks where queries may belong to unknown classes.
This work describes a fine-grained plant identification system consisting of an ensemble of deep convolutional neural networks, within an open-set identification framework.
Two wellknown deep learning architectures of VGGNet and GoogLeNet, pretrained on the object recognition dataset of ILSVRC 2012, are fine-tuned using the plant dataset of LifeCLEF 2015.
Moreover, GoogLeNet is finetuned using plant and non-plant images, to serve for rejecting samples from non-plant classes.
Our systems have been evaluated on the test dataset of PlantCLEF 2016 by the campaign organizers and the best proposed model has achieved an official score of 0.738 in terms of the mean average precision, while the best official score was announced to be 0.742.

A parallel algorithm for coverage optimization on multi-core architectures Ran Wei Alan T. Murray To cite this article: Ran Wei Alan T. Murray (2016)
A parallel algorithm for coverage optimization on multi-core architectures, International Journal of Geographical Information Science, 30:3
, 432-450, DOI:
10.1080/13658816.2015.1030750
To link to this article: http dx.doi.org/10.1080/13658816.2015.1030750

This is the third years that Tsinghua University Information Retrieval Group (THUIR) participates in Novelty task of TREC.
Our research on this year’s novelty track mainly focused on four aspects 1) text feature selection and reduction 2) improved sentence classification in finding relevant information 3)efficient sentence redundancy computing 4) effective result filtering.
All experiments have been performed on TMiner IR system, developed by THU IR group last year.

This report describes HCIR 2009, the third international workshop on Human-Computer Interaction and Information Retrieval (HCIR held in October 2009 at The Catholic University of America in Washington, DC.
The workshop attracted over 50 participants from across the world and was the largest HCIR workshop to date.
The event brought together representatives from academia, industry, and government to discuss research, present work in process, and advance ideas in the area of HCIR.
The workshop consisted of a keynote presentation, panel presentations, a poster session, and two guided discussion sessions.

Despite the widespread use of BM25, there have been few studies examining its effectiveness on a document description over single and multiple field combinations.
We determine the effectiveness of BM25 on various document fields.
We find that BM25 models relevance on popularity fields such as anchor text and query click information no better than a linear function of the field attributes.
We also find query click information to be the single most important field for retrieval.
In response, we develop a machine learning approach to BM25-style retrieval that learns, using LambdaRank, from the input attributes of BM25.
Our model significantly improves retrieval effectiveness over BM25 and BM25F. Our data-driven approach is fast, effective, avoids the problem of parameter tuning, and can directly optimize for several common information retrieval measures.
We demonstrate the advantages of our model on a very large real-world Web data collection.

We propose a new method for information retrieval (IR The main goal of this method is to enhance the core IR-process of finding relevant and only relevant documents in a set of documents.
More precisely the method aims at increasing precision at top ranked documents.
It is founded on previous works on graph matching and acknowledges the significance of structural similarity in making analogies.
The method we propose is based on graph vertices comparison and involves recursive computation of similarity.
It extends the principle stating that the resemblance of two vertices can be computed over the similarities of the vertices to which they are connected and takes into account the concept of similarity propagation within a graph.
First results from conducted experiments show the feasibility and the effectiveness of our approach.
Indeed, our method highly outperforms the vector-based cosine model and more than doubles the precision until top sixty returned documents.

Validation of Remote Sensing Content-Based Information Retrieval (RS-CBIR) systems requires innovative strategies to overcome the scarcity of labeled data.
CBIR systems validation by means of precision/recall measures based on either, user’s feedback or a-priori known categories, are hard to apply to RS-CBIR systems.
We propose to apply a datadriven (unsupervised) quality assessment strategy analogous to the DAMA strategy applied for the validation of classification methods used in thematic mapping.
The strategy is intended for quality assessment when little or no ground truth is available.
The proposed strategy deals with the RS-CBIR validation problem by giving a quantitative and qualitative evidence of the relative (subjective) quality of RS-CBIR systems without a-priori knowledge.

Personalized information retrieval and access is critical to applications in the medical and healthcare domain where the accuracy of the retrieved information and obtaining it in a time critical situation are extremely important.
Existing personalized information retrieval and access approaches in the medical and healthcare domain have improved in the last decade, but the centralized system design principle is impeding their ability to be flexible and adaptive to changing demands.
These demands arise from the environment in which the approaches are used and from changing user requirements.
In this paper we present our preliminary ideas regarding a new component-based design philosophy for personalized information retrieval and access services.
We believe this paper offers a new way of thinking about the retrieval of time critical, personalized information from medical information systems.

We propose to use MapReduce to quickly test new retrieval approaches on a cluster of machines by sequentially scanning all documents.
We present a small case study in which we use a cluster of 15 low cost machines to search a web crawl of 0.5 billion pages showing that sequential scanning is a viable approach to running large-scale information retrieval experiments with little effort.
The code is available to other researchers at: http sourceforge.net/projects/mirex/

This paper applies a fuzzy ontology framework to information retrieval system for enterprise management information system.
The framework includes three parts: concepts, properties of concepts and values of properties, in which property's value can be either standard data type or linguistic values, i. e. fuzzy concepts.
The fuzzy semantic query expansion is constructed by order relation, equivalence relation, inclusion relation and complement relation between fuzzy concepts defined in fuzzy linguistic variable ontology.
The application to retrieve project information shows that the framework can overcome the localization of other fuzzy ontology's models, and this research facilitates the semantic retrieval of enterprise management information through fuzzy concepts on the Semantic Web.

The increase of voice-based interaction has changed the way people seek information, making search more conversational.
Development of effective conversational approaches to search requires better understanding of how people express information needs in dialogue.
This paper describes the creation and examination of over 32K spoken utterances collected during 34 hours of collaborative search tasks.
The contribution of this work is three-fold.
First, we propose a model of conversational information needs (CINs) based on a synthesis of relevant theories in Information Seeking and Retrieval.
Second, we show several behavioural patterns of CINs based on the proposed model.
Third, we identify effective feature groups that may be useful for detecting CINs categories from conversations.
This paper concludes with a discussion of how these findings can facilitate advance of conversational search applications.

One of the problems to solve in Music Information Retrieval (MIR) is the modelization of music style.
The system could be trained to identify the main features that would characterize music genres or style so as to look for that kind of music over large musical corpus.
So in this paper multimodal approach, pattern recognition approach and co-updating approach is been studied for identifying the style from different genre of the music.
Considering the intuitive feelings of similarity from the listeners perspective, the focus on features that are computed using similarity metrics for melodies, harmonies, and audio signals for style identification.
A multimodal approach mostly considered support vector machine as a binary classifier to determine if two songs or music played by the same artist given their similarity metrics in the three aspects and also discussed the experimental methodologies of the two different approaches.

–This paper focuses on the needs of developing an automated Digital Library Management system .The purpose is to automate the task of analyzing data containing in raster image documents for the purpose of intelligent information retrieval in digital library.
An efficient and computationally fast method for segmenting text and graphics part of document images based on multi-scale wavelet analysis and statistical pattern recognition is presented.
The extracted text is further classified into Title, Author name, name of the publication etc and being stored in the database for further Library related operations.
We do not assume any a priori information regarding the font size, scanning resolution, type of layout, etc.
of the document in our segmentation scheme.
Keywords Document segmentation, daubechies wavelet, Multiscale wavelet analysis, priori Information, Fourier transform.

In this paper, we propose the employment of a combination of multi-document summarization[2] and automated layout[3] as a post-processing step in document retrieval.
We examine the use of an interactive textual fisheye that employs automated layout techniques to present a generated natural language summary as a replacement to the standard ranked list.
Our presentation system is novel because it employs a natural language summarization system to generate informative as well as indicative summaries of varying lengths to create an effective layout given the constraints of a specific amount of available screen space.
In addition, the system leverages the topical structure of the generated summary to create a textual fisheye for navigating and interacting with the presentation where the actual length of the text is varied to provide different levels of detail rather than the more conventional change in rendering parameters such as font size.

This paper presents a study of incorporating domain-specific knowledge (i.e information about concepts and relationships between concepts in a certain domain) in an information retrieval (IR) system to improve its effectiveness in retrieving biomedical literature.
The effects of different types of domain-specific knowledge in performance contribution are examined.
Based on the TREC platform, we show that appropriate use of domain-specific knowledge in a proposed conceptual retrieval model yields about 23% improvement over the best reported result in passage retrieval in the Genomics Track of TREC 2006.

This paper proposes an HCI approach to supporting interactive document retrieval in unorganized open information space.
On the assumption that taxonomical thought are one of the most important and skilled operations for us when we organize or store information, we may discuss that clustering or classification techniques are applied to interactive interface.
The proposed system, therefore, provides two-dimensional space to visualize categories of document icons for interactive interface.
The features of the proposed approach are (1) visualization of document categories for interaction 2) initialization of categories by hierarchical clustering method 3) customization of categories by support vector machine techniques 4) user-depend additional attributes for individual implicit, and (5) cognitive aspects integration of these techniques for IR interface.
We made a preliminary experiment on text categorization for customization, using two test collections to estimate practicability.

This paper describes an extension of our work presented in the robust English-to-French bilingual task of the CLEF 2007 workshop, a knowledge-light approach for query translation in Cross-Language Information Retrieval systems.
Our work is based on the direct translation of character n-grams, avoiding the need for word normalization during indexing or translation, and also dealing with out-of-vocabulary words.
Moreover, since such a solution does not rely on language-specific processing, it can be used with languages of very different nature even when linguistic information and resources are scarce or unavailable.
The results obtained have been very positive, and support the findings from our previous English-to-Spanish experiments.

Verbose query reduction and query term weighting are automatic techniques to deal with verbose queries.
The objective is either to assign an appropriate weight to query terms according to their importance in the topic, or outright remove unsuitable terms from the query and keep only the suitable terms to the topic and user's need.
These techniques improve performance and provide good results for ad hoc information retrieval.
In this paper we propose a new approach to deal with long verbose queries in Social Information Retrieval (SIR) by taking Social Book Search as an example.
In this approach, a new statistical measure was introduced to reduce and weight terms of verbose queries.
Next, we expand the query by exploiting the similar books mentioned by users in their queries.
We find that the proposed approach improves significantly the results.

Peer-to-Peer Information Retrieval System (P2PIR) is a system that can query multiple information retrieval systems and merges ranked results list into a single result of documents.
Classical methods are generally based on linear combination schemes.
A major shortcoming of the classical methods is that there is no defined way to study dependencies and interactions existing among relevance criteria.
In this paper, we propose a result merging method based on the Choquet Integral, called Choquet-Based Merging (CBM The experimental results obtained on the test collection provided by TREC Contextual Suggestion track shows the effectiveness of our proposal.

Arabic language is very different and difficult structure than other languages, that’s because it is a very rich language with complex morphology.
Many stemmers have been developed for Arabic language but still there are many weakness and problems.
There is still lack of usage of Arabic stemming in search engines.
This paper introduces a rooted word Arabic stemmer technique.
The results of the introduced technique for six Arabic sentences are used in famous search engines Google Chrome, Internet Explore and Mozilla Firefox to check the effect of using Arabic stemming in these search engines in terms of the total number of searched pages and the search time ratio for actual sentences and their stemming results.
The results show that Arabic words stemming increase and accelerate the search engines output.
Keywords—Information Retrieval; Arabic Stemming; Search Engine; Arabic Morphology

In this paper we present a study on music mood classification using audio and lyrics information.
The mood of a song is expressed by means of musical features but a relevant part also seems to be conveyed by the lyrics.
We evaluate each factor independently and explore the possibility to combine both, using natural language processing and music information retrieval techniques.
We show that standard distance-based methods and latent semantic analysis are able to classify the lyrics significantly better than random, but the performance is still quite inferior to that of audio-based techniques.
We then introduce a method based on differences between language models that gives performances closer to audio-based classifiers.
Moreover, integrating this in a multimodal system (audio+text) allows an improvement in the overall performance.
We demonstrate that lyrics and audio information are complementary, and can be combined to improve a classification system.

Increasingly more clinicians use web Information Retrieval (IR) systems to assist them in diagnosing di cult medical cases, for instance rare diseases that they may not be familiar with.
However, web IR systems are not necessarily optimised for this task.
For instance, clinicians’ queries tend to be long lists of symptoms, often containing phrases, whereas web IR systems typically expect very short keywordbased queries.
Motivated by such di↵erences, this work uses a preliminary study of 30 clinical cases to reflect on rare disease retrieval as an IR task.
Initial experiments using both Google web search and o✏ine retrieval from a rare disease collection indicate that the retrieval of rare diseases is an open problem with room for improvement.

The theory of concept (or Galois) lattices provides a natural and formal setting in which to discover and represent concept hierarchies.
In this paper we present a system, GALOIS, which is able to determine the concept lattice corresponding to a given set of objects.
GALOIS is incremental and relatively efficient, the time complexity of each update ranging from O(n) to O(n2) where n is the number of concepts in the lattice.
Unlike most approaches to conceptual clustering, GALOIS represents and updates all possible classes in a restricted concept space.
Therefore the concept hierarchies it finds are always justified and are not sensitive to object ordering.
We experimentally demonstrate, using several machine learning data sets, that GALOIS can be successfully used for class discovery and class prediction.
We also point out applications of GALOIS in fields related to machine learning (i.e information retrieval and databases).

Network administration is a task that requires experience in relating symptoms of network problems with possible causes and corrective actions.
We describe the design of a system and more specifically its information retrieval component, which aims to retrieve articles relevant to a given problem case from a collection of articles describing previously solved problems and their associated solutions.
An article is described by a term vector.
We present a methodology for defining the vocabulary and preliminary results for assessing the quality of expert-proposed modifications to the vocabulary.
We obtain vocabulary-derived document classes from a self- organising map and assess vocabulary quality using the quality of classification into these classes.

A source of potential systematic errors in information retrieval is identiied and discussed.
These errors occur when base-form reduction is applied with a (necessarily) nite dictionary.
Formal methods for avoiding this error source are presented, along with some practical complexities met in its implementation.

Geographical entities often appears in very different forms in text collections, such as when a foreign name is used instead of the English one, or when the citation of some region or place omits the name of a larger geographical entity containing them.
This is a known problem in the field of Information Retrieval.
The use of an ontology like WordNet can help in addressing this issue.
In this paper we propose an automatic method to expand the geographical terms in queries by using the WordNet ontology and another method that expands the terms during the indexing phase.
The proposed methods exploits the synonymy, meronymy and holonymy relationships provided by WordNet, together with some information extracted from the gloss.

Recent emerging technologies such as internetworking and the World Wide Web have signiicantly expanded the types of data available to an information management system.
Textual data is the most prevalent of these data types.
In this paper we discuss an approach based on domain ontologies (expressed as E-R Models) and SQL for modeling and querying textual data as implemented in the InfoSleuth project at MCC.
We identify the basic shortcomings of current approaches for textual data on the web a) lack of precision; and (b) lack of interoperation and discuss how our approach helps alleviate the above shortcomings.
Techniques for mapping concepts in a domain ontology to the underlying textual data and for translation of queries expressed in SQL to the underlying information retrieval operations are presented.
Limitations of current indexing technologies in supporting expressions translated from SQL are identiied and heuristic approaches are proposed to overcome the same.

Peter B. Danzig, Jongsuk Ahn, John Nell, Katia Obraczka Computer Science Department University of Southern California Los Angeles, California 90089-0782

These indexes were prepared by William S. Stalcup, Steven A. Holton and Anthony E. Petrarca, Department of Computer and Information Science, The Ohio State University with the aid of programs developed by W. Michael Lay as part of his Doctoral research.
The technique used for production of these indexes is a variation of the Double-KWIC Coordinate Indexing Technique various aspects of which have been described by A. E. Petrarca and W. M. Lay in &lt;u&gt;J. Chem.
Doc 9&lt;/u&gt 256(1969
lt;u&gt;ASIS Proceedings, 6&lt;/u&gt 277(1969 lt;u&gt;Proceedings of the Seventh Annual National Colloquium on Information Retrieval&lt;/u&gt 155(1970 and by W. M. Lay in his Ph.D. Dissertation, The Ohio State University, 1973.
The index entries are derived from the titles of the conference papers augmented by one or more ACM classification codes (a dictionary of which appears at the end of these indexes).

Web is loaded with information and is getting overloaded with data each passing day.
There needs to be efficient development in the area of information retrieval so the required data can be fetched accurately and efficiently.
In this paper the two promising areas Natural Language Processing and Web technologies which can be combined together to enable the enterprise to combine the unstructured and structured data in ways that was not handled efficiently by traditional tools.
The better understanding of web information can be done by integrating NLP and Web portal.
The paper also explores various techniques used both areas.
Also the NLP frameworks which can be used for the future work in this area.

A key problem in Tor’s architecture is that it requires users to maintain a global view of the system, which will become costly as the size of the network increases.
Several peer-to-peer approaches have been proposed in order to alleviate the scalability concerns of the Tor network, but they are only able to provide heuristic security; in fact, the security community has been quite successful at breaking the state of the art systems using both passive and active attacks.
In this paper, we explore new primitives for scalable anonymous communication, with a focus on providing provable security guarantees.
First, we propose a new approach for secure peer-topeer anonymous communication based on a reciprocal neighbor policy.
Secondly, we propose PIR-Tor, a clientserver scalable architecture for anonymous communications based on Private Information Retrieval.

The information structure (IS) model is used in information economics to evaluate the expected payoff from imperfect information and to recommend an optimal decision strategy.
This paper applies the IS model to the Information Filtering (IF) problem.
Using the suggested model, IF systems can be rank ordered, when the user payoff matrix is known.
In some situations the ordering is “strong that is, independent of the payoff matrix.
The relation between the commonly used precision and recall performance measure and the IS model parameters is derived.
The strong relation is translated into the recall precision plot, widely used in the Information Retrieval community.
It is proved that there is a relation between the precision and recall parameters for which systems can be rank ordered, using the Blackwell theorem, regardless of the user payoff matrix.
Topic Areas Evaluation, Performance, Formal Models

In recent years, Nonnegative Matrix Factorization (NMF) has received considerable interest from the data mining and information retrieval fields.
NMF has been successfully applied in document clustering, image representation, and other domains.
This study proposes an online NMF (ONMF) algorithm to efficiently handle very large-scale and/or streaming datasets.
Unlike conventional NMF solutions which require the entire data matrix to reside in the memory, our ONMF algorithm proceeds with one data point or one chunk of data points at a time.
Experiments with one-pass and multi-pass ONMF on real datasets are presented.

This special issue of the Journal of Information Retrieval contains selected and substantially extended works presented and published at the second International Conference on the Theory of Information Retrieval (ICTIR held at Microsoft Research in Cambridge, UK, on 10–11 September 2009.
The ICTIR conference series is a biennial international conference that provides an opportunity for the presentation of the latest work describing advances in the theoretical and formal aspects of Information Retrieval
The conference is run under the auspices of the British Computer Society’s Information Retrieval Specialist Group and the proceedings are now being published in the Lecture Notes in Computer Science (LNCS) series by Springer.
The first ICTIR was held in Budapest in October 2007, organized by Sándor

Despite the growing popularity of Library and Information Science (LIS) education in the Gulf Co-operation Council (GCC) states, there is evidence of significant shortcomings in resources, expertise and facilities which may seriously threaten the future sustainability of the field.
In other parts of the world, trends in regional and international collaboration and co-operation among LIS specialists are contributing to the growth and sustainability of this area of education, and generating innovative ways of addressing resource limitations and other weaknesses.
This paper draws on existing literature to examine the current state of LIS education in the GCC states and to provide examples of the types of collaborative models and strategies that might be adopted from other parts of the world in order to improve LIS education in this region.
The potential challenges to this process are discussed and some recommendations for consideration by LIS specialists in the GCC are put forward.

This paper describes an on-going effort to combine Information Retrieval (IR) and Information Extraction (IE) technologies, to leverage the benefits provided by both approaches to add value for the end-user, as compared with IR or IE in isolation.
The main aim of the combined system is to pool together information from multiple sources to improve the quality of results.
On one hand, multiple mentions of the same event or related events should be presented in a coherent fashion.
On the other hand, grouping related events should improve the system’s confidence in the discovered facts.
We describe our approach and the results achieved in the project to date.

Information Retrieval (IR) forms the basis of many information management tasks.
Information management itself has become an extremely important area as the amount of electronically available information increases dramatically.
There are numerous methods of performing the IR task both by utilising different techniques and through using different representations of the information available to us.
It has been shown that some algorithms outperform others on certain tasks.
Combining the results produced by different algorithms has resulted in superior retrieval performance and this has become an important research area.
This paper introduces a probability-based fusion technique probFuse that shows initial promise in addressing this question.
It also compares probFuse with the common CombMNZ data fusion technique.

MIREX 2005 is the second evaluation of algorithms related to music information retrieval (MIR This document describes our submission to the MIREX audio melody extraction contest addressing the task of identifying the melody pitch contour from polyphonic musical audio.
We use mainly a data-driven approach implementing standard audio signal processing techniques like Fourier analysis, instantaneous frequency estimation and sinusoidal extraction.
Nevertheless, some high level knowledge is applied.
Examples of such knowledge include perceptually motivated methods (psychoacoustics, auditory stream segregation) as well as basic voice-leading principles.
The result of the Mirex 2005 evaluation proves that our algorithm performs best in respect of runtime and overall accuracy, a measure which combines voicing detection (discrimination between melody and non-melody parts) and pitch estimation.

This research hypothesises the array data-model as a key ingredient for the integration of information retrieval and database technology.
The main research goals focus on a seamless blending of the efficient evaluation of sparse array computations in the relational domain, with the challenges posed by information retrieval applications.

The ability to accurately judge the semantic similarity between words is critical to the performance of several applications such as Information Retrieval and Natural Language Processing.
Therefore, in this paper we propose a semantic similarity measure that uses in one hand, an online English dictionary provided by the Semantic Atlas project of the French National Centre for Scientific Research (CNRS) and on the other hand, a page counts based metric returned by a social website whose content is generated by users.

In traditional commercial environment, business information retrieval is limited in searching scope and quality.
As enterprise turns to electronic commerce (EC) environment, the quality of business information on supply and sale directly affects the level of enterprise operations.
This paper analyses the problem of business information retrieval in EC environment, and proposes a Hopfield neural network based business information retrieval model using Internet information resources by means of Artificial Intelligence (AI) and Information Retrieval (IR) theory.
Through generating extended query terms by Hopfield neural network and ranking the results, the proposed model can expand the searching scope and increase the precision of retrieval, so as to provide valuable business information for enterprises.

We present general-purpose methods for recognizing certain types of structure in HTML documents.
The methods are implemented using WHIRL, a "soft" logic that incorporates a notion of textual similarity developed in the information retrieval community.
In an experimental evaluation on 82 Web pages, the structure ranked first by our method is "meaningful i.e a structure that was used in a hand-coded "wrapper or extraction program, for the page--nearly 70% of the time.
This improves on a value of 50% obtained by an earlier method.
With appropriate background information, the structure-recognition methods we describe can also be used to learn a wrapper from examples, or for maintaining a wrapper as a Web page changes format.
In these settings, the top-ranked structure is meaningful nearly 85% of the time.

Salience is an important characteristic of information influencing users’ cognitive and emotional states.
For example, salient parts of a document are those that readers will find moving or provoking.
This article analyzes the main characteristics of salience and the different meanings of the concept in information retrieval and linguistics.
It also presents a generic approach for identifying linguistically salient segments in a text using readers’ textual feedback.
The method supports any kind of text and textual feedback.
We evaluated the effectiveness of the method with a corpus of blog posts and readers’ comments.
Our preliminary experiments show that the method has promising results with an fscore of 0.65.
The method could also be used on 90% of commented posts which proves that it can be used on a large scale.
Salience, Information Retrieval, Feedback

Taylor Francis makes every effort to ensure the accuracy of all the information (the “Content contained in the publications on our platform.
However, Taylor Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content.
Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor Francis.
The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information.
Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.

This paper details our participation in the NTCIR-11 Temporalia task including Temporal Query Intent Classification (TQIC) and Temporal Information Retrieval (TIR
In the TQIC subtask, we explore the rich temporal information in the labeled and unlabeled search queries.
Semi-supervised and supervised linear classifiers are learned to predict the temporal classes for each search query.
In the TIR subtask, we perform temporal ranking based on the technique of learning-to-rank.
Two classes of features are investigated for estimating the document relevance.

Axiomatic approach provides a systematic way to think about heuristics, identify the weakness of existing methods, and optimize the existing methods accordingly.
This tutorial aims to promote axiomatic thinking that can benefit not only the study of IR models but also the methods for many IR applications.

The explosion of the World Wide Web as a global information network brings with it a number of related challenges for information retrieval and automation.
The link structure, which is the main feature of the hypermedia environment, can be a rich source of information for exploration.
This paper is centered around the exploiting of hyperlinks in the subject of automatic discovery.
In this paper, we will P S C Z L M the design of site traversing algorithms and identify some important parameters for hyperlinkbased information discovery on the Web.
Meanwhile, we propose a new evaluation method, the search depth, for comparing various hyperlink traversing algorithms.

We report our experiments in TREC 2009 Million Query track and Adhoc task of Web track.
Our goal is to evaluate the effectiveness of axiomatic retrieval models on the large data collection.
Axiomatic approaches to information retrieval have been recently proposed and studied.
The basic idea is to search for retrieval functions that can satisfy all the reasonable retrieval constraints.
Previous studies showed that the derived basic axiomatic retrieval functions are less sensitive to the parameters than the other state of the art retrieval functions with comparable optimal performance.
In this paper, we focus on evaluating the effectiveness of the basic axiomatic retrieval functions as well as the semantic term matching based query expansion strategy.
Experiment results of the two tracks demonstrate the effectiveness of the axiomatic retrieval models.

The SIFT (Segmented Information Fusion Techniques) group in UCD is dedicated to researching Data Fusion in Information Retrieval.
This area of research involves the merging of multiple sets of results into a single result set that is presented to the user.
As a means of both evaluating the effectiveness of this work and comparing it against other retrieval systems, the group entered Category B of the TREC 2010 Web Track.
This involved the use of freely-available Information Retrieval tools to provide inputs to the data fusion process.
This paper outlines the strategies of the 3 candidate fusion algorithms entered in the ad-hoc task, discusses the methodology employed for the runs and presents a preliminary analysis of the provisional results issued by TREC.

This paper explores the application of a multilingual, WordNet-like lexical knowledge base, EuroWordNet, to Cross-Language Text Retrieval.
EuroWordNet (EWN) is a multilingual database with basic semantic relations between words for some European languages (Dutch, Italian, Spanish and English
In addition to the relations in WordNet 1.5, EWN includes domain labels, cross-language and crosscategory relations, which are directly useful for Multilingual Information Retrieval.
We propose conceptual, language-independent indexing on the basis of EWN Interlingual Index (ILI) as a promising approach to cross-language text retrieval.

This study describes how metadata can be used to organize documents into hierarchical structures that filter against each other.
It then discusses several experiments that were conducted to test the underlying usability concerns of this type of organizational system.

In this paper, we propose a robust mobile-based music information retrieval (MIR) system.
In the real mobile environment, a query music signal is captured by a cellular phone.
A major problem in this environment is distortions contained in the features of the query sound due to the mobile network and environmental noise.
In order to minimize the effect of these noises and so improve the system performance, we adopt a simple MA (moving average) filter which has relatively simple structure and low computational complexity.
Then a SFS (sequential forward selection) feature optimization method is implemented to further improve and stabilize the system performance.
The proposed system has been tested with cellular phones in the real world and it shows about 68% of average retrieving success rate.
We include experimental results under the real world mobile environments.

In this study we provide a practical mechanism for earthquake disaster risk assessment and management in the tourism industry, especially focusing on insurance and prevention.
The primary methodology is to integrate the information sciences, earth sciences, civil engineering and insurance with the concepts of disaster risk management.
There are four primary components within this system: stochastic event generator, hazard analysis procedure, vulnerability analysis procedure, financial analysis procedure.
Based on the analysis results of the above generator and procedures, the model can produce annual exceeding probability curve, average annual loss, and probable maximum loss, which are very useful information for earthquake risk control strategy and management.
Several scenario data are taken into consideration with the characteristics and sources of earthquake-related damage.

This article summarizes Information Retrieval (IR) research conducted at the Universidade Federal de Minas Gerais (UFMG over more than a quarter of a century.
The work of the UFMG IR group has covered some of the key areas in modern IR from crawling, indexing, compression and ranking methods to search engines and recommender systems.
Further, its focus on addressing practical problems of relevance to society and on building prototypes to validate the proposed solutions has led to the spin-off of two key start-up companies in Brazil, one of them acquired by Google Inc. to become its R&D center for Latin America.

The purpose of an information retrieval (IR) system is to help users accomplish a task.
IR system evaluation should consider both task success and the value of support given over the entire information seeking episode.
Relevance-based measurements fail to address these requirements.
In this paper, usefulness is proposed as a basis for IR evaluation.

With the rapid development of technology and society, academic field tends to be informationization.
Electronic information on the Internet attracts more and more attention by the majority of scholars.
The article executed systematic study on special text format and language style for Chinese scientific papers proposed ontology-based classification framework of Chinese scientific papers, constructed Information Science ontology.
The new feature extraction algorithm and the text semantic similarity algorithm were applied to the whole process of the framework.
Experiment results show that the proposed method has got pretty good effect in recall precision, and F1 value.

Based on the fact that face-to-face and virtual meetings in organizational units increasingly take place in today’s business processes, it is not surprising that a lot of valuable information is generated during such events.
Further, meeting information can be a valuable knowledge source for teaching and learning activities, e.g. it can support trainers and teachers in identifying emerging topics for vocational trainings and for the provision of illustrative real-life examples.
Indeed, staff members and learners get access to this helpful information in order to solve problems and improve their skills.
However, there is still a gap in order to integrate meeting information into teaching and learning activities.
In order to overcome this problem, this paper focuses on smart meeting information retrieval supporting teaching and learning purposes in companies and research institutions.

A system for classification of mail pieces using range images is proposed.
Position, orientation, size and shape parameters of volumetric models of single mail pieces are recovered using least squares minimization of a fitting function.
The models are superquadrics with global deformations.
The recovered parameters serve for classification.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-88-57.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/681 SHAPE RECOVERY OF MAIL PIECES USING DEFORMABLE MODELS Franc Solina and Ruzena
Bajcsy MS-CIS-88-57 GRASP LAB
150 Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 191 04

Music Information Retrieval (MIR) is a crucial topic in the domain of information retrieval.
According to major characteristics of music, Query-by-Humming system retrieves interesting music by finding melody that contains similar or equal melody to the humming query.
Basing on the designed fuzzy inference model a novel Query-by-Humming/Singing system is proposed to extract pitch contour information from WAV and MIDI files in this paper.
To verify the effectiveness of the presented work, the MIREX QBSH Database is employed as our experimental database and a large amount of human vocal data is used as query to test the robustness of MIR.
Then, the Longest Common Subsequence (LCS) is served as an approximate matching algorithm to identify the most related top 5 music as an evaluation standard for the system.
Experimental results show that the proposed system achieves 85% accuracy in the top 5 retrievals.

The empirical nature of Information Retrieval (IR) mandates strong experimental practices.
The Cranfield/TREC evaluation paradigm represents a keystone of such experimental practices.
Within this paradigm, the generation of relevance judgments has been the subject of intense scientific investigation.
This is because, on one hand, consistent, precise and numerous judgements are key to reduce evaluation uncertainty and test collection bias; on the other hand, however, relevance judgements are costly to collect.
The selection of which documents to judge for relevance (known as pooling) has therefore great impact in IR evaluation.
In this paper, we contribute a set of 8 novel pooling strategies based on retrieval fusion methods.
We show that the choice of the pooling strategy has significant effects on the cost needed to obtain an unbiased test collection; we also identify the best performing pooling strategy according to three evaluation measure.

The rapid development of network technologies has seen the use of network as a media for file storage and sharing.
Files stored on network are expected to be shared in a safe and effective manner.
Most recent researches on secure network storage focus solely on security and give little concern to support for information retrieval and in turn information sharing.
This paper presents a novel network storage model which integrates techniques in compression, encryption and information retrieval.
It effectively satisfies the concern for both security and sharing.
Its performance is also demonstrated via experiments on a prototype system.

Most Music Information Retrieval (MIR) researchers will agree that understanding users' needs and behaviors is critical for developing a good MIR system.
The number of user studies in the MIR domain has been gradually increasing since the early 2000s reflecting the need for empirical studies of users.
However, despite the growing number of user studies and the wide recognition of their importance, it is unclear how large their impact has been in the field; on how systems are developed, evaluation tasks are created, and how we understand critical concepts such as music similarity or music mood.
In this paper, we present our analysis on the growth, publication and citation patterns, and design of 155 user studies.
This is followed by a discussion of a number of issues/challenges in conducting MIR user studies and distributing the research results.
We conclude by making recommendations to increase the visibility and impact of user studies in the field.

a Dipartimento di Ingegneria dell’Informazione, Università di Padova, Italy, and College of Computing, Georgia Institute of Technology, Atlanta, USA b
Universidad de Chile, DCC/Center for Web Research, Santiago, Chile, and ICREA Professor, Departament de Tecnología, Universitat Pompeu Fabra, Barcelona, Spain c
Dipartimento di Ingegneria dell’Informazione, Università di Padova,
Via Gradenigo 6, 35131 Padova, Italy

This paper presents our system report on our participation in the shared task on “Detecting Paraphrases in Indian Languages (DPIL organized in the “Forum for Information Retrieval Evaluation
(FIRE)”2016, in both the tasks (Task1 and Task2) defined in this shared task in four Indian languages (Tamil, Malayalam, Hindi and Punjabi We made use of different similarity measures and machine translation evaluation metrics as features and used machine learning framework to take paraphrase decision between a pair of text snippets.
We obtained the accuracies of 97.08 94.2 97.32% and 98.29% in Task1 and 86.68 77.37 84% and 98.77% in Task2 for Tamil, Malayalam, Hindi and Punjabi respectively on the respective training sets using a 10-fold cross validation framework.

Interactive analysis of datacube, in which a user navigates a cube by launching a sequence of queries is often tedious since the user may have no idea of what the forthcoming query should be in his current analysis.
To better support this process we propose in this paper to apply a Collaborative Work approach that leverages former explorations of the cube to recommend OLAP queries.
The system that we have developed adapts Approximate String Matching, a technique popular in Information Retrieval, to match the current analysis with the former explorations and help suggesting a query to the user.
Our approach has been implemented with the open source Mondrian OLAP server to recommend MDX queries and we have carried out some preliminary experiments that show its efficiency for generating effective query recommendations.

Logical models offer a sound theoretical framework for information retrieval (IR Fuzzy logic provides a convenient extension of classic logic based approaches.
In particular it makes it possible to grasp the gradually of some relevant concepts and to model both imprecision and uncertainty inherent to the retrieval process.
In the paper we show a few possible fuzzy logic based approaches to information retrieval.
In particular we study how well-known keywords weights semantics may be recovered in some more advanced models of this type.

This is a worksheet to accompany the slides for my ICTIR 2013 tutorial.
It is meant to explain how to perform standard significance tests in R and how to use simulation to investigate tests more deeply.
My hope is that the reader that follows along will reach the same conclusion that I did: that there are so many factors that can affect the computation of a p-value, and so many of those that do not accurately reflect the reality of IR experimentation, that no objective meaning should be assigned to any p-value, no matter how carefully it has been computed.

Positive design leads to positive change in our society.
In most cases, discussions focus on those who receive the design.
However, positive design may also have a positive, but often overlooked, effect on the designers themselves.
Learning about difficulties others face and developing solutions is a benefit that can contribute to individual designers’ education and general sense of well-being.
Having a broader understanding of alternative views and lifestyles makes one a better person.
In addition, positive design may benefit the entire field of information science by improving its ability to renew itself and attract new, young talent.

Term weighting methods have been shown to give significant increases in information retrieval performance.
The presence of pronomial references in documents reduces the term frequencies of associated words with a consequent effect on term weights and information retrieval behaviour.
This investigation explores the impact on information retrieval performance of broad coverage automatic pronoun resolution.
Results indicate that this approach has potential to improve both precision at fixed cutoff levels and average precision.

Philosophy of information is a relatively new endeavor.
In the context of information reality, it analyses a group of classical philosophical problems such as the principles, origins and structures of knowledge, the nature of existence, the problems of mind, logical structures of language and meaning, the principles of logical reasoning and critical thinking, theories of truth, and ethical issues (e.g Floridi 2002, 2004a, 2004b; Adriaans and van Bethem 2008; Himma and Tavani 2008 Information science is also a relatively new theoretical field.
It develops in several different directions and analyses thus different aspects of information reality, such as to name only some of the relevant aspects information retrieval, information systems, knowledge organization, information/knowledge management, information behavior, information quality, information literacy, bibliometrics and artificial intelligence.
NO. 4, DECEMBER, 2010

Large collections of full-text documents are now commonly used in automated information retrieval.
When the stored document texts are long, the retrieval of complete documents may not be in the users' best interest.
In such circumstance, efficient and effective retrieval results may be obtained by using passage retrieval strategies designed to retrieve text excerpts of varying size in response to statements of user interest.
New approaches are described in this study for implementing selective passage retrieval systems, and identifying text passages responsive to particular user needs.
An automated encyclopedia search system is used to evaluate the usefulness of the proposed methods.

In this report, we describe a practical relevance ranking procedure, as it is implemented and integrated in the interim prototype of the SPIRIT search engine.
We review the theoretical models and ideas presented in the previous three deliverables of WP5, and state the practical decisions and refinements made during implementation.
Possible improvements are identified which will lead to an advanced and final version.
SPIRIT project
Practical Similarity Ranking IST-2001-35047 D18:5302

Starting from an unsolved problem of information retrieval this paper presents an ontology-based model for indexing and retrieval.
The model combines the methods and experiences of cognitive-to-interpret indexing languages with the strengths and possibilities of formal knowledge representation.
The core component of the model uses inferences along the paths of typed relations between the entities of a knowledge representation for enabling the determination of hit quantities in the context of retrieval processes.
The entities are arranged in aspect-oriented facets to ensure a consistent hierarchical structure.
The possible consequences for indexing and retrieval are discussed.

This paper describes a general model of information retrieval systems and processes and its implementation as a workbench for information retrieval experimentation in Common Lisp.
A brief overview discusses the motivation for and goals of such a workbench.
A general model of retrieval systems is presented which identifies two functional components, partitioners and transformers and a single aggregate data type, collections, and how they interact.
The workbench and its implementation are discussed in general terms, including the core classes and methods and a small group of applications which have been developed for it.
Finally, there is a brief discussion of the usefulness of Common Lisp as the implementation language for the workbench and its conceptual affinity for information retrieval system modeling.

Geographic Information Retrieval (GIR) has become a very attractive area of research.
GIR is a specialization of a traditional information retrieval system, which may index and search Web documents based on their spatial footprints.
Research in this new field may be categorized into crawling spatial-related documents, modeling the geographic scope of a document, indexing these documents using textual and spatial features, and the building of spatially-enabled searching and ranking.
This paper presents a method for modeling the geographic scope.
The proposed model is based on both the statistics collected from the detected references; and the spatial distribution of the places involved in a given document.
This model aims to simplify the indexing and searching processes.
Furthermore, the number of spatial operations is reduced, as a consequence the overall performance is improved.

A novel and complex form of information access is cross-language information retrieval: searching for texts written in foreign languages based on native language queries.
Although the underlying technology for achieving such a search is relatively well understood, the appropriate interface design is not.
This paper presents three user evaluations undertaken during the iterative design of Clarity, a cross-language retrieval system for low density languages, and shows how the user interaction design evolved depending on the results of usability tests.
The first test was instrumental to identify weaknesses in both functionalities and interface; the second was run to determine if query translation should be shown or not; the final was a global assessment and focussed on user satisfaction criteria.
Lessons were learned at every stage of the process leading to a much more informed view of what a cross-language retrieval system should offer to users.

The improvements to ad-hoc IR systems over the last decades have been recently criticized as illusionary and based on incorrect baseline comparisons.
In this paper several improvements to the LM approach to IR are combined and evaluated: Pitman-Yor Process smoothing, TF-IDF feature weighting and modelbased feedback.
The increases in ranking quality are significant and cumulative over the standard baselines of Dirichlet Prior and 2-stage Smoothing, when evaluated across 13 standard ad-hoc retrieval datasets.
The combination of the improvements is shown to improve the Mean Average Precision over the datasets by 17.1% relative.
Furthermore, the considered improvements can be easily implemented with little additional computation to existing LM retrieval systems.
On the basis of the results it is suggested that LM research for IR should move towards using stronger baseline models.

As methods in artist similarity identification using Web Music Information Retrieval perform well on known evaluation sets, we investigate the application of such a method to a more realistic data set.
We notice that ambiguous artist names lead to unsatisfying results.
We present a simple, efficient and unsupervised method to deal with ambiguous artist names.

This paper describes cross-language informationretrieval experiments carried out for TREC-6.
Our retrieval method, cross-language latent semantic indexing (CL-LSI is completely automatic and we were able to use it to create a 3-way EnglishFrench-German IR system.
This study extends our previous work in terms of the large size of training and testing corpora, the use of low-quality training data, the evaluation using relevance judgments, and the number of languages analyzed.

Assessing the effectiveness of different visualizations for judgments of positional uncertainty Grant McKenzie, Mary Hegarty, Trevor Barrett Michael Goodchild
To cite this article: Grant McKenzie, Mary Hegarty, Trevor Barrett Michael Goodchild (2015
Assessing the effectiveness of different visualizations for judgments of positional uncertainty, International Journal of Geographical Information Science, DOI:
10.1080/13658816.2015.1082566
To link to this article: http dx.doi.org/10.1080/13658816.2015.1082566

We examine the feasibility of fusing the outputs of multiple text retrieval engines to improve accuracy.
We tested three Web-based search engines (Excite for Web Servers, Infoseek's Ultraseek Server, and Sony Search Engine) over a 74,520 document collection (TREC Wall Street Journal articles from 1990-92) with a set of 125 natural-language queries with relevance judgements (TREC topics 51-175 We show that a weighted combination of scores produces higher precision over the top 5, 105 20, and 30 documents than any single engine over the same data set.
We also compare favorably against the state-of-the-art heuristics in merging search engines.
Our results suggest that fusing the results from the most dissimilar engines (those with the least overlap in the retrieved sets) is a more effective strategy than simply weighting the best engines more heavily.

reason: By allowing us to project beyond our basic-level experience, conceptual metaphor makes possible science, philosophy, and 167 all other forms of abstract theoretical reasoning Lakoff Johnson 1999 556-557] Consider the abstract idea, the higher or more senior an employee is in an organization, the more power they have This abstract concept is built upon the metaphor:

An analysis patterns are conceptual and abstract models of domain to understand the domain and its problem.
They support the analysis process and are useful of making and reusing of domain knowledge.
Also, they can be transformed to more flexible design and apply for any specific domain or application and solve the problem.
In this paper, we show the analysis patterns for information retrieval in intelligent responsive space (IRS For our work, we describe the information retrieval (IR) analysis pattern with refined template for analysis patterns.
And we applied IR analysis pattern for our specific domain to understand the domain and construct the ontology based knowledge base.

For our first participation in the CLEF evaluation campaign, our aim is to explore a translation-free technique for multilingual information retrieval.
This technique is based on an ontological representation of documents and queries.
We use a multilingual ontology for documents/queries representation.
For each language, we use the multilingual ontology to map a term to its corresponding concept.
The same mapping is applied to each document and each query.
Then, we use a classic vector space model for the indexing and the querying.
The main advantages of our approach are: no merging phase is required, no dependency on automatic translators between all pairs of languages exists, and adding a new language only requires a new mapping dictionary to the multilingual ontology.

In previous work we have established the design of an information system intended to provide efficient information retrieval services over a wide-area network.
In this paper we show that throughput increases nearly as fast w the number of sites for most topologies, analyze response time in more detail, and show that natural update propagation is sufficient to eliminate obsolete data from the system in most cases.

Probabilistic modelling of recommender systems naturally introduces the concept of prior probability into the recommendation task.
Relevance-Based Language Models, a principled probabilistic query expansion technique in Information Retrieval, has been recently adapted to the item recommendation task with success.
In this paper, we study the effect of the item and user prior probabilities under that framework.
We adapt two priors from the document retrieval field and then we propose other two new probabilistic priors.
Evidence gathered from experimentation indicates that a linear prior for the neighbour and a probabilistic prior based on Dirichlet smoothing for the items improve the quality of the item recommendation ranking.

Success in Information Retrieval (IR) depends on many variables.
Several interdisciplinary approaches try to improve the quality of the results obtained by an IR system.
In this paper we propose a new way of using word sense disambiguation (WSD) in IR.
The method we develop is based on Naïve Bayes classification and can be used both as a filtering and as a re-ranking technique.
We show on the TREC ad-hoc collection that WSD is useful in the case of queries which are difficult due to sense ambiguity.
Our interest regards improving the precision after 5, 10 and 30 retrieved documents (P@5, P@10, P@30 respectively, for such lowest precision queries.

This paper describes our different retrieval approaches conducted on a cultural heritage (CH) content testcollection.
In our study, our main objective is to measure the relative merit of various query expansion and semantic enrichment techniques.
To do so we considered different strategies based on blind-query expansion as well as using external resource, namely Wikipedia.
In using external knowledge bases we consider different strategies in order to select the most appropriate concepts to be added to the initial query.
Our results show that expansion with both pseudo-relevant documents and external resources improve the retrieval.
While the impact of expansion using an external resource is more significant.

A typed, modular paradigm for polynomial time computation is proposed.
Comments University of Pennsylvania Department of Computer and Information Sciences
Technical Report No.
MSCIS-91-59.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/338 Bounded Linear Logic: A Modular Approach to Polynomial Time Computability MS-CIS-91-59
LOGIC COMPUTATION 36
Jean-Yves Girard Andre Scedrov Philip J. Scott Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389

Previous works showed that the use of document passages like basic unit of information, to calculate the relevance of a document to a question, improve the results of the information retrieval systems sensibly.
However, IR community has not arrived to a consent about how to define those text passages so that the system can improve the efficiently.
This paper reports on experiments with IR-n system, a information retrieval system based on the selection of passages of variable size as basic unit of information, in the monolingual (Spanish) and bilingual (Spanish-English) tasks at CLEF-2001.
The IR-n system has been developed this year in the Language Processing and Information Systems research group at the University of Alicante.

The modernization of the library information sector largely depends on the quality of higher education that is provided by universities of culture.
The most important factor in the formation of common cultural and professional competencies in future employees of libraries, information analytical centers, and agencies is the organization of cooperation between universities and potential employers.
Professional and public accreditation is regarded as one of the effective mechanisms for organizing this interaction.
The interactive technologies that are used in the implementation of educational programs strengthen the ties between library-information science and practice.

The amount of Arabic electronic information is growing drastically on the web.
Statistics shows that the number of Internet users in the Middle East has increased enormously since the year 2000 due to increase in ICT awareness and its importance within Arab countries.
As a result this has raised the need to find effective methods and techniques for allocating and retrieving the Arabic-based content from the web.
This paper presents major Information Retrieval (IR) tools and techniques and it highlights few challenges in this regard.

Currently there is no standard test collection for evaluation of Farsi information retrieval systems.
In this paper we introduce Mahak, the first complete test collection generally available for evaluating Farsi information retrieval systems.
We also discuss our construction process in detail.
As a goal, we hope that Mahak will foster the development of Farsi information retrieval systems.

Text Categorization (TC also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set.
If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task.
TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers.
In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents.

At present, most biomedical Information Retrieval and Extraction tools process abstracts rather than full-text articles.
The increasing availability of full text will allow more knowledge to be extracted with greater reliability.
To investigate the challenges of full-text processing, we manually annotated a corpus of cited articles from a Molecular Interaction Map (Kohn, 1999
Our analysis demonstrates the necessity of full-text processing; identifies the article sections where interactions are most commonly stated; and quantifies both the amount of external knowledge required and the proportion of interactions requiring multiple or deeper inference steps.
Further, it identifies a range of NLP tools required, including: identifying synonyms, and resolving coreference and negated expressions.
This is important guidance for researchers engineering biomedical text processing systems.

Experiment Components configuration Evaluation view
Recall-Precision-Graph User profile with achievements We present a web-based tool for learning the information retrieval process.
An interactive approach helps students gain practical knowledge.
Our focus is the arrangement and configuration of IR components and their evaluation.
The incorporation of game mechanics counteracts an information overload and motivates progression.

A key role for the knowledge sharing on the Web is the constructive interaction between system and user.
It is presented an application implemented on Cicerobot, a RWI B21 autonomous robot.
It is based on a cognitive architecture and works on tasks related with guided tours in the Archaeological Museum of Agrigento.
Interaction between Cicerobot and the user is based on a first step towards semantics sharing between them.
To this purpose knowledge is represented through sub-symbolically model inside the robot.
Being Cicerobot a node on the Web, its knowledge can be shared and virtual interaction during the visit is permitted.
Experimental results are also presented showing the effectiveness of the retrieval system.

The citations in 187 articles on bibliographic instruction published in thirteen library science journals were analyzed to determine the extent to which authors cited sources from library and information science compared to sources from traditional subject disciplines.
The results suggest an insularity of user instruction literature not only from other subject disciplines but from the larger field of librarianship as well.

In this paper, we present CoMMA (Corporate Memory Management through Agents an open, agent-based system for the management of a corporate memory that was realized integrating several emerging technologies: agent technology, knowledge modeling, XML technology, information retrieval and machine learning techniques.
In particular, the system has been realized to help users in the management of an organization corporate memory and in particular to facilitate the creation, dissemination, transmission and reuse of knowledge in an organization.
CoMMA system is a FIPA compliant agent system and has been implemented by using the JADE agent development software framework.
CoMMA is the result of an international project funded by European Commission.

A common approach to solving the term mismatch between a user’s query and relevant documents is query expansion, which augments a user’s query with additional, related terms in order to increase coverage of relevant documents.
The system proposed in this paper presents an alternative means of ameliorating the term mismatch problem for certain corpora.
Instead of using query expansion, this system uses document expansion.
Specifically, for corpora containing inter-document references (e.g links or citations this system uses those references to expand the set of terms contained within a given document.
Preliminary evaluation of the results of using document expansion in a sample implementation indicates that this approach can provide mild benefit to the perceived helpfulness of a user’s query.

At present, the majority of biomedical Information Retrieval tools process abstracts rather than full-text articles.
The increasing availability of full text will allow more knowledge to be extracted with greater reliability.
The first step of this is to extract sentences and passages from the text which report scientific results.
We investigate the challenges of sentence retrieval, using an annotated corpus of articles cited in a Molecular Interaction Map (Kohn, 1999) developed by McIntosh and Curran (
From the annotated facts we generate keywords for sentence retrieval, and analyse the impact of various query relaxation strategies on performance.
We also investigate the impact of hedging and commitment in the reporting of scientific results on retrieval.
Finally, we look at whether linguistic properties such as anaphora and negation have an impact on retrieval performance.

The study of contextual features has been widely discussed in Information Retrieval (IR but concrete applications on real data streams are not common.
In this paper, we aim at doing retweet recommandation.
Considering a user interest, we introduce a model to perform real-time online filtering of the Twitter stream using several contextual features.
The model separates content and contextual aspects, achieving a very high velocity.
Experiments were performed on the TREC Microblog 2015 collection.
Results show that integrating contextual features has a positive impact on the effectiveness of the filtering without penalizing efficiency.
MOTS-CLÉS RI Contextuelle, Filtrage temps-réel, Microblogs.

The article describes a method for constructing a model for ranking the search engine delivery on the Internet using inductive GMDH algorithms.
The method makes it possible to enhance substantially the relevance of scientific and technical information search on the Internet provided to sift spam and the commercial information.
The process of discovering the web resources ranking model is described for known search engines and comparing its effectiveness with the constructed model.

Much work in information retrieval focuses on using a model of documents and queries to derive retrieval algorithms.
Model based development is a useful alternative to heuristic development because in a model the assumptions are explicit and can be examined and refined independent of the particular retrieval algorithm.
We explore the explicit assumptions underlying the na&#239;ve framework by performing computational analysis of actual corpora and queries to devise a generative document model that closely matches text.
Our thesis is that a model so developed will be more accurate than existing models, and thus more useful in retrieval, as well as other applications.
We test this by learning from a corpus the best document model.
We find the learned model better predicts the existence of text data and has improved performance on certain IR tasks.

Previous research has shown the words in natural language documents exist as a small world network.
Thus it might be feasible to use extensive physics algorithms for extracting community structure.
We present a novel method for semantically clustering a large collection of documents using small world communities.
We combine specially modified physics algorithms with traditional information retrieval techniques.
A term network is generated from the document collection, the terms are clustered into small world communities, and the semantic term clusters are used to generate overlapping document clusters.
Clustering 90K documents took 20 seconds, generating good quality community clusters in nearly linear running time, O(n log
n) where n is the size of the lexicon in the document collection.

A novel similarity measure for bag-of-words type large scale image retrieval is presented.
The similarity function is learned in an unsupervised manner, requires no extra space over the standard bag-of-words method and is more discriminative than both L2-based soft assignment and Hamming embedding.
The novel similarity function achieves mean average precision that is superior to any result published in the literature on the standard Oxford 5k, Oxford 105k and Paris datasets/protocols.
We study the effect of a fine quantization and very large vocabularies (up to 64 million words) and show that the performance of specific object retrieval increases with the size of the vocabulary.
This observation is in contradiction with previously published results.
We further demonstrate that the large vocabularies increase the speed of the tf-idf scoring step.

Aiming at the low conversion rate of the patent technological achievements, the paper puts forward an idea about conversion support system of patent technological achievements based on knowledge discovery in databases.
Combined with Information Retrieval and Statistics, the system uses clustering, association and SVM (Support Vector Machine) to achieve the goal of the patent Data-Mining at both the macro and micro.
Then, the system discovers the unknown information of the designated patent, which provides important references for the consumers such as enterprises, universities, research institutes to make decisions.

We intend to implement a Coordinated Multi-layer Multi-domain Optical Network (COMMON) Framework for Large-scale Science Applications.
In the COMMON project, specific problems to be addressed include 1) anycast/multicast/manycast request provisioning, 2) multi-layer, multidomain quality of service (QoS and 3) multi-layer, multi-domain path survivability.
In what follows, we outline the progress in categories 1) and 2 Year 1 deliverables The work conducted in this report was performed at the University of Massachusetts, Dartmouth in the Computer and Information Science department.
Dr. Vinod Vokkarane (principal investigator) is the director of the Advanced Computer Networks Lab (
ACNL which comprises of the following research team:
Post-Doctoral Research Associate: Dr. Arush Gadkar Visiting Fulbright Scholar:
Joan Triay (Universitat Politècnica de Catalunya (UPC Spain)
Graduate Students: Jeremy Plante (BS/MS)
Bharath Ramaprasad (MS)
Derek Rousseau (MS)

In this paper we identify location names that appear in queries written in Indonesian using geographic gazetteer.
We built the gazetteer by collecting geographic information from a number of geographic resources.
We translated an Indonesian query set into English using a machine translation technique.
We also made an attempt to improve the retrieval effectiveness using a query expansion technique.
The result shows that identifying locations in the queries and applying the query expansion technique can help improve the retrieval effectiveness for certain queries.

Todays, The internet has become a source of multi-lingual content.
Users are not aware of multiple languages, so the language diversity becomes a great barrier for world communication.
Cross-Language Information Retrieval (CLIR) provides a solution for this language barrier, where a user can search in his native language and get the relevant information in the required language.
Currently, distributed word vector representation has a trend in various Natural Language Processing (NLP) task.
These word vectors are used to identify similar contextual words.
In this paper, we analyze the effectiveness of word vectors across the languages in Hindi-English CLIR.
Skip-Gram Model (SGM) is used to learn bi-lingual word vectors from sentence aligned comparable corpus.
IBM model is used to align the source language and target language words from sentence aligned comparable corpus.
Best target language translation is selected with the help of top-n word alignments and word vectors.

This article proposes an electronic commerce paradigm for agent-based Information Discovery (ID ID is the synthesis of Information Retrieval and Information Filtering, thus coping with dynamics from both elds.
Agent technology, having a number of valuable properties to attack problems with dynamics, is used in ID, resulting in a paradigm with three types of agents: user, broker, and source agents.
Cooperation, which is the keyword for the agents to fullll ID tasks, is seen as negotiation, thus obtaining a very general and exible way to describe agents' interaction.
The possibilities for ID agents to negotiate are investigated.
In addition, remarks stemming from the ID background are made about other issues concerning negotiation such as protocols, strategies, and coordination.

Discovering relations among Named Entities (NEs) from large corpora is both a challenging, as well as useful task in the domain of Natural Language Processing, with applications in Information Retrieval (IR Summarization (SUM Question Answering (QA) and Textual Entailment (TE
The work we present resulted from the attempt to solve practical issues we were confronted with while building systems for the tasks of Textual Entailment Recognition and Question Answering, respectively.
The approach consists in applying grammar induced extraction patterns on a large corpus Wikipedia for the extraction of relations between a given Named Entity and other Named Entities.
The results obtained are high in precision, determining a reliable and useful application of the built resource.

This paper addresses an important problem related to the use of semantics in IR.
It concerns the representation of document semantics and its proper use in retrieval.
The approach we propose aims at representing the content of the document by the best semantic network called document semantic core in two main steps.
During the first step concepts (words and phrases) are extracted from a document, driven by an external general-purpose ontology, namely WordNet.
The second step a global disambiguation of the extracted concepts regarding to the document leads to build the best semantic network.
Thus, the selected concepts represent the nodes of the semantic network whereas similarity measure values between connected nodes weight the links.
The resulting scored concepts are used for the document conceptual indexing in Information Retrieval.

The structural features of XML components are an extra source of information that should be used in a content-oriented retrieval task on this type of documents.
In this paper we explore one of the structural features from the INEX collection [1] that could be used in content-oriented search.
We analyse the gain this knowledge could add to the performance of an information retrieval system and present a first approach on how this structural information could be extracted from a relevance feedback process to be used as priors in a language modelling framework.

This paper discusses research on query translation events in Malay-English Cross-Language Information Retrieval (CLIR) system.
We assume that by improving query translation accuracy, we can improve the information retrieval performance.
The dictionary-based CLIR system facing three main problems: translation ambiguity; compound and phrase handling and proper names translation.
The use of natural language processing (NLP) techniques, such as stemming, Part-of-Speech (POS) tagging is useful in query translation process.
Hence, n-gram matching technique has successfully applied to information retrieval (IR) system for phrases and proper names translation.
The proposed query translation architecture consist of stemming, Part-of-Speech (POS) tagging and n-gram matching techniques is useful in CLIR system as well as search engine application.

This paper focuses on a generalized approach to providing user interface to a web-based expert system (WBES We examine MVC and MVP design patterns used traditionally to construct a web application user interface.
In order to leverage the strength of the MVC/MVP design patterns we propose a special ontology representing a user communication domain.
We describe a self-service networked infrastructure for automatic deployment of command line interface (CLI) applications.
We demonstrate how to apply the proposed ontology for the design of a WBES aimed at supporting client software re-execution in clouds.
In particular, we address the problems existing in the area of software development for music information retrieval algorithms implementation.

Die Fachtagung wurde gemeinsam vom Hochschulverband Informationswissenschaft, den
GI-Fachgruppen Hypertext, Information Retrieval und Multimediale elektronische Dokumente, der Östereichischen Computer Gesellschaft sowie der Schweizer Informa-tiker Gesellschaft veranstaltet.

In this work we propose a feature location approach that targets models as the feature realization artifacts.
The approach combines Genetic Algorithms and Information Retrieval techniques.
Given a model and a feature description, model fragments extracted from the model are evolved using genetic operations.
Then, Formal Concept Analysis is used to cluster the model fragments based on their common attributes into feature realization candidates.
Finally, Latent Semantic Analysis is used to rank the candidates based on the similarity with the feature description.
As a result, the genetic algorithm evolves the population of model fragments to find the set of most suitable feature realizations.
We have evaluated the approach with an industrial case study, locating features with precision and recall values around 90 baseline obtains less than 40
Finally, we provide recommendations on how to provide the input to the approach to improve the location of features over the models.

The number of users of an on-line shopping websites is continuously increasing.
Such website often provides facility for the users to give comments and ratings to the products being sold on the websites.
This information can be useful as the recommendation for other users in making their purchase decision.
This paper investigates the problem of predicting rating based on users' comments.
A classifier based on information retrieval model is proposed for the prediction.
In addition, the effect of integrating sentiment analysis for the rating prediction is also investigated.
Based on the results, an improvement in prediction performance can be expected with sentiment analysis where an increase of 54% is achieved.

The combination of evidence for Information Retrieval has been studied extensively in order to increase effectiveness.
In this paper, we study the selective application of different retrieval approaches on a per-query basis for Web Information Retrieval.
Our methodology is based on the assumption that not all queries benefit from a uniform retrieval approach.
In order to select the most appropriate retrieval approaches for each query, we use evidence from the hyperlink structure of the retrieved documents and also from the distribution of aggregates, that is the groups of documents from the same domain.
Our experimental results show that it is possible to obtain important improvements in retrieval effectiveness by using simple statistical decision mechanisms on a per-query basis.

As GSM devices are widely spread around the world.
They are today used for many different purposes.
This paper presents a new approach in using PDA or GSM devices to establish a system architecture for personalized retrieval of user defined Web information.
The solution does not need any special or proprietary hardware devices other than GSM or PDA device with short message service (SMS) capability and PC computer with Internet connection acting as a server.

This paper presents the experiments carried out at Jadavpur University as part of the participation in the Forum for Information Retrieval Evaluation (FIRE)
2010 in ad-hoc mono-lingual information retrieval task for English and Bengali languages.
The experiments carried out by us for FIRE 2010 are based on stemming, zonal indexing, theme identification, TF-IDF based ranking model and positional information.
The document collection for English and Bengali contained 1,23,047 and 1,25,586 documents respectively.
Each query was specified using title, narration and description format.
75 queries were used for training the system while the system was tested with 50 queries in each of English and Bengali.

The probabilistic formalism of quantum physics is said to provide a sound basis for building a principled information retrieval framework.
Such a framework can be based on the notion of information need vector spaces where events, such as document relevance or observed user interactions, correspond to subspaces.
As in quantum theory, a probability distribution over these subspaces is defined through weighted sets of state vectors (density operators and used to represent the current view of the retrieval system on the user information need.
Tensor spaces can be used to capture different aspects of information needs.
Our evaluation shows that the framework can lead to acceptable performance in an ad-hoc retrieval task.
Going beyond this, we discuss the potential of the framework for three active challenges in information retrieval, namely, interaction, novelty and diversity.

In information retrieval (IR) systems, there are a query and a collection of documents compared with this query and ranked according to a particular similarity measure.
Since texts with the same content can be written by different authors, the writing styles of the documents change as well accordingly.
This observation brings the idea of investigating text by means of style.
In this paper, we analyze text documents in terms of stylistic features of the written text and measure effectiveness of these features in an IR system.
Our main focus is on Turkish text documents.
Although there are many studies about broadening IR systems with style based enhancement, there is no similar application for Turkish which performs retrieval depending purely on style.

The paper presents the PortEdu Project (an Educational Portal which consists of a MAS (MultiAgent System) architecture for a leaning environment in the web strongly based on personalized information retrieval.
Experiencing search mechanisms it has been detected that the success of distance learning mediated by computer is linked to the contextual search tools quality.
Our goal with this project is to aid the student in his learning process and retrieve information pertaining to the context of the problems, which are being studied by the students.

In TREC-9, we participated in the English-Chinese Cross-Language Information Retrieval (CLIR) track.
Our work involved two aspects: finding good methods for Chinese IR, and finding effective translation means between English and Chinese.
On Chinese monolingual retrieval, we investigated the use of different entities as indexes, pseudorelevance feedback, and length normalization, and examined their impact on Chinese IR.
On EnglishChinese CLIR, our focus was put on finding effective ways for query translation.
Our method incorporates three improvements over the simple lexicon-based translation 1) word/term disambiguation using co-occurrence 2) phrase detecting and translation using a statistical language model and (3) translation coverage enhancement using a statistical translation model.
This method is shown to be as effective as a good MT system.

In this letter, we propose two new support vector approaches for ordinal regression, which optimize multiple thresholds to define parallel discriminant hyperplanes for the ordinal scales.
Both approaches guarantee that the thresholds are properly ordered at the optimal solution.
The size of these optimization problems is linear in the number of training samples.
The sequential minimal optimization algorithm is adapted for the resulting optimization problems; it is extremely easy to implement and scales efficiently as a quadratic function of the number of examples.
The results of numerical experiments on some benchmark and real-world data sets, including applications of ordinal regression to information retrieval, verify the usefulness of these approaches.

Publishing in general and newspaper organizations specifically confront rising challenges to collect information faster at lower cost.
These challenges must be coordinated with organization's strategic, tactical, and operational needs via the alignment of Information Technology (IT) with the business strategy.
The ultimate goal is a Modern Publishing and Newspaper Industries.
This paper provides an innovative solution to accomplish this goal by accelerating publishing and newspaper processes time.
The process' time is accelerated via an Intelligent Search Lifecycle Architecture (ISLA which integrates Search Engine, Information Extraction, Information Retrieval, Data Mining and Data Warehouse using Service-Oriented Architecture (SOA).

Simulation of human users engaged in interactive information retrieval (IIR) may be a key resource to enable evaluation of IR systems in interactive settings in ways that are scalable, objective, and cheap to implement.
This paper considers generation of simulated users in an operationalist framework.
It identifies several challenges due to the cognitive nature of IIR and its highly conditionalized interactions with information systems and suggests a program for user simulation must rely on results from user studies and will need to overcome several difficult modeling problems.

Information retrieval is an important activity especially for cross-language environment.
When the knowledge is represented by some means/method, it will be easy to retrieve the information.
So, to represent knowledge ontology is a rich source, which may give better approach for information retrieval especially for cross language searching.
Cross-language information retrieval (CLIR) is a retrieval process in which the user presents queries in one language to retrieve information in another language.
CLIR has gained popularity among Information Retrieval (IR) researches in recent years.
CLIR is very much needed; especially when the user only knows his/her native language and it may not be possible to process native language all the time.
Simple approaches have been developed for CLIR by using multi-lingual dictionary or Word Net.
Ontology will be better choice for CLIR, as it covers the entire context and its relationships, which will be helpful for both user and system provider.

In recent years, data mining has been increasingly advocated in academia and industries.
Its applications are widespread in such disciplines as engineering, marketing, biology, and web analysis.
Data mining is a multidisciplinary field, drawing work from areas such as database technology, artificial intelligence, machine learning, neural networks, statistics, pattern recognition, knowledge based systems, knowledge acquisition, information retrieval, high performance computing and data visualization.
A more complete definition of data mining is as follows Data mining is the process of selection, exploration and modelling of large quantities of data to discover regularities or relations that are at first unknown with the aim of obtaining clear and useful results for the owner of the database
In this study we used the prediction tools of PolyAnalyst for three datasets’ and compared the results of the analysis by the predictive accuracies.

Approaches for extracting related words (terms) by co-occurrence work poorly sometimes.
Two words frequently co-occurring in the same documents are considered related.
However, they may not relate at all because they would have no common meanings nor similar semantics.
We address this problem by considering the page designer’s intention and propose a new model to extract related words.
Our approach is based on the idea that the web page designers usually make the correlative hyperlinks appear in close zone on the browser.
We developed a browser-based crawler to collect “geographically” near hyperlinks, then by clustering these hyperlinks based on their pixel coordinates, we extract related words which can well reflect the designer’s intention.
Experimental results show that our method can represent the intention of the web page designer in extremely high precision.
Moreover, the experiments indicate that our extracting method can obtain related words in a high average precision.

Magnus Jonsson School of Information Science, Computer and Electrical Engineering Halmstad University Halmstad, Sweden

The annual Text Retrieval Conference (TREC) has become a major event in information retrieval (IR) research.
Over the last few years, its main focus has changed from the common ad hoc and routing tasks to tracks focused on special interests, such as cross-language retrieval, question answering, and large database/Web retrieval.
In each of the tracks, a common task and test collection is developed each year for all participating groups to undertake.
One problem the tracks have is a paucity of face-to-face meetings, since the only certain time each track actually meets is at the annual TREC meeting.
For many tracks, this time is too early to work out all the details of the coming yearg track, and most additional work is done via email.

Advances in digital capture and storage technologies mean that it is now possible to capture and store one’s entire life experiences in a Human Digital Memory (HDM
However, these vast personal archives are of little benefit if an individual cannot locate and retrieve significant items from them.
While potentially offering exciting opportunities to support a user in their activities by providing access to information stored from previous experiences, we believe that the features of HDM datasets present new research challenges for information retrieval which must be addressed if these possibilities are to be realised.
Specifically we postulate that effective retrieval from HDMs must exploit the rich sources of context data which can be captured and associated with items stored within them.
User’s memories of experiences stored within their memory archive will often be linked to these context features.
We suggest how such contextual metadata can be exploited within the retrieval process.

In this paper we investigate how retrieving information can be improved through task related indexing of documents based on ontologies.
Different index types, varying from content­based keywords to structured task­based indexing ontologies are compared in an experiment that simulates the task of creating instructional material from a database of source material.
To be able to judge the added value of task and ontology related indexes, traditional information retrieval performance measures are extended with new measures reflecting the quality of the material produced with the retrieved information.
The results of the experiment show that a structured task­based indexing ontology improves the quality of the product created from retrieved material only to some extent, but that it certainly improves the efficiency and effectiveness of search and retrieval and precision of use.

Exploration of text corpora using self-organizing maps has shown promising results in recent years.
Topographic map approaches usually use the original vector space model known from Information Retrieval for text document representation.
In this paper I present a two stage model using features based on sentence categories as alternative approach which includes contextual information.
Algorithmic optimizations required by this computationally expensive model are shown and evaluated.
Also a method for model independent comparison of document maps by evaluation of document distribution on maps is introduced and used to compare results obtained with both the new model and the vector space model.

This paper presents an extended model for image representation and retrieval (EMIR This model combines different interpretations of an image to build a complete description of it, each interpretation being represented by a particular view.
The set of views considered in EMIR2 include the physical view and the logical view, which is an aggregation of four main views: the structural view, the spatial view, the perceptive view, and the symbolic view.
A description of the model concepts is given using a mathematical notation, yieldingthe framework EMIR2
We defined a first correspondence function that estimates the similarity between two images, one being the query.

The paper presents the CROSSMARC approach for the complex task of identification of interesting web sites and web pages and the extraction of information from them.
This task is hard because most of the information on the Web today is in the form of HTML documents, which are designed for presentation purposes and not for automatic extraction systems.
This task becomes even harder in a multilingual context, where web pages in different languages need to be considered.
CROSSMARC approach focuses on the easy customization of web information retrieval and extraction technology to new domains and languages.
This is achieved by adopting and implementing an open, multi-lingual and multi-agent architecture that integrates the CROSSMARC components into a web-based prototype system, as well as by providing an infrastructure that facilitates customization of its components to new domains and languages.

One of the challenges in cross lingual information retrieval is the retrieval of relevant information for a query expressed in a native language.
While retrieval of relevant documents is slightly easier, analyzing the relevance of the retrieved documents and the presentation of the results to the users are non-trivial tasks.
A method for information retrieval for a query expressed in a native language is presented in this paper.
It uses insights from data mining and intelligent search for formulating the query and parsing the results.
It also uses heuristic methods for the categorization of documents in terms of relevance.
Our approach compliments the search engine’s inbuilt methods for identifying and displaying the results of queries.
A prototype has been developed for analyzing Tamil-English corpora.
The initial results have shown that this approach is suitable for on the fly retrieval of documents.

To facilitate the search for relevant information across a setof online distributed collections, a federated information retrieval system typically represents each collection, centrally, by a set of vocabularies or sampled documents.
Accurate retrieval is therefore related to how precise each representation reflects the underlying content stored in that collection.
As collections evolve over time, collection representations should also be updated to reflect any change, however, a current solution has not yet been proposed.
In this study we examine both the implications of out-of-date representation sets on retrieval accuracy, as well as proposing three different policies for managing necessary updates.
Each policyis evaluated on a testbed of forty-four dynamic collections over an eight-week period.
Our findings show that out-of-date representations significantly degrade performance overtime, however, adopting a suitable update policy can minimise this problem.

The paper describes evaluation resources for concept-based, cross-lingual information retrieval in the medical domain.
All resources were constructed in the context of the MuchMore project and are freely available through the project website.
Available resources include: a bilingual, parallel document collection of German and English medical scientific abstracts, a set of queries and corresponding relevance assessments, two manually disambiguated test sets for semantic annotation (sense disambiguation two evaluation lists for German morphological decomposition of medical terms.

Isis Truck, Jacques Malenfant.
Towards A Unification Of Some Linguistic Representation Models: A Vectorial Approach.
The 9th International FLINS Conference on Computational Intelligence in Decision and Control, Aug 2010, Chengdu, China.
World Scientific, Computational Intelligence: Foundations and Applications, Proceedings of the 9th International FLINS Conference (World Scientific Proceedings Series on Computer Engineering and Information Science pp.610-615, 2010
hal-00567019>

Bibster is a semantics-based Peer-to-Peer system for exchanging bibliographic data among researchers.
Bibster exploits ontologies in data storage, query formulation, query routing and answer presentation.
While the original Bibster system assumed a globally shared domain ontology, we here describe extensions to the Bibster system, that allow to learn personalized ontologies from the local bibliographic metadata.
These personal ontologies can not only be used for subsequently classifying the bibliographic metadata, but also for supporting an improved query refinement process.

In the present work a model is proposed which is useful for text summarization of the given document by using pattern recognition techniques for improving the retrieval performance of the relevant information.
The design and implementation of the proposed systems is concerned with methods for summarizing of the retrieving information from a collection of documents or corpuses.
The quality of a system is measured by how useful it is to the typical users of the system.
In the basic approach, a query is considered generated from an “ideal” document that satisfies the information need.
The system’s job is then to estimate the likelihood of each document in the collection being the ideal document and rank them accordingly.
The recent development of related techniques stimulates new modeling and estimation methods that are beyond the scope of the traditional approaches.

This paper introduces an intelligent sink mechanism based on the observation and analysis of non-uniform energy consumption problem in sensor network, in which more energy consumed at nodes near the sink than at nodes far away.
The proposed intelligent sink utilizes dynamic sink proxies for localized data query and collection, so as to achieve highly distributed and low energy consumption among sensor nodes.
The evaluation result reveals that the proposed sink mechanism reduces the energy consumption of information retrieval in the network and obtains balanced energy consumption among sensor nodes.

The effectiveness of three stop words lists for Arabic Information Retrieval General Stoplist, CorpusBased Stoplist, Combined Stoplist were investigated in this study.
Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling.
The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval.
The LDC (Linguistic Data Consortium)
Arabic Newswire data set was used with the Lemur Toolkit.
The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight.
The overall performance of a general stoplist was better than the other

In recent years, the algorithms of learning to rank have been proposed by researchers.
However, in information retrieval, instances of ranks are imbalanced.
After the instances of ranks are composed to pairs, the pairs of ranks are imbalanced too.
In this paper, a cost-sensitive risk minimum model of pairwise learning to rank imbalanced data sets is proposed.
Following this model, the algorithm of cost-sensitive supported vector learning to rank is investigated.
In experiment, the standard Ranking SVM is used as baseline.
The document retrieval data set is used in experiment.
The experiment results show that the performance of cost-sensitive support vector learning to rank is better than Ranking SVM on two rank imbalanced data sets.

With the rising popularity of rich media services such as Flickr, YouTube, and Jumpcut, new challenges in large scale multimedia information retrieval have emerged that not only rely on meta-data but on content-based information retrieval combined with the collective knowledge of users and geo-referenced meta-data that is captured during the creation process.
For the future, it is envisioned that multimedia search in mobile environments or on P2P networks will take off on a large scale.

The last 30 years have witnessed the emergence and growth of the discipline Computational Linguistics.
This field, which is also known as linguistic data processing, language data processing, natural language processing, and information linguistics, reflects the interests of many different disciplines ranging from philology, linguistics, computer and information science, psychology, and psycholinguistics.

In this paper we examine the domain of information search and propose a “goal-based” approach to study search strategy.
We describe “goal-based information search” using a framework of Knowledge Discovery.
We identify two Information Retrieval (IR) goals using the constructs of Knowledge Acquisition (KA) and Knowledge Explanation (KE We classify these constructs into two specific information problems: An exploration-exploitation problem and an implicitexplicit problem.
Our proposed framework is an extension of prior work in this domain, applying an IR Process Model originally developed for Legal-IR and adapted to Medical-IR.
The approach in this paper is guided by the recent ACM-SIG Medical Information Retrieval (MedIR)
Workshop definition methodologies and technologies that seek to improve access to medical information archives via a process of information retrieval.

Requirements tracing is a central activity for software systems quality management.
However, in large-scale evolving systems, maintaining traceability information manually can become a tedious task.
To address this problem, several dynamic techniques were introduced to provide automatic traceability links generation.
These techniques are usually based on information retrieval (IR) methods which link different artifacts based on their syntactic information.
This paper reports an ongoing experimental investigation of using semantics-enabled IR methods to generate traceability links.
Our goal is to explore dynamic, accurate, and conceptually rich ways to generate and maintain traceability information.

INTERNATIONAL JOURNAL OF ENGINEERING SCIENCES RESEARCH TECHNOLOGY CONTENT BASED INFORMATION RETRIEVAL FOR DIGITAL LIBRARY
USING DOCUMENT IMAGE Roshni S. Tadse L. H. Patil, C. U. Chauhan Research Scholar, Department of Computer Science and Engineering, Priyadarshini Institute of Engineering and Technology Nagpur (MS India Asst.
Professor, Department of Computer Science and Engineering, Priyadarshini Institute of Engineering and Technology Nagpur (MS India Asst.
Professor, Department of Computer Science and Engineering, Priyadarshini Institute of Engineering and Technology Nagpur (MS India

We present an on-line learning framework tailored towards real-time learning from observed user behavior in search engines and other information retrieval systems.
In particular, we only require pairwise comparisons which were shown to be reliably inferred from implicit feedback (Joachims et al 2007; Radlinski et al 2008b
We will present an algorithm with theoretical guarantees as well as simulation results.

The 2016 CLEF eHealth Task 3 aims to evaluation the effectiveness of information retrieval systems when searching for health content on the web.
The ClueWeb12 B13 data set is utilized in this task.
This paper presents our work on the 2016 CLEF eHealth Task 3.
We propose a Web-based query expansion method and a combination method to better understand and satisfy the task.
In particular, we test our Web-based query expansion method in many medical data sets and achieve an outstanding performance.

We consider the problem of labeling a partially labeled graph.
This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings.
It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance.
Our approach develops a framework for regularization on such graphs.
The algorithms are very simple and involve solving a single, usually sparse, system of linear equations.
Using the notion of algorithmic stability, we derive bounds on the generalization error and relate it to structural invariants of the graph.
Some experimental results testing the performance of the regularization algorithm and the usefulness of the generalization bound are presented.

In this paper, the task of text segmentation is approached from a topic modeling perspective.
We investigate the use of latent Dirichlet allocation (LDA) topic model to segment a text into semantically coherent segments.
A major benefit of the proposed approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment.
This information is of potential use in applications like segment retrieval and discourse analysis.
The new approach outperforms a standard baseline method and yields significantly better performance than most of the available unsupervised methods on a benchmark dataset.

We propose a construction of a single database private information retrieval system using fully homomorphic encryption.
The construction results in a system with Ω(log2 n) communication complexity, and polylogarithmic computational overhead, in the size of the database.
An improved construction is also proposed, which reduces the client’s communication overhead, and leaves open the possibility of reducing the total communication overhead.
This results in a construction which has higher communication overhead than the current optimal system, but which is more computationally efficient.

Rhetorical structure theory (RST) is one of the leading theories in computational linguistics.
It improved the ability of extracting the semantic behind the processed text.
Researches showed that different application (information retrieval, text summarization, text generation etc) have proved to give better result using RST.
The applicability of RST to process and understand texts has been studied in several languages, but none of those studies addressed the Arabic language.
In this paper, we present a framework of applying RST on Arabic language in order to rhetorically parse and understand the Arabic texts

E cient and accurate information retrieval is a key issue in image databases.
Since image databases use image features for retrieval, traditional alphanumeric indexing methods are not particularly suitable for content-based retrieval.
Therefore, new indexing methods must be designed and implemented speci cally for image retrieval.
In this paper, we propose to use competitive learning clustering algorithm to produce an indexing structure for Montage, which is an image database supporting content-based retrieval using color, texture, sketch, and shape for Hong Kong's fashion, textile, and clothing industry.
Competitive learning is a stochastic and e cient clustering method which provides good cluster center approximation for image database indexing.
Using synthetic data, we demonstrate the Recall and Precision performance of nearest neighbor feature retrieval based on the indexing structure generated by competitive learning clustering and show that the algorithm works well.

The authors use the information visualization software x2014; CiteSpace II to conduct the knowledge mapping based on the data of cross-language information retrieval(CLIR) downloaded from the Web of Science at Institute for Scientific Information (ISI including SCI, SSCI, and A&amp;HCI.
The paper maps the co-citation network mapping to reveal the representative scientists and documents of CLIR.
Then draws the term and keyword co-appearance network to analyze the research hotspots of CLIR, and concludes that query translation, query expansion and machine translation are the hot topics of CLIR.
At last, we map the research front mapping to analyze the research fronts of CLIR and find that the research fronts and development trend are the methods, strategies, and evaluation of CLIR.

Maintaining mobile agent availability in the presence of agent server crashes is a challenging issue since developers normally have no control over remote agent servers.
A popular technique is that a mobile agent injects a replica into stable storage upon its arrival at each agent server.
However, a server crash leaves the replica unavailable, for an unknown time period, until the agent server is back on-line.
This paper uses exception handling to maintain the availability of mobile agents in the presence of agent server crash failures.
Two exception handler designs are proposed.
The first exists at the agent server that created the mobile agent.
The second operates at the previous agent server visited by the mobile agent.
Initial performance results demonstrate that although the second design is slower it offers the smaller trip time increase in the presence of agent server crashes.

The production systems used by the subsea petroleum industry are knowledge and information intensive.
Any problem needs to be solved quickly and efficiently avoiding decommissioning or waiting for the symptoms to be escalated.
This requires precise information to be supplied on-time.
For this reason we have proposed rule-based monitoring of device performance.
However, covering all possible cases by rules is a labour-intensive and not trivial task.
Therefore, in this paper we propose a scenario-driven information retrieval approach to complement rule-based monitoring.
The main objective is to automatically formulate a query that is sent to a vector-space model information retrieval engine every time incomplete inference happens, i.e. when a specific case has no rules defined.

In this paper, a supervised neural network has been used to classify pairs of terms as being multiwords or non-multiwords.
Classification is based on the values yielded by different estimators, currently available in literature, used as inputs for the neural network.
Lists of multiwords and nonmultiwords have been built to train the net.
Afterward, many other pairs of terms have been classified using the trained net.
Results obtained in this classification have been used to perform information retrieval tasks.
Experiments show that detecting multiwords results in better performance of the IR methods.

This discussion takes the position that information retrieval systems are fundamentally linguistic in nature in essence, the languages of document representation and searching are dialects of natural language.
Because of this, the discipline of the Philosophy of Language should have some bearing on the problems of document representation and search query formulation.
The philosophies of Austin, Searle, Grice and Wittgenstein are briefly examined and their relevance to information retrieval theory is discussed.

— Information Retrieval is a process made by a user to obtain relevant information which meets his needs using an Information Retrieval System (IRS However the IRS shows some differences between user relevance and system relevance.
These variations are primarily related to the imperfection of the indexing process (approach directly related to the IR) and specially the non consideration of the user profile.
This paper presents a study about formalisms proposed in the literature and addresses some reflections to deal with problems arising from this survey, in order to satisfy the final user in Information Retrieval process.

We apply a recent formalization of visualization as information retrieval to linear projections.
We introduce a method that optimizes a linear projection for an information retrieval task: retrieving neighbors of input samples based on their low-dimensional visualization coordinates only.
The simple linear projection makes the method easy to interpret, while the visualization task is made well-defined by the novel information retrieval criterion.
The method has a further advantage: it projects input features, but the input neighborhoods it preserves can be given separately from the input features, e.g. by external data of sample similarities.
Thus the visualization can reveal the relationship between data features and complicated data similarities.
We further extend the method to kernel-based projections.

Mathematical Morphology is a theory concerned with the processing and analysis of images or signals using filters and other operators that modify them.
This paper studies how the original images and signals can be retrieved using fuzzy property-oriented concept lattices and fuzzy relation equations.

The Unified Medical Language System (UMLS) is an extensive collection of terms and concepts.
The UMLS includes biomedical terms from standard classifications.
The semantic network (SN) links the concepts, sometimes ambiguously.
In this paper we try, on one hand to describe the relationship between concepts more efficiently and on the other hand to find new relationships.
Assuming that re-usability and automatic extraction of knowledge from existing thesaurus enables an improvement of the metatheasaurus, we cross the SN with linked concepts from the ADM (Assisted Medical Diagnosis Results are presented and our discussion concerns firstly the use of the SN only; secondly the improvement that allows pre-selection of linked concepts, and thirdly the possibility to coincide with other developments that improve the metathesaurus.

BACKGROUND Millions of consumers perform health information retrieval (HIR) online.
To better understand the consumers' perspective on HIR performance, we conducted an observation and interview study of 97 health information consumers.
METHODS Consumers were asked to perform HIR tasks and we recorded their view regarding performance using several differ-ent subjective measurements: finding the desired information, usefulness of the information found, satisfaction with the information, and intention to continue searching.
Statistical analysis was applied to verify if the multiple subjective measurements were redundant.
The measurements ranged from slight agreement to no agreement among them.
A number of reasons were identified for this lack of agreement.
Although related, the four subjective measurements of HIR performance are distinct from each other and carried different useful information

Open domain question answering has become a very active research area over the past few years, due in large measure to the stimulus of the TREC Question Answering track.
This track addresses the task of finding <b>answers</b> to natural language (NL) questions (e.g
i>How tall is the Eiffel Tower?
Who is Aaron Copland
i from large text collections.
This task stands in contrast to the more conventional IR task of retrieving <b>documents</b> relevant to a query, where the query may be simply a collection of keywords (e.g i>Eiffel Tower, American composer, born Brooklyn NY 1900</i

The promise of the Semantic Web is to democratise access to data, allowing anyone to make use of and contribute back to the global store of knowledge.
Within the scope of the OMRAS2Music Information Retrieval project, we have made use of and contributed to Semantic Web technologies for purposes ranging from the publication of music recording metadata to the online dissemination of results from audio analysis algorithms.
In this paper, we assess the extent to which our tools and frameworks can assist in research and facilitate distributed work among audio and music researchers, and enumerate and motivate further steps to improve collaborative efforts in music informatics using the Semantic Web.
To this end, we review some of the tools developed by the OMRAS2 project, examine the extent to which our work reflects the Semantic Web paradigm, and discuss some of the remaining work needed to fulfil the promise of online music informatics research.

Applied Social Sciences Index Abstracts (ASSIA Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index DBLP;
GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Research Articles

Cutwidth minimization problem (CMP) consists of finding a linear layout of a graph such that the maximal number of cuts of a line separating consecutive vertices is minimized.
CMP has significant applications in VLSI design, network communications, automatic graph drawings and information retrieval but it is proven to be a NP hard problem.
Exact results of cutwidth are known for some classes of graphs but no algorithm has been proposed for the general graphs.
In this paper, we present a hybrid evolutionary algorithm (HEA) for CMP which uses the depth first search of graph to generate the initial population and incorporates the simulated annealing in the selection process.
HEA achieves the known optimal cutwidth of all the standard graphs tested.
We also conjecture the cutwidth of some circulant graphs and generalized Peterson graphs supported by our experimental results.

In this paper we propose to report a work on evaluation of a knowledge based application that leads to the constitution of a benchmark such as those that exist in Information Retrieval evaluations.
This benchmark enables to perform quantitative evaluation by classic metrics such as precision and recall.
We had also conducted a qualitative analysis that helps the elaboration of guidelines and methodological indications for ontology evaluation and enhancement.

A Mongolian Information Retrieval System based on Solr is proposed in this paper.
The system implements the retrieval of Mongolian data store in our local machine.
Firstly, in this paper, we built a Mongolian corpus with one million words, which has been corrected manually.
Secondly, after being transcoded, theses data was represented by Latin characters.
Finally, we used Solr to build indexes and documents so that we do queries on millions of data within seconds.

Complex documents are used in many environments, e.g information retrieval (IR Such documents contain subdocuments, which may contain further subdocuments, etc.
Powerful tools are needed to facilitate their retrieval, restructuring, and analysis.
Existing IR systems are poor in com plex document restructuring and data aggregation.
However, in practice, IR system users would often want to obtain aggregation information on subdocuments of complex documents.
In this paper we address this problem and provide a truly declarative and powerful interface for the users.
Our interface is based on the non-fkst-normal-form (NF2) relational model.
It allows intuitive and systematic modeling of complex documents.

The algebra-based and ontology-based information retrieval model are investigated in this paper,from which we know the two reinforce each other to some extent.
Based on this observation, an integrated method for information retrieval is presented by taking the advantage of these two models.
Then, an effective matching algorithm for pairing of different characteristic words is presented to improve the similarity calculating between the query and text.
The experimental results show that, compared with the algebra-based and ontology-based models, the proposed method obviously gets a better precision and recall.

Incident reporting is becoming increasingly important in large organizations.
Legislation is progressively being introduced to deal with this information.
One example is the European Directive No. 94/95/EC, which obliges airlines and national bodies to collect and collate reports of incidents.
Typically these organizations use manual files and standard databases to store and retrieve incident reports.
However, research has established that database technology needs to be enhanced in order to deal with incidents.
This paper describes the design and implementation of InRet, an incident report retrieval system that endeavours to find similarities and patterns between incidents by combining the strengths of Case-Based Reasoning and Information Retrieval techniques in an integrated system.
Preliminary results from InRet are presented and are encouraging.

In this thesis, we present a kind of chat corpus in Chinese word segmentation and we also present its construction process.
This kind of chat corpus works in the way of combining application of automatic segmentation technology with the method of manual correction.
Thereinto, the automatic segmentation is performed in the way of using the Natural Language Processing Information Retrieval (NLPIR
As to manual correction, errors from NLPIR will be categorized and some annotation suggestions will be put forward.
Combining using these two methods above, our study, which is a preliminary study, could be very easy extended to other Chats texts.
What's more, the corpus, which produced in our works, could provide a good standard for the research of Chinese word segmentation, especially in the part of dialogue.

Inspired by the great success of information retrieval (IR) style keyword search on the web, keyword search on XML has emerged recently.
Existing methods cannot resolve challenges addressed by using keyword search in Temporal XML documents.
We propose a way to evaluate temporal keyword search queries over Temporal XML documents.
Moreover, we propose a new ranking method based on the time-aware IR ranking methods to rank temporal keyword search queries results.
Extensive experiments have been conducted to show the effectiveness of our approach.
Keywords—temporal XML; Keyword Search; ranking

Users who downloaded this article also downloaded:
N.J. BELKIN, R.N. ODDY, H.M. BROOKS 1982
ASK FOR INFORMATION RETRIEVAL:
PART I. BACKGROUND AND THEORY Journal of Documentation, Vol.
38 Iss 2 pp.
61-71 http dx.doi.org/10.1108/eb026722 DAVID ELLIS 1989
A BEHAVIOURAL APPROACH TO INFORMATION RETRIEVAL SYSTEM DESIGN Journal of Documentation, Vol.
45 Iss 3 pp.
171-212 http
dx.doi.org/10.1108/eb026843 María J. López-huertas 1997 Thesaurus structure design: a conceptual approach for improved interaction Journal of Documentation, Vol.
53 Iss 2 pp.
139-177 http dx.doi.org/10.1108/ EUM0000000007197

Recently, various language model approaches have been proposed in the information retrieval realm, with their promising performances in general document and Web page retrieval applications.
Based on these achievements, in this paper, we investigate and discuss whether language model approaches can be adapted to content based image retrieval (CBIR based on the x0201C;bag of visual words&#x0201D; image representation.
A critical element of language model estimation is smoothing, which adjusts the maximum likelihood estimation to overcome the data sparseness problem.
Therefore, we perform extensive studies over different smoothing methods, strategies, and parameters, by showing their impacts to the retrieval performances.
Experiments are performed over two popular image retrieval databases, together with some insightful conclusions to facilitate the adaptation of language model approaches to CBIR.

Traditional information retrieval systems rank documents according to their relevance to users' input queries.
State of the art commercial search engines (SEs) train ranking models and suggest query refinements by exploiting collective intelligence implicitly using global users' query logs.
However, they do not provide an explicit channel for users to communicate with each other in the search process.
By asking or discussing with other users on the fly, a user could find relevant information more conveniently and gain a better search experience.
In this paper, we present a demo of novel Search Engine with a live Chat Channel (SECC SECC can group users automatically based on their input queries and allow them to communicate with each other in real time through a chat interface.

Searching for relevant information on the worldwide web is often a difficult and frustrating task.
The information one is looking for, is hidden among thousands of documents returned by a search engine.
One way of making search for relevant information easier, is to create better interfaces to the search engines; interfaces that facilitate quick and efficient browsing through the multitude of returned documents.
In this paper, we present FIRE a multimodal interface for information retrieval deployed in the Intelligent Room at the MIT AI Lab.
FIRE differs from most other interfaces for information retrieval in that it combines a couple of interaction modalities to improve the search process.

This paper describes our IR (Information Retrieval) based method for SemEval 2016 task 1, Semantic Textual Similarity (STS
The main feature of our approach is to extend a conventional IR-based scheme by incorporating word alignment information.
This enables us to develop a more fine-grained similarity measurement.
In the evaluation results, we have seen that the proposed method improves upon a conventional IR-based method on average.
In addition, one of our submissions achieved the best performance for the “postediting” data set.

In this paper we describe a mechanism to improve Information Retrieval (IR) on the web.
The method is based on Formal Concepts Analysis (FCA) that it is makes semantical relations during the queries, and allows a reorganizing, in the shape of a lattice of concepts, the answers provided by a search engine.
We proposed for the IR an incremental algorithm based on Galois lattice.
This algorithm allows a formal clustering of the data sources, and the results which it turns over are classified by order of relevance.
The control of relevance is exploited in clustering, we improved the result by using ontology in field of image processing, and reformulating the user queries which make it possible to give more relevant documents.
Keywords-FCA; Galois lattice; IR; Ontology; Query Reformulation)

Taxonomy is a method used to ease information retrieval for an effective knowledge management system.
Studies have found information technology is vital for knowledge to be successfully managed and shared.
We decided therefore to survey IT taxonomy studies using literature review and classification of articles from 1989 to 2008 in order to explore how taxonomy applications have developed in this period.
Based on 62 articles found, the study classifies the taxonomy research into main topics and scope, methodologies most often applied, theories and models, and contributing countries.
Based on the findings, we offer suggestions on improving research in this field.

The rapid and uncontrolled growth of the World Wide Web (WWW) imposes severe constraints when meeting the diverse needs of today‘s information society.
The main challenge resides on: location, retrieval, integration and coherent presentation of quality and relevant ubiquitous information (anytime, anywhere and anyhow with the correct format for the required device) from WWW, quickly and accurately, to satisfy a user‘s singular information needs.
Taking into consideration the unique features of the WWW environment, the limitations of existing solutions and the fact that agents are efficient tools for the development of Web Information Retrieval (WIR) systems; the current paper describes UWIRS (acronym of Ubiquitous Web Information Retrieval Solution a web information retrieval system that includes aspects to keep in mind for the ideal WIR solution.

Usually, user accesses to information on the web using search engines.
However, a significant factor, which is the quality of the restored documents, is underestimated by most of these engines.
This is due to not taking into account all dimensions of a web document during the indexing and retrieval process.
In recent years, language models have been proposed on Information Retrieval and increased in popularity, due to their simplicity, clear probabilistic meaning.
In addition these models offer possibility of integrating a priori document information.
Several features have been used to estimate the prior probability of a document such as: document length.
However these features depend only on the document.
A web page is part of website.
The idea that we explore in this article is to use website features which contains the concerned page to estimate the page prior.

Evaluation is a central aspect of information retrieval (IR) research.
In the past few years, a new evaluation methodology known as living labs has been proposed as a way for researchers to be able to perform in-situ evaluation.
The first CIKM workshop on Living Labs for IR evaluation (LL’13) was held on 1st November 2013 in San Francisco, USA.
The workshop consisted of an industrial keynote, four oral paper presentations, three demo presentations, and a discussion session.
This report presents an overview of the scope and contents of the workshop and outlines the major outcomes.

Future Information Retrieval, especially in connection with the internet, will incorporate the content descriptions that are generated with social network extraction technologies and preferably incorporate the probability theory for assigning the semantic.
Although there is an increasing interest about social network extraction, but a little of them has a significant impact to information retrieval.
Therefore this paper proposes a model of information retrieval from the social network extraction.

Nowadays, large amount of data is available everywhere.
Therefore, it is very important to analyze this data in order to extract some useful information and to develop an algorithm based on this analysis.
This can be achieved through data mining and machine learning.
Machine learning is an integral part of artificial intelligence, which is used to design algorithms based on the data trends and historical relationships between data.
Machine learning is used in various fields such as bioinformatics, intrusion detection, Information retrieval, game playing, marketing, malware detection, image deconvolution and so on.
This paper presents the work done by various authors in the field of machine learning in various application areas.

The performance of an IR system is deteriorated by factors including short and vague queries put up by the users, ever increasing volume of documents on the web, users not knowing their exact information need etc.
Relevance feedback (RF) and web search document clustering are techniques to improve the performance of an Information Retrieval (IR) system.
Relevance feedback provides a method to get more relevant search result from an IR system using documents that are marked relevant by the user as a feedback to reformulate query.
This refined query is then used to retrieve the documents.
In document clustering approach, the search result is divided into thematic groups where documents of one group are similar to each other and dissimilar to the documents of other groups.
This paper presents a report on the effectiveness of relevance feedback technique as compared to document clustering in context of web information retrieval and why document clustering is the most preferred approach.

It has been shown that the use of topic models for Information retrieval provides an increase in precision when used in the appropriate form.
Latent Dirichlet Allocation (LDA) is a generative topic model that allows us to model documents using a Dirichlet prior.
Using this topic model, we are able to obtain a fitted Dirichlet parameter that provides the maximum likelihood for the document set.
In this article, we examine the sensitivity of LDA with respect to the Dirichlet parameter when used for Information retrieval.
We compare the topic model computation times, storage requirements and retrieval precision of fitted LDA to LDA with a uniform Dirichlet prior.
The results show there there is no significant benefit of using fitted LDA over the LDA with a constant Dirichlet parameter, hence showing that LDA is insensitive with respect to the Dirichlet parameter when used for Information retrieval.

Abstract— To expand ontology meanings, an effective ontology mapping approach is needed to map related or similar knowledge from heterogeneous sources together.
Especially, the mapping approach also can be applied to support image recognition in order to enhance its retrieval information.
In this paper, we propose the ontology mapping with back propagation method to learn image objects, and link to their personal information to increase the completeness of user inquiry.
In our approach, we adopt the multilayer feed-forward neural network couple with back propagation algorithm to learn the image patterns, store their similarity values, check and map with target images, and link to their target information from knowledge domains at the final process.
Our experiments clearly present the accuracy of overall testing which can reach to 81.3%.

Mobile agents need to communicate and interact with each other to perform a global task.
Coordination models deal with interactions between concurrent entities.
A context-aware objective coordination model (COCM) is proposed for mobile agent applications.
The context aware coordination model transfers interactions between agents from globally coupling interactions to locally uncoupling tuple space interactions.
Programmable tuple space is introduced to solve the problems of context related coordination because of mobility and data heterogeneity in mobile agent systems.
Furthermore a sample system on information retrieval in mobile agent applications is carried out to evaluate the performance of the proposed model.

In this paper we define a type of cohesive subgroups called <i>communities</i in hypergraphs, based on the edge connectivity of subhypergraphs.
We describe a simple algorithm for the construction of these sets and show, based on examples from image segmentation and information retrieval, that these groups may be useful for the analysis and accessibility of large graphs and hypergraphs.

The 11th edition of the annual Dutch-Belgian Information Retrieval workshop (DIR 2011) took place on February 4 in Amsterdam.
It was organized by the University of Amsterdam and the Centrum Wiskunde 38; Informatica.
The focus of this year&#226 s workshop was on interaction, with the goal of facilitating and increasing interaction, especially within the local research community, and between industry and academia.
The scientific program included demos, research papers, and compressed contributions.
The keynotes by Nick Belkin and Gabriella Kazai provided intriguing outlooks on the future of IR evaluation.

The conventional boolean retrieval system does not provide ranked retrieval output because it cannot compute similarity coefficients between queries and documents.
Extended boolean models such as fuzzy set, Wailer-Kraft, Paice, P-Norm and Infinite
-One have been proposed in the past to support ranking facility for the boolean retrievats ystem.
In this paper, we analyze the behaviourat aspects of the previous extended boolean models and address important mathematical properties to affect retrieval effectiveness.
We concentrate our description on evaluation formulas for AND and OR operations and query weights.
Our analyses show that P-Nolm is the most suitable for achieving high retrieval effectiveness.

Recently, in the fall of 1974, San Jose State University (SJSU) launched a new master's program in Computer and Information Science (CIS The main function of this paper is to describe the program&#8212;detailing its origins, content, administration, present status and potential future.
A secondary purpose is to present material about one of the program's options&#8212;Management Information Systems.(MIS
In order to accomplish the above goals, the paper is divided into five sections as follows: Background, Program Description, Current Results, MIS Option, and Summary.

In this study, we are concerned by a field which represents an intellectual, social, and economic practice, strongly linked to a semi-automatic knowledge organization.
lnformational Competitive lntelligence is characterized by two major distinctive features: transition from the classical activity of Information Retrieval to organised lnformation Filtering, then conversion of filtered information into Knowledge to help decision

In this paper we use information retrieval metrics to evaluate the effect of a document sanitization process, measuring information loss and risk of disclosure.
In order to sanitize the documents we have developed a semiautomatic anonymization process following the guidelines of Executive Order 13526 (2009) of the US Administration.
It embodies two main steps i) identifying and anonymizing specific person names and data, and (ii) concept generalization based on WordNet categories, in order to identify words categorized as classified.
Finally, we manually revise the text from a contextual point of view to eliminate complete sentences, paragraphs and sections, where necessary.
For empirical tests, we use a subset of the Wikileaks Cables, made up of documents relating to five key news items which were revealed by the cables.

INSPECTOR is a proprietary software system that is designed to be used in an information retrieval environment.
Specifically, it is oriented toward the on-line retrieval of microfilmed documents through the indexing of certain key terms relating to the document itself.
Items such as date, account number, name, customer name or number, purchase order number, etc. might be considered as key descriptive terms.
Thus by indexing these elements on a randomly accessible disk drive, the location of the filmed image of <underline>all</underline> original documents pertaining to a particular descriptive term may be quickly located by the computer and the location displayed to the operator.
Alternatively, if used in conjunction with the Eastman Kodak IC-5/PR-1 microfilm retrieval unit, the computer system will cause the film display unit to automatically advance to the correct frame(s keeping operator intervention to an absolute minimum.

Key terms related to information searching and search models are defined.
A historic context is provided to illustrate the evolution of the four main digital environments that users interact with in their search process to offer readers background information regarding the transition from manual information systems to computer-based information retrieval (IR) systems, as well as the transition from intermediary searching to end-user searching.
Emphasis is placed on the review of different levels of information searching from search tactics/moves, search strategies, and usage patterns, to search models and associated factors in relation to task, user knowledge structure, IR system design, and social-organization context.
Search models are further classified into two types, with one type illustrating information search process (ISP) and the other type emphasizing the factors that influence the process.
In addition, unsolved problems and future research are discussed and suggested.

The effectiveness of queries in information retrieval can be improved through query expansion.
This technique automatically introduces additional query terms that are statistically likely to match documents on the intended topic.
However, query expansion techniques rely on fixed parameters.
Our investigation of the effect of varying these parameters shows that the strategy of using fixed values is questionable.

In multiple-source information systems, attribute values are often assessed in linguistic terms belonging to different vocabularies.
The request itself, which may include preferences, may be expressed using terms of another vocabulary, raising the problem of matching the query and the information in a semantic manner.
The fuzzy pattern matching framework allows us to compute matching degrees between queries and data represented by fuzzy sets, even if they do not perfectly match.
The qualitative pattern matching no longer requires a fuzzy set representation thanks to the use of ontologies for computing similarity degrees between terms.
This allows us to deal with information querying in face of heterogeneous sources of information.
This chapter presents this tool and its application to database and textual information retrieval on two examples.

Post-war telecommunications network is completely damaged by aerial bombings as mostly towers and elevated structures are bombarded.
This brings hindrances to the postwar information retrieval as it becomes extremely difficult to communicate using the previously established communication system.
The aim of this paper is to fabricate a MANET based emergency communication system containing Damage Assessment Subsystem, Data Processing Subsystem and a Data Localization Subsystem.
Apart from theoretically formulating the Post War Information Retrieval System (PWIRS this paper focuses on collaborative information gathering and independent processing of data through the MANET based networking infrastructure.

We propose to use MapReduce to quickly test new retrieval approaches on a cluster of machines by sequentially scanning all documents.
We present a small case study in which we use a cluster of 15 low cost machines to search a web crawl of 0.5 billion pages showing that sequential scanning is a viable approach to running large-scale information retrieval experiments with little effort.
The code is available to other researchers at: http mirex.sourceforge.net

A citation analysis of core library and information science journals was conducted to identify factors associated with subjective rankings of a journal's value in promotion and tenure decisions.
Prestige rankings from a 1982 survey of ARL directors and library school deans were correlated with nine citation measures: total citation count, impact factor, immediacy index, references per paper, Price's Index, self-citation rate, popularity factor, citation factor, and consumption factor, with and without controlling for journal orientation, age, circulation, and index coverage.
Results indicate that deans and directors may differ in their weighting of scholarliness and timeliness when rating journal value, especially when the practitioner-research orientation of the journal is considered.

The notion that information seeking is not always a solitary activity, and that people working in collaboration for information intensive tasks should be studied and supported, has become more prevalent in the recent years than ever before.
Several new research questions, methodologies, and systems have emerged around this notion that may even prove to be useful beyond the field of collaborative information seeking (CIS with relevance to the broader area of information seeking and behavior.
This position paper attempts to identify challenges and opportunities for both seasoned and novice CIS researcher/practitioner.
While the context for this paper is set around CIS, it is my hope that just as this narrative has benefited from fields of Information Retrieval (IR Computer-Supported Cooperative Work (
CSCW and Human-Computer Interaction (HCI the lessons presented here will in turn be helpful to researchers working in those domains.

In this paper we analyze queries and sessions intended to satisfy children's information needs using a large-scale query log.
The aim of this analysis is twofold: i)
To identify differences between such queries and sessions, and general queries and sessions; ii) To enhance the query log by including annotations of queries, sessions, and actions for future research on information retrieval for children.
We found statistically significant differences between the set of general purpose and queries seeking for content intended for children.
We show that our findings are consistent with previous studies on the physical behavior of children using Web search engines.

When an image database is queried with a particular example image show me similar images the corresponding feature vector is computed and the most similar feature vectors from the database are searched to display the most similar images in the database.
This paper presents the design and implementation of a high-dimensional index application to facilitate the speedy searching in feature based image information retrieval, and the improvement for the k nearest neighbor query algorithm based on X-tree which is designed for high-dimensional indexing.
Finally the performance evaluations are presented to show the merit of the algorithm.

In this paper, we explore several statistical methods to find solutions to the problem of query translation ambiguity.
Indeed, we propose and compare a new possibilistic approach for query translation derived from a probabilistic one, by applying a classical probability-possibility transformation of probability distributions, which introduces a certain tolerance in the selection of word translations.
Finally, the best words are selected based on a similarity measure.
The experiments are performed on CLEF-2003 French-English CLIR collection, which allowed us to test the effectiveness of the possibilistic approach.

The statistical machine translation (SMT) component of cross-lingual information retrieval (CLIR) systems is often regarded as black box that is optimized for translation quality independent from the retrieval task.
In recent work [10 SMT has been tuned for retrieval by training a reranker on $k$-best translations ordered according to their retrieval performance.
In this paper we propose a decomposable proxy for retrieval quality that obviates the need for costly intermediate retrieval.
Furthermore, we explore the full search space of the SMT decoder by directly optimizing decoder parameters under a retrieval-based objective.
Experimental results for patent retrieval show our approach to be a promising alternative to the standard pipeline approach.

In this paper, we describe a new way of representing a symbolic picture by a two-dimensional string.
A picture query can also be specified as a 2-D string.
The problem of pictorial information retrieval then becomes a problem of 2-D subsequence matching.
We present algorithms for encoding a symbolic picture into its 2-D string representation, reconstructing a picture from its 2-D string representation, and matching a 2-D string with another 2-D string.
We also prove the necessary and sufficient conditions to characterize ambiguous pictures for reduced 2-D strings as well as normal 2-D strings.
This approach thus allows an efficient and natural way to construct iconic indexes for pictures.

Emails, SMS, Chat transcripts and Social Network Messages play a vital role in present day‟s communication.
These digitally communicated data sets are different from well typed English documents like books, news papers Etc in 2 ways.
One is the use of acronyms instead of typing full form of the term and another one is the existence of misspelled words.
Because of these reasons, information retrieval methods used for analyzing well typed English documents may not be well suitable to analyze them.
In this paper we propose an information retrieval method to address the high recall requirements of retrieving information from digitally communicated data sets.

Research in Information Retrieval has traditionally focused on serving the best results for a single query.
Real users, however, often begin an interaction with a search engine with a sufficiently under-specified information need that they will need to reformulate before they find either the one thing or every thing they are looking for.
We define a session as the sequence of queries and interactions that a user performs in service of an information need.
The first workshop on Information Retrieval
Over Query Sessions was held at ECIR 2011 in Dublin, Ireland, with the purpose of investigating questions of measuring, analyzing, and optimizing IR system behavior over a session of reformulations.

Three measures of effectiveness of an information retrieval system are formulated in terms of a user's estimate of the relevance of items output.
In each instance the type of question logic allowed is postulated without specification of certain parameters which denote the weights attached to the question terms.
The parameters are then determined to maximise the effectiveness.
Their values depend on certain statistics of the data base.
The search effectiveness is then optimum for the permitted form of question, the measure of output relevance, and for data bases of similar statistics.
The techniques used are analogous to those used to define a matched filter and a Wiener root-meansquare filter.

Shared evaluation tasks have become popular over the last decades as ways of making communities of researchers advance together.
This paper presents the organization of five new shared task evaluation campaigns for image indexing and retrieval.
We have designed these campaigns based on our previous experience of participating in or organizing various text retrieval campaigns such as TREC, AMARYLLIS and CLEF.
Our purpose behind these campaigns is to minimize the gap between technology evaluation and user-oriented evaluation in the field of information retrieval.

We study information retrieval (IR) on Turkish texts using a large-scale test collection that contains 408,305 documents and 72 ad hoc queries.
We examine the effects of several stemming options and query-document matching functions on retrieval performance.
We show that a simple word truncation approach, a word truncation approach that uses language dependent corpus statistics, and an elaborate lemmatizer-based stemmer provide similar retrieval effectiveness in Turkish IR.
We investigate the effects of a range of search conditions on the retrieval performance; these include the scalability issues, query and document length effects, and the use of stopword list in indexing.

One of the core components in information retrieval(IR) is the document-term-weighting scheme.
In this paper,we will propose a novel learning-based term-weighting approach to improve the retrieval performance of vector space model in homogeneous collections.
We first introduce a simple learning system to weighting the index terms of documents.
Then, we deduce a formal computational approach according to some theories of matrix computation and statistical inference.
Our experiments on 8 collections will show that our approach outperforms classic tfidf weighting, about 20%∼45%.

We observe through simulations that an information sharing scheme using variable identifiers in mobile ad-hoc networks, which we previously proposed, enables us to heuristically find desired information by having identifiers varied.
In the information sharing, people create identifiers that represent their interest, and information published by the people are probabilistically propagated using distances between identifiers of the people.
The identifiers of the people dynamically change depending on people surrounding each person.
Under a situation that an identifier is variable, we set how many times people obtained desired information to be an observation item.
Also, we consider a process that people heuristically find desired information into a simulation model and set how many times people obtain such information to be an observation item.
In simulations, we show examples of observation for these items.

The Multimedia Information Retrieval (MIR) in the P2P networks has been widely studied.
In this paper, we propose a new comprehensive similarity function to calculate the similarity of peers in the P2P networks so as to classify these peers.
We also apply the relevance feedback in the process of retrieval in order to improve the speed and accuracy of retrieval.
In simulation, we compare our algorithm to the traditional method on the basis of the performance of the test which includes four types of thousands of files (text, image, video, and audio
The results show that our algorithm performs better on both speed and accuracy.

Query by Singing/Humming (QBSH) is a most natural way for music recognition.
A QBSH system can help find songs by matching a part of melody users sing or hum.
Many Music Information Retrieval (MIR) techniques have been used to carry out the QBSH goal.
In the past, little studies had ever been revealed about how to design and implement a simple QBSH system.
In this article, we combine Short-Time-Fourier-Transform, Fourier Transform and Spectrum Analysis techniques at the client end, and Dynamic Time Warping (DTW) and dynamic programming techniques at the server end to implement Music Information Retrieval And Gathering Engine (
MIRAGE which can take input queries issued by users from web-based client-ends toward MIRAGE deployed at the server-end.
MIRAGE can identify melody when a user only knows the rhythm of a song without any other information.

The CiteSeer digital library stores and indexes research articles in Computer Science and related fields.
Although its main purpose is to make it easier for researchers to search for scientific information, CiteSeer has been proven as a powerful resource in many data mining, machine learning and information retrieval applications that use rich metadata, e.g titles, abstracts, authors, venues, references lists, etc.
The metadata extraction in CiteSeer is done using automated techniques.
Although fairly accurate, these techniques still result in noisy metadata.
Since the performance of models trained on these data highly depends on the quality of the data, we propose an approach to CiteSeer metadata cleaning that incorporates information from an external data source.
The result is a subset of CiteSeer, which is substantially cleaner than the entire set.
Our goal is to make the new dataset available to the research community to facilitate future work in Information Retrieval.

In the process of controlling crimes, public security often faces a lot of information which is incomplete, fuzzy and random information.
How to extract useful and potential information and knowledge from above information has become a difficult problem for public security.
The data explosion but the data mining technology rapid development and application of lack of knowledge of the status quo, data mining technology is introduced into public service, in order to improve the crime prevention and control, information science and the construction of public security has become the direction of further development, and a data mining method based on sample data is introduced in this paper.

As there is availability of large amount of data on the web, but due to constraints web is only used for browsing and searching.
Traditional IE uses NLP techniques such as lexicons, grammars, whereas web applies machine learning and pattern mining techniques to exploit the syntactical patterns or layout structures of the template-based documents.
Information Retrieval is the art of presentation, storage, organization of and access to information items.
IR now–-days mainly deals with retrieving information based on user queries.
The paper deals with basic understanding of IR and IR models and shows Support Vector Machines is a good technique fir classification of huge data sets.
General Terms Information Retrieval (IR)

We describe a neural information retrieval system (NIRS now in production within the Boeing Company, which has been developed for the identification and retrieval of engineering designs.
Two-dimensional and three-dimensional representations of engineering designs are input to adaptive resonance theory (ART-1) neural networks to produce clusters of similar parts.
The trained networks are then used to recall an appropriate cluster when queried with a new part design.
This application is of great practical value to industry because it aids in the identification, retrieval, and reuse of engineering designs, potentially saving large amounts of nonrecurring costs.
In this paper, we review the application, the neural architectures and algorithms, and then give the current status and the lessons learned in developing a neural network system for production use in industry.

This paper reports the benefits of Probabilistic language modeling in template messaging domain.
Through a Statistical Machine Translation (SMT) sentences written with short forms, misspelled words and chatting slang can be corrected.
Given a source-language (e.g Short message) sentence, the problem of machine translation is to automatically produce a target-language (e.g Long form English) translation, to be used by the young generation for messaging.
The main goal behind this project is to analyze the improvement in efficiency as the size of bilingual corpus increases.
Machine learning and translation systems, dictionary and textbook preparations, patent and reference searches, and various information retrieval systems are the main applications of the project.

Implementing keyword search and other IR tasks on top of relational engines has become viable in practice, especially thanks to high-performance column-store technology.
Supporting complex combinations of structured and unstructured search in real-world heterogeneous data spaces however requires more than “just” IR-on-DB.
In this work, we walk the reader through our industrial-strength solution to this challenge and its application to a real-world scenario.
By treating structured and unstructured search as first-class citizens of the same computational platform, much of the integration effort is pushed from the application level down to the data-management level.
Combined with a visual design environment, this allows to model complex search engines without a need for programming.

The Technology survey task deals with the retrieval of information that can best answer a scientific question.
This task is more challenging in biomedical and chemistry domains due to diverse conventions applied for naming the entities.
In order to address this challenge, the work reported here presents an ad-hoc retrieval task that has been evaluated during the TRECCHEM-2011 for its ability to support retrieval from the biomedical and chemistry literature.
The core of the framework contains nearly 1.3 million patents and full-text articles that were indexed with pre-selected biomedical concepts.
Altogether, four runs were submitted based on different query formulation strategies and they exhibited competitive results.

The 2nd International Conference on the Philosophy of Information (ICPI 2015) took place in Vienna June 5–6, 2015 as a major Section of the Vienna 2015 Summit Conference on the Response and Responsibility of the Information Sciences.
At the ICPI, Wu Kun and others presented evidence for a current integration and convergence of the philosophy and science of information, under the influence of the unique characteristics of information itself.
As I have shown, my extension of logic to real systems (Logic in Reality; LIR) applies to and explicates many of the properties of information processes.
In this paper, I apply LIR as a framework for understanding the operation of information in three areas of science and philosophy that were discussed at the Summit.
The utility of this approach in support of an information commons is suggested the abstract section.

In this paper we present
YaSemIR, a free open-source Semantic Information Retrieval system based on Lucene.
It takes one or more ontologies in OWL format and a terminology associated to each ontology in SKOS format to index semantically a text collection.
The terminology is used to annotate concepts in documents, while the ontology is used to exploit the taxonomic information in order to expand these with their subsumers.
YaSemIR is a flexible system that may be configured to work with different ontologies, on various types of documents.

Few-shot learning refers to understanding new concepts from only a few examples.
We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime.
We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously.
In particular, we view each batch point as a ‘query’ that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings.
Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.

In this paper we describe the design and implementation of MuST, a multilingual information retrieval, summarization, and translation system.
MuST integrates machine translation and other text processing services to enable users to perform cross-language information retrieval using available search services such as commercial Internet search engines.
To handle non-standard languages, a new Internet indexing agent can be deployed, specialized local search services can be built, and shallow MT can be added to provide useful functionality.
A case study of augmenting MuST with Indonesian is included.
MuST adopts ubiquitous web browsers as its primary user interface, and provides tightly integrated automated shallow translation and user biased summarization to help users quickly judge the relevance of documents.

Which is more important: the journey or the destination?
Classical Interactive Information Retrieval (IIR) based on work-task scenarios usually puts the emphasis on the destination of the search (the results) with metrics such as precision and recall rather than the search journey.
But social media, mobile devices and other pervasive technologies have made information accessible to people in leisure scenarios and open up casual-leisure search behaviours motivated by hedonistic need such as having fun, or relaxing instead of a well-defined information need.
During search sessions users might find irrelevant information but they may keep exploring because the IR system satisfies their current leisure need.
This research aims to understand better casual-leisure search behaviour and design new IR systems to support autotelic search experiences.

The first international workshop on Exploitation of Social Media for Emergency Relief and Preparedness (SMERP) was held in conjunction with the 2017 European Conference on Information Retrieval (ECIR) in Aberdeen, Scotland, UK.
The aim of the workshop was to explore various technologies for extracting useful information from social media content in disaster situations.
The workshop included a peer-reviewed research paper track, a data challenge, two keynote talks, and discussion sessions on the relevant open research challenges.
This report presents an overview of the workshop, including the motivations behind organizing the workshop, and summaries of the research papers and keynote talks at the workshop.
We also reflect on the future directions as inferred from discussion sessions during the workshop

The paper argues that information science can best serve the needs of interdisciplinary scholarship (which is of increasing importance) by developing universal classifications of the phenomena studied by scholars and the theories and methods applied by scholars.
Present systems of document classification are grounded in disciplinary terminology and thus serve interdisciplinary scholarship poorly.
The second part of the paper outlines the importance of the recommended type of system of classification, the limitations of present systems, and the effects of these limitations on interdisciplinary scholarship.
The third part argues that such a system of classification is feasible, and that it is best developed through a combination of induction and deduction.

The paper reports on the formal framework to design strategies for multi-issue non-symmetric meaning negotiations among software agents in a distributed information retrieval system.
The advancements of the framework are the following.
A resulting strategy compares the contexts of two background domain theories not concept by concept, but the whole context (conceptual graph) to the other context by accounting the relationships among concepts, the properties and the constraints over properties.
It contains the mechanisms for measuring contextual similarity through assessing propositional substitutions and to provide argumentation through generating extra contexts.
It uses presuppositions for choosing the best similarity hypotheses and to make the mutual concession to common sense monotonic.
It provides the means to evaluate the possible eagerness to concede through semantic commitments and related notions of knowledgeability and degree of reputation

With an ever increasing number of databases that people access on a daily basis, user privacy becomes an increasing issue.
In order to ensure that not even the owners of the database can determine the records that a particular user is trying to access, plenty of research has been devoted to developing Private Information Retrieval schemes capable of masking a user’s queries.
Such schemes can be broken into two categories: the information-theoretic and the computationally-bounded approaches.
Both styles will be examined, along with a quick look at how they can be combined to improve performance.

The correspondence between natural deduction proofs and λ-terms is presented and discussed.
A variant of the reducibility method is presented, and a general theorem for establishing properties of typed (first-order) λ-terms is proved.
As a corollary, we obtain a simple proof of the Church-Rosser property, and of the strong normalization property, for the typed λ-calculus associated with the system of (intuitionistic) first-order natural deduction, including all the connectors and falsity with or without η-like rules Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-93-01.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/418 On the Correspondence Between Proofs and A-Terms MS-CIS-93-01
LOGIC COMPUTATION 54

The number of scientific publications is constantly increasing, and the results published on Empirical Software Engineering are growing even faster.
Some software engineering publishers have began to collaborate with research groups to make available repositories of software engineering empirical data.
However, these initiatives are limited due to issues related to the available search tools.
As a result, many researchers in the area have adopted a semi-automated approach for performing searches for systematic reviews as a mean to extract empirical evidence from published material.
This makes this activity labor intensive and error prone.
In this paper, we argue that the use of techniques from information retrieval, as well as text mining, can support systematic reviews and improve the creation of repositories of SE empirical evidence.

In this paper, we present Concept Chain Queries (CCQ a special case of text mining in document collections focusing on detecting links between two topics across text documents.
We interpret such a query as finding the most meaningful evidence trails across documents that connect these two topics.
We propose to use link-analysis techniques over the extracted features provided by Information Extraction Engine for finding new knowledge.
A graphical text representation and mining model is proposed which combines information retrieval, association mining and link analysis techniques.
We present experiments on different datasets that demonstrate the effectiveness of our algorithm.
Specifically, the algorithm generates ranked concept chains and evidence trails where the key terms representing significant relationships between topics are ranked high.

The amount of information on the Web is growing at an exponential rate.
Information overload has brought a heavy burden for modern life.
Keyword based search engines no long fill the needs of many people.
This paper introduces an approach towards intelligent information retrieval by providing clustered Web pages and minded concepts based on results of search engines.
Web page clustering is based on SVD (singular value decomposition concepts mining is implemented with a revision of Apriori algorithm.
Experiments on three different kinds of keyword as queries to information retrieval have showed a promising result

A scalable, parallel, relational-database driven information retrieval engine is described.
To support portability across a wide-range of execution environments, including parallel multicomputers, all algorithms strictly adhere to the SQL-92 standard.
By incorporating relevance feedback algorithms, accuracy was significantly enhanced over prior database-driven information retrieval efforts.
Algorithmic modifications to our earlier prototype resulted in significantly enhanced scalability.
Currently our information retrieval engine sustains near-linear speedups using a 24-node parallel database machine.
Experiments using the TREC data collections are presented to validate the described approaches.

Clothes products contain many characteristics which are difficult to describe in keywords, such as texture, shape, or the relationship between object and space.
Based on this issue, we develop an image-based visual clothing retrieval system, which extracts and uses the features of clothing images to find objects that are difficult to describe by text, or without text annotations.
We integrate techniques from image processing and information retrieval, and develop our retrieval system which provides region-of-interest visual search.
Experiment on an image database with about 1891 clothing images shows that our approach is effective and efficient.

Modelling the document scores returned from an IR system for a given query using parameterised score distributions is an area of research that has become more popular in recent years.
Score distribution (SD) models are useful for a number of IR tasks.
These include data fusion, query performance prediction, determining thresholds in filtering applications, and tasks in the area of distributed retrieval.
The inference of performance metrics, such as average precision, from these SD models is an important consideration.
In this paper, we study the accuracy of a number of methods of inferring average precision from an SD model.

This paper introduces a novel vision for further enhanced Internet of Things services.
Based on a variety of data x2014; such as location data, ontology-backed search queries, in- and outdoor conditions x2014; the Prometheus framework is intended to support users with helpful recommendations and information preceding a search for context-aware data.
Adapted from artificial intelligence concepts, Prometheus proposes user-readjusted answers on umpteen conditions.
A number of potential Prometheus framework applications are illustrated.
Added value and possible future studies are discussed in the conclusion.

The paper reports on the evaluation of a system that allows users to search information using spoken queries.
The front end is a large vocabulary continuous speech recognizer which translates the query from speech to text and puts it through an information retrieval engine to retrieve the set of relevant documents for that query.
The system is designed for Spanish language.
The performance of the system was evaluated using the test suites of CLEF, which is an evaluation forum similar to TREC.
10 different speakers were recorded reading the queries.
Results of different experiments are reported.
Best results were obtained with a language model of 60,000 words and medium length queries: loss of precision of 23.79 compared to using perfect transcription of the queries) with a WER of 18.4 Overall, these results are encouraging and provide a solid basis for the feasibility of building speech driven information retrieval systems.

The 1st Workshop on Social Media for Personalization
And Search was held on April 9, 2017 in conjuction with the 39th European Conference on Information Retrieval (ECIR 2017
The scientific program included paper presentations, posters, and a final discussion.
The keynote was delivered by Dr. Fabrizio Silvestri, who provided an overview of how recent advances in the use of embeddings can be effectively employed in advertisement and recommendation.

IRiSS (Information Retrieval based Software Search) is a software exploration tool that uses an indexing engine based on an information retrieval method.
IRiSS is implemented as an add-in to the Visual Studio .NET
development environment and it allows the user to search a C project for the implementation of concepts formulated as natural language queries.
The results of the query are presented as ranked list of software methods or classes, ordered by the similarity to the user query.
A second component of IRiSS provides another searching method based on regular expression matching.
This method is based on the existing “find” feature form the Visual Studio environment and it has an improved format for the display of the search results.

We present an experiment in which an information retrieval system using a forest of decision trees was trained using Utgoff's ITI algorithm on two test collections.
The system was then compared with a conventional inverted indexing engine and found to give a superior performance.
We argue that the method has the potential to be used in real applications where the task domain is homogenous.

The main challenge in the area of Information Retrieval (IR) and Natural Language Processing (NLP) is the characteristics of synonymy and polysemy that exist in words of natural language.
The capability of natural language interfaces to the semantic search engine can be improved by both the knowledge extraction and semantic data.
The combining of information can make the integration and sharing of distributed data sources easily.
This will assist the user to have the required information efficiently and easily.
In this paper, some concerns of evolving algorithm to capture the semantic similarity among sentences based on WordNet semantic dictionaryis presented.
The proposed algorithm will be relying on a number of resources including Ontology and WordNet.

Not only since the advent of XML, many applications call for efficient structured document retrieval, challenging both Information Retrieval (IR) and database (DB) research.
Most approaches combining indexing techniques from both fields still separate path and content matching, merging the hits in an expensive join.
This paper shows that retrieval is significantly accelerated by processing text and structure simultaneously.
The Content-Aware DataGuide (CADG) interleaves IR and DB indexing techniques to minimize path matching and suppress joins at query time, also saving needless
I/O operations during retrieval.
Extensive experiments prove the CADG to outperform the DataGuide [11,14] by a factor 5 to 200 on average.
For structurally unselective queries, it is over 400 times faster than the DataGuide.
The best results were achieved on large collections of heterogeneously structured textual documents.

This paper reports a novel semantic web application developed to deliver a collaborative tagging system for a digital on-line museum.
The key features of our application called the Virtual Museum of the Pacific concern the browsing and retrieval interface based on Formal Concept Analysis the extensible distributed data model to support collaborative tagging and its web services implementation.

Thare are various systems using Boolean, vector and other models for representation of documents.
Each of these models has limitations that do not allow the user to find all relevant documents.
This paper describes the approach to retrieving of topic development based on aglomerative clustering.
Standard methods of IR does not allow us such kind of queries for appropriate solution of information problem.
The goal of presented method is to find list of documents that are bearing on topic, represented by user-selected document, sorted with respect to historical development of the topic.

The Ant World system facilitates collaborative information retrieval on the Internet, i.e. to make it easier to find useful information on the Internet.
It asks users’ opinion on whether the documents they view are useful for their goals, and uses this feedback to guide other users in their searches.
Our approach [Z] is conceptually motivated by two important concepts from the domain of biology: neuronal networks and communication via pheromones.
At the global level we enable the network to update its search information dynamically like a vast neuronal network, while at the local level the acquisition and deposition of link information is akin to pheromonic marking of sites and trails by insects.
With this analogy in mind we will refer to the information about the value of links as Digital Information Pheromones (DIP The resulting network is continuously adaptive.

Users of information retrieval systems (IRS) know and use many relationships between concepts a long time before these find their way into textbooks, printed thesauri, or classification schemes.
We present here an IRS component called TEGEN, which taps this expertise by automatically drawing conclusions from actual search behavior about possible thesaurus entries.
This is done during an iterative knowledge acquisition process: only after explicit or implicit confirmation by other users of the IRS during the knowledge verification process, the results are incorporated into a thesaurus.
TEGEN is written in PASCAL using a knowledge-based programming method.
It uses the relational database system IMF2 and is implemented at the Technical University of Munich and at the Leibniz Computer Center of the Bavarian Academy of Sciences.

Temporal Information Retrieval is an emerging research area in the field of Information Retrieval.
Due to the immense amount of data in the WWW, and because the contents of documents are strongly time-dependent, it is very tough for the user to retrieve the relevant documents.
Traditional Information Retrieval approaches based on topic similarity alone is not sufficient for the search in temporal document collections.
The time dimension available in the documents should be incorporated with document ranking for efficient retrieval.
This survey gives an introduction to Temporal Information Retrieval and explores the different time-aware retrieval methods and temporal ranking methods for specific types of time-sensitive queries.

The interest for information retrieval has existed long before the Internet.
Boolean retrieval is the most simple of these retrieval methods and relies on the use of Boolean operators.
The terms in a query are linked together with AND, OR and NOT.
This method is often used in search engines on the Internet because it is fast and can therefore be used online.
This method has also its problems.
The user has to have some knowledge to the search topic for the search to be efficient, e.g a wrong word in a query could rank a relevant document non relevant.
The retrieved documents are all equally ranked with respect to relevance and the number of retrieved documents can only be changed by reformulating the query.
We consider p-norm approach, Max score and wand exact optimization techniques for ranked keyword retrieval that can be adopted via low cost screening process.

Recommendation Systems apply Information Retrieval techniques to select the online information relevant to a given user.
Collaborative Filtering (CF) is currently most widely used approach to build Recommendation System.
CF techniques uses the user’ behavior in form of user-item ratings as their information source for prediction.
There are major challenges like sparsity of rating matrix and growing nature of data which is faced by CF algorithms.
These challenges are been well taken care by Matrix Factorization (MF
In this paper we attempt to present an overview on the role of different MF model to address the challenges of CF algorithms, which can be served as a roadmap for research in this area.

Existing work on indexing and retrieving documents from large on-line collections has had great success at treating both documents and queries as simple, unstructured collections of individual words (terms) with dependencies among these terms largely ignored.
However, natural language text has a great deal of structure.
In particular, at a scale close to that of the individual word, there are interactions and dependencies that many IR systems ignore.
Those systems that do attempt to capture some of these dependencies do so in rather indirect ways.

This paper proposes a semantic search system with the NLS (Natural Language Semantics) query parser and metadata tagging for unstructured data, which establishes high quality search, that is, information retrieval.
Thus, the proposed system collects data from unstructured data sources and adds tags to the data as metadata.
And, data with metadata becomes structured data with semantic information, which makes the data analysis much easier.
And, the system parses the NLS input by the user to retrieve information from the search engine.
Then, information result with search engine follows the common format that is easy to read and analyze.
Besides, it will be clearly illustrated how much risk of the systems has been mitigated relative to the production of a full prototype systems.

In this report, we summarize the outcome of the "Evaluation-as-a-Service" workshop that was held on the 5th and 6th March 2015 in Sierre, Switzerland.
The objective of the meeting was to bring together initiatives that use cloud infrastructures, virtual machines, APIs (Application Programming Interface) and related projects that provide evaluation of information retrieval or machine learning tools as a service.

Computational prediction of transcription factor's binding sites and regulatory target genes has great value to the biological studies of cellular process.
Existing practices either look into first-hand gene expression data which could be costly for large scale analysis, or apply statistical or heuristic learning methods to discover potential binding sites which have limited accuracy due to the complexity of the data.
Based on well-studied information retrieval theories, this paper proposes a novel systematic approach for transcription factor target gene prediction.
The key of the approach is to model the prediction problem as a classification task by representing the features of the sequential data into vector data points in a higher-order domain.
The proposed approach has produced satisfactory results in our controlled experiment on Auxin Response Factor (ARF) target gene prediction in Arabidopsis

Support Vector Machines (SVMs) have been one of the major breakthroughs in machine learning, both in terms of their practical success as well as their learning-theoretic properties.
This talk presents a generic extension of SVM classification to the case of structured classification, i.e. the task of predicting output variables with some meaningful internal structure.
As we will show, this approach has many interesting applications in information extraction, information retrieval, document categorization and natural language processing, including supervised training of Markov Random Fields and probabilistic context-free grammars.

Recent advances in computer technology have made it now possible to create and display three-dimensional virtual environments for real-time exploration and interaction by a user.
This paper surveys some of the research done in this field at such places as: NASA's Ames Research Center, MIT's Media Laboratory, The University of North Carolina at Chapel Hill, and the University of New Brunswick.
Limitations to the "reality" of these simulations will be examined, focusing on input and output devices, computational complexity, as well as tactile and visual feedback.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report
No. MSCIS-92-10.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/406
The Reality of Virtual Environments WPE I1 Paper MS-CIS-92-10 GRAPHICS LAB 49

Many traditional information retrieval (IR) tasks, such as text search, text clustering or text categorization, have natural language documents as their first-class objects, in the sense that the algorithms that are meant to solve these tasks require explicit internal representations of the documents they need to deal with.
In IR documents are usually given as extensional vectorial representation, in which the dimensions (features) of the vector representing a document are the terms occurring in the document.
The approach to term representation that the IR community has almost universally adopted is known as the bag-of-words approach: a document dj is represented as a vector of term weights dj ω1j ωrj where r is the cardinality of the dictionary and 0 ωkj 1 represents the contribution of term tk to the specification of the semantics of dj This article analyses and compares many different bag-of-words approaches.

Department of Software Engineering, School of Information Science and Engineering, Yanshan University, Qinhuangdao, China, 2 The Key Laboratory for Computer Virtual Technology and System Integration of Hebei Province, Yanshan University, Qinhuangdao, China, Department of Neurology, First Hospital of Qinhuangdao, Qinhuangdao, China, Department of Computer Science and Technology, School of Mathematics and Information Science and Technology, Hebei Normal University of Science and Technology, Qinhuangdao, China, 5 Swartz Center for Computational Neuroscience, University of California, San Diego, San Diego, CA, United States

Background.
Several researchers have proposed creating after-the-fact structure among software artifacts using trace recovery based on Information Retrieval (
IR Due to significant variation points in previous studies, results are not easily aggregated.
We aim at an overview picture of the outcome of previous evaluations.
Based on a systematic mapping study, we perform a synthesis of published research.
Our synthesis shows that there are no empirical evidence that any IR model outperforms another model consistently.
We also display a strong dependency between the Precision and Recall (P-R) values and the input datasets.
Finally, our mapping of P-R values on the possible output space highlights the difficulty of recovering accurate trace links using nai&#x0308;ve cut-off strategies.
Conclusion.
Based on our findings, we stress the need for empirical evaluations beyond the basic P-R 'race'.

A method to identify ontology components is presented in this article.
The method relies on Natural Language Processing (NLP) techniques to extract concepts and relations among these concepts.
This method is applied in the legal field to build an ontology dedicated to information retrieval.
Legal texts on which the method is performed are carefully chosen as describing and conceptualizing the legal domain.
We suggest that this method can help legal ontology designers and may be used while building ontologies dedicated to other tasks than information retrieval.

Within a Music Information Retrieval perspective, the goal of the study presented here is to investigate the impact on sound features of the musician’s affective intention, namely when trying to intentionally convey emotional contents via expressiveness.
A preliminary experiment has been performed involving 10 tuba players.
The recordings have been analysed by extracting a variety of features, which have been subsequently evaluated by combining both classic and machine learning statistical techniques.
Results are reported and discussed.

Word Sense Disambiguation (WSD) has become a popular method for solving the ambiguous meaning of the words in Information Retrieval (IR) field area.
Under the Natural Language Processing (NLP) community, WSD has been described as the task which able to select the appropriate meaning among the ambiguous meanings to a given word.
Among three approaches, supervised based, unsupervised based and knowledge based approaches to WSD, this paper focuses on both supervised based and knowledge based approaches by proposing new Jaccard coefficient-based WSD algorithm to overcome the vocabulary miss match problem.
WordNet and corpus external knowledge resources are utilized as the sense repositories by linking up with the new WSD algorithm to consider additional semantic for WSD.
According to sample testing, IR system with new WSD algorithm attains more about 20 percent of total accuracy rate than traditional IR system.

Topic models are of broad interest.
They can be used for query expansion and result structuring in information retrieval and as an important component in services such as recommender systems and user adaptive advertising.
In large scale applications both the size of the database (number of documents) and the size of the vocabulary can be significant challenges.
Here we discuss two mechanisms that can make scalable solutions possible in the face of large document databases and large vocabularies.
The first issue is addressed by a parallel distributed implementation, while the vocabulary problem is reduced by use of large and carefully curated term set.
We demonstrate the performance of the proposed system and in the process break a previously claimed x2018;world record&#x2019; announced April 2010 both by speed and size of problem.
We show that the use of a WordNet derived vocabulary can identify topics at par with a much larger case specific vocabulary.

The ability to find relevant materials in large document collections is a fundamental component of legal research.
The emergence of large machine-readable collections of legal materials has stimulated research aimed at improving the quality of the tools used to access these collections.
Important research has been conducted within the traditional information retrieval, the artificial intelligence, and the legal communities with varying degrees of interaction between these groups.
This article provides an introduction to text retrieval and surveys the main research related to the retrieval of legal materials.

The identification of noun compound as multi-word lexical units is very important task in natural language processing applications that require some degree of semantic interpretation such as, machine translation, information retrieval and text summarization.
In this paper, we used the hybrid method for extracting the noun compound from Arabic corpus that is based on linguistic knowledge and statistical measures.
For the candidate identification, we have used some linguistic analysis tools such as lemmatization and POS in order to filter the candidates and determine the variations.
The association measures have been computed for each candidate to rank the candidates.
After that, we have evaluated the association measures by using the n-best evaluation method.
We reported the precision values for each association measure in each n-best list.
The experimental results showed that the log-likelihood ratio is the best association measure that achieved highest precision.

The design objectives and the mechanisms for achieving those objectives are considered for each of three systems, Java, Erlang, and TIL.
In particular, I examine the use of types and intermediate representations in the system implementation.
In addition, the systems are compared to examine how one system's mechanisms may (or may not) be applied to another.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-98-05.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/796 Types and Intermediate Representations

Music Information Retrieval (MIR) is an interdisciplinary research area that has grown out of the need to manage burgeoning collections of music in digital form.
Its diverse disciplinary communities, exemplified by the recently established ISMIR conference series, have yet to articulate a common research agenda or agree on methodological principles and metrics of success.
In order for MIR to succeed, researchers need to work with real user communities and develop research resources such as reference music collections so that the wide variety of techniques being developed in MIR can be meaningfully compared with one another.
Out of these efforts, a common MIR practice can emerge.

We explore the limits of single-server computational private information retrieval (PIR) for the purpose of preserving client access patterns leakage.
We show that deployment of non-trivial single server PIR protocols on real hardware of the recent past would have been orders of magnitude less time-efficient than trivially transferring the entire database.
We stress that these results are beyond existing knowledge of mere “impracticality” under unfavorable assumptions.
They rather reflect an inherent limitation with respect to modern hardware, likely the result of a communication-cost centric protocol design.
We argue that this is likely to hold on non-specialized traditional hardware in the foreseeable future.
We validate our reasoning in an experimental setup on modern off-the-shelf hardware.
Ultimately, we hope our results will stimulate practical designs.

The Linked Open Data provides useful information on huge number of related entities gathered from numerous distributed data sources residing in the cloud.
Searching for the required information in this big data is one of the key challenges, in the domain of Information Retrieval, where various studies have been conducted for multiple purposes.
These studies highlighted three main challenges: querying ambiguity, information incompleteness, and visualization techniques.
In this paper, we introduce a visualization approach which provides a more informative view and additional understanding of semantic data over DBpedia.
We show that our proposed solution, considered as efficient for users, overcomes the other existing ones by hiding complexity, and by offering users the information of interest.

Predicting gender by names is one of the most interesting problems in the domain of Information Retrieval and expert finding task.
In this research paper, we propose a machine learning approach for gender prediction task.
We propose a new feature, that is, combination of letters in names which gives 86.54% accuracy.
Our data collection consists of 3000 Urdu language names written using English Alphabets.
This technique can be used to extract names from email addresses and hence is also valid for emails.
To the best of our knowledge, it is the firstever attempt for predicting gender from Pakistani (Urdu) names written using English alphabets.
Urdu; Semantic Web; Gender Prediction; Expert Profiling; Machine Learning

The main obstacle in realizing semantic-based image retrieval is from the web that semantic description of an image is difficult to capture in low-level features.
Text based keywords can be generated from web documents to capture semantic information for narrowing down the search space.
We use an effective approach to integrate keywords, which is extracted from Web documents and low-level features such as color-texture features to take advantage of their complementing strengths.
Experimental results show that the keywords can be replaced with low-level features for successful query refinement and improved precision of retrieval.

for many-body systems and quantum computation Daniel A. Lidar, Ali T. Rezakhani, and Alioscia Hamma Department of Chemistry, University of Southern California, Los Angeles, California 90089, USA Department of Electrical Engineering, University of Southern California, Los Angeles, California 90089, USA Department of Physics, University of Southern California, Los Angeles, California 90089, USA Center for Quantum Information Science and Technology, University of Southern California, Los Angeles, California 90089, USA Perimeter Institute for Theoretical Physics, 31 Caroline St. N, Waterloo, Ontario N2L 2Y5, Canada Massachusetts Institute of Technology, Research Laboratory of Electronics, 77 Massachusetts Ave Cambridge, Massachusetts 02139, USA

Conjunctive Boolean queries are a fundamental operation in web search engines.
These queries can be reduced to the problem of intersecting ordered sets of integers, where each set represents the documents containing one of the query terms.
But there is tension between the desire to store the lists effectively, in a compressed form, and the desire to carry out intersection operations efficiently, using non-sequential processing modes.
In this paper we evaluate intersection algorithms on compressed sets, comparing them to the best non-sequential arraybased intersection algorithms.
By adding a simple, low-cost, auxiliary index, we show that compressed storage need not hinder efficient and high-speed intersec-

Semantic query optimisation uses semantic knowledge to transform a query into another form that can be executed in a more efficient manner but still yields the same result as the original query.
Commonly this semantic knowledge is in the form of rules which are generated either during the query process itself or are constructed according to defined heuristics.
Over a period of time the rule set may, therefore, become very large and the number of semantically equivalent queries which may be derived rises exponentially.
The problem is to identify a near optimal alternative query in a time which is minimal and also short relative to the overall query execution time.
In this paper we propose a method of measuring the effectiveness of each rule and present a fast algorithm which selects the most cost effective transformations to yield a near optimal alternative query.
Experiments carried out, on a large publicly available dataset, show worthwhile savings using the approach.

Cooperative search engine (CSE) is a distributed search engine, which can update index in very short time for the purpose of fresh information retrieval.
Although CSE's retrieval response time is longer than general centralized search engines, we develop some techniques to accelerate CSE's retrieval response time.
As a routine work, CSE finds new documents and new words, and updates the index.
In this daily update process, removed documents and words were not considered due to update processing speed.
So we develop a new daily update process, differential update.
Differential update considers added, modified and removed documents and words.
We describe the behavior and evaluation of the differential update.

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.

One critical aspect of an information system is indexing, that is, the representation of concepts to get a well formed data structure for search.
Until recently, this task was accomplished by creating a bibliographic citation that references the original text.
This approach allows a reduction in time and space bounds, although it does not facilitate the user from finding relevant information.
In effect, it is the combination of the words and their semantic implications that contain the value of these concepts, which leads us to more sophisticated index representations, such as context-free grammars.
This information is inherent in the document and query.
So, matching becomes a possible mechanism to extract a common pattern from multiple data and we could use it for indexing and querying in information retrieval systems.

The workshop on Medical Information Retrieval took place at SIGIR 2014 in Gold Coast, Australia on July 11.
The workshop included eight oral presentations of referred papers and an invited keynote presentation.
This allowed time for lively discussions among the participants.
These showed the significant interest in the medical information retrieval domain and the many research challenges arising in this space which need to be addressed to give added value to the wide variety of users that can profit from medical information search, such as patients, general health professionals and specialist groups such as radiologists who mainly search for images and image related information.

D. Jasmine Guna Sundari Assistant professor Department of Computer Science, Government Arts College for women, Ramanathapuram Email:
durkarthi@gmail.com D. Sundar Assistant professor Department of Computer Science, Government Arts College, Thiruvadanai Email:
sundarums@gmail.com
ABSTRACT Text mining is an exciting research area that tries to discover useful information can be derived from this unstructured data by using techniques from machine learning, natural language processing (NLP data mining, information retrieval (IR and knowledge management.
Text mining involves the pre-processing techniques to harvest data and initial understanding of the patterns that exist in the data.
The techniques such as Information Retrieval, Information Extraction, Categorization, Clustering and Summarization that are used to analyse these intermediate representations such as distribution analysis, association rules and visualisation of the results.

Recent research and applications for evaluation and quality estimation of Machine Translation require statistical measures for comparing machine-predicted ranking against gold sets annotated by humans.
Additional to the existing practice of measuring segment-level correlation with Kendall tau, we propose using ranking metrics from the research field of Information Retrieval such as Mean Reciprocal Rank, Normalized Discounted Cumulative Gain and Expected Reciprocal Rank.
These reward systems that predict correctly the highest ranked items than the one of lower ones.
We present an open source tool ”R E providing implementation of these metrics.
It can be either run independently as a script supporting common formats or can be imported to any Python application.

In this paper, we explore approaches to multi-lingual information retrieval for Greek, Latin, and Old Norse texts.
We also describe an information retrieval tool that allows users to formulate Greek, Latin, or Old Norse queries in English and display the results in an innovative clustering and visualization facility.

Recently, Top-k queries in distributed applications have attracted much interest in many different areas such as network and monitoring systems, information retrieval, sensor networks.
Some efficient algorithms, such as TA, BPA, are regarded as centralized process in distributed applications.
However, to some ad hoc ranged Top-K queries have less flexibility towards updating and low efficiency in ranged searching with much cost of network communication.
So we propose an efficient strategy for ranged Top-k query over web services and
our performance evaluation shows that such strategy has less network communication and queries timely updating ranged top k result efficiently.

The purpose of this study is to output information which users hope as high ranking results on narrow display of mobile information terminals.
The concept of the proposed algorithm is to understand the meanings and the contents expressed by keywords and to retrieve appropriately related information.
Concretely, we propose an information retrieval technique to evaluate the relationships between words using an association mechanism.
A new test collection was made by 100 examinees who judged the retrieval results according to text meanings.
Retrieval performance was objectively confirmed with the experiment based on the test collection.

Modeling data via artificial neural networks (ANN) is not a new concept.
Most of the underlying techniques have been known since the 1940s.
It has to be pointed out though that a series of recent advances in how the networks are trained and utilized form the foundation of today's Deep Learning ecosystem.
It is a fair statement that the recent advances brought forward by Deep Learning reflect a new era in Machine Learning (ML) that revolutionized many domains of signal and information processing.
This holds true beyond the commonly discussed speech and object recognition science but also branches into computer vision, natural language processing, or information retrieval related fields.

Diversified ranking is a fundamental task in machine learning.
It is broadly applicable in many real world problems, e.g information retrieval, team assembling, product search, etc.
In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples.
We formulate it as an optimization problem and show that in general it is NP-hard.
Then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the (1 1/e) near-optimal solution.
Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.

In this paper we show how to maintain a partially-ordered hierarchy of patterns by subpattern-of for efficient associative retrieval.
The techniques described here are most applicable for databases of complex data structures such as graphs or matrices as opposed to simple data structures such as relations, sets, lists or frames that are seen in most information retrieval systems.
Any representation scheme for the patterns in the database may be used as long as the user provides the system with the function that compares two patterns in the representation scheme and determines if one pattern is a subpattern of the other, if the patterns are identical or the patterns are incomparable.
The user also provides utilities for reading and writing the patterns.

Previous research has shown that features based on term proximity are important for effective retrieval.
However, they incur substantial costs in terms of larger inverted indexes and slower query execution times as compared to term-based features.
This paper explores whether term proximity features based on approximate term positions are as effective as those based on exact term positions.
We introduce the novel notion of approximate positional indexes based on dividing documents into coarse-grained buckets and recording term positions with respect to those buckets.
We propose different approaches to defining the buckets and compactly encoding bucket ids.
In the context of linear ranking functions, experimental results show that features based on approximate term positions are able to achieve effectiveness comparable to exact term positions, but with smaller indexes and faster query evaluation.

Context analysis is a crucial issue in dynamic meeting scenarios for online information service and offline semantic information retrieval.
Four high-level semantic contexts, that is monologue presentation discussion" and "break are considered and analyzed in dynamic meeting scenarios in this paper.
To characterize four semantic contexts by event fusion and inference, four kinds of group events are considered, such as participant's motion events, speaking events, appearance of person events, and using the projector screen event.
Hierarchical dynamic Bayesian networks, which model group events and situation context, are constructed to bridge the gap between physical audio features and semantic concepts.
Rao-Blackwellized particle filter is applied for online inference in our hierarchical dynamic Bayesian network.
The experimental results demonstrate the effectiveness of the proposed approaches.

Query is often treated as a bag of words by search engines, but when people are formulating queries, they use “concepts” as building blocks.
Can we automatically segment the query to recover the concepts?
How can we use the identified concepts to improve search relevance?
In this talk, I will present some techniques to some techniques for query concept identification and how can we use them to improve search relevance from query rewriting and machine learning

In this paper, we try to exploit the semantic richness of Arabic language for Information Retrieval
The semantics of Arabic words may be extracted from dictionaries or corpora, which are analyzed and minded using Natural Language Processing (NLP) and text mining tools.
This allows modeling the contextual dependencies between words, which help identify the meaning of queries in the search process.
Thus, the queries are enriched by semantic knowledge, which enhances search performance.
In this context, this paper describes a text mining-based approach for Arabic semantic IR, which considers senses of query terms.
Experiments and results based on a standard Arabic Test collection are discussed through this communication.
In the one hand, we compare dictionary versus corpus-based approaches for modeling semantics.
On the other hand, we compare some Arabic NLP tools in the preprocessing step.
Thus, we study the effect of Arabic morphology on the semantic interpretation of queries.

During a three-day workshop in February 2012, 45 Information Retrieval researchers met to discuss long-range challenges and opportunities within the field.
The result of the workshop is a diverse set of research directions, project ideas, and challenge areas.
This report describes the workshop format, provides summaries of broad themes that emerged, includes brief descriptions of all the ideas, and provides detailed discussion of six proposals that were voted "most interesting" by the participants.
Key themes include the need to: move beyond ranked lists of documents to support richer dialog and presentation, represent the context of search and searchers, provide richer support for information seeking, enable retrieval of a wide range of structured and unstructured content, and develop new evaluation methodologies.

Quantum technologies based on photons will likely require integrated optics architectures for improved performance, miniaturization and scalability.
We demonstrate high-fidelity silica-on-silicon integrated optical realizations of key quantum photonic circuits and the first integrated quantum algorithm.

Information retrieval capability of recurrent neural networks and performances of their formerly-proposed design procedures are questioned in this thesis work.
Five novel design methods for discrete Hopfield recurrent network model to restore prototype static vectors from their distorted versions along the operation on a finite state-space are then introduced.
Qualitative properties provided by these methods are verified analytically, while quantitative ones are estimated by conducting computer experiments.
A comparison of each proposed method with the conventional design procedures is presented in terms of these properties.
The performances of the resulting networks are finally demonstrated on benchmark static information retrieval applications, namely character recognition and image reconstruction.

Query difficulty prediction aims to identify, in advance, how well an information retrieval system will perform when faced with a particular search request.
The current standard evaluation methodology involves calculating a correlation coefficient, to indicate how strongly the predicted query difficulty is related with an actual system performance measure, usually Average Precision.
We run a series of experiments based on predictors that have been shown to perform well in the literature, comparing these across different TREC runs.
Our results demonstrate that the current evaluation methodology is severely limited.
Although it can be used to demonstrate the performance of a predictor for a single system, such performance is not consistent over a variety of retrieval systems.
We conclude that published results in the query difficulty area are generally not comparable, and recommend that prediction be evaluated against a spectrum of underlying search systems.

This work presents an analysis of the meaning of strategically oriented information science education and the possibilities of using modern information technology in gaining strategic knowledge.
The framework of methodology used in information science education is defined Variety of knowledge levels are analysed from the viewpoint of the transfer of strategic knowledge.
The contents of information science education are grouped according to the levels of complexity which are defined by the degree of the variety of knowledge.
The activities of strategically oriented information science education are defined and analysed from the standpoint of creating the guidelines for the development of informatization of education on the principles of creativity and innovation.

Pseudo-relevance feedback is a common technique in information retrieval.
Content Based Information Retrieval systems (CBIR like visual retrieval engines, suffer from low performance due to the distance between the semantic interpretation of an image and its visual features.
This is the reason why such a systems are empowered when textual information is associated to the images in the collection and the CBIR is combined with a textual retrieval system.
In this paper, some experiments on applying text-based pseudo-relevance feedback on CBIR are reported.
The results obtained show that these techniques may enhance the behavior of traditional fusion approaches.

In this paper, we present our work in TREC 2016 Clinical Decision Support Track.
Among five submitted runs, two of them are based on summary topics and the others on note topics.
In summary version run, we expand the original text with external data on web.
Note topics are much longer than the summary, which contain a significant number of medical abbreviations as well as other linguistic jargon and style.
An automatic method and a manual method are applied to process note topics.
In the automatic method, we utilize KODA, a well-known knowledge drive annotator, to extract key information from the original text.
In the manual one, we ask medical experts to diagnose and give their advice.
For all of the five runs, we adopt Terrier search engine to implement various retrieval models.
Furthermore, results combinations are applied to improve the performance of our model.

Information retrieval evaluation has typically been performed over several dozen queries, each judged to near-completeness.
There has been a great deal of recent work on evaluation over much smaller judgment sets: how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available.
In light of this, it should be possible to evaluate over many more queries without much more total judging effort.
The Million Query Track at TREC 2007 used two document selection algorithms to acquire relevance judgments for more than 1,800 queries.
We present results of the track, along with deeper analysis: investigating tradeoffs between the number of queries and number of judgments shows that, up to a point, evaluation over more queries with fewer judgments is more cost-effective and as reliable as fewer queries with more judgments.
Total assessor effort can be reduced by 95% with no appreciable increase in evaluation errors.

In recent years, quite a number of projects started to apply case-based reasoning technology to textual documents instead of highly structured cases.
For this the term Textual CBR has been coined.
In this paper, we give an overview over the main ideas of Textual CBR and compare it with Information Retrieval techniques.
We also present some preliminary results obtained from three projects performed which further demonstrate major advantages of Tex-tual CBR.

This paper describes our retrieval system for NTCIR-2 Japanese/English CLIR and MLIR tasks.
We integrate query and document translation with monolingual retrieval to improve retrieval accuracy, and perform clustering to improve browsing efficiency.
We also introduce an entropy-driven technique in evaluating clustering methods.

The HIA'15 workshop aims to bring together information retrieval practitioners from industry and academic researchers concerned with heterogeneous information access and search federation.
We would like to create a forum to encourage discussion and exchange of ideas on heterogeneous information access in different contexts.
To facilitate the discussion, we encourage submissions on ideas and results from different aspects of heterogeneous information access including aggregated search, composite retrieval, personal search, structured search, etc.
Another objective of the workshop is to encourage submissions with novel ideas (e.g. new applications) on heterogeneous information access and potential future directions of this area.

The digital libraries of the future will include not only (ASCII) text information but scanned paper documents as well as still photographs and videos.
There is, therefore, a need to index and retrieve information from such multi-media collections.
The Center for Intelligent Information Retrieval (CIIR) has a number of projects to index and retrieve multi-media information.
These include: 1.
The extraction of text from images which may be used both for finding text zones against general backgrounds as well as for indexing and retrieving image information.
Indexing hand-written and poorly printed documents using image matching techniques (word spotting 3.
Indexing images using their content.

Australian Business Deans Council (ABDC Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; Gale Directory of Publications Broadcast Media; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Thomson Reuters; Ulrich’s Periodicals Directory;
Web of Science Research Articles

Retrieving patent and other Intellectual Property (IP) documents is quite critical in various areas including science and technology, marketing, intellectual property right management, business and so on.
In the past, much of the focus for patent and IP retrieval has been from the database community, and not from the Information Retrieval (IR) community.
It is partly because the research and development in IR has tended to place emphasis on the generalized systems, which are effective for any kinds of documents and any kinds of queries.
It is partly because the text genre of the patents has highly specific characteristics both in semantics and syntax of the text and has highly specialized ways of use by professional users.
We assumed that this is the right time to organize a workshop since supporting technology for specialized retrieval is ready.

Learning to rank has become a popular research topic in several areas such as information retrieval and machine learning.
Pair-wise ranking, which learns all the order preferences between pairs of examples, is a typical method for solving the ranking problem.
In pair-wise ranking, Rank SVM is a widely-used algorithm and has been successfully applied to the ranking problem in the previous work.
However, Rank SVM suffers from the critical problem of long training time needed to deal with a huge number of pairs.
In this paper, we propose a data selection technique, Pruned Rank SVM, that selects the most informative pairs before training.
Experimental results show that the performance of Pruned Rank SVM is on par with Rank SVM while using significantly fewer pairs.

My research interests include all areas of information retrieval and human-computer interaction.
As my thesis work, I am focusing on challenges in providing an effective search system for personal information management (PIM Having finished my seventh semester in the MS/Ph.D program in UMass Computer Science department, I am starting to establish myself as an Information Retrieval researcher, making several contributions in the area of my research.
In the remainder of this statement I will describe my research experience and interested projects for internship in detail.

Collection Edited by: The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL)
Graduate School of Library and Information Science University of Illinois at Urbana-Champaign

Self-organizing Semantic Map for Information Retrieval Xia Lin Dagobert Soergel Gary Marchionini College of Library and Information Services University of Maryland College Park
, A neural network’s unsupervised learning algorithm, Kohonen’s feature map, is applied to constructing a selforganizing semantic map for information retrieval.
The semantic map visualizes semantic relationships between input documents, and has properties of economic representation of data with their interrelationships.
The potentials of the semantic map include using the map as a retrieval interface for an online bibliographic system.
A prototype system that demonstrates this potential is described.

Bilingual parallel corpora are very important in various filed of natural language processing (NLP
The quality of a Statistical Machine Translation (SMT) system strongly dependent upon the amount of training data.
For low resource language pairs such as Persian-English, there are not enough parallel sentences to build an accurate SMT system.
This paper describes a new approach to use the Wikipedia as a comparable corpus to extract Persian-English parallel sentences and eventually improve SMT system performance This new approach is also applicable to other low resource language pairs.
In order to calculate the similarity score between two sentences, a novel bi-directional translation-based information retrieval system is proposed.
A length penalty score is introduced to increase the accuracy of extracted corpus.
Using extracted parallel sentences, the performance of existing Persian-English SMT is improved drastically.

The size of theWeb as well as user bases of search systems continue to grow exponentially.
Consequently, providing subsecond query response times and high query throughput become quite challenging for large-scale information retrieval systems.
Distributing different aspects of search (e.g crawling, indexing, and query processing) is essential to achieve scalability in large-scale information retrieval systems.
The 8th Workshop on Large-Scale Distributed Systems for Information Retrieval (LSDS-IR'10) has provided a venue to discuss the current research challenges and identify new directions for distributed information retrieval.
The workshop contained two industry talks as well as six research paper presentations.
The hot topics in this year's workshop were collection selection architectures, application of MapReduce to information retrieval problems, similarity search, geographically distributed web search, and optimization techniques for search efficiency.

The notion of ontology is introduced to information retrieval systems and semantic Web-oriented architecture of personalized intelligent information retrieval system is presented, and hierarchical tree model of local domain ontological repository of biomedical diseases system is constructed, in which the OWL language is used to detail annotate syntax and semantics of concepts, their attributes and relations among the concepts.
Automatic acquirement of remote information on the Internet and pretreatment of local domain ontological repository are implemented by inference rules and information semantics-based intelligent clustering algorithm in the architecture, and preference database of doctors are managed logically according to their retrieval interests.
Consequently, the system will meets the requirement of personalized intelligent information search for diseases diagnosis, and further improves recall and precision of information retrieval on biomedical diseases system.

Modern Information Retrieval (IR) systems often employ document weighting models with many parameters that require to be appropriately set for effective retrieval performance.
To obtain these parameter settings, quality training is usually required, where assessors have manually labelled the relevance of retrieved items for many queries.
In this work, we examine the usefulness of high-quality click-through data for training an IR system, on searching the .gov vertical domain of the Web.
We find that, compared to training using relevance judgements created using human assessors, the click-through trained settings are as good and occasionally better.

Recent and continuing advances in online information systems are creating many opportunities and also new problems in information retrieval.
Gathering the information in different natural language is the most difficult task, which often requires huge resources.
Cross-language information retrieval (CLIR) is the retrieval of information for a query written in the native language.
This paper deals with various classification techniques that can be used for solving the problems encountered in CLIR.

I will present an analysis of how path dependent IR is support in the Ostensive Model for retrieval.
It will involve a discussion of appropriate similarity measures and an explanation of how to predict and present a choice of objects for a user in terms of the path traversed in the object (or interpretation) space.

Opinion mining is a recent discipline combining Information Retrieval and Computational Linguistics which is concerned with the opinion a document expresses and not just with the topic in the document.
Online forums, newsgroups, blogs, and specialized sites provide voluminous information feeds from where opinions can be retrieved.
Opinion’s polarity is established through application of machine learning techniques for classification of textual reviews as either a positive or negative class.
In this paper, it is proposed to extract the feature set from reviews using Inverse document frequency and the reviews are classified as positive or negative using Bagging algorithms.
The proposed method is evaluated using a subset of Internet Movie Database (IMBd General Terms Data Mining, Classification Accuracy.

This paper describes the work done by the Information Retrieval Research Group at Océ Technologies
B.V for the Cross-Language Evaluation Forum (CLEF) 2001.
We have participated in the Dutch Monolingual task and submitted three runs.
In this paper we introduce a new method for relevance computation using two new query operators.
This method is meant to be applied to Multilingual Retrieval in the future.

The experiments presented in this paper explore topics surrounding video information retrieval (IR This paper will discuss in detail our participation at TRECVID 2004.
A video retrieval system named ViewFinder was developed to search and browse the TRECVID 2004 test data, and both manual and interactive search experiments were carried out.
Each of the performed search experiments were in agreement with the task definitions and conference guidelines developed by TRECVID coordinators.
This paper will present our approach for TRECVID participation which includes the development of ViewFinder and other supportive tools, and the experimental designs of our search runs.
Results for each experimental search run are also presented.

E-government processes are dedicated to the improvement of the efficiency, inexpensiveness and accessibility of public administration services: dematerialization activities, introduced to manage bureaucratic digital documents in a proper way, are among the main tasks of the e-government works.
In this paper we present a novel RDF model of digital documents for improving the dematerialization effectiveness, that constitutes the starting point of an information system able to manage documental streams in the most efficient way.
Such model takes into account the important need that is required in several e-government applications which, depending on authorities or final users or time, provides different representations of the same multimedia contents.

Successful classification, information retrieval and image analysis tools are intimately related with the quality of the features employed in the process.
Pixel intensities, color, texture and shape are, generally, the basis from which most of the features are computed and used in such fields.
This papers presents a novel shape-based feature extraction approach where an image is decomposed into multiple contours, and further characterized by Fourier descriptors.
Unlike traditional approaches we make use of topological knowledge to generate well-defined closed contours, which are efficient signatures for image retrieval.
The method has been evaluated in the CBIR context and image analysis.
The results have shown that the multi-contour decomposition, as opposed to a single shape information, introduced a significant improvement in the discrimination power.
2008 Elsevier B.V.
All rights reserved.

Information Retrieval Systems typically distinguish between content bearing words and terms on a stop list.
But “content-bearing is relative to a collection.
For optimal retrieval efficiency, it is desirable to have automated methods for custom building a stop list.
This paper defines the notion of serial clustering of words in text, and explores the value of such clustering as an indicator of a word bearing cent ent.
The numerical measures we propose may also be of value in assigning weights to terms in requests.
Experimental support is obtained from natural text databases in three different languages.

Faculty of Computing Sciences and Engineering, De Montfort University UK Department of Computer and Information Science, University of Konstanz Germany Definition: Video streaming refers to a video transmission method that allows the receiver to view the video continuously after only a short delay.

In this paper, we propose mobile access to peer-reviewed medical information based on textual search and content-based visual image retrieval.
Web-based interfaces designed for limited screen space were developed to query via web services a medical information retrieval engine optimizing the amount of data to be transferred in wireless form.
Visual and textual retrieval engines with state-of-the-art performance were integrated.
Results obtained show a good usability of the software.
Future use in clinical environments has the potential of increasing quality of patient care through bedside access to the medical literature in context.

The development of a new drug takes over 10 years and costs approximately US $2.6 billion.
Virtual compound screening (VS) is a part of efforts to reduce this cost.
Learning-to-rank is a machine learning technique in information retrieval that was recently introduced to VS.
It works well because the application of VS requires the ranking of compounds.
Moreover, learning-to-rank can treat multiple heterogeneous experimental data because it is trained using only the order of activity of compounds.
In this study, we propose PKRank, a novel learning-to-rank method for ligand-based VS that uses a pairwise kernel and RankSVM.
PKRank is a general case of the method proposed by Zhang et al.
with the advantage of extensibility in terms of kernel selection.
In comparisons of predictive accuracy, PKRank yielded a more accurate model than the previous method.

To date there has been very little research conducted on the behaviour of music information retrieval (MIR) users, in spite of the immense popularity of free music retrieval systems available on the Internet.
In this study we examine the issue of music seeking behaviour through the examination of users life style effect of three different age groups using questionnaires.
It was found that lifestyles had a significant impact on users need for music and hence their music seeking behaviour.
The importance of social networks in music information seeking was reinforced in this study.
An experiment was conducted with three different types of search on the Kazaa MIR system and the participants interviewed in order to collect data.
Users found the Kazaa system intuitive and easy to use.
Searchers used both song titles and lyrics for finding relevant music items.
The insights provided by this study can be of assistance in the development of user focused Internet MIR systems.

Many practical systems in physics, biology, engineering, and information science exhibit impulsive dynamical behaviors due to abrupt changes at certain instants during the dynamical processes.
In this paper, stability analysis and stabilization synthesis problems are investigated for switched impulsive systems which consisting of a family of linear constant subsystems and a rule that orchestrates the switching between them.
Furthermore, there exist impulses at the switching instants.
A switched quadratic Lyapunov function is introduced to check asymptotic stability of such systems.
Two equivalent necessary and sufficient conditions for the existence of such a Lyapunov function are established, respectively.
The conditions are in linear matrix inequality form and can be used to solve stabilization synthesis problem.
The results are extended to the uncertain systems case as well

A new indexing method.
called Compressed Multi-Framed Signature File (C-MFSF that uses a partial query evaluation strategy with compressed signature bit slices is presented.
a signature tile is divided into variable sized compressed vertical frames with different on-bit densities to optimize the response time.
Experiments with a real database of 152,850 records show that a response time less than I50 milliseconds is possible.
For multi-term queries C-MFSF obtains the query results with fewer disk accesses than the inverted tiles.
The method requires no indexing vocabulary.
These attributes have important implications; for example, web search engines process multi-term queries in very large databases with sizeable vocabularies.

This paper describes the results of scientific and practical work on the creation of the Information Retrieval Thesauri for Social Sciences and Humanities for the Automated Information System of the Institute of Scientific Information on Social Sciences of the RAS series, viz the problems of forming a lexical array, the properties of terminology and introduction of branch thesauri, and the role that is played by the structure of thesauri as an instrument for expanding the navigation potential in the conceptual space of documents from a bibliographic fund.

The hardware and architecture approach to randomized algorithms is defined not only by the evaluation of systems that paved the way for the emulation of digital-to-analog converters, but also by the essential need for expert systems.
After years of important research into IPv7, we show the understanding of the Turing machine.
Our focus here is not on whether the infamous psychoacoustic algorithm for the important unification of SCSI disks and systems by M. Bharadwaj et al.
runs in O(n) time, but rather on proposing an analysis of linked lists.

The Semantic Web (SW) was originally positioned as a combination of Knowledge Representation (KR) and the Web.
However, most applications that use SW data today lean more towards the Information Retrieval spectrum.
The reason for this is that traditional KR systems are designed to work with datasets that are small, curated, homogeneous, and application-specific.
However, the SW is large-scale, messy, and heterogeneous.
We present a software project called WebQR in which we take a traditional KR application: Qualitative Reasoning, but implement it on top of the SW.
We show that reimagining a traditional KR system in the context of the SW opens up many opportunities but presents several problems for application development as well.

This document is intended to introduce the key elements of the Nuprl Proof Development System (Nuprl, for short) from the perspective of a Nuprl user, as opposed to the perspective of someone intimately involved in developing or extending Nuprl.
As such, it may be more appropriate than other Kuprl-related documents for readers who are primarily concerned with uses of Nuprl and not fine details of Nuprl's mathematical foundation.
It introduces and illustrates key Kuprl concepts -such as types, terms, displayforms, and tactics in the framework of a model of calculational predicate logic inference.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-01-32.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/822 A User-Level Introduction to the Nuprl Proof Development System Eric Aaron Department of Computer and Information Science University Of Pennsylvania

We show that any 1-round 2-server Private Information Retrieval Protocol where the answers are one bit long must ask questions that are at least n−2 bits long, which is nearly equal to the known n−1 upper bound.
This improves upon the approximately 0.25n lower bound of Kerenidis and deWolf while avoiding their use of quantum techniques.

In this paper, we introduce LAICOS, a social Web search engine as a contribution to the growing area of Social Information Retrieval (SIR Social information and personalization are at the heart of LAICOS.
On the one hand, the social context of documents is added as a layer to their textual content traditionally used for indexing to provide Personalized Social Document Representations.
On the other hand, the social context of users is used for the query expansion process using the Personalized Social Query Expansion framework (PSQE) proposed in our earlier works.
We describe the different components of the system while relying on social bookmarking systems as a source of social information for personalizing and enhancing the IR process.
We show how the internal structure of indexes as well as the query expansion process operated using social information.

The proposed research defines an approach to combine Information Retrieval based analysis of the textual information embedded in software artifacts with program static and dynamic analysis techniques to support key activities of the incremental change of software, such as concept and feature location.

Microblogging sites are important sources of situational information during disaster situations.
Hence it is important to design and evaluate Information Retrieval (IR) systems that retrieve information from microblogs during disaster situations.
The primary contribution of this paper is to develop a test collection for evaluating IR systems for microblog retrieval in disaster situations.
The collection consists of about 50,000 microblogs posted during the Nepal earthquake in April 2015, a set of five topics (information needs) that are practically important during a disaster, and the gold standard annotations of which microblogs are relevant to each topic.
We also present some IR models that can be suitable in this evaluation setup, including a standard language model based retrieval, and word embedding based retrieval.
We find that the term embedding based retrieval performs better for short, noisy microblogs.

This paper proposes a human information retrieval system with face extraction and recognition function.
A user watches the TV and if he encounters with interesting but unknown persons, he can specify them on the TV display by a mouse device.
The proposed system can extract and recognize the specified persons and retrieve their information from human information database via interet and output it on a display unit as well as through voice synthesizer.
We used subspace methods for a face extraction and recognition.
In face extraction, the sub-image within a scanning window is projected to facial subspace and it is regarded as the facial region, if the projection amount has a local peak.
In face recognition, the facial subspace of individual person is constructed at first in an observation space using respective training data.
Then an input facial region is projected to the individual subspace and classified into the person with the maximum projection amount.

Nowadays, cross-lingual Information Retrieval (IR) is one of the greatest challenges to deal with.
Besides, one of the most important issues in IR consists in the corpus vocabulary reduction in order to make possible to use in real situations some methods of IR such as the well-known vector space model.
In this work, we have considered a vocabulary reduction process based on the selection of mid-frequency terms.
Our approach enhances precision, but in order to obtain a better recall, we have conducted an enrichment process based on the addition of co-ocurrence terms.
By using this approach, we have obtained an improvement of 40% in the corpus of the BiEnEs WebCLEF 2005 task.
The obtained results in the current mixed monolingual task of the WebCLEF 2006 have shown that the text enrichment must be done before the vocabulary reduction process in order to get the best performance.

The dominant approaches to information retrieval system design are based on rational theory and cognitive engineering.
However, these theories as well as approaches in other disciplines, reviewed in this paper, do not account for communication, or interaction, among design participants which is critical to design outcomes.
This research attempts to develop a descriptive design model that accounts for communication among users, designers, and developers throughout the design process.
A pilot study has been completed and a preliminary model that represents a first step in understanding participants' evolving perceptions and expectations of the design process and its outcomes is described in this paper.

Enterprise Resource Planning (ERP) is an important tool for various enterprises and organizations supporting and managing business and making decision.
The technology foundation of ERP is based on information techniques.
The theory base includes application management theory, system theory, mathematic, computer science and information science.
ERP is the information system which combines with advanced management thoughts, methods and instruments.
Only if modern management theories and methods integrate into ERP, it can play an important role in management and help enterprises make decisions[2 This article focuses on the teaching mode of ERP (Enterprise resource planning) course in colleges and universities.
It recommends an innovative three dimensional teaching mode, composed of three main parts.
The new experimental arrangement is good to improve the students&apos; practical ability, innovation and teamwork ability.
It changes the ERP course from

We briefly survey the major thrusts of computer graphics activities, examining trends and topics rather than offering a comprehensive survey of all that is happening.
The directions of professional activities, hardware, software, and algorithms are outlined.
Within hardware we examine workstations, personal graphics systems, high performance systems, and low level VLSI chips; within software, standards and interactive system design; within algorithms, visible surface rendering and shading, three-dimensional modeling techniques, and animation.
Disciplines Computer Engineering Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-84-14.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/1008

The main motive behind raga identification is that it can be used as good basis for music information retrieval of carnatic music songs or film songs based on carnatic music.
The input polyphonic music signal is analyzed and made to pass through a signal separation algorithm to separate the instrument and the vocal signal.
The frequency components of the signal are then determined and we map these frequency in to swara sequence ant thereby determine the raga of the particular song.30- 40 a sample of Base Ragas from 72 is being identified which will help more for music learners and musicians.
Still now only vocal type of identification is being carried out and we provide the identification with string type Instruments of 1 or 2 Instruments mixture (violin/sitar/both).

An algorithm for information retrieval is developed.
It searches documents in a data set for those that are relevant to a user query.
It applies the Golub Kahan bidiagonalization algorithm to the term document matrix, starting from the query vector.
It is a further development of the vector space method, and needs less work and is more exible than latent semantic indexing (LSI Numerical tests on both small (Medline) and larger (Financial Times data from the TREC4 collection) data sets are reported.

On the 20 th of June, 2006 a workshop on the European Conference in Information Retrieval was held which aimed to develop a set of guidelines for authors and reviewers of ECIR papers.
These draft guidelines have been complied based upon the presentations from the workshop, and have been extracted from the full workshop report, which is available to download from the BCS-IRSG website (http irsg.bcs.org
We hope that this document will serve as the basis for helping authors wishing to submit to ECIR (especially student and first time authors along with aiding reviewers in the refereeing process.
These guidelines, are exactly, that guidelines, and authors and referees need to use their common sense, experience and discretion when using and interpreting these guidelines.
If you have any questions, comments, suggestions or contributions for the draft guidelines please contact the BCS-IRSG Chair, Leif Azzopardi

Preferences have gained a tremendous impact on the personalization of user queries.
In this paper, we present an approach for preference elicitation in a multimedia information retrieval scenario.
The approach is based on inductive preferences, which provide an intuitive means for stating preferences on actual result documents.
Additionally, they do not demand further knowledge of the underlying retrieval system of the user.
This work focuses on the user interaction of preference formulation and presents a prototype for a preference-based multimedia information retrieval system.

Medicine is a discipline that requires both judgment and action.
Information science can help in several aspect.
It can help the physician in collecting complete and relevant data.
It can support the physician by providing access to the rapidly increasing sets of medical knowledge through different kinds of the data bases.
It can facilitate the management of medical records which may be used for clinical follow-up of patients, clinical research, evaluation of medical action and education.
In all these aspects, information science gives indirect help to medical decision.

One of the main bottlenecks in the progress of the Music Information Retrieval (MIR) research field is the limited access to common, large and annotated audio databases that could serve for technology development and/or evaluation.
The aim of this paper is to present in detail the ENST-Drums database, emphasizing on both the content and the recording process.
This audiovisual database of drum performances by three professional drummers was recorded on 8 audio channels and 2 video channels.
The drum sequences are fully annotated and will be, for a large part, freely distributed for research purposes.
The large variety in its content should serve research in various domains of audio signal processing involving drums, ranging from single drum event classification to complex multimodal drum track transcription and extraction from polyphonic music.

Day by day, the amount of digital data grows exponentially.
Furthermore, these data come from heterogeneous content and structure resources.
Both factors are the major handicap that limits the correct functioning of information systems.
Being on the same wavelength, researchers have fixed as a challenge to reach targeted information retrieval which allows an easy access to the desired resource then return relevant and correct outcomes corresponding to a specific request.
Therefore, semantic annotation is chosen as a paramount solution that may handle all these requirements, while ensuring a good understanding of document content then exploit and share it readily afterward.
In this paper, we treat semantic annotation of documents using multi-ontologies, by stressing the effect of that on the relevance and the quality of semantic annotation results.
We present a survey of different multi-ontologies based semantic annotation approaches and conclude with a synthesis of the study made.

In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatictopic extraction, and fast information retrieval or filtering.
In this paper, we propose a novel method for clustering documents using regularization.
Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer.
So we call our algorithm <i>Clustering with Local and Global Regularization (
We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods.
Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.

The performance of cross-language information retrieval (CLIR) systems has been improved to the level of practical use.
The next step is to inform potential users that CLIR technologies are ready to be used.
A good way of doing this is to present attractive scenarios of using multilingual information sources.
For this purpose, we need to obtain more knowledge on the occasions when CLIR is more beneficial as compared with monolingual information retrieval from the utility perspective.
The difficulty lies in the inclusion of scenario building into the research activities.
This paper introduces a framework named the remarkable search topicfinding task to examine the way in which we can pursue this objective as part of the CLIR evaluation framework.
An example process implementing the task and some unresolved issues are discussed.

The objective of this paper is to develop software framework to improve information creation, maintenance and retrieval by introducing semantic technologies.
This paper analyzes the drawbacks of traditional keyword based search engines and proposes the need for semantic based intelligent information retrieval systems.
It also analyzes technologies specified by W3C, procedure for the creation of ontology from scratch, the evolution of ontology.
This paper presents Ontology Based Information Retrieval (IR) for Sports and Eminent Personalities Domain which is developed using Protégé tool.

The amount of unstructured and semi-structured data available in the form of text documents, images, audio, and video files by far exceeds the volume of data stored in relational databases.
Yet, in order to optimally utilize this data, it is necessary to devise methods and tools that extract relevant information and support efficient, content-oriented access to it.
There is considerable interest, commercial as well as academic, in this domain which has sparked research in machine learning methods for information access.
Some of the devised techniques have already led to improved tools that are incorporated in today’s information retrieval and knowledge management systems.

 Legal texts usually have a complex structure and reading through them is a time-consuming and strenuous task.
Hence it is essential to provide the legal practitioners a concise representation of the text.
Catchphrases are those phrases which state the important issues present in the text, thus effectively characterizing it.
This paper proposes an approach for the subtask 1 of the task IRLed (Information Retrieval from Legal Documents FIRE 2017.
The proposed algorithm uses a three step approach for extracting catchphrases from legal documents.

In many domains there are specific attributes in documents that carry more weight than the general words in the document.
This paper proposes the use of information extraction techniques in order to identify these attributes for the domain of calls for papers.
The utilisation of attributes into queries imposes new requirements on the retrieval method of conventional information retrieval systems.
A new model for estimating the relevance of documents to user requests is also presented.
The effectiveness of this model and the benefits of integrating information extraction with information retrieval are shown by comparing our system with a typical information retrieval system.
The results show a precision increase of between 45% and 60% of all recall points.

Qiang Lai 1 ID Akif Akgul 2, Chunbiao Li 3, Guanghui Xu 4 and Ünal Çavuşoğlu 5 1 School of Electrical and Automation Engineering, East China Jiaotong University, Nanchang 330013,
China 2 Department of Electrical and Electronics Engineering, Faculty of Technology, Sakarya University, Serdivan 54187, Turkey; aakgul@sakarya.edu.tr 3 School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing 210044, China; chunbiaolee@nuist.edu.cn 4 School of Electrical and Electronic Engineering, Hubei University of Technology, Wuhan 430068, China;
xgh@hbut.edu.cn 5 Department of Computer Engineering, Faculty of Computer and Information Sciences, Sakarya University, Serdivan 54187, Turkey; unalc@sakarya.edu.tr Correspondence: chaos1963@ecjtu.jx.cn or laiqiang87@126.com;
Tel 86-0791-8704-6216

I. Research Goals and Objectives
The basic structure of the electrical power grid has remained unchanged for a hundred years.
It has become increasingly clear, however, that the hierarchical, centrally-controlled grid of the twentieth century is ill-suited to the needs of the twenty-first.
A future grid, in which modern sensors, communication links, and computational power are used to improve efficiency, stability, and flexibility, has become known as the “smart grid Much of the hardware that will enable smart grids is in development or already exists smart” meters and appliances that respond to pricing signals, distributed wireless sensor networks, improved batteries for plug-in hybrid electric vehicles (PHEVs) that enable distributed storage, and so on.

This paper considers several practical heuristics for the design of good, if not optimal, partial-match data bases in situations which are typical of real life information retrieval systems Submitted to Information Processing Letters Work supported in part by U.S. Energy Research and Development Administration under Contract E(Oh.-3)-515 Work supported in part by National Science Foundation Grant

An efficient immune query optimization algorithm for information retrieval is proposed in this paper.
The main characteristics of this algorithm are as follows:
The genetic individual is a query, each gene corresponds to a weighted term, immune operator is used to avoid degeneracy, local search procedure based on the concept of neighborhood is used to speed up the abilities of finding better query vector.
Experimental results show that the proposed algorithm can efficiently improve the performance of the query search.

The main area of work in computer music related to information systems is known as music information retrieval (MIR Databases containing musical information can be classified into two main groups: those containing audio data (digitized music) and those that file symbolic data (digital music scores The latter are much more abstract that the former ones and contain a lot of information already coded in terms of musical symbols, thus MIR algorithms are easier and more efficient when dealing with symbolic databases.
The automatic extraction of the notes in a digital musical signal (automatic music transcription) permits applying symbolic processing algorithms to audio data.
In this work we analize the performance of a neural approach and a well known non parametric algorithm, like nearest neighbours, when dealing with this problem using spectral pattern identification.

This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments.
It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable.
As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has.
These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results.
In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.

The Information Grid (InfoGrid) is a framework for building information access applications that provides a user interface design and an interaction model.
It focuses on retrieval of application objects as its top level mechanism for accessing user information, documents, or services.
We have embodied the InfoGrid design in an object-oriented application framework that supports rapid construction of applications.
This application framework has been used to build a number of applications, some that are classically characterized as information retrieval applications, others that are more typically viewed as personal work tools.

This paper considers the proposed algorithm submitted to theMusic Information Retrieval Evaluation eXchange(MIREX)
2011 “AudioMelody Extraction” task.
The proposed melody pitch extraction algorithm can be divided into three steps 1) a spectral analysis using variable length window 2) a pitch candidate estimation, and (3) a pitch sequence identification.
In the first step, the short-time Fourier transform (STFT) with variable length window is performed to be robust against dynamic variation of melody line.
In the second step, melody pitch candidates of each frame are obtained from weights of a harmonic structure in the spectrum.
In the third step, a single pitch sequence (melody line) is selected from the many possible pitch sequences based on the general properties of melody line.

Many inference problems are naturally formulated using hard and soft constraints over relational domains: the desired solution must satisfy the hard constraints, while optimizing the objectives expressed by the soft constraints.
Existing techniques for solving such constraints rely on efficiently grounding a sufficient subset of constraints that is tractable to solve.
We present an eager-lazy grounding algorithm that eagerly exploits proofs and lazily refutes counterexamples.
We show that our algorithm achieves significant speedup over existing approaches without sacrificing soundness for real-world applications from information retrieval and program analysis.

Within the last few years very little as been made of the usefulness of Connectivity Analysis to Information Retrieval on the WWW.
This document discusses hyperlinks on the WWW and our experiments on the exploitation of the immediate neighbourhood of a web page in an effort to improve search results.
In order to test the hypothesis that Connectivity Analysis can increase precision in the top ranked documents from a set of relevant documents, we developed a software application to generate and re-rank a query relevant subset of the WT2g Dataset.
We discuss our software in depth and the problems that we encountered during development.
Our experiments are based on implementing a number of re-ranking formulae, each of which tests the usefulness of different approaches to reranking a set of relevant pages, ranging from basic context analysis (inLink ranking) to combined content and context analysis approaches.

Personalized Information Retrieval systems (PIR) are of great need now a day.
With growing size of database requirement of precise data, PIR are of great importance.
But this area is still being under research for the best methodology of searching.
The PIR system instead of providing irrelevant data along with relevant one, provide us with just the possible relevant data matching our need requirement.
In this paper a survey is done on different algorithms that are being worked on so far on PIR systems.
Their drawbacks new changes that can be inculcated.
Different algorithms are being used to retrieve data in the PIR systems.
Each algorithm was applied to the database their results were noted.
Then their drawbacks were noticed some changes were made to overcome those.

Summary form only given.
We have designed and implemented an efficient stop-word removal algorithm for Arabic language based on a finite state machine (FSM An efficient stop-word removal technique is needed in many natural language processing application such as: spelling normalization, stemming and stem weighting, Question answering systems and in information retrieval systems (IR Most of the existing stop-word removal techniques bases on a dictionary that contains a list of stop-word, it is very expensive, it takes too much time for searching process and required too much space to store these stop-words.
The new Arabic removal stop-word technique has been tested using a set of 242 Arabic abstracts chosen from the Proceedings of the Saudi Arabian National Computer conferences, and another set of data chosen from the holy Q'uran, and it gives impressive results that reached approximately to 98%.

Geographic information retrieval (GIR) systems aim to provide advanced functionalities for storage, organisation, searching and representation of geographic information.
Digital maps, as a well–established and powerful information medium, have been widely used in GIR systems as a representation tool for geographic knowledge and geographic relationships, and provide users with capabilities to understand and reason about the geographic environment that is correlated with the retrieved information.
This paper addresses several key challenges in designing and implementing of retrieval systems for geo-tagged Web content by developing an online map-based GIR system called GeoTagMapper.
The components of the proposed are described in detail and the effectiveness and the usefulness of our system are demonstrated by applying it to a large collection of geo-tagged Web pages.

Discovering the inherent structure in data has become one of the major challenges in data mining applications.
It requires the development of stable and adaptive models that are capable of handling the typically very high-dimensional feature spaces.
In this paper we present the Growing Hierarchical Self-Organizing Map (GH-SOM a neural network model based on the self-organizing map.
The main feature of this extended model is its capability of growing both in terms of map size as well as in a three-dimensional tree-structure in order to represent the hierarchical structure present in a data collection.
This capability, combined with the stability of the self-organizing map for high-dimensional feature space representation, makes it an ideal tool for data analysis and exploration.
We demonstrate the potential of this method with an application from the information retrieval domain, which is prototypical of the high-dimensional feature spaces frequently encountered in today’s

This research indicates how information retrieval activity is a social process helping to foster a network of organizational expertise.
After describing knowledge management practices in a distributed research and development laboratory we identify recent introduction of Experts' Retrieval System (ERS A four steps model
Information Retrieval, Caring (social support Negotiation, and Reification is suggested.
This model specifies DemonD an ERS relying on transparent profile construction based on user's activity, community's participation and shared documents.
DemonD eventually encourages the emergence of informal knowledge networks and competencies awareness in a distributed context.

This paper summarizes the experience of Math Information Retrieval team of Masaryk University (MIRMU) with the NTCIR-12 MathIR arXiv Main Task and its subtasks.
We based our approach on the MIaS system.
Based on NTCIR-11 Math-2 Task relevance judgements, we developed an evaluation platform.
Using this platform we rigorously evaluated combinations of new features and picked the most promising ones for the NTCIR-12 evaluation.
The new features tested are mostly aimed at further canonicalizing MathML input, structurally unifying formulae for syntactic-based similarity search and query expansion when combining text and math query terms.

Engineers spend a significant amount of time searching for the right product information during the design and manufacturing process.
A large amount as well as various types of product information have been generated and are available within the engineering databases.
But representing and indexing the products effectively and retrieving them efficiently, remains a challenge.
The purpose of this survey is to document the current state of research and development.
We then identify avenues for exploration and provide a comparison of the advantages, disadvantages, and limitations among the various techniques.
This survey concludes by suggesting possible future research directions.

Question-answering has recently received more and more attention from researchers.
It is widely regarded as the advanced stage of information retrieval.
This paper provides a novel domain-independent question-answering system which is based on information retrieval in a large-scale collection of texts, and an improved system similarity model is developed and applied in it which improves the performance of the system.
Many natural language processing technologies are adopted to increase the accuracy of the system.
Several useful tools are incorporates as external auxiliary resources.
In addition, some external knowledge such as knowledge from Internet is also widely used in this system.
Test data collection and evaluation methodology from 2006
Text Retrieval Conference's Question Answering Track are used to evaluate the system, and the results are comparatively satisfying

We explore how Private Information Retrieval (PIR) can help users keep their sensitive information from being leaked in an SQL query.
We show how to retrieve data from a relational database with PIR by hiding sensitive constants contained in the predicates of a query.
Experimental results and microbenchmarking tests show our approach incurs reasonable storage overhead for the added privacy benefit and performs between 7 and 480 times faster than previous work.

We discuss in a compact way how the implicit relations between spatiotemporal relatedness of information items, spatiotemporal relatedness of users, social relatedness of users and semantic relatedness of information items may be exploited for an information retrieval architecture that operates along the lines of human ways of searching.
The decentralized and agent oriented architecture mirrors emerging trends such as upcoming mobile and decentralized social networking as a new paradigm in social computing and is targetted to satisfy broader and more subtly interlinked information demands beyond immediate information needs which can be readily satisfied with current IR services.
We briefly discuss why using spatio-temporal references as primary information criterion implicitly conserves other relations and is thus suitable for such an architecture.
We finally shortly point to results from a large evaluation study using Wikipedia articles.

The paper presents a research in Arabic Information Retrieval
It surveys the impact of statistical and morphological analysis of Arabic text in improving Arabic IR relevancy.
We investigated the contributions of Stemming, Indexing, Query Expansion, Text Summarization (TS Text Translation, and Named Entity Recognition (NER) in enhancing the relevancy of Arabic IR.
Our survey emphasizing on the quantitative relevancy measurements provided in the surveyed publications.
The paper shows that the researchers achieved significant enhancements especially in building accurate stemmers, with accuracy reaches 97 and in measuring the impact of different indexing strategies.
Query expansion and Text Translation showed positive relevancy effect.
However, other tasks such as NER and TS still need more research to realize their impact on Arabic IR.

This article provides an overview of studies that have used citation analysis in the field of humanities in the period 1951 to 2010.
The work is based on an exhaustive search in databases—particularly those in library and information science—and on citation chaining from papers on citation analysis.
The results confirm that use of this technique in the humanities is limited, and although there was some growth in the 1970s and 1980s, it has stagnated in the past 2 decades.
Most of the work has been done by research staff, but almost one third involves library staff, and 15% has been done by students.
The study also showed that less than one fourth of the works used a citation database such as the Arts Humanities Citation Index and that 21% of the works were in publications other than library and information science journals.
The United States has the greatest output, and English is by far the most frequently used language, and 13.9% of the studies are in other languages.

This poster describes a potential problem with a relatively well used measure in Information Retrieval research: Kendall's Tau rank correlation coefficient.
The coefficient is best known for its use in determining the similarity of test collections when ranking sets of retrieval runs.
Threshold values for the coefficient have been defined and used in a number of published studies in information retrieval.
However, this poster presents results showing that basing decisions on such thresholds is not as reliableas has been assumed.

Nepali is an inflectionally rich language.
The unavailability of a stemmer, that effectively eliminates inflections, makes the task of searching, text mining, information retrieval and natural language processing in Nepali challenging.
Stemming helps in mapping all variants of inflected words to their respective stems.
Stemmers are widely used in Text Mining, Information Retrieval and Natural Language Processing systems to improve the performance of such systems.
This paper presents a new rule-based stemmer for Nepali language.
We have composed 128 suffix rules, which are executed in step-by-step and iterative manner to eliminate inflections.
The proposed stemmer was tested on 5000 Nepali words and the overall accuracy was around 88.78%.

This paper aims at presenting a methodology for automatic thesaurus construction in order to help the search of documents and we want to obtain the development of classes for specific topics (for a given corpus) without a priori semantic information.
Information contained in the thesaurus lead to new search formulations via automatic and/or user feedback.
This presentation even being theoretical is oriented toward a database implementation.

Video contains multiple types of audio and visual information, which are difficult to extract, combine or trade-off in general video information retrieval.
This paper provides an evaluation on the effects of different types of information used for video retrieval from a video collection.
A number of different sources of information are present in most typical broadcast video collections and can be exploited for information retrieval.
We will discuss the contributions of automatically recognized speech transcripts, image similarity matching, and video OCR in the contexts of experiments performed as part of 2001 TREC Video Retrieval Track evaluation performed by the National Institute of Standards and Technology.
For the queries used in this evaluation, image matching and video OCR proved to be the most important aspects of video information retrieval.

Private Information Retrieval (PIR despite being well studied, is computationally costly and arduous to scale.
We explore lower-cost relaxations of information-theoretic PIR, based on dummy queries, sparse vectors, and compositions with an anonymity system.
We prove the security of each scheme using a flexible differentially private definition for private queries that can capture notions of imperfect privacy.
We show that basic schemes are weak, but some of them can be made arbitrarily safe by composing them with large anonymity systems.

Measuring the information retrieval effectiveness of Web search engines can be expensive if human relevance judgments are required to evaluate search results.
Using implicit user feedback for search engine evaluation provides a cost and time effective manner of addressing this problem.
Web search engines can use <i>human evaluation</i> of search results without the expense of human evaluators.
An additional advantage of this approach is the availability of real time data regarding system performance.
Wecapture user relevance judgments actions such as print, save and bookmark, sending these actions and the corresponding document identifiers to a central server via a client application.
We use this implicit feedback to calculate performance metrics, such as precision.
We can calculate an overall system performance metric based on a collection of weighted metrics.

The Exploration, Navigation and Retrieval of Information in Cultural Heritage Workshop (ENRICH 2013) offers a forum to 1) discuss the challenges and opportunities in Information Retrieval research in the area of Cultural Heritage; 2) encourage collaboration between researchers engaged in work in this specialist area of Information Retrieval, and to foster the formation of a research community; and 3) identify a set of actions which the community should undertake to progress the research agenda.
The workshop will foster a new stream of Information Retrieval research and support the design of search tools that can help end-users fully exploit the wonderful Cultural Heritage material that is available across the globe.

There is a growing need to access and retrieve relevant information across media, across languages and across modalities.
The research context concerning information access and retrieval aims at modelling, designing and implementing systems able to provide a fast and effective content-based access to a large amount of multimedia information.
The aim of such systems is to estimate the relevance of documents to a user information need.
This is a very hard and complex task due to several distinct reasons that a large volume of research has attempted to analyse and tackle.
Information Retrieval (IR) can be considered as the first historical research area aimed at defining systems for the automatic access to huge amounts of information (together with DBMSs whose main aim is to manage and access huge amounts of data).

The analysis of web-based documents using quantitative techniques is a wellestablished area of research within the realm of information science.
This paper builds on some of that work and presents the results of statistical analysis carried out on the web link structure text files of 111 UK universities.
Summary statistics are produced using Alternative Document Models and the results of the statistical analysis are also graphically displayed, including trendline equations.
Mathematical linear relationships were observed between certain bivariate data with subsequent Pearson correlation analysis revealing a number of very strong correlation relationships, particularly between site size and number of source target directories and pages.
This seems to support previous research by suggesting that the directory Alternative Document Model has some advantages over the domain Alternative Document Model.

This paper describes our Korean-Chinese cross-language information retrieval system for NTCIR-6.
Our system uses a bilingual dictionary to perform query translation.
We expand our bilingual dictionary by extracting words and their translations from the Wikipedia site, an online encyclopedia.
To resolve the problem of translating Western people's names into Chinese, we propose a transliteration mapping method.
We translate queries form Korean query to Chinese by using a co-occurrence method.
When evaluating on the NTCIR-6 test set, the performance of our system achieves a mean average precision (MAP) of 0.1392 (relax score) for title query type and 0.1274 (relax score) for description query type.

In an era where highly accurate Question Answering (QA) systems are being built using complex Natural Language Processing (NLP) and Information Retrieval (IR) algorithms, presenting the acquired answer to the user akin to a human answer is also crucial.
In this paper we present an answer presentation strategy by embedding the answer in a sentence which is developed by incorporating the linguistic structure of the source question extracted through typed dependency parsing.
The evaluation using human participants proved that the methodology is human-competitive and can result in linguistically correct sentences for more that 70% of the test dataset acquired from QALD question dataset.

Private Information Retrieval (PIR) protocols aim at ensuring a user that he can retrieve some part Di of a distributed database D without revealing the index i to the server(s
Most of known PIR protocols focus on decreasing the communication complexity between the client and the server(s
Recently, the use of PIR codes by Fazeli et.
al. also lead to a huge reduction of the storage overhead supported by the servers.
However, only a few works address the issue of the computational complexity of the servers.
In this paper, we show that transversal designs and their generalizations provide PIR schemes achieving simultaneously reasonable communication complexity, low storage overhead, optimal computational complexity for the servers, and resistance to a collusion of some of them.

This paper proposes a new distinguishing-type attack to identify block ciphers.
This attack utilises a multidisciplinary approach to the problem.
It is grounded in a neural network, by means of a linguistic and an information retrieval approach, from patterns found on a set of cipher texts.
This result is possible due to the existence of intrinsic properties in the mathematical basis of ciphers, which create signatures in the cipher texts.
Experiments were performed on a set of cipher texts, which were encrypted by the finalist algorithms of AES contest: MARS, RC6, Rijndael, Serpent and Two fish, with a unique 128-bit key.
The processes of clustering and classification were successful, allowing the formation of well-defined groups, where cipher texts encrypted by the same algorithm stayed close to each other, from a topological standpoint, which allow the identification of the cipher.

Information retrieval (IR) extracts and organizes natural language information found in unstructured text.
Many of the challenges faced by software engineers can be addressed using IR techniques on the unstructured text provided by source code and its associated documents.
A survey of IR-based techniques applied to software engineering challenges during the initial development process is presented.
In particular, the following problems are considered: requirements discovery, maintaining software repositories, establishing traceability links, efficient software reuse, and effective software metrics.
These techniques highlight the bright future that IR brings to addressing SE problems.

Information Retrieval: Common information-retrieval techniques either rely on a specific encoding of available information (e.g. fixed classification codes) or simple full-text analysis.
Both approaches suffer from severe shortcomings.
Using an ontology in order to explicate the vocabulary can help overcome some of these problems.
When used for the description of available information as well as for query formulation an ontology serves as a common basis for matching queries against potential results on a semantic level.

By and large, three classic framework models have been used in the process of retrieving information: Boolean, Vector Space and Probabilistic.
Boolean model is a light weight model which matches the query with precise semantics.
Because of its boolean nature, results may be tides, missing partial matching, while on the contrary, vector space model, considering term-frequency, inverse document frequency measures, achieves utmost relevancy in retrieving documents in information retrieval.
This paper implements and discusses the issues of information retrieval system with vector space model using MATLAB on Cranfield data collection of aerodynamics domain.

We consider optimization problems with polynomial inequality constraints in non-commuting variables.
These non-commuting variables are viewed as operators acting on a Hilbert space whose dimension is not fixed, and the associated polynomial inequalities as semidefinite positivity constraints.
Such problems arise naturally in quantum theory and quantum information science.
To solve them, we introduce a hierarchy of semidefinite programming relaxations which generates a monotone sequence of lower bounds on the global minimum of the original problem.
In the case that the constraints defining our problem guarantee that the operators are bounded, we prove that our sequence of lower bounds converges to the global solution.
We also introduce a criterion to detect whether the global optimum is reached at a given relaxation step and show how to extract a global optimizer from the solution of the corresponding semidefinite programming problem.

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.

This paper presents the ITEM multilingual search engine.
This search engine performs full lexical processing (morphological analysis, tagging and Word Sense Disambiguation) on documents and queries in order to provide language-neutral indexes for querying and retrieval.
The indexing terms are the EuroWordNet/ITEM InterLingual Index records that link wordnets in 10 languages of the European Community (the search engine currently supports Spanish, English and Catalan
The goal of this application is to provide a way of comparing in context the behavior of different Natural Language Processing strategies for Cross-Language Information Retrieval (CLIR) and, in particular, different Word Sense Disambiguation strategies for query translation and conceptual indexing.

This paper proposes a novel approach in taking audio feature into account for better event recognition performance in recognizing complex events in real movies, since audio can provide strong evidence to certain events.
In our method, local-space time feature and audio feature are firstly extracted from the video sequences and then an individual video sequence is represented as a SOFM density map; finally we integrate such density map with SVM for recognition events.
To evaluate effectiveness of this method, this paper uses the public Hollywood dataset, in this dataset the shot sequences have been collected from 32 different Hollywood movies and it includes 8 event classes.
The presented result justifies the proposed method explicitly, improve the average accuracy and average precision compared to other relative approaches 2014 Elsevier B.V.
All rights reserved.

A study of the “real world”
Web information searching behavior of 206 college students over a 10-month period showed that, contrary to expectations, the users adopted a more passive or browsing approach to Web information searching and became more eclectic in their selection of Web hosts as they gained experience.
The study used a longitudinal transaction log analysis of the URLs accessed during 5,431 user days of Web information searching to detect changes in information searching behavior associated with increased experience of using the Web.
The findings have implications for the design of future Web information retrieval tools.

User classification is a critical problem in modern library.
But there has been little research into intelligent methods for improving the accuracy of user classification quality.
In this article, we propose a HMM of library user classification (HMMLUC) to recognize the different type of library users.
Using user previous behavior, HMMLUC learns a probabilistic model over the types of the user, and then applies this model at every step of the data entry process to improve user service quality.
The experiment results proof that the performance of the model is sound.
The average precision ratio of the HMMLUC is 91.2, while the recall ratio is 85.3.

The academic research field of music information retrieval is expanding as rapidly as the MP3 collection of a stereotypical teenager.
This could be no coincidence the benefit of an automated genre classifier increases when the music collection contains several thousand tracks.
Of course, there are other applications of music information retrieval.
Here we highlight a few that make use of a simple, visual, representation of an audio signal, based on three easy-to-calculate audio features.
The applications range from simple navigation around consumer recordings of broadcasts, to a music video production planning tool, to a short term "Listen Again" eye-catching display.
This document was originally published at the 123rd Audio Engineering Society Convention, New York, USA, October 2007
Additional key words:

Accessing online information remains an inexact science.
While valuable information can be found, typically many irrelevant documents are also retrieved and many relevant ones are missed.
Terminology mismatches between the user's query and document contents is a main cause of retrieval failures.
Expanding a user's query with related words can improve search performance, but the problem of identifying related words remains.
This research uses corpus linguistics techniques to automatically discover word similarities directly from the contents of the untagged TREC database and to incorporates that information in the SMART information retrieval system.
The similarities are calculated based on the contexts in which a set of target words appear.
Using these similarities, user queries are automatically expanded, resulting in conceptual retrieval rather than requiring exact word matches between queries and documents.

1University of Patras, Department of Computer Engineering and Informatics, GR-265 00, Patras, Greece 2Carleton University, School of Computer Science, Ottawa, ON, K1S 5B6, Canada 3Wesleyan University, Department of Mathematics, Middletown, CT 06459, USA 4University of Patras, Department of Computer Engineering and Informatics, GR-265 00, Patras, Greece also Computer Technology Institute, Kolokotroni 3, GR-262 21, Patras, Greece 5University of Patras, Department of Computer Engineering and Informatics, GR-265 00, Patras, Greece Email: kaporis@ceid.upatras.gr, kirousis@ceid.upatras.gr, kranakis@scs.carleton.ca, dkrizanc@wesleyan.edu, stamatiu@ceid.upatras.gr, estavrop@ceid.upatras.gr

Sergio Boixo, Troels F. Rønnow, Sergei V. Isakov, Zhihui Wang, David Wecker, Daniel A. Lidar, John M. Martinis, and Matthias Troyer∗2 Information Sciences Institute and Department of Electrical Engineering, University of Southern California, Los Angeles, CA 90089, USA Theoretische Physik, ETH Zurich, 8093 Zurich, Switzerland Department of Chemistry and Center for Quantum Information Science Technology, University of Southern California, Los Angeles, California 90089, USA Quantum Architectures and Computation Group, Microsoft Research, Redmond, WA 98052, USA 5 Departments of Electrical Engineering, Chemistry and Physics, and Center for Quantum Information Science Technology, University of Southern California, Los Angeles, California 90089, USA Department of Physics, University of California, Santa Barbara, CA 93106-9530, USA

The results and achievements of research in information retrieval have had little influence on the types of retrieval mechanism implemented in the large commercial on-line retrieval systems.
Commercial systems still use simple Boolean techniques while experiments have shown that other techniques, such as those making use of relevance information, perform better.
Reasons for this are suggested.
A strategy is described for the implementation of high-powered techniques which overcomes this problem an intelligent terminal is used in conjunction with an existing commercial I. R. system.
The added processing capability permits the implementation of more sophisticated retrieval techniques very cheaply.
Retrieval methods that can be implemented on such a terminal used in this way are described.
Particular emphasis is placed upon the practical implementation of a term weighting scheme based on relevance feedback information which generates a ranked list of documents in answer to a query.

Read more and get great!
That's what the book enPDFd introduction to modern information retrieval 3rd edition will give for every reader to read this book.
This is an on-line book provided in this website.
Even this book becomes a choice of someone to read, many in the world also loves it so much.
As what we talk, when you read more every page of this introduction to modern information retrieval 3rd edition, what you will obtain is something great.

The standard system-based evaluation paradigm has focused on assessing the performance of retrieval systems in serving the best results for a single query.
Real users, however, often begin an interaction with a search engine with a sufficiently under-specified query that they will need to reformulate before they find what they are looking for.
In this work we consider the problem of evaluating retrieval systems over test collections of multi-query sessions.
We propose two families of measures: a model-free family that makes no assumption about the user's behavior over a session, and a model-based family with a simple model of user interactions over the session.
In both cases we generalize traditional evaluation metrics such as average precision to multi-query session evaluation.
We demonstrate the behavior of the proposed metrics by using the new TREC 2010 Session track collection and simulations over the TREC-9 Query track collection.

Traditional expert systems for medical diagnosis have a major pitfall, in that it is difficult to adopt new knowledge, particularly from descriptive, incomplete and unstructured information.
Information retrieval sounds a natural answer to these problems.
This paper presents an information retrieval based approach to computer aided diagnosis in order to tackle this problem.
Preliminary experimental results with traditional Chinese medicine expertise show that it can effectively help to diagnose based on the descriptive symptoms provided.

The standard criteria for evaluation of information retrieval (IR) systems: effectiveness, efficiency, usability, satisfaction, cost-benefit seem as applicable to the interactive multimedia context as to the non-interactive, text-based context in which they have been developed.
However, the operationalizations, measures and methods developed in the traditional context are, for a variety of reasons, almost wholly inadequate for the new context.
This paper discusses some of the problematic aspects of evaluation in this new context, and suggests some strategies for developing new measures and methodologies for the evaluation of interactive multimedia IR systems.

This paper describes aspects of an information system being constructed for intelligent information retrieval in maritime navigation applications.
The system is based largely on digital nautical charts, and the construction of a taxonomy of chart features and concepts related to the maritime navigation domain is described.
This taxonomy is used in formulating queries and sub-queries for disparate sources of information that supply information about navigation hazards, aids to navigation, and other relevant local information.

In expanding from an application-oriented hierarchical database model information system to a system integrated with information-oriented relational database model, IBM offers its IMS/DB2 dual database strategy.
There are also other non-IBM database technologies challenging DB2 as the only alternative for IMS installation.
Automobile industries themselves are going through such a transition in developing their fourth-generation information systems.
A couple major automobile corporations with IMS-based information systems are brought up as examples.
These corporations have developed their design strategies and system architectures.
Such an integration has influence upon the operating environment and the decision support for the end-user-driven information retrieval applications.

Vijay V. Raghavan Hayri Sever
The Center for Advanced Computer Studies
The Department of Computer Science University of Southwestern Louisiana University of Southwestern Louisiana Lafayette, LA 70504, USA Lafayette, LA 70504, USA e-mail
: raghavan@cacs.usl. edu Information Retrieval (IR) systems exploit user feedback by generating an optimal query with respect to a particular information need.
Since obtaining an optimal query is an expensive process, the need for mechanisms to save and reuse past optimal queries for future queries is obvions.
In this article, we propose the use of a query base, a set of persistent past optimal queries, and investigate similarity measures between queries.
The query base can be used either to answer user queries or to formulate optimal queries.
We justify the former case analytically and the latter case by experiment.

We use game theory to analyze meta-learning algorithms.
The objective of meta-learning is to determine which algorithm to apply on a given task.
This is an instance of a more general problem that consists of allocating knowledge consumers to learning producers.
Solving this general problem in the field of meta-learning yields solutions for related fields such as information retrieval and recommender systems.

Users often search in information retrieval systems iteratively.
We claim that changing the underlying retrieval algorithm and query expansion algorithm during these iterations yields improved results.
In the rst part of this paper, results of preliminary experiments are presented which indicate the viability of our claim.
In the second part, we focus on our approach for the dynamic selection of the proper retrieval algorithms and query expansion algorithms.
Our approach is based on case-based planning:
A retrieval algorithm sequence which holds certain quality criteria is set up due to the consideration of previous experiences and cases.

Digital Library; Australian Business Deans Council (ABDC Bacon’s Media Directory; Burrelle’s Media Directory; Cabell’s
Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; Gale Directory of Publications Broadcast Media; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Research Articles

This paper presents our contribution to enhance literature-based discovery with information retrieval techniques.
We propose the joint use of a flexible Information Retrieval model and MeSH Concepts for knowledge discovery in biomedical literature.
The Information Retrieval model contributes to filter MEDLINE biomedical literature to the most relevant documents.
Utilizing MeSH concepts allows to quickly identifying candidate concepts that could potentially validate a hypothesis.
We have tested our approach by replicating the Swanson's first discovery on fish oil and Raynaud's disease correlation.
The obtained results show the effectiveness of our approach.

Song and Bruza
[6] introduce a framework for Information Retrieval(IR) based on Gardenfor's three tiered cognitive model; Conceptual Spaces[4 They instantiate a conceptual space using Hyperspace Analogue to Language (HAL[3] to generate higher order concepts which are later used for ad-hoc retrieval.
In this poster, we propose an alternative implementation of the conceptual space by using a probabilistic HAL space (pHAL To evaluate whether converting to such an implementation is beneficial we have performed an initial investigation comparing the concept combination of HAL against pHAL for the task of query expansion.
Our experiments indicate that pHAL outperforms the original HAL method and that better query term selection methods can improve performance on both HAL and pHAL.

The large scale of scholarly publications poses a challenge for scholars in information seeking and sense-making.
Bibliometrics, information retrieval (IR text mining, and NLP techniques could help in these activities, but are not yet widely implemented in digital libraries.
The 2nd joint BIRNDL workshop was held at the 40th ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017) in Tokyo, Japan.
BIRNDL 2017 intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric, and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale.
The workshop incorporated three paper sessions and the 3rd edition of the CL-SciSumm Shared Task.

The language modeling approach to information retrieval (IR) is a new framework that has been proposed and developed within the past five years, although its roots in the IR literature go back more than twenty years.
Research carried out at a number of sites has confirmed that the language modeling approach is a theoretically attractive and potentially very effective probabilistic framework for building IR systems.

Library; Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Research Articles

Data uncertainty is a main problem of information science to handle the data and information.
Many theories handle the uncertainty problem.
This paper analysed soft set reduction and described how a data set is converted into binary information system and also analysed how it is better to reduce the dimension of the data.
Like rough set, fuzzy set etc are dealing with uncertainty.
Soft set theory also do vital role to handle the uncertainty problem.

We present a study to understand the effect that negated terms (e.g no fever and family history (e.g family his- tory of diabetes have on searching clinical records.
Our analysis is aimed at devising the most effective means of handling negation and family history.
In doing so, we explicitly represent a clinical record according to its different content types: negated, family history and normal content; the retrieval model weights each of these separately.
Empirical evaluation shows that overall the presence of negation harms retrieval effectiveness while family history has little effect.
We show negation is best handled by weighting negated content (rather than the common practise of re- moving or replacing it
However, we also show that many queries benefit from the inclusion of negated content and that negation is optimally handled on a per-query basis.
Additional evaluation shows that adaptive handing of negated and family history content can have significant benefits.

This article is concerned with the design and implementation of Information Retrieval Systems (IRS We show how theories and models from the domain of Human Computer Interaction (HCI) can be applied to the design of IRS.
We first study the user’s tasks by modelling the mental activities of the user while accomplishing a task.
Adopting a system perspective, we consider the processing tasks of an IRS and organize them in a design space.
We then build upon the design space to consider the implications of such data processing and levels of abstraction on software design.
Finally we present PAC-Amodeus, a software architecture model and illustrate the applicability of the approach with the implementation of an IRS: the TIAPRI system.

While there are now a number of languages and frameworks that enable computer-based systems to search stored data semantically, the optimal design for effective user interfaces for such systems is still unclear.
Such interfaces should mask unnecessary query detail from users, yet still allow them to build queries of arbitrary complexity without significant restrictions.
We developed a user interface supporting semantic query generation for SemanticOrganizer, a tool used by scientists and engineers at NASA to construct semantic networks of knowledge and data.
Through this interface users can select node types, node attributes and node links to build ad-hoc semantic queries for searching the SemanticOrganizer network.

A Geographic Information Retrieval (GIR) system for answering geographic queries has to cope with various information needs, which have a wide range of contexts and implicit requirements.
A user, for example, who is looking for a place to spend his or her holidays certainly has a different understanding of distance than a user looking for a bar in the city
he or she lives in To get a better understanding of geographic information needs and their implications for GIR systems
, we analysed real world (geographic) queries with regard to different facets of geographic references in queries.
The results of this analysis are presented in this paper, the aim of which was a classification of the geographic aspects of information needs.
We present empirical results and line out possible classification criteria, which could be helpful in designing GIR systems that are able to consider different semantics of geographic references in queries.

Experiments carried out within evaluation initiatives for information retrieval have been building a substantial resource for further detailed research.
In this study, we present a comprehensive analysis of the data of the Cross Language Evaluation Forum (CLEF) from the years 2000 to 2004.
Features of the topics are related to the detailed results of more than 100 runs.
The analysis considers the performance of the systems for each individual topic.
Named entities in topics revealed to be a major influencing factor on retrieval performance.
They lead to a significant improvement of the retrieval quality in general and also for most systems and tasks.
This knowledge, gained by data mining on the evaluation results, can be exploited for the improvement of retrieval systems as well as for the design of topics for future CLEF campaigns.

The Informedia Digital Video Library Project at Carnegie Mellon University is creating large digital libraries of video and audio data available for full content retrieval by integrating natural language understanding, image processing, speech recognition and information retrieval.
These digital video libraries allow users to explore multi-media data in depth as well as in breadth.
The Informedia system automatically processes and indexes video and audio sources and allows selective retrieval of short video segments based on spoken queries.
Interactive queries allow the user to retrieve stories of interest from all the sources that contained segments on a particular topic.
Informedia will display representative icons for relevant segments, allowing the user to select interesting video paragraphs for playback.

We propose two Japanese-language information retrieval methods that enhance retrieval effectiveness by using relationships between words.
The first method uses dependency relationships between words in a sentence, while the second method uses proximity relationships, in particular the ordered co-occurrence information of words in a sentence as an approximation to the dependency relationships between them.
We construct these two methods on the Structured Index, which represents dependency relationships between words in a sentence as a set of binary trees.
Structured Index is created by morphological analysis, dependency analysis, and compound noun analysis.
We show the result of retrieval experiments using NTCIR–2, and discuss the effect of using relationships between words on Japanese information retrieval.

The TIPSTER collection is unusual because of both its size and detail.
In particular, it describes a set of information needs, as opposed to traditional queries.
These detailed representations of information need are an opportunity for research on different methods of formulating queries.
This paper describes several methods of constructing queries for the INQUERY information retrieval system, and then evaluates those methods on the TIPSTER document collection.
Both AdHoc and Routing query processing methods are evaluated.

We present a neural-network based selforganizing approach that enables visualization of the information retrieval while at the same time improving its precision.
In computer experiments, two-dimensional documentary maps in which queries and documents were mapped in topological order according to their similarities were created.
The ranking of the results retrieved using the maps was better than that of the results obtained using a conventional TFIDF method.
Furthermore, the precision of the proposed method was much higher than that of the conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall.

In many applications involving multi-media data, the definition of similarity between items is integral to several key tasks, e.g nearest-neighbor retrieval, classification, and recommendation.
Data in such regimes typically exhibits multiple modalities, such as acoustic and visual content of video.
Integrating such heterogeneous data to form a holistic similarity space is therefore a key challenge to be overcome in many real-world applications.
We present a novel multiple kernel learning technique for integrating heterogeneous data into a single, unified similarity space.
Our algorithm learns an optimal ensemble of kernel transformations which conform to measurements of human perceptual similarity, as expressed by relative comparisons.
To cope with the ubiquitous problems of subjectivity and inconsistency in multimedia similarity, we develop graph-based techniques to filter similarity measurements, resulting in a simplified and robust training procedure.

This paper describes the participation of MRIM team in Task 3: Patient-Centered Information Retrieval-IRTask 1: Ad-hoc search of CLEF eHealth Evaluation lab 2016.
The aim of this task is to evaluate the effectiveness of information retrieval systems when searching for health content on the web.
Our submission investigates the effectiveness of word embedding for query expansion in the health domain.
We experiment two variants of query expansion method using word embedding.
Our first run is a baseline system with default stopping and stemming.
The other two runs expand the queries using two different word embedding sources.
Our three runs are conducted on Terrier platform using Dirichlet language model.

How useful are topic models based on song lyrics for applications in music information retrieval?
Unsupervised topic models on text corpora are often difficult to interpret.
Based on a large collection of lyrics, we investigate how well automatically generated topics are related to manual topic annotations.
We propose to use the kurtosis metric to align unsupervised topics with a reference model of supervised topics.
This metric is well-suited for topic assessments, as it turns out to be more strongly correlated with manual topic quality scores than existing measures for semantic coherence.
We also show how it can be used for a detailed graphical topic quality assessment.

The following popers were presented at an open technical meeting* held by the ACM Computer Language Committee on Information Retrieval on October 20-21, 1961 at the RCA David Sarnoff Research Laboratory, Princeton, New Jersey.
This meeting was one in the series of ACM Special Interest Symposia, sponsored by a special interest group, limited to a special interest field, and approved by the ACM Program Committee.
Several of the papers presented and some of the introductory comments offered for discussion were published prior to the meeting, in the September 1961 issue of the Communications.

An information theoretic approach to security and privacy called Secure And Private Information Retrieval (SAPIR) is introduced.
SAPIR is applied to distributed data storage systems.
In this approach, random combinations of all contents are stored across the network.
Our coding approach is based on Random Linear Fountain (RLF) codes.
To retrieve a content, a group of servers collaborate with each other to form a Reconstruction Group (RG SAPIR achieves asymptotic perfect secrecy if at least one of the servers within an RG is not compromised.
Further, a Private Information Retrieval (PIR) scheme based on random queries is proposed.
The PIR approach ensures the users privately download their desired contents without the servers knowing about the requested contents indices.
The proposed scheme is adaptive and can provide privacy against a significant number of colluding servers.

Neda Kabiri, Ali Jannati, Nafiseh Vahed, Mina Mahami Oskouei.
Ph.D. Student in Health Policy, Iranian Center of Excellence in Health Management, School of Management and Medical Informatics, Tabriz University of Medical Sciences, Tabriz, Iran; Research Center for Evidence Based Medicine Iran (RCEBM Tabriz University of Medical Sciences, Tabriz, Iran; Associated Professor in Health Management, Iranian Center of Excellence in Health Management, School of Management and Medical Informatics, Tabriz University of Medical Sciences, Tabriz, Iran; M.Sc Medical Library Information Science, School of Management and Medical Informatics, Tabriz University of Medical Sciences, Tabriz, Iran;
M.Sc. of Medical Library Information Science,
Iranian Center of Excellence in Health Management; Department of Medical Library Information Science, Faculty of Management and Medical Informatics, University of Medical Sciences, TabrizIran.

We propose an online access panel to support the evaluation process of Interactive Information Retrieval (IIR) systems called IIRpanel.
By maintaining an online access panel with users of IIR systems we assume that the recurring effort to recruit participants for web-based as well as for lab studies can be minimized.
We target on using the online access panel not only for our own development processes but to open it for other interested researchers in the field of IIR.
In this paper we present the concept of IIRpanel as well as first implementation details.

The Internet of things has profoundly changed the way we imagine information science and architecture, and smart homes are an important part of this domain.
Created a decade ago, the few existing prototypes use technologies of the day, forcing designers to create centralized and costly architectures that raise some issues concerning reliability, scalability, and ease of access which cannot be tolerated in the context of assistance.
In this paper, we briefly introduce a new kind of architecture where the focus is placed on distribution.
More specifically, we respond to the first issue we encountered by proposing a lightweight and portable messaging protocol.
After running several tests, we observed a maximized bandwidth, whereby no packets were lost and good encryption was obtained.
These results tend to prove that our innovation may be employed in a real context of distribution with small entities.

We investigated two strategies for improving Information Retrieval thanks to incoming and outgoing citations.
We first started from settings that worked last year and established a baseline.
Then, we tried to rerank this run.
The incoming citations’ strategy was to compute the number of incoming citations in PubMed Central, and to boost the score of the articles that were the most cited.
The outgoing citations’ strategy was to promote the references of the retrieved documents.
Unfortunately, no significant improvement from the baseline was observed.

Information Dissemination applications are gaining increasing popularity due to dramatic improvements in communications bandwidth and ubiquity.
The sheer volume of data available necessitates the use of selective approaches to dissemination in order to avoid overwhelming users with unnecessaryinformation.
Existing mechanisms for selective dissemination typically rely on simple keyword matching or “bag of words” information retrieval techniques.
The advent of XML as a standard for information exchangeand the development of query languages for XML data enables the development of more sophisticated filtering mechanisms that take structure information into account.
We have developed several index organizations and search algorithms for performing efficient filtering of XML documents for large-scale information dissemination systems.
In this paper we describe these techniques and examine their performance across a range of document, workload, and scale scenarios.

The use of Vector Space Models (VSM) in the area of Information Retrieval is an established practice, thanks to its very clean and solid formalism that allows us to easily represent objects in a vector space and to perform calculations on them.
The goal of this work is to investigate the impact of VSM on Recommender Systems (RS) performance.
Specifically, we will introduce two approaches: the first is based on a dimensionality reduction technique called Random Indexing, while the second extends the previous one by integrating a negation operator implemented in the Semantic Vectors open-source package.
The results emerged from the experimental evaluation confirmed the predictive accuracy of the model.
This work summarizes the results already presented in the RecSys 2010 Doctoral Consortium.

Bug localization involves the use of information about a bug to assist in locating sections of code that must be modified to fix the bug.
Such a task can involve a considerable amount of time and effort on the part of software developers and/or maintainers.
Recently, several automated bug localization techniques based on information retrieval (IR) models have been developed to speed the process of bug localization.
Another code analysis technique involves locating duplicated sections of code in software projects, called code clones.
We examine the application of code clone location techniques in the context of bug localization.
We attempt to determine the advantages of extending existing code clone location techniques through the inclusion of IR models in the analysis process.
We also examine a technique for extending the use of bug logging repositories and version control systems by analyzing the two using IR techniques.

This Perspectives is the outgrowth of work begun at Maryland under the Informatics Task Force and its national and international advisory groups.
In a theoretical discussion of what information science can contribute to the health professions, the authors address questions of definition and describe application and knowledge models for the emerging profession of informatics.
A review of existing programs includes curriculum models and provides details on informatics programs emphasizing information and computer science; programs emphasizing the health sciences; and specialized informatics programs (undergraduate, master, and doctoral level Focus is placed on models for informatics program development.
The authors hope to build upon the database reported on in this article, and thereby foster the informatics education for the professions.

This paper describes the system used by the LIPN team in the Semantic Textual Similarity task at SemEval 2013.
It uses a support vector regression model, combining different text similarity measures that constitute the features.
These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similarity measure based on syntactic dependencies.

Authors: Heimonen Tomi Name of article: Mobile Findex: Facilitating Information Access in Mobile Web Search with Automatic Result Clustering Year of publication: 2008 Name of journal: Advances in Human-Computer Interaction Volume:
2008 Number of issue: 680640 Pages: 1-14 ISSN: 1687-5907 Discipline: Natural sciences Computer and information sciences Language:
en School/Other Unit: School of Information Sciences

Information that is available on the World Wide Web (WWW) is already more vast than can be comprehensibly studied by individuals and is increasing at a staggering pace.
Health consumerism is fuelled by knowledgeable patients.
A key to health consumerism is locating reliable health information on the WWW.
Unfortunately, much health information on the web is of suspect quality.
Differences between individual and clinician information seeking requirements are discussed.
An agent-based health Information Retrieval (IR) system is developed using validated existing technology.
The agent system satisfies the health information requirements for individuals.
A simulation of the system indicates a significant reduction in information overload and also high perceived reliability of information.

This paper describes a framework that integrates case-based reasoning capabilities in a BDI agent architecture as well as its application to the design of Web information retrieval agents.
The research proposed in this paper generates two key insights.
First, it shows that the integration of case-based reasoning in a BDI agent architecture is a non-trivial exercise that suggests interesting ways of building BDI agents with learning capabilities.
Second, it demonstrates the efficacy of the resulting framework by presenting the design of intelligent Web information retrieval agents that are effective in well-demarcated domains.

Learning to rank refers to machine learning techniques for training the model in a ranking task.
Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining.
Intensive studies have been conducted on the problem and significant progress has been made [1 2
This short paper gives an introduction to learning to rank, and it specifically explains the fundamental problems, existing approaches, and future work of learning to rank.
Several learning to rank methods using SVM techniques are described in details.
key words: learning to rank, information retrieval, natural language processing, SVM

The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding.
Relevant research areas include: contentbased multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic-web, MPEG-7 and MPEG-21 standards, user interfaces and human factors.
In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented.
These advances will be integrated in the SCHEMA module-based, expandable reference system.

ABSTRACT For the NTCIR Workshop 8 we organized a Geographic and Temporal Information Retrieval Task called “NTCIR
The focus of this task is on search with Geographic and Temporal constraints.
This overview describes the data collections (Japanese and English news stories topic development, assessment results and lessons learned from the NTCIR GeoTime task, which combines GIR with time-based search to find specific events in a multilingual collection.
Eight teams submitted Japanese runs (including unofficial three teams who provided runs to expand the pools) and six teams submitted English runs.
One team participated in both Japanese and English.

The field of Information Science is constantly changing.
Therefore, information scientists are required to regularly review and if necessary redefine its fundamental building blocks.
This article is one of a group of four articles, which resulted from a Critical Delphi study conducted in 2003-2005.
The study, Knowledge Map of Information Science, was aimed at exploring the foundations of information science.
The international panel was composed of 57 leading scholars from 16 countries, who represent (almost) all the major subfields and important aspects of the field.
This particular article documents 130 definitions of data, information, and knowledge formulated by 45 scholars, and maps the major conceptual approaches for defining these three key concepts.

The graph partitioning problem is as follows Given a graph G N E where N is a set of weighted nodes and E is a set of weighted edges and a positive integer p nd p subsets N N Np of N such that pi
Ni N and Ni
p i p where W i and W are the sums of the node weights in Ni and N respec tively the cut size i e the sum of weights of edges cross ing between subsets is minimized
This problem is of interest in areas such as VLSI placement and routing and e cient parallel implementations of nite element methods
In this survey we summarize the state of the art of sequential and parallel graph partitioning algorithms

Text categorization is one of the important steps of many applications, e.g. Web page classification, indexing in search engine and information retrieval.
When the number of documents available is huge, active learning could help relief the training time and cost.
Moreover, active learning is able to filter out noisy samples for training and therefore may achieve better generalization capability.
In this work, we adopt the localized generalization error model to active learning for text categorization.
In our approach, the samples yielding the highest generalization error for those unseen samples local to it is selected as the next training sample.
The feature extraction from raw documents is also discussed.
Experimental results show that the proposed method is effective in reducing the number of training samples and achieves good generalization capability

In conventional information retrieval (IR documents in a collection are indexed before the retrieval process, and the document collection is generally static, organised and homogeneous.
But, information on the Web is vast, dynamic, unorganised and heterogeneous.
Current search engines and IR systems on the Web are based on the conventional indexing approach and have limitations including the need for frequent update of the index.
We propose a non-indexing approach for information retrieval on the Web and show that the IR system based on this approach performs better than popular search engines.

XML Information Retrieval is approach to identify the appropriate answer granularity and controlling to elements overlap.
Recently, the demand for integrating Full Text Search and relational search has increased dramatically.
The RDBMS implementation is generally much worse in the performance than the IR engine implementation.
Especially, when a query is processed in the RDBMS, the number of join operation increases in proportion to the number of relationships in the query.
In order to solve these problems, we propose in this paper a novel approach to extend the inverted index for support query processing, namely Absolute Document XPath Indexing that allows supporting and reducing the length of time on Score Sharing scheme.
In terms of processing time, our system required an average of one second per topic on INEX-IEEE and an average of ten seconds per topic on INEX-Wiki better than GPX system.

Multimedia data by definition comprises several different types of content modalities.
Music specifically inherits e.g. audio at its core, text in the form of lyrics, images by means of album covers, or video in the form of music videos.
Yet, in many Music Information Retrieval applications, only the audio content is utilised.
Recent studies have shown the usefulness of incorporating other modalities; in most of them, textual information in the form of song lyrics or artist biographies, were employed.
Following this direction, the contribution of this paper is a large-scale evaluation of the combination of audio and text (lyrics) features for genre classification, on a database comprising over 20,000 songs.
We present the audio and lyrics features employed, and provide an in-depth discussion of the experimental results.

This paper proposes an Agent Community based Peer-to-Peer information retrieval method called ACP2P method, which uses agent communities to manage and look up information related to users.
An agent works as a delegate of its user and searches for information that the user wants by communicating with other agents.
The communication between agents is carried out in a peer-to-peer computing architecture.
In order to retrieve information related to a user query, an agent uses a content file, which consists of retrieved documents, and two histories a query/retrieved document history(Q/RDH) and a query/sender agent
history(Q/SAH
We implemented this method with Multi-Agents Kodama[1 and conducted preliminary experiments to test the hypothesis.
The empirical results showed that the method was much more efficient than a naive method employing 253;multicast&#253; techniques only to look up a target agent.

The 2-Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval.
The variables concerned are within-document term tkequency, document length, and within-query term frequency.
Simple weighting functions are developed, and tested on the TREC test collection.
Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.

In this paper we explore some of the most important areas of information retrieval.
In particular, Crosslingual Information Retrieval (CLIR) and Multilingual Information Retrieval (MLIR CLIR deals with asking questions in one language and retrieving documents in different language.
MLIR deals with asking questions in one or more languages and retrieving documents in one or more different languages.
With an increasingly globalized economy, the ability to find information in other languages is becoming a necessity.
We also presented the evaluation initiatives of information retrieval domain.
Finally we have presented the overall review of the research works in Indian and Foreign languages.

This paper addresses the problem of named entity recognition (NER) in travel-related search queries.
NER is an important step toward a richer understanding of user-generated inputs in information retrieval systems.
NER in queries is challenging due to minimal context and few structural clues.
NER in restricted-domain queries is useful in vertical search applications, for example following query classification in general search.
This paper describes an efficient machine learningbased solution for the high-quality extraction of semantic entities from query inputs in a restricted-domain information retrieval setting.
We apply a conditional random field (CRF) sequence model to travel-domain search queries and achieve high-accuracy results.
Our approach yields an overall F1 score of 86.4% on a heldout test set, outperforming a baseline score of 82.0% on a CRF with standard features.
The resulting NER classifier is currently in use in a real-life travel search engine.

The Kendall and AP rank correlation coefficients have become mainstream in Information Retrieval research for comparing the rankings of systems produced by two different evaluation conditions, such as different effectiveness measures or pool depths.
However, in this paper we focus on the expected rank correlation between the mean scores observed with a test collection and the true, unobservable means under the same conditions.
In particular, we propose statistical estimators of and AP correlations following both parametric and non-parametric approaches, and with special emphasis on small topic sets.
Through large scale simulation with TREC data, we study the error and bias of the estimators.
In general, such estimates of expected correlation with the true ranking may accompany the results reported from an evaluation experiment, as an easy to understand figure of reliability.
All the results in this paper are fully reproducible with data and code available online

Several computer file formats for musical notation and their storage and interchange have been developed in the world.
One of them, text-based MusicXML format is robust, and user friendly, designed for interchange, analysis, and performance of musical information.
It shows potential for being developed as a standard for musical notation interchange.
Musical themes could be represented in formal and graph models and used in music information retrieval, which is mostly based on XML platform.

The advent of modem information technology is significantly affecting many aspects of the system design process as well as the resulting user products.
One tendency seems to be in the direction of integrated work stations which aim at supporting the broad diversification of tasks which professional users have to cope with.
Therefore earlier efforts originating within the fields of “ergonomics” and “human factors” are undergoing a radical development in the direction of a serious concern with understanding and dealing with the basic interactions between people and their actual work situation.
Major areas include user modelling, task performance, decision making and user-system communications.
This concern has spawned an activity called cognitive engineering having the overall goal of providing good human-work interfaces and incorporating as a principle ingredient a cognitive task analysis (Pejtersen and Rasmussen 1986).

Cloud computing is a promising information technique (IT) that can organize a large amount of IT resources in an efficient and flexible manner.
Increasingly numerous companies plan to move their local data management systems to the cloud and store and manage their product information on cloud servers.
An accompanying challenge is how to protect the security of the commercially confidential data, while maintaining the ability to search the data.
In this paper, a privacy-preserving data search scheme is proposed, that can support both the identifier-based and feature-based product searches.
Specifically, two novel index trees are constructed and encrypted, that can be searched without knowing the plaintext data.
Analysis and simulation results demonstrate the security and efficiency of our scheme.

This paper presents the evaluation setting for the SemEval-2010 Word Sense Induction (WSI) task.
The setting of the SemEval-2007 WSI task consists of two evaluation schemes, i.e. unsupervised evaluation and supervised evaluation.
The first one evaluates WSI methods in a similar fashion to Information Retrieval exercises using F-Score.
However, F-Score suffers from the matching problem which does not allow 1) the assessment of the entire membership of clusters, and (2) the evaluation of all clusters in a given solution.
In this paper, we present the use of V-measure as a measure of objectively assessing WSI methods in an unsupervised setting, and we also suggest a small modification on the supervised evaluation.

Borko Furht Department of Computer Science and Engineering Florida Atlantic University, 777 Glades Road Boca Raton, FL 33431-0991, USA E-mail:
borko@cse.fau.edu Daniel Socek Department of Computer Science and Engineering Florida Atlantic University, 777 Glades Road Boca Raton, FL 33431-0991, USA E-mail:
dsocek@brain.math.fau.edu
Ahmet M. Eskicioglu Department of Computer and Information Science Brooklyn College of the City University of New York 2900 Bedford Avenue, Brooklyn, NY 11210, USA E-mail: eskicioglu@sci.brooklyn.cuny.edu

Medical information search refers to methodologies and technologies that seek to improve access to medical information archives via a process of information retrieval (IR Such information is now potentially accessible from many sources including the general web, social media, journal articles, and hospital records.
Health-related content is one of the most searched-for topics on the internet, and as such this is an important domain for IR research.
Medical information is of interest to a wide variety of users, including patients and their families, researchers, general practitioners and clinicians, and practitioners with specific expertise such as radiologists.
There are several dedicated services that seek to make this information more easily accessible, such as the ‘Health on the Net’ system for

With the rapid growth of online social networks, massive information are produced and shared daily.
Information retrieval in online social network is quite different with it in traditional web.
Users in online social network often set their data to a limit range of audience, which put forward a requirement of access control in search engine.
The access control issue is one of the main challenges since a generic model is still needed to characterize different access control strategies adopted in online social networks.
In this paper, we analysis different social network structures, and then propose two access control models: K-step Model and Inheritance Model.
Experiments show that the models are practically feasible in retrieval efficiency.

The ability to filter quantum states is a key capability in quantum information science and technology, where one-qubit filters, or polarizers, have found wide application.
Filtering on the basis of entanglement requires extension to multi-qubit filters with qubit-qubit interactions.
Such devices have many important applications to quantum technologies.

Probability models have been used in cross-modal multimedia information retrieval recently by building conjunctive models bridging the text and image components.
Previous studies have shown that cross-modal information retrieval system using the topic correlation model (TCM) outperforms state-of-the-art models in English corpus.
In this paper, we will focus on the Chinese language, which is different from western languages composed by alphabets.
Words and characters will be chosen as the basic structural units of Chinese, respectively.
We also set up a test database, named Ch-Wikipedia, in which documents with paired image and text are extracted fromChinese website ofWikipedia.
We investigate the problems of retrieving texts (ranked by semantic closeness) given an image query, and vice versa.
The capabilities of the TCM model is verified by experiments across the Ch-Wikipedia dataset.

Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval.
While other tasks, such as beat tracking and chord recognition have seen improvement with the application of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f0 and melody tracking, primarily due to the scarce availability of labeled data.
In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamental frequencies, trained using a large, semi-automatically generated f0 dataset.
We demonstrate the effectiveness of our model for learning salience representations for both multi-f0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f0 and melody datasets.
We conclude with directions for future research.

Authority-based approaches are widely used in expert retrieval from social media.
However, most of these approaches are applied to either topic-independent networks, or more topic-dependent networks which still contain topic-irrelevant users as nodes and interactions as edges.
Therefore, authority estimation over these graphs is still not topic-specific enough.
This paper proposes a more topic-focused authority network construction approach which provides more effective topic-specific authority modeling of users.
Focusing the computational effort to more topic-specific authority networks also leads to significant gains in running time for authority estimation.

Users of a given information retrieval system may have different requirements at various times (sometimes emphasizing precision, sometimes recall and the corpus may have different degree of emphasis regarding subjects.
Therefore, a general purpose system should have parameters to allow adjustable performance.
This article investigates the integration of several evaluation methods into a single system so that a user may choose an appropriate method or a combination of methods for his query evaluation.

The vast amount of music available electronically presents considerable challenges for information retrieval.
There is a need to annotate music items with descriptors in order to facilitate retrieval.
In this paper we present a process for determining the music genre of an item using a new set of descriptors.
A Wavelet Packet Transform is applied to obtain the signal representation at different levels.
Time and frequency features are extracted from these levels taking into account the nature of music.
Using <i>round-robin</i> and <i>
one-against-all</i> ensembles of simple classifiers, together with feature selection methods, we evaluate the best signal representation for music genre classification.
Ensembles based on different feature sub-spaces are explored as well in order to overcome over-fitting issues.
Our evaluation shows that Wavelet Packet analysis together with ensemble methods achieves very good classification accuracy.

The paper investigates the application of fuzzy logic based concept summarization and formal concept analysis in automatically building concept hierarchies from a text corpora.
The context of a term has been modeled using its syntactic relations with the most frequent verbs, which act as attributes.
This context information has been used to produce a concept lattice, which retains the concept hierarchies as well as the membership weights of the objects.
The concepts within each hierarchy have been summarized using a fuzzy logic based soft least upper bound approach.
An information retrieval model is proposed, which uses fuzzy formal concepts to get the relevance degree between the document and the query.
Results for ontology evaluation are shown on two domain ontologies.

Even though, one of the most famous technique to Similarity Information Retrieval are the Self-Organizing Maps (
SOM They can not answer questions like k-Nearest Neighbors easily.
This paper presents a new family of constructive SOM called SAM-SOM family which incorporates Spatial Access Methods to perform more specific queries like k-NN and range queries.
Using this family networks, the patterns have to be presented only once.
This approach speeds up dramatically the SOM training process with a minimal number of parameters.
Keywords— Constructive Self-Organizing Maps, Neural Networks, Spatial Access Methods, Stability, Plasticity.

This paper is concerned with the development of grammars suitable for full-text Information Retrieval.
It rst sets out some of the design criteria which should be taken into account in writing such a grammar.
Then the notation of AAx Grammars over a Finite Lattice (agfl) is described, a simple formalism for the morphosyntactic description of natural languages which has an ecient implementation.
It is shown how the agfl formalism helps in writing grammars with the properties demanded by Information Retrieval applications.
Within the agfl framework, grammars for English and Dutch have been written for Information Retrieval purposes, which are available for use by other research groups.
Some properties of those grammars and some experiences with their use are reported.

We organized a workshop at SIGIR'01 to explore the area of information retrieval techniques for speech applications.
Here we summarize the results of that workshop

Thank you for reading imageclef experimental evaluation in visual information retrieval.
As you may know, people have look numerous times for their chosen readings like this imageclef experimental evaluation in visual information retrieval, but end up in infectious downloads.
Rather than enjoying a good book with a cup of coffee in the afternoon, instead they cope with some infectious virus inside their laptop.

Relevance judgment has traditionally been considered a personal and subjective matter.
A user’s search and the search result are treated as an isolated event.
To consider the collaborative nature of information retrieval (IR) in a group/organization or even societal context, this article proposes a method that measures relevance based on group/peer consensus.
The method can be used in IR experiments.
In this method, the relevance of a document is decided by group consensus, or more specifically, by the number of users (or experiment participants) who retrieve it for the same search question.
The more users who retrieve it, the more relevant the document will be considered.
A user’s search performance can be measured by a relevance score based on this notion.
The article reports the results of an experiment using this method to compare the search performance of different types of users.
Related issues with the method and future directions are also discussed.

This paper is concerned with information retrieval in the context of supporting complex litigation by managing large numbers of documents.
It is shown that the application is sufficiently different from searching for case/statute text or reasoning with the law, so as to render the techniques developed for the latter inappropriate, A new approach to information representation and system design is identified and developed.
The paper presents an architecture that takes into account the peculiar characteristics of the application and enables the utilisation of existing skills of professionals, thereby facilitating rapid and consistent encoding.
An extended object-oriented paradigm underlies the architecture.
Using this paradigm, it has been possible to combine techniques developed for large databases with the purposive or functional similarity approach to search and retrieval taken in case-based design systems.

The creation of a labelled dataset for Information Retrieval (IR) purposes is a costly process.
For this reason, a mix of crowdsourcing and active learning approaches have been proposed in the literature in order to assess the relevance of documents of a collection given a particular query at an affordable cost.
In this paper, we present the design of the gamification of this interactive process that draws inspiration from recent works in the area of gamification for IR.
In particular, we focus on three main points: i) we want to create a set of relevance judgements with the least effort by human assessors, ii) we use interactive search interfaces that use game mechanics, iii)
we use Natural Language Processing (NLP) to collect different aspects of a query.

This paper introduces an expressive formal Information Retrieval model developed for the Web.
It is based on the Bayesian inference network model and views IR as an evidential reasoning process.
It supports the explicit combination of multiple Web document representations under a single framework.
Information extracted from the content of Web documents and derived from the analysis of the Web link structure is used as source of evidence in support of the ranking algorithm.
This content and link-based evidential information is utilised in the generation of the multiple Web document representations used in the combination.

This paper presents the design and implementation of an agent society within the con nes of IBM s Aglet advanced programming interface speci cation
The society exists with the sole purpose of nd ing MP s throughout a given network A sociohistorical context of the problem of nding MP s is provided The core design issues of agent tracking agent messaging social structure agent mobility methods and agent security are addressed and resolved An end user interface is es tablished along with a detailed discussion of design strategy

In this paper, we devise an information retrieval system which can filter and rank tweets according to relevance to the query.
We devise methods to understand relationships among entities and action verbs from a small set of manually annotated tweets.
We further use these relationships to filter tweets and rank them accordingly.
Our results (as published by FIRE Microblog Track) show that we have high precision score in detection of topmost 20 tweets.

With the increase in amount of data and information available on the web, there have been high demands on personalized information retrieval services to provide context-aware services for the web users.
This paper proposes a novel dynamic multi-agent context-awareness user profile construction method based on ontology to incorporate concepts and properties to model the user profile.
This method comprehensively considers the frequency and the specific of the concept in one document and its corresponding domain ontology to construct the user profile, based on which, a fuzzy c-means clustering method is adopted to cluster the user’s interest domain, and a dynamic update policy is adopted to continuously consider the change of the users’ interest.
The simulation result shows that along with the gradual perfection of the our user profile, our proposed system is better than traditional semantic based retrieval system in terms of the Recall Ratio and Precision Ratio.

Concept location, the problem of associating human oriented concepts with their counterpart solution domain concepts, is a fundamental problem that lies at the heart of software comprehension.
Recent research has attempted to alleviate the impact of the concept location problem through the application of methods drawn from the Information Retrieval (IR) community.
Here we present a new approach based on a complimentary IR method which also has a sound basis in cognitive theory.
We compare our approach to related work through an experiment and present our conclusions

Cyberinfrastructure, GIS, and spatial optimization: opportunities and challenges Wenwen Li, Kai Cao Richard L. Church To cite this article: Wenwen Li, Kai Cao Richard L. Church (2016)
Cyberinfrastructure, GIS, and spatial optimization: opportunities and challenges, International Journal of Geographical Information Science, 30:3, 427-431, DOI:
10.1080/13658816.2015.1112906
To link to this article: http dx.doi.org/10.1080/13658816.2015.1112906

In Cooper, part of the student support will be provided by a Question Answering application in the form of a webservice.
Question Answering allows a user to use the content of project document as input to find related documents as well as related experts.
Latent Semantic Analysis as an underlying technique is briefly discussed followed by a description of our Latent Semantic Analysis engine and the software architecture that was developed.
Issues for further development are also mentioned.
The final section contains a specific case study of an environment in which an implementation is planned.

OntoQuest is a physician decision support system that mines the hospital data base for previous decisions made in cases similar to the current one.
For example, OntoQuest displays a list of the medications prescribed to similar historical patients from which the physician may compare his choice of medication for the current patient.
This information retrieval is accomplished using ontological queries.
Unlike a regular database query, an ontological query is able to account for semantic similarity between patients.
To implement the ontological query, we propose a method for computing the ontological similarity between patients represented by sets of ICD-9 diagnoses.
We have tested the OntoQuest prototype on a pilot data set of 2077 patients.
Finally, we compare the OntoQuest performance to conventional database queries.
We believe that OntoQuest can be extended to compare services, quality and outcomes among patient and provider groups.

In many diagnosis-and-repair domains, diagnostic reasoning cannot be abstracted from repair actions, nor from actions necessary to obtain diagnostic information.
We call these exploratory-corrective domains.
In TraumAID 2.0, a consultation system for multiple trauma management, we have developed and implemented a framework for reasoning in such domains which integrates diagnostic reasoning with planning and action.
In this paper, we present Goal-Directed Diagnosis (GDD the diagnostic reasoning component of this framework.
Taking the view that a diagnosis is only worthwhile to the extent that it can affect subsequent decisions, GDD focuses on the formation of appropriate goals for its complementary planner.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report
No. MSCIS-92-81.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/296 More On Goal-Directed Diagnosis MS-CIS-92-81 LINC LAB 239

To deploy deep learning models for ad-hoc information retrieval, suitable representations of query-document pairs are needed.
Such representations ought to capture all relevant information required to assess the relevance of a document for a given query, including uni-gram term overlap as well as positional information such as proximity and term dependencies.
In this work, we investigate the use of similarity matrices that are able to encode such position-specific information.
Extensive experiments on Trec Web Track data confirm that such representations can yield good results.

As the number of non-English documents is increasing dramatically on the web nowadays, the study and design of information retrieval systems for these languages is very important.
The Persian language is the official language of Iran, Afghanistan and Tajikistan and is also spoken in some other countries in the Middle East, so there are significant amount of Persian documents available on the web.
In this study, we will present and compare our English-Persian cross language text retrieval experiments on Hamshahri text collection.
Also, we will present Combinatorial Translation Probability (CTP) calculation method for query translation that estimates translation probabilities based on the collection itself.

In this paper we concern with the automatic generation of document temporal metadata, and how to exploit it in current Information Retrieval and Topic Detection systems.
Specifically, we first propose a method to automatically detect the time references appearing in a document and to translate them into a formal time model.
Then, we describe an algorithm that constructs the event-time period of a document starting from its extracted time references.
Finally, we demonstrate through a clustering experiment that such generated event-time periods are as accurate as regarding all the time references of the document.
This greatly simplifies the representation of the document temporality and its management within IR and databases systems.

This paper presents an information retrieval approach which uses a rough concept clustering in conjunction with Latent Semantic Analysis(LSA) to provide better document retrieval results matched to queries.
The conceptual context defined in this article can be local, so no domain expert has to be involved in this approach.
Our experiment consists of word clustering by similarity and rough concept recognition, associated to a basic LSA retrieval system.
Our information retrieval process is illustrated through our experimentation model and results are compared in two different aspects.
Experiment results show that retrieval performance benefit can be gained from this approach and further performance benefits can also be obtained according to the further work, which needs researching about parameter settings and algorithm development.

Learning to rank has attracted great attention recently in both information retrieval and machine learning communities.
However, the lack of public dataset had stood in its way until the LETOR benchmark dataset (actually a group of three datasets) was released in the SIGIR 2007 workshop on Learning to Rank for Information Retrieval (LR4IR 2007
Since then, this dataset has been widely used in many learning to rank papers, and has greatly speeded up the corresponding research.
Recently, we released the latest version, LETOR3.0.
LETOR3.0 makes a lot of improvement over previous two versions, and it is more useful and reliable.
In this paper, we describe the details of datasets in LETOR3.0, including collection information, query sets and implementation of feature extraction.
We also benchmark several widely used learning to rank methods on LETOR3.0, illustrating the results of these methods, suggesting new directions for research, and providing baseline results for future study.

The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions.
A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word.
Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved.
The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit.
Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.

We present an approach to information retrieval based on context distance and morphology.
Context distance is a measure we use to assess the closeness of word meanings.
This context distance model measures semantic distances between words using the local contexts of words within a single document as well as the lexical co-occurrence information in the set of documents to be retrieved.
We also propose to integrate the context distance model with morphological analysis in determining word similarity so that the two can enhance each other.
Using the standard vector-space model, we evaluated the proposed method on a subset of TREC-4 corpus (AP88 and AP90 collection, 158,240 documents, 49 queries Results show that this method improves the 11-point average precision by 8.6%.

Information Retrieval is crucial in today’s Internet World.
System should have complete knowledge to provide the information to the user based on their query.
This paper explores the use of relations in information retrieval system for Tourism domain.
A relation provides syntactic and semantic information between pair of concepts.
Relation extraction captures the relationship between multiple concepts.
In this paper, concepts and binary relations have been identified for Tourism domain.
It extracts the relation between the concepts.
Our approach explores the new relationship even if it is not available.
The extracted information gives meaningful and complete information.
It also appends additional information into knowledge base.
Keywords-Information Retrieval, Reasoning, Tourism

In this paper we describe the participation of the Language Technologies Lab of INAOE at ImageCLEF 2016 teaser 1: Text Illustration (TI The goal of the TI task consists in finding the best image that describes a given document query.
For evaluating this task, there is a dataset containing web pages having text and images.
We address the TI as a purely Information Retrieval (IR) task, for a given document query we search for the most similar web pages and use the associated images to them as illustrations.
In this way, queries are used to retrieve related images from web pages, but the retrieval result are only the associated images.
For this, we represent the web pages and queries using state-of-the-art text representations.
Those representations incorporate information that allows us to exploit textual or semantic aspects.
According to ImageCLEF 2016 evaluation, the proposed approach holds the best performance for the TI task.

Semantic search has been one of the major envisioned benefits of the Semantic Web since its emergence in the late 90’s
Our demo shows a proposal towards this goal.
One way to view a semantic search engine is as a tool that gets formal queries (e.g. in RDQL, RQL, SPARQL, or the like) from a client, executes them against an ontologybased knowledge base, and returns tuples of ontology values (resources) that satisfy the query [2 While this conception of semantic search brings enormous advantages already, our work aims at taking a step beyond this.
In our view of Information Retrieval in the Semantic Web, a search engine returns documents, rather than (or in addition to) exact values, in response to user queries.
The engine should rank the documents, according to conceptbased relevance criteria.
The overall retrieval process is illustrated in Figure 1 (see [3] for more details of our research).

Utilising the Grid for conducting scientific research often involves a certain level of security risk, in particular when limited knowledge about the service provider is known before hand.
This paper reports our experience of integrating an Attack-Tolerant Information Retrieval (ATIR) service with Taverna, a popular workflow tool among the UK e-science community, to support secure information query in a biology context.
This paper presents the system architecture that has been used for the integration and the corresponding implementation details.
Performance studies show that the overhead of ATIR server side processing is trivial 5 compared with the total processing time of the integrated Taverna.
Our experimental results also show that the major processing overhead is caused by the Taverna enactor operations which consume no less than 50% of the total processing time.

With the Privacy issues drawing more and more concerns, privacy protection techniques based on Computational Private Information Retrieval (CPIR) allow a user to retrieve data from a service provider without revealing the users query information.
For large-scale applications, there exists a gap between privacy protection techniques and its feasibility.
In this paper, we propose an optimized CPIR-V based algorithm SCPIR-V for privacy protection nearest neighbor query which reduces the computational cost and communication cost efficiently.
Inclusion relation among the candidate data sets of nearest neighbor points are utilized to compress the matrix where data are stored, and new data structures and query algorithms are designed.
Compared to the existing work, the computation cost is reduced by 2-5 times and the communication cost by nearly 2 times.

This paper deals with Information Retrieval from audio-visual recordings.
Such recordings are often quite long and users may want to find the exact starting points of relevant passages they search for.
In Passage Retrieval, the recordings are automatically segmented into smaller parts, on which the standard retrieval techniques are applied.
In this paper, we discuss various techniques for segmentation of audio-visual recordings and focus on machine learning approaches which decide on segment boundaries based on various features combined in a decision-tree model.
Our experiments are carried out on the data used for the Search and Hyperlinking Task and Similar Segments in Social Speech Task of the MediaEval Benchmark 2013.

The growth of available online collections of documents comes with a growth in users’ expectations for retrieval services.
Contextual representations and interaction processes are becoming a real challenge to accurately navigate among large amounts of data.
This paper presents an intuitive framework for the creation of multilevel interactive maps of multimedia content.
Our framework allows the user to build interactive 2D representations of multimedia documents.
The visualization is based on a graph layout and filtering and generates maps through document clustering.
Users may interactively modify the map according to their criteria of interests, defining their own viewpoints on the collection.
They can then dig or surf depending on the interaction mode they choose.
Those functionalities allow them to navigate, browse, and analyze quickly and precisely sets of multimedia documents, allowing deep investigation of collections.

For many decades researchers in the domain of NLP (Natural Language Processing) and its applications like Machine Translation, Text Mining, Question Answering, Information Extraction and Information retrieval etc. have been posed with a challenging area of research i.e. WSD (Word Sense Disambiguation) WSD can be defined as the ability to correctly ascertain the meaning of a word, with reference to the context in which the word was used.
Linguistics, has defined context as the passage, sentence or text in which the word appears that is used for ascertaining its meaning.
Thus, context is dependent on the POS (Part Of Speech) where the word is used e.g. Adverb, Adjective, Pronoun, Verb and Noun.
In the following study we recommend a unique way of WSD based on Context through WordNet, multimodal algorithm that is knowledge based, map-reduce and soft sense WSD.

We compare different strategies to apply statistical machine translation techniques in order to retrieve documents which are a plausible translation of a given source document.
Finding the translated version of a document is a relevant task, for example, when building a corpus of parallel texts that can help to create and to evaluate new machine translation systems.
In contrast to the traditional settings in cross-language information retrieval tasks, in this case both the source and the target text are long and, thus, the procedure used to select what words or phrases will be included in the query has a key effect on the retrieval performance.
In the statistical approach explored here, both the probability of the translation and the relevance of the terms are taken into account in order to build an effective query.

In this paper we discuss how the Vector Space Model of Information Retrieval can be used in a new way by combining connectionist ideas about distributed representations with the concept of propositional structure (semantic case structure) derived from mainstream Natural Language Understanding research.
We show how distributed representations may be used to capture both amorphous concept representations and propositional structures and we discuss a prototype Information Retrieval system, PELICAN, which has been constructed in order to experiment with these ideaa.

The Agent-Community-based Peer-to-Peer Information Retrieval (ACP2P) method uses agent communities to manage and look up information of interest to users.
An agent works as a delegate of its user and searches for information that the user wants by communicating with other agents.
The communication between agents is carried out in a peer-to-peer computing architecture.
The ACP2P is implemented using the Multi-Agent Kodama framework.
This paper presents how the ACP2P method works in an agent community network and show the experimental results to illustrate the validity of this approach.

This paper reviews several evaluation measures developed for evaluating XML information retrieval (IR) systems.
We argue that these measures, some of which are currently in use by the INitiative for the Evaluation of XML Retrieval (INEX are complicated, hard to understand, and hard to explain to users of XML IR systems.
To show the value of keeping things simple, we report alternative evaluation results of official evaluation runs submitted to INEX 2004 using simple metrics, and show its value for INEX.

The size of a document archive is a very important parameter for resource selection in distributed information retrieval systems.
In this paper, we present a method for automatically detecting the size (i.e. number of documents) of a document archive, in case the archive itself does not provided such information.
In addition, a method for detecting the incremental change of the archive size is also presented, which can be useful for deciding if a resource description has become obsolete and needs to be regenerated.
An experimental evaluation of these methods shows that they provide quite accurate information.

By analyzing explicit &amp; implicit feedback information retrieval systems can determine topical relevance and tailor search criteria to the user's needs.
In this paper we investigate whether it is possible to infer what is relevant by observing user affective behaviour.
The sensory data employed range between facial expressions and peripheral physiological signals.
We extract a set of features from the signals and analyze the data using classification methods, such as SVM and KNN.
The results of our initial evaluation indicate that prediction of relevance is possible, to a certain extent, and implicit feedback models can benefit from taking into account user affective behavior.

In this extended abstract I examine the notion of <i>
noise</i> and its application to Information Retrieval and urge the reader to consider noise as an intrinsic property of information, not merely a problem to be eliminated.

Software Product Line has proven to be an effective methodology for developing a diversity of software products at lower costs, in shorter time, and with higher quality.
However, the adoption and maintenance of traceability in the context of product lines is considered a difficult task, due to the large number and heterogeneity of assets developed during product line engineering.
Furthermore, the manual creation and management of traceability relations is difficult, error-prone, time consuming and complex.
In this sense, Traceability Information Retrieval Tool (TIRT) was proposed in order to mitigate the maintenance traceability problem.
An experimental study was performed in order to identify the viability of the proposed tool and traceability scenarios.

Source code re-use has been usually faced from a compiler perspective.
Considering the source code as a piece of text, we are able to use natural language techniques for the detection of source code re-use.
This paper describes the use of ensemble models in the task of source code re-use detection.
Ensembles of Information Retrieval (IR) models are constructed using common classifiers.
The IR-inspired models are compared with the ensembles in C and Java programming languages.
The use of ensemble classifiers shows promising results for detecting source code re-use.

Relevance is one of the most important concepts in information retrieval.
This paper discusses various approaches for relevance calculation available in literature.
In Information Retrieval, relevance is how well a retrieved document or set of documents meets the information need.
The paper summarizes, what relevance is and a survey of different methods of calculating relevance.
Our goal is to study existing methods and to identify usefulness of existing methods for multilingual data and to understand the prerequisites required for these methods while using mime types likes webpages, urls, xml apart from plain text.
General Terms Information Retrieval, Pattern Recognition, Algorithms

Companies often receive thousands of resumes for each job posting and employ dedicated screeners to short list qualified applicants.
In this paper, we present PROSPECT, a decision support tool to help these screeners shortlist resumes efficiently.
Prospect mines resumes to extract salient aspects of candidate profiles like skills, experience in each skill, education details and past experience.
Extracted information is presented in the form of facets to aid recruiters in the task of screening.
We also employ Information Retrieval techniques to rank all applicants for a given job opening.
In our experiments we show that extracted information improves our ranking by <b>30 b> there by making screening task simpler and more efficient.

This article reports findings of empirical research that investigated information searchers' online query refinement process.
Prior studies have recognized the information specialists' role in helping searchers articulate and refine queries.
Using a semantic network and a Problem Behavior Graph to represent the online search process, our study revealed that searchers also refined their own queries in an online task environment.
The information retrieval system played a passive role in assisting online query refinement, which was, however, or that confirmed Taylor's four-level query formulation model.
Based on our empirical findings, we proposed using a process model to facilitate and improve query refinement in an online environment.
We believe incorporating this model into retrieval systems can result in the design of more 8220;intelligent&#8221; and useful information retrieval systems.

Chinese is written without using spaces or other word delimiters.
Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries.
Interpreting a text as a sequence of words is benecial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and keyphrase extraction.
We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression.
It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.
This simple and general method performs well with respect to specialized schemes for Chinese language segmentation.

Electronic patient health records encompass valuable information about patient’s medical problems, diagnoses, and treatments offered including their outcomes.
However, a problem for medical professionals is an ability to efficiently access the information that are documented in the form of free-text.
Therefore, the work presented here exhibits an information retrieval platform for efficient processing of e-health records.
The system offers facilities for keyword searches, semantic searches, and ontological searches.
An open evaluation during the TRECMED 2011 demonstrated competitive results.

This paper presents the results of an experimental study of some similarity measures used for both Information Retrieval and Document Clustering.
Our results indicate that the cosine similarity measure is superior than the other measures such as Jaccard measure, Euclidean measure that we tested.
Cosine Similarity measure is particularly better for text documents.
Previously these measures are compared with the conventional text datasets but the proposed system collects the datasets with the help of API and it returns the collection of XML pages.
These XML pages are parsed and filtered to get the web document datasets.
In this paper, we compare and analyze the effectiveness of these measures for these web document datasets.

The objective of this work is to determine if people are interacting in TV video by detecting whether they are looking at each other or not.
We determine both the temporal period of the interaction and also spatially localize the relevant people.
We make the following four contributions i) head detection with implicit coarse pose information (front, profile, back ii) continuous head pose estimation in unconstrained scenarios (TV video) using Gaussian process regression iii) propose and evaluate several methods for assessing whether and when pairs of people are looking at each other in a video shot; and (iv) introduce new ground truth annotation for this task, extending the TV human interactions dataset (Patron-Perez et al.
The performance of the methods is evaluated on this dataset, which consists of 300 video clips extracted from TV shows.
Despite the variety and difficulty of this video material, our best method obtains an average precision of 87.6 in a fully automatic manner.

Research in Information Retrieval has traditionally focused on serving the best results for a single query.
In practice however users often enter queries in sessions of reformulations.
The Sessions Track at TREC 2010 implements an initial experiment to evaluate the effectiveness of retrieval systems over single query reformulations.

The purpose of this study is to improve effectiveness of geotemporal information retrieval with semantic role labeling (SRL) for sentences in topics and documents, especially focusing on locational and temporal facets.
We propose a combination of four language models (LM) representing different semantic roles and scopes of models for documents and a rank aggregation method.
The rationale is based on observation that sentence-based language models using SRL retrieved relevant documents that are not ranked high by a general LM approach.
Although we did not get the comparison result between the general model and our proposed method from NTCIR-9 minutely, we obtained meaningful improvement with the NTCIR-8 GeoTime corpus.
Given that the current result is based on our initial effort under the time limitation, we believe that further exploration along the idea of using SRL would give a significant improvement in the geotemporal information retrieval.

Web retrieval is now one of the most important issues in computer science, and we believe that applying multi-agent systems to this area is a promising approach.
We introduce Kodama1 system, which is being developed and in use at Kyushu University, as a multi-agent-based approach to build a distributed Information Retrieval (IR) system that lets users retrieve relevant distributed information from the Web.
We reported methods to agentify the Web, and to cluster the agentified domain into communities.
In order to investigate the performance of our system, we carried out several experiments in multiple Server Agent domains and developed a smart query routing mechanism for routing the user’s query.
The results ensure that the idea of Web page agentification, clustering and routing techniques promise to achieve more relevant information.

Social networking sites are visibly the latest trend and have changed the way the 21st century communicates each other.
Its usage becomes more and more in the world of information, its presence in the library and information science becomes challenging and exciting.
The aim of this study was to know the trend of posting, sharing, commenting and like a Facebook post, to understand the participation level and response against Facebook group post, and also to explore the type of content generated by library and information science community through Facebook Group.
Group postings/contents of four different Facebook group pages (LIS Group, LIS-XPRESS, S.M.I.T. library students, students of LIS operating and use by LIS community during February 2012 February 2015 are collected, read and analysed.
The analysis of those contents found that sharing photos, links and statuses are major types of group posts in every group and members hardly posting videos and create events for the community.

Natural Language Processing (NLP) involves many phases of which the significant one is Word-sense disambiguation (WSD WSD includes the techniques of identifying a suitable meaning of words and sentences in a particular context by applying various computational procedures.
WSD is an Artificial Intelligence problem that needs resolution for ambiguity of words.
WSD is essential for many NLP applications like Machine Translation, Information Retrieval, Information Extraction and for many others.
The WSD techniques are mainly categorized into knowledge-based approaches, Machine Learning based approaches and hybrid approaches.
The assessment of WSD systems is discussed in this study and it includes comparisons of different WSD approaches in the context of Indian languages.

Automated information retrieval relies heavily on statistical regularities that emerge as terms are deposited to produce text.
This paper examines statistical patterns expected of a pair of terms that are semantically related to each other.
Guided by a conceptualization of the text generation process, we derive measures of how tightly two terms are semantically associated.
Our main objective is to probe whether such measures yield reasonable results.
Specifically, we examine how the tendency of a content bearing term to clump, as quantified by previously developed measures of term clumping, is influenced by the presence of other terms.
This approach allows us to present a toolkit from which a range of measures can be constructed.
As an illustration, one of several suggested measures is evaluated on a large text corpus built from an on-line encyclopedia.

Bug locating usually involves intensive search activities and incurs unpredictable cost of labor and time.
An issue of information retrieval on bug locations is particularly addressed to facilitate identifying bugs from software code.
In this paper, a novel bug retrieval approach with co-location shrinkage (CS) is proposed.
The proposed approach has been implemented in open-source software projects collected from real-world repositories, and consistently improves the retrieval accuracy of a state-of-the-art Support Vector Machine (SVM) model.

Knowledge discovery from text and ontology learning are relatively new fields.
However their usage is extended in many fields like Information Retrieval (IR) and its related domains.
Human Plausible Reasoning based (HPR) IR systems for example need a knowledge base as their underlying system which is currently made by hand.
In this paper we propose an architecture based on ontology learning methods to automatically generate the needed HPR knowledge base.
Keywords—Ontology Learning, Human Plausible Reasoning, knowledge extraction, knowledge representation.

I feel honored in inviting you, the learned participants, to take part in the 14th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2015) sponsored by the IEEE Computer Society and the International Association for Computer and Information Science
(ACIS ICIS is a forum that brings together researchers, scientists, engineers, industry practitioners, and students to present their original work, discuss, encourage and exchange new ideas, research results, and experiences on all aspects of Computer and Information Science.

Multiword units are groups of words that occur together more often than expected by chance in sub-languages.
Président de la République, Coupe du monde and Traité de Maastricht are multiword units.
Unfortunately, most of the machine-readable dictionaries contain clearly insufficient information about multiword units.
Therefore, their automatic extraction from corpora is an important issue not only for natural language processing but also for applications on Information Retrieval, Information Extraction and Machine Translation.
In this paper, we propose a new extraction system based on a new association measure, the Mutual Expectation, and a new acquisition process based on an algorithm of local maxima, the LocalMax algorithm.

Term weighting is a crucial task in many Information Retrieval applications.
Common approaches are based either on statistical or on natural language analysis.
In this paper, we present a new algorithm that capitalizes from the advantages of both the strategies.
In the proposed method, the weights are computed by a parametric function, called Context Function, that models the semantic influence exercised amongst the terms.
The Context Function is learned by examples, so that its implementation is mostly automatic.
The algorithm was successfully tested on a data set of crossword clues, which represent a case of Single-Word Question Answering.

A long-standing challenge in information retrieval is to disambiguate query words for more precise search results.
However, two or more meanings of a word in a query, or polysemy, deteriorate the precision effectiveness of information retrieval systems.
There is a need for correct and effective information retrieval in many information systems such as health care and customer relationship management.
This paper examines three topic language models that are mentioned in the literature for their ability to handle polysemy in query words.
The three topic lanauge models are--latent semantic analysis, probabilistic latent semantic analysis, and latent Dirichlet allocation.
We review these models and compare their performance in query disambiguation.
Our study provides guidance on the use of these models in information retrieval systems.

The combination of different text representations and search strategies has become a standard technique for improving the effectiveness of information retrieval.
combination, for example, has been studied extensively in the TREC evaluations and is the basis of the “meta-search” engines used on the Web.
This paper examines the development of this technique, including both experimental results and the retrieval models that have been proposed as formal frameworks for combination.
We show that combining approaches for information retrieval can be modeled as combining the outputs of multiple classifiers based on one or more representations, and that this simple model can provide explanations for many of the experimental results.
We also show that this view of combination is very similar to the inference net model, and that a new approach to retrieval based on language models supports combination and can be integrated with the inference net model.

Private information retrieval (PIR) is normally modeled as a game between two players: a user and a database.
The user wants to retrieve some item from the database without the latter learning which item.
Most current PIR protocols are ill-suited to provide PIR from a search engine or large database: i) their computational complexity is linear in the size of the database; ii) they assume active cooperation by the database server in the PIR protocol.
If the database cannot be assumed to cooperate, a peer-to-peer user community is a natural alternative to achieve some query anonymity: a user submits a query on behalf of another user in the community.
A peer-to-peer PIR system is described in this paper which relies on an underlying combinatorial structure to reduce the required key material and increase availability.

This paper describes the Corinna system which integrates a theoretical approach to dialogue modeling with text generation techniques to conduct cooperative dialogues in natural language.
It is shown how the dialogue model COR can be augmented by adding discourse relations as an additional level of description which is particularly valuable for the generation of dialogue acts.

In this work a brief overview of somefizzy approaches to deal with vagueness in Information Retrieval System is presented.
In particular some extensions of the documents' representation and of the query language are described.

In this paper we give a technical description of the PRIME system prototype.
PRIME allows us to provide an operational side to the theoretical work we have done in the “Modelling and Multimedia Information Retrieval MRIM) team.
PRIME is designed to provide a generic way to express the storing, manipulating and retrieval of multimedia data.
These tasks are separated into two parts, namely the strict database tasks and the information retrieval tasks.
This paper focus on this generic part, and we address more specifically the problem of managing and retrieving images.
We describe the implementation of a medical application managing Magnetic Resonance Images based on the generic core.

Frans van der Sluis Human Media Interaction, University of Twente P.O. Box 217, 7500AE Enschede, The Netherlands f.vandersluis@utwente.nl Betsy van Dijk Human Media Interaction, University of Twente P.O. Box 217, 7500AE Enschede, The Netherlands bvdijk@ewi.utwente.nl
Egon L. van den Broek Human Media Interaction, University of Twente P.O. Box 217, 7500AE Enschede, The Netherlands vandenbroek@acm.org

The Structured InformationManager (SIM) is an electronic documentmanagement system designed to provide access to multigigabyte document collections.
SIM was designed to provide both information retrieval and database capabilities.
Central to achieving this goal was the incorporationwithin SIM of international documentmanagement standards including SGML and Z39.50.
This paper describes the SIM architecture and the techniques used to support these standards.
It describes how a standard based approach can be used to support structured queries and presents the techniques used by SIM for query evaluation.

The query-performance prediction task is to estimate retrieval effectiveness with no relevance judgments.
retrieval prediction methods operate prior to retrieval time.
Hence, these predictors are often based on analyzing the query and the corpus upon which retrieval is performed.
We propose a em corpus-independent} approach to pre-retrieval prediction which relies on information extracted from Wikipedia.
Specifically, we present Wikipedia-based features that can attest to the effectiveness of retrieval performed in response to a query em regardless} of the corpus upon which search is performed.
Empirical evaluation demonstrates the merits of our approach.
As a case in point, integrating the Wikipedia-based features with state-of-the-art pre-retrieval predictors that analyze the corpus yields prediction quality that is consistently better than that of using the latter alone.

Much research has been performed investigating how links between web pages can be exploited in an Information Retrieval setting [1, 4
In this poster, we investigate the application of the Barabási-Albert model to link structure analysis on a collection of web documents within the language modeling framework.
Our model utilizes the web structure as described by a Scale Free Network and derives a document prior based on a web document’s age and linkage.
Preliminary experiments indicate the utility of our approach over other current link structure algorithms and warrants further research.

This paper presents a novel ranking framework for content-based multimedia information retrieval (CBMIR The framework introduces relevance features and a new ranking scheme.
Each relevance feature measures the relevance of an instance with respect to a profile of the targeted multimedia database.
We show that the task of CBMIR can be done more effectively using the relevance features than the original features.
Furthermore, additional performance gain is achieved by incorporating our new ranking scheme which modifies instance rankings based on the weighted average of relevance feature values.
Experiments on image and music databases validate the efficacy and efficiency of the proposed framework 2011 Elsevier Ltd.
All rights reserved.

This paper will explore the use of autoencoders for semantic hashing in the context of Information Retrieval.
This paper will summarize how to efficiently train an autoencoder in order to create meaningful and low-dimensional encodings of data.
This paper will demonstrate how computing and storing the closest encodings to an input query can help speed up search time and improve the quality of our search results.
The novel contributions of this paper involve using the representation of the data learned by an auto-encoder in order to augment our search query in various ways.
I present and evaluate the new gradient search augmentation (GSA) approach, as well as the more well-known pseudo-relevance-feedback (PRF) adjustment.
I find that GSA helps to improve the performance of the TF-IDF based information retrieval system, and PRF combined with GSA works best overall for the systems compared in this paper.

The paper presents the basic idea involved in information retrieval using fuzzy set theory and fuzzy logic.
It attempts to clarify the definition and some terms and discuss the possible sources of fuzziness within this sub-field.
It also tries to prove that how this way retrieving information is more realistic and expressive than their crisp counterparts.

Translation memories (TM) are widely used in the localization industry to improve consistency and speed of human translation.
Several approaches have been presented to integrate the bilingual translation units of TMs into statistical machine translation (SMT We present an extension of these approaches to the integration of partial matches found in a large, monolingual corpus in the target language, using cross-language information retrieval (CLIR) techniques.
We use locality-sensitive hashing (LSH) for efficient coarse-grained retrieval of match candidates, which are then filtered by finegrained fuzzy matching, and finally used to re-rank the n-best SMT output.
We show consistent and significant improvements over a state-of-the-art SMT system, across different domains and language pairs on tens of millions of sentences.

This paper presents a user-study showing the effectiveness of a linked-based, virtual integration infrastructure that gives users access to relevant online resources, and empowering users to design an information-seeking path that is specifically relevant to their context.
IntegraL provides a lightweight approach to improve and augment search functionality by dynamically generating context-focused “anchors” for recognized elements of interest generated by library services.
This paper includes a description of how IntegraL’s design supports users’ informationseeking behavior.
A full user study with both objective and subjective measures of IntegraL, and hypothesis testing regarding IntegraL’s effectiveness of the user’s information seeking experience, is described along with data analysis, implications arising from this kind of virtual integration, and possible future directions.

In this paper, we will employ a multi-agent for the crop production in a distributed environment.
We will use an Integrator Agent in the proposed model on the component integration.
The MIRA-CP(Mobile Information Retrieval Agent for Crop Production) will address the inadequacy of other data mining tools in processing performance and efficiency when use for knowledge discovery.
The Integrator Agent was developed based on CORBA architecture for search and extraction of data from heterogeneous servers in the distributed environment.
Our experiment shows that the MIRA generated essential association rules which can be practically explained for decision making purposes.
Keyword Mobil Agent, Component integration, Crop Production Information Retrieval, Crop Production Knowledge Discovering, Data Mining

Most geographic information retrieval systems depend on the detection and disambiguation of place names in documents, assuming that the documents with a specific geographic scope contain explicit place names in the text that are strongly related to the document scopes.
However, some non-geographic names such as companies, monuments or sport events, may also provide indirect relevant evidence that can significantly contribute to the assignment of geographic scopes to documents.
In this paper, we analyze the amount of implicit and explicit geographic evidence in newspaper documents, and measure its impact on geographic information retrieval by evaluating the performance of a retrieval system using the GeoCLEF evaluation data.

XML allows users to define elements using arbitrary words and organize them in a nested structure.
These features of XML offer both challenges and opportunities in information retrieval, document management, and data mining.
In this paper, we propose a new methodology for preparing XML documents for quantitative determination of similarity between XML documents by taking account of XML semantics (i.e meanings of the elements and nested structures of XML documents Accurate quantitative determination of similarity between XML documents provides an important basis for a variety of applications of XML document mining and processing.
Experiments with XML documents show that our methodology provides a 50-1 00% improvement in determining similarity, over the traditional vector-space model that considers only term-frequency and 100% accuracy in identibins the category of each document from an on-line bookstore.

World Wide Web (WWW) is information rich environment.
The access and document posted on WWW has rapidly grown since it was popularized in 1990s.
The abundance of information on WWW cause information overload problem, where users find hard to filter and select information return by the search system.
Moreover, the growth of new inexperienced users creates new challenges for information retrieval researchers.
It has become increasingly difficult for these users to find relevant documents that satisfying their information need.
This problem is due to the fact that this category of user is unable to express his exact need of information which leads to the use of inappropriate keywords or queries.
This paper presents a study of the user searching behavior on query formulation.
In this paper, query formulation is divided into two categories; breadth and depth query strategies.
The findings of this study show that breadth query strategy is the most popular strategy.

Fuzzy logic has been recognized as a useful approach for information retrieval.
It helps identify partially matched documents for a given query so that ranking the relevant documents becomes straightforward.
Unfortunately, research on fuzzy information retrieval only focuses on the estimation of relevancy of the retrieved documents, but lacks the estimation of imprecise probability of obtaining such relevant documents.
This paper makes an attempt to the latter.
It extends the fuzzy probability (FP) calculation method proposed by Huang and Shi to an enhanced FP calculation method.
Furthermore, it proposes a novel FP calculation method.
An example is provided to illustrate the use of fuzzy probability estimation.

Several methods have been proposed to evaluate queries over a native XML DBMS, where the queries specify both path and keyword constraints.
These broadly consist of graph traversal approaches, optimized with auxiliary structures known as structure indexes; and approaches based on information-retrieval style inverted lists.
We propose a strategy that combines the two forms of auxiliary indexes, and a query evaluation algorithm for branching path expressions based on this strategy.
Our technique is general and applicable for a wide range of choices of structure indexes and inverted list join algorithms.
Our experiments over the Niagara XML DBMS show the benefit of integrating the two forms of indexes.
We also consider algorithmic issues in evaluating path expression queries when the notion of relevance ranking is incorporated.
By integrating the above techniques with the Threshold Algorithm proposed by Fagin et al we obtain instance optimal algorithms to push down top k computation.

The Structured Audio Information Retrieval System (STAIRS) project targets environments where workers need access to information, but cannot use traditional hands-and-eyes devices, such as a PDA.
The information to be accessed is stored in an information base, either as pre-recorded audio or as text to be run through a text-to-speech engine.
Given the inherent limitations of the simple audio interface used in STAIRS, it is important to structure the information base in a way which makes navigation as easy as possible for the user.

Based on an analysis of the 377 documents that cited Griffith's publications in the ISI citation databases, it has been found that Griffith made pioneer and significant contributions with his collaborators to the fields of bibliometrics and scholarly communication among scientists.
His research work has also greatly influenced people from all over the world conducting research in psychology, bibliometric information science, and social studies of science in the past several decades.

Expertise search is a well-established field in information retrieval.
In recent years, the increasing availability of data enables accumulation of evidence of talent and expertise from a wide range of domains.
The availability of big data significantly benefits employers and recruiters.
By analyzing the massive amounts of structured and unstructured data, organizations may be able to find the exact skill sets and talent they need to grow their business.
The aim of this workshop is to provide a forum for industry and academia to discuss the recent progress in talent search and management, and how the use of big data and data-driven decision making can advance talent acquisition and human resource management.

A computer-based course addressing the topic of" applying Natural Language resources and techniques to Information Retrieval is presented.
The course provides several Internet on-line resources to support a learning by doing approach in a real world context.
Rationale for the design of the course is presented and a detailed description of the course structure and content is given.

The National Congress on Computing (ENC) has become one of the most important conferences at national level given that provides a forum of exposition of the advances in Computer Sciences and, at the same time, a gathering space of the related community.
On the ENC ’03 edition the Mexican scientific community proposed seven workshops: Advances in Databases and Information Retrieval, Mobile Computing, Reconfigurable Computing and FPGA’s, Logic and Agents, Learning Objects, Learning Machine and Software Engineering.

Separating the rib cages and lungs from other irrelevant structures in a given chest x-ray image is a major problem for the professionals of medical domain involved in chest x-rays.
To provide an algorithm and automation solution for separating chest x-ray structures especially rib cages and lungs are the main focus of this thesis.
An algorithm has been proposed using object recognition and template matching techniques of image processing domain.
The results of the proposed solution have been verified for different chest x-ray images and it works with complete accuracy and efficiency.
The proposed algorithm can be enhanced to accommodate the study of structures in chest x-ray image and report any inconsistencies.
The Journal of American Science.
2009;5(4):43-48 ISSN 1545-1003)

This article discusses the opportunities and challenges of applying modern information retrieval techniques to the cultural heritage domain.
Although the fi eld of information retrieval is closely associated with computer science, it originally emerged from library science also one of the main disciplines concerned with access to cultural heritage material.
Hence we are, in a sense, exploring what happens if we bring these strands of research back together again.
The article consists of three parts.
In the fi rst part, we explain the fi eld of information retrieval and its multidisciplinary nature.
In the second part, we discuss how and why the problem of providing access to cultural heritage can be cast naturally as an information retrieval problem.
In the third and main part, we present a detailed case study of applying the modern information retrieval approach in practice within a museum.

We establish the following, quite unexpected, result: replication of data for the computational Private Information Retrieval problem is not necessary.
More speciically, based on the quadratic resid-uosity assumption, we present a single database, computationally-private information-retrieval scheme with O(n) communication complexity for any 0.

Multilingual access is an important area of research, especially given the growth in multilingual users of online resources.
A large body of research exists for Cross-Language Information Retrieval (CLIR however, little of this work has considered the language skills of the end user, a critical factor in providing effective multilingual search functionality.
In this paper we describe an experiment carried out to further understand the effects of language skills on multilingual search.
Using the Google Translate service, we show that users have varied language skills that are non-trivial to assess and can impact their multilingual searching experience and search effectiveness.

Informativeness measures have been used in interactive information retrieval and automatic summarization evaluation.
Indeed, as opposed to adhoc retrieval, these two tasks cannot rely on the Cranfield evaluation paradigm in which retrieved documents are compared to static query relevance document lists.
In this paper, we explore the use of informativeness measures to evaluate adhoc task.
The advantage of the proposed evaluation framework is that it does not rely on an exhaustive reference and can be used in a changing environment in which new documents occur, and for which relevance has not been assessed.
We show that the correlation between the official system ranking and the informativeness measure is specifically high for most of the TREC adhoc tracks.

We focus on a property of natural language enabling the processing of information conveyed by linguistic expressions: structural asymmetry.
We provide evidence that structural asymmetry is a property of argument structure.
We focus on Information Retrieval and Question Answering systems and we provide evidence that these systems fail to recover natural language argument structure asymmetrical relations and thus they may fail to retrieve relevant documents from large databases and to provide relevant answers to questions.
The processing of the underlying asymmetric relations will contribute to the optimization of Information Retrieval and Question Answering systems.

Evaluation is one major step of an IR process.
It helps to classify and compare Information Retrieval (IR) systems according to their e ectiveness or their e ciency.
Many IR works carried di erent kinds of studies about evaluation methods and many evaluation measures have been proposed.
Most of these measures are based on the ranking of documents retrieved by IR systems in response to queries.
The retrieved documents are ranked according to their retrieval status values if these are monotonously increasing with the probability of relevance of documents.
However, few IR works tried to know more about these RSV and their possible use for IR evaluation.
In this work, we analyze di erent RSV computations and investigate the links between the RSVs and the IR systems evaluation.

In this paper, we describe a program of research designed to explore how a lexical semantic theory may be exploited for extracting information from corpora suitable for use in Information Retrieval applications.
Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the ~ultomatic construction of predictions about semantic relationships among words appearing in coltocatioz~al systems.
We illustrate the at)proach for the acquisition of lexical information for several classes of nominals.

Melody extraction from polyphonic audio, which consists of number of different instruments, is one of the most challenging tasks in the field of Music Information Retrieval (MIR This paper aims at implementation of extracting melody using two-way mismatch dual pitch tracking (TWMDPT) and harmonic cluster tracking (HCT) algorithms.
In this work, melody of singing voice is estimated in presence of multi-accompaniment.
In the first method temporal features of voice harmonics are used to find the voice pitch and it is based on salience function.
The second method depends upon strong higher harmonics for robustness against distortion by the first harmonic due to low frequency accompaniments.
Performance of these algorithms is evaluated on MIR-1k database and accuracy is estimated using Raw Pitch Accuracy (RPA Raw Chroma Accuracy (RCA) and timbre feature.

Usually, we can use a classification or clustering machine learning algorithm to manage knowledge and information retrieval.
If we have a small size of known information with a large scale of unknown data, a <i>semi-supervised learning (
SSL i> algorithm is often preferred.
Under the cluster or manifold assumption, usually, the larger amount of unlabeled data are used for learning, the bigger gains of the SSL approaches are achieved.
In the paper, we adopt the graph-based SSL algorithm to solve the problem.
However the graph-based SSL algorithms are unable to be learnt with large-scale unlabeled samples and originally can only work in a transductive setting.
In the paper, we propose a scalable graph-based SSL algorithm to attack the problems aforementioned by Gaussian mixture model label propagation.
Experiments conducted on the real dataset illustrate the effectiveness of the proposed algorithm.

As the information available in the internet increases in not only its quantity but also the variety of its written languages, that is, more and more documents provided through the internet are written in non-English languages, an imperative for access to information written in many languages is becoming increasingly apparent.
Crosslanguage information retrieval deals with the problem of information retrieval with documents and queries presented in di erent languages.
This paper considers crosslanguage information search on the Web.
A general CLIR model which is based on query translation method is proposed in a formal form and according to the model a wrapper interface for already existing Web search system can be easily constructed.
We focuses on the description of design details on CLIR interface construction among English, Japanese and Chinese.

This paper presents a new technique for determining the information associated to the nationality words.
The method makes use of a compressed search graph, namely directed acyclic word-graph (DAWG) and our introduced refinement technique.
The former allows us to search quickly a nationality word in only one time scanning.
The latter is introduced in our work because a DAWG cannot determine information to each key uniquely.
The refinement is a set of simple rules.
The if-parts are just character comparison of the terminal states of DAWG.
The then-part provides the linguistic values.
Our approach has been applied to the nationality words in English for being used via Internet in the task of Who is Who?

The SAIL-GRS system is based on a widely used approach originating from information retrieval and document indexing, the TF -IDF measure.
In this implementation for spoken dialogue system grammar induction, rule constituent frequency (CF and inverse rule frequency (IRF measures are used for estimating lexical and semantic similarity of candidate grammar rules to a seed set of rule pattern instances.
The performance of the system is evaluated for the English language in three different domains, travel, tourism and finance and in the travel domain, for Greek.
The simplicity of our approach makes it quite easy and fast to implement irrespective of language and domain.
The results show that the SAILGRS system performs quite well in all three domains and in both languages.

Visual information retrieval in images and video has been developing rapidly in our daily life and is an important research field in content-based information indexing and retrieval, automatic annotation and structuring of images.
Visual information system can make the use of relevance feedback so that the user progressively refines the search result by marking images in the result as relevant not relevant or neutral to the search query and then repeating the search with the new information.
With a comprehensive review as the main portion, this paper also suggested some novel solutions and perspectives throughout the discussion.
Introduce the concept of Negative bootstrap, opens up interesting avenues for future research.
KeywordsBootstrapping, CBIR (Content Based Image Retrieval Relevance feedback VIR (Visual Information Retrieval).

In this paper, we present a study based on the effect of using a fuzzy find matching technique with the objective of increasing the precision of textual information retrieval for image analysis while allowing for mismatches.
This technique is very helpful for searching those areas of interest where chances of misspelling are more likely, for example retrieving text information from an image.
The technique we propose can be used as an embedded component or a post-processing tool for image analysis resulting in a faster retrieval of corresponding information from images with given keywords.
In the case of no hits, our technique outcome would suggest few close matches that can be further searched in keywords to get a closer match with improved speed of searching.

Requirements traceability is linking requirements to software artifacts, such as source code, test-cases and configuration files.
For stakeholders of software, it is important to understand which requirements were tested, whether sufficiently, if at all.
Hence tracing requirements in test-cases is an important problem.
In this paper, we build on existing research and use features, realization of functional requirements in software [15 to automatically create requirements traceability links between requirements and test-cases.
We evaluate our approach on a chat system, Apache Pool [21] and Apache Log4j
[11 We obtain precision/recall levels of more than 90 an improvement upon currently existing Information Retrieval approaches when tested on the same case studies.

This paper addresses the problem of building expert interfaces to information retrieval systems.
In particular, the problem of augmenting the capabilities of such interfaces with user modeling features is discussed and the main benefits of this approach are outlined.
The paper presents a prototype system called IR-NLI II, devoted to model by means of artificial intelligence techniques the human intermediary to information retrieval systems.
The overall organization of the IR-NLI II system is presented, together with a short description of the two main modules implemented so far, namely the Information Retrieval Expert Subsystem and the User Modeling Subsystem.
An example of interaction with IR-NLI II is described.
Perspectives and future research directions are finally outlined.

Recommender systems aim at automatically providing objects related to user’s interests.
The angular stone of such systems is a way to identify documents to be recommended.
Indeed, the quality of these systems depends on the accuracy of its recommendation selection method.
Thus, the selection method should be carefully chosen in order to improve end-user satisfaction.
In this paper, we first compare two sets of approaches from the literature to underline that their results are significantly different.
We also provide the conclusion of a survey done by thirty four students showing that diversity is considered as important in recommendation lists.
Finally, we show that combining existing recommendation selection methods is a good mean to obtain diversity in recommendation lists.
Keywords-Information Retrieval; Recommender System; Document similarity; Diversity

In this paper we report on a series of experiments designed to investigate the combination of term and document weighting functions in Information Retrieval.
We describe a series of weighting functions, each of which is based on how information is used within documents and collections, and use these weighting functions in two types of experiments: one based on combination of evidence for ad-hoc retrieval, the other based on selective combination of evidence within a relevance feedback situation.
We discuss the difficulties involved in predicting good combinations of evidence for ad-hoc retrieval, and suggest the factors that may lead to the success or failure of combination.
We also demonstrate how, in a relevance feedback situation, the relevance assessments can provide a good indication of how evidence should be selected for query term weighting.
The use of relevance information to guide the combination process is shown to reduce the variability inherent in combination of evidence.

Arizona State University has created a new School of Computing and Informatics (SCI SCI currently consists of the Department of Computer Science and Engineering and the Department of Biomedical Informatics.
SCI was created to house the core computer science, computer engineering, software engineering, biomedical informatics and other computing and informatics programs and concentrations.
The informatics programs will serve as trans-disciplinary bridges to other disciplines.
One of the initial activities of the new school was the creation of an information sciences certificate.
This paper describes the process used in developing this certificate, the tradeoffs made and the detailed course requirements.
Lessons learned regarding how computer science departments can better collaborate with other disciplines on meeting computing educational needs will also be shared

This introduction to the special issue summarizes and contextualizes six novel research contributions at the intersection of information retrieval (IR) and crowdsourcing (also overlapping crowdsourcing’s closely-related sibling, human computation
Several of the papers included in this special issue represent deeper investigations into research topics for which earlier stages of the authors’ research were disseminated at crowdsourcing workshops at SIGIR and WSDM conferences, as well as at the NIST TREC conference.
Since the first proposed use of crowdsourcing for IR in 2008, interest in this area has quickly accelerated and led to three workshops, an ongoing NIST TREC track, and a great variety of published papers, talks, and tutorials.
We briefly summarize the area in order to help situate the contributions appearing in this special issue.
We also discuss some broader current trends and issues in crowdsourcing which bear upon its use in IR and other fields.

Geography Markup Language (GML) is becoming the de facto standard for electronic data exchange among the applications of Web and distributed geographic information systems.
However, the conventional query languages (e. g. SQL and its extended versions) are not suitable for direct querying and updating of GML documents.
Even the effective approaches working well with XML could not guarantee good results when applied to GML documents.
XQuery is a powerful standard query language for XML.
This study will investigate a query language specification to support spatial queries over GML documents by extending XQuery.
The data model, algebra, and formal semantics as well as various spatial functions and operations of this proposed query language are presented in detail.

A multi database model of distributed information retrieval is presented in which people are assumed to have access to many searchable text databases
In such an environment full text information retrieval consists of discovering database contents ranking databases by their expected ability to satisfy the query searching a small number of databases and merging results returned by di erent databases
This paper presents algorithms for each task It also discusses how to reorganize conventional test collections into multi database testbeds and evaluation methodologies for multi database experiments
A broad and diverse group of experimental results is presented to demonstrate that the algorithms are e ective e cient robust and scalable

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.

This paper introduces a novel vision for further enhanced Internet of Things services.
Based on a variety of data such as location data, ontology-backed search queries, in-and outdoor conditions the Prometheus framework is intended to support users with helpful recommendations and information preceding a search for context-aware data.
Adapted from artificial intelligence concepts, Prometheus proposes user-readjusted answers on umpteen conditions.
A number of potential Prometheus framework applications are illustrated.
Added value and possible future studies are discussed in the conclusion.

Two types of data are used in pattern recognition, object and relational data.
Object data is the most common type of data and is in the form of the usual data set of feature vectors.
Relational data is less common than object data and consists of the pairwise relations (similarities or dissimilarities) between each pair of implicit objects.
Such a relation is usually stored in a relation matrix and no other knowledge is available about the objects being clustered.
Because relational data is less common than object data, relational pattern recognition methods are not as well developed as their object counterparts, particularly in the area of robust clustering.
However, relational methods are becoming a necessity as relational data becomes more and more common.
For instance, information retrieval and data mining are all applications which could greatly benefit from pattern recognition methods that can deal with relational data.

The information explosion calls for adequate and efficient approaches to information retrieval.
Integrated information retrieval (IIR) in grid computing environment is becoming more and more attractive for integration and share of heterogeneous resource to provide users integrated retrieval services.
This paper proposes IIR service infrastructure on grid platform, GIIRS, which used Resource Description Framework (RDF) as data representation specification.
And we designed a query mechanism to implement IIR of heterogeneous and semi-structured web data.
The GIIRS can be easily deployed on grid platform and have feature of semantic interoperability.

This report explores the intellectual processes entailed during information seeking, as information needs are generated and information is sought and evaluated for relevance.
It focuses on the details of cognitive processing, reviewing a number of models.
In particular, Popper’s model of the communication process between an individual and new information is explored and elaborated from the perspective of Pask’s Conversation Theory.
The implications of this theory are discussed in relation to the development of what Cole has termed “enabling” information retrieval systems.

Conceptual Information Systems provide a multi-dimensional conceptually structured view on data stored in relational databases.
On restricting the expressiveness of the retrieval language, they allow the visualization of sets of related queries in conceptual hierarchies, hence supporting the search of something one does not have a precise description but only a vague idea of.
Information Retrieval is considered as the process of nding speciic objects (documents etc out of a large set of objects which t to some description.
In some data analysis and knowledge discovery applications, the dual task is of interest: The analyst needs to determine, for a subset of objects, a description for this subset.
In this paper we discuss how Conceptual Information Systems can be extended to support also the second task.

With a boom in the internet, social media text has been increasing day by day.
Much of the user generated content on internet is written in a very informal way.
Usually people tend to write text on social media using indigenous script.
To understand a script different from ours is a difficult task.
Moreover, nowadays queries received by the search engines are large number of transliterated text.
Hence providing a common platform to deal with the problem of transliterated text becomes really important.
This paper presents our approach to handle labeling of queries as part of the FIRE2015 shared task on Mixed-Script Information Retrieval.
Tokens in the query are labeled on basis of a hybrid approach which involves rule based and machine learning techniques.
Each annotation has been dealt separately but sequentially.

This paper provides an overview of the current version of SIGA, a system that supports the organization of information retrieval (IR) evaluations.
SIGA was recently used in Págico, an evaluation contest where both automatic and human participants competed to find answers to 150 topics in the Portuguese Wikipedia, and we describe its new capabilities in this context as well as provide preliminary results from Págico.

In this paper we present initial results of the ongoing project, building a Conversational Agent (chatterbot) Marvin.
Marvin is designed to simulate intelligent conversation with students of Information Sciences at the Faculty of Humanities and Social Sciences in Zagreb.
It is capable of providing basic feedback to students via textual methods.
The primary goal of this work is to inform the user of points of interest, to provide support, capture data from the user and promote study of information sciences.
Furthermore, the goal is to enhance the presentation of information to students, especially information regarding the undergraduate study, obligatory and elective courses, ECTS (European Credit Transfer System) points and exams.
Finally, the objective is to teach the students how to improve the quality of user experience using human-like conversational agents.

This paper integrates five linguistic resources, including Cilin, a Chinese-English dictionary, ASBC corpus, SemCor, and WordNet, to construct a Chinese-English WordNet.
The result is employed in Chinese-English information retrieval.
Under TREC-6 text collection, TREC topics 301-350, and Smart information retrieval system, the model for CLIR achieves 69.23% of monolingual information retrieval.
It also gains 10.02% increase relative to a model that resolves translation ambiguity and target polysemy together using a Chinese-English dictionary.

The first International Workshop on MAchine Learning and Information Retrieval for Software Evolution (MALIR-SE) was held on the 11th of November 2013.
The workshop was held in conjunction with the 28th IEEE/ACM International Conference on Automated Software Engineering (ASE) in Silicon Valley, California, USA.
The workshop brought researchers and practitioners that were interested in leveraging machine learning and information retrieval techniques to automate various software evolution tasks.
During the workshop, papers on the application of machine learning and information retrieval techniques to bug fix time prediction and anti-pattern detection were presented.
There were also discussions on the presented papers and on future direction of research in the area.

The partial success in inferring the characteristics of offenders from their criminal behaviour offender profiling has relied on limited data and subjective judgments.
We therefore sought to determine if Information Retrieval techniques and in particular Language Modelling could be applied directly to existing police digital records of criminal events to identify significant characteristics of offenders.
The categories selected were gender and age group.
Results showed that distinct differences in characteristics do exist.

Although many different visual information retrieval systems have been proposed, few have been tested, and where testing has been performed results were often inconclusive.
Further, there is very little evidence of benchmarking systems against a common standard.
An approach for testing novel interfaces is proposed which uses bottom-up, stepwise testing to allow evaluation of a visualization, itself, rather than restricting evaluation to the system instantiating it.
This approach not only makes it easier to control variables, but the tests are also easier to perform.
The methodology will be presented through a case study, where a new visualization technique is compared to more traditional ways of presenting data.

This paper presents a mechanism considering both browse and query for semantic information portals to retrieve information based on Knowledge Base.
our browse mechanism utilizes ontology query languages to let users browse freely, and a graphical query interface is proposed to facilitate users formulating ontology -based query.

Tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation.
A token is an instance of token a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.
New software tool and algorithm to support the IRS at tokenization process are presented.
Our proposed tool will filter out the three computer character Sequences: IP-Addresses, Web URLs, Date, and Email Addresses.
Our tool will use the pattern matching algorithms and filtration methods.
After this process, the IRS can start a new tokenization process on the new retrieved text which will be free of these sequences.
Keywords—Information Retrieval; Tokenization; pattern matching; and Sequences Filtration.

We review some of the scientific opportunities and technical challenges posed by the exploration of the large digital sky surveys, in the context of a Virtual Observatory (VO The VO paradigm will profoundly change the way observational astronomy is done.
Clustering analysis techniques can be used to discover samples of rare, unusual, or even previously unknown types of astronomical objects and phenomena.
Exploration of the previously poorly probed portions of the observable parameter space are especially promising.
We illustrate some of the possible types of studies with examples drawn from DPOSS; much more complex and interesting applications are forthcoming.
Development of the new tools needed for an efficient exploration of these vast data sets requires a synergy between astronomy and information sciences, with great potential returns for both fields.

Annotation of very large corpora is a complex and multifarious task, in the process of which the annotating teams are confronted with various aspects of linguistics and information science.
In the present paper, we discuss the attempts to balance the degree of automatic processing, the accuracy of the annotation and the requirements laid by the amount of annotated data as realized in the course of the annotation of the Prague Dependency Treebank (PDT We will focus on some points connected with the issues of the depth of annotation, of the robustness of the annotating scheme and the coverage.

For the space of two identical systems of arbitrary dimensions, we introduce a continuous family of bases with the following properties: i) the bases are orthonormal, ii) in each basis, all the states have the same values of entanglement, and iii) they continuously interpolate between the product basis and the maximally entangled basis.
The states thus constructed may find applications in many areas related to quantum information science including quantum cryptography, optimal Bell tests and investigation of enhancement of channel capacity due to entanglement.
PACS Number:
Corresponding author, email:vahid@sharif.edu email:laleh@mehr.sharif.edu

Two methods are given to improve weighting schemes by using relevance information of a set of queries.
The first method is to estimate parameter values of two independence models in information retrieval 8212; the binary independence model and the non-binary independence model.
The parameters estimated here are used to calculate optimal weights for terms in a different set of queries.
Performance of this estimation is compared to the inverse document frequency method, the cosine measure, and the statistical similarity measure.
The second method is to learn optimal weights of the non-binary independence model adaptively by a learning formula.
Experiments are performed on three different document collections CISI, MEDLARS, and CRN4NUL for both methods, and results are reported.
Both methods show improvements compared to the existing weighting schemes.
Experimental results show that the second method gives slightly better performance than the first one, and has simpler implementation.

The paper presents a GPS-based positioning method mainly used to field rescue.
The position and orientation of the rescuer and the trapped are acquired using GPS chip and digital compass.
Using the GPS data and compass data, the relative distance and orientation between them are calculated from the geometric relationships based on a series of formulas in Geographic Information Science (GIS Using this technology, we develop a solar-powered portable field-rescue device.
The experiments prove that the device can do accurate coordination via wireless communication, helping rescuers to search the trapped more accurately and quickly.

is the author of articles and books on linguistics, information retrieval, and in particular machine translation-many available from his website Abstract: This review of developments in machine translation (MT) covers the usage and development of computer aids and systems, production systems for large corporations, Internet aids for individuals, and the wide range of research activity in Europe and North America.

In this paper we present DelfosnetX, an Information Retrieval (IR) system intended to evaluate different relevance analysis and ranking techniques for metadataenabled IR, and more specifically, XML-based IR.
The theoretical background that supports the proposed model is also discussed here.

New general purpose ranking functions are discovered using genetic programming.
The TREC WSJ collection was chosen as a training set.
A baseline comparison function was chosen as the best of inner product, probability, cosine, and Okapi BM25.
An elitist genetic algorithm with a population size
100 was run 13 times for 100 generations and the best performing algorithms chosen from these.
The best learned functions, when evaluated against the best baseline function (BM25 demonstrate some significant performance differences, with improvements in mean average precision as high as 32% observed on one TREC collection not used in training.
In no test is BM25 shown to significantly outperform the best learned function.

This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique.
We first demonstrate, through the use of Monte Carlo techniques, that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability.
In order to further understand this result, additional deterministic experiments were performed.
The results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration.
Back Propagation is Sensitive to Initial Conditions John F. Kolen Jordan B. Pollack Laboratory for Artificial Intelligence Research Computer and Information Science Department
The Ohio State University Columbus, Ohio 43210, USA kolen-j@cis.ohio-state.edu, pollack@cis.ohio-state.edu

We describe an extension of ML with records where inheritance is given by ML generic polymorphism.
All operations on records introduced by Wand in [Wan87] are supported, in particular the unrestricted extension of a field, and other operations such as renaming of fields are added.
The solution relies on both an extension of ML, where the language of types is sorted and considered modulo equations [Rem9Ob and on a record extension of types
The solution is simple and modular and the type inference algorithm is efficient in practice.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-90-73.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/641 Type Inference For Records
In A Natural Extension Of ML MS-CIS-90-73 LOGIC COMPUTATION 24

When conducting research within a framework of Geographic Information Science (GISc the scientific validity of this work can be argued as highly dependent upon the extent to which the methods employed are reproducible, and that, in the strictest sense, can only be fully achieved by implementing transparent workflows that utilize both open source software and openly available data.
After considering the scientific implications of non-reproducible methods, we provide a review of both open source Geographic Information Systems (GIS) and openly available data, before describing an integrated model for Open GISc.
We conclude with a critical review of this embryonic paradigm, with directions for future development in supporting spatial data infrastructure.
ARTICLE HISTORY Received 8 November 2014 Accepted 29 December 2015

This paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain.
The data consists of the total of 1,508 real user queries in English translated to Czech, German, and French.
We describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system.

A genetic algorithm is described that models changing user interests in an on-line information retrieval support system.
User interests are represented as a population of queries which are issued to a World-Wide Web search engine.
User feedback on the queries and their results provides the fitness of each query.
A single point crossover operator recombines query terms and two mutation operators replace query terms with randomly selected words and their synonyms, respectively.
Initial user testing has shown that the GA approach is more effective in a simple learning task than a non-GA approach.

Code generation from hybrid system models, a promising approach for producing reliable embedded systems, has been our research focus for some time now.
In this report, we summarize the progress made thus far and provide directions for research towards realization of this goal.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
. MSCIS-05-03.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/6 Sound Code Generation from Hybrid System Models:
Some Theoretical Results Madhukar Anand, Jesung Kim, and Insup Lee Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 {anandm,jesung,lee}@saul.cis.upenn.edu

Information retrieval for geo-locatable places and services must be provided with the most according and personalized content to a user&#x02019;s information request, based this on his or her spatio-temporal situation.
One initiative that describes a set of geo-locable services is the Chefmoz project, in which different restaurants from the entire world are semantically described; in this paper is presented an approach that takes advantage of the information contained in Chefmoz in order to apply semantic Web technologies for the contextual and personalized query of services.
In order to this, a extended version for the FOAF vocabulary which describes the personal context of a user was used.
The precision and recall results for an experimental case are presented.
Finally the conclusions and future work is mentioned.

Today, to effectively meet the needs of users, multilingual information retrieval is an important issue regarding to the continually increasing volume of information in various languages.
This paper addresses indexing and retrieval in a trilingual corpus:
Arabic, French, English.
The proposed system is founded on a knowledge representation formalism, namely semantic graphs which supports a domain ontology.
Documents and queries are also represented in this formalism.
The domain ontology constitutes the kernel of the system and is used both for indexing and retrieval

The mankind entered the 21st century as a highly advanced information society.
Fast progress in science combined with the emergence and expansion of information and telecommunication technologies has caused that information has become a main production resource of human.
On the one hand, information is available for everybody, on the other hand, an enormous increase in the amount of electronic documents makes accessing this information very difficult or even impossible.
One of the possible ways out of this situation is to provide effective tools for information retrieval, which enable to find relevant information.
The rest of the paper presents the main models of information retrieval in a repository of textual documents, describes the concept of association list and its use for improving the quality of retrieval.
Moreover, it compares the models with an example of information retrieval in a repository of press notes.

In this paper I present my research goals and what I have obtained so far into my first year of PhD.
In particular this paper is about a novel architecture for closed domain question answering and a possible application in the cultural heritage context.
Unlike open domain question answering, which makes intensive use of Information Retrieval (IR) techniques, closed domain question answering systems might be built on top of a formal model with the possibility to apply formal logics and reasoning.
Natural language question answering poses some nontrivial problems to tackle.
We investigate such problems and propose some solutions based on AI techniques, picking the Cultural Heritage domain as a target application.

The major question raised in information retrieval on semi-structured documents relates to the manner of effectively handling the structure and the contents of the document for better answering the user's needs.
These needs can be formulated by queries composed of only key words or key words and structural constraints.
In this paper, we are interested in Information Retrieval in semi-structured document like XML.
For these purposes, we present a model for the semi-structured information retrieval, based on the possibilistic networks.
The document elements and elements terms relations are modelled by measures of possibility and necessity.
In this model, the user's query starts a process of propagation to recover documents or portions of documents necessarily or at least possibly relevant.
An example of such a research is proposed in order to illustrate the presented approach.

Extracting singing melody from audio signals is one essential component for musical information retrieval.
One can identify melody of different types of music easily even without professional training, but it is difficult for computer to achieve this goal.
In this paper, we present a new feature which provides not only a convenient way to observe music but also a tool to extract singing melody.
Also, we propose pitch likelihood, note and energy transition probabilities to perform Viterbi search.
The experimental results show that our method can extract singing melody accurately.

Digital Preservation (DP) has evolved into a specialized, interdisciplinary research discipline of its own, seeing significant increases in terms of research capacity, results, but also challenges.
With this specialization, however, core IT know-how that is needed to tackle the significant problems that we are facing in DP is not sufficiently present within the DP research community.
This paper outlines some research challenges in DP, highlighting the need for the DP community to reach out to IT research in general to jointly develop solutions.
It also shows some examples of integrating other computer science disciplines such as Information Retrieval Machine Learning, or Software Engineering, to address DP challenges, concluding with a brief overview of activities at the Department of Software Technology and Interactive Systems at the Vienna University of Technology in this domain.

This paper performs a profile based Information Retrieval from printed document image collections.
Keywords are valuable indexing tools and if they can be identified at the image level, extensive computation during recognition will be avoided.
Printed documents can be scanned to produce document images.
Instead of converting entire document images into text equivalent, word profiles are identified to match the word images in Bilingual document images.(English and
Tamil During retrieval, the same profile could be extracted from the user specified word and can be matched with the word images in the document.
This yields a faster result even in a quality-degraded document.
This kind of Information Retrieval (Keyword Based Search) can be adapted in Digital Libraries, which employs digitized documents instead of text processing.
This promotes efficient search in document images irrespective of the language.

This position paper argues that a user perspective is needed to focus mid-term research in Cross-Language Information Retrieval (CLIR and proposes different search scenarios that pose different challenges for CLIR researchers and represent plausible user needs.

Classical information retrieval (IR) methods often lose valuable information when aggregating weights, which may diminish the discriminating power between documents.
To cope with this problem, the paper presents an approach for ranking documents in IR, based on a vector-based ordering technique already considered in fuzzy logic for multiple criteria analysis purpose.
Moreover, the proposed approach uses a possibilistic framework for encoding the retrieval status values.
The approach, applied to a benchmark collection, has been shown to improve IR precision w.r.t.
classical approaches.

This paper provides a set of criteria to evaluate the usability of trader user interfaces with respect to resource discovery.
The criteria were identified because trader user interface developers sometimes make unrealistic assumptions about the information seeking behavior of their users, leading to interfaces that are difficult to use for resource discovery.
This paper uses some insights from the cognitive aspects of the information science field to justify the usability criteria.
The results of this paper are general enough to aid the developer of a user interface to any resource discovery tool.

Two methods defining a logic model for Information Retrieval are introduced.
The first one is based on the conditional logic as suggested by van Rijsbergen, while the second is based on the Nie's variations of the uncertainty principle of van Rijsbergen.
A Hypercard based system has been also designed and implemented to support small-scale experiments.
It has been used to evaluate the performance of the new model in comparison with the standard retrieval methods.
Though we imposed some simplifying assumptions to the model in order to have a workable system, the experimental results allow us to claim that the new methods are comparable to the already existing ones.

Arabic IR (Information Retrieval) has recently become a focus of research and commercial development.
Very few standards for evaluation of such tools are known and available.
A concrete evaluation for Arabic IR systems is necessary for the advancement of this field.
In this paper we discuss available resources for testing Arabic IR systems and we propose a strategy to accelerate the development and the reliable evaluation of systems that can than be exploited by a wide range of users of varied interests.

Information retrieval systems provide access to collections of thousands, or millions, of documents, from which, by providing an appropriate description, users can recover any one.
Typically, users iteratively refine the descriptions they provide to satisfy their needs, and retrieval systems can utilize user feedback on selected documents to indicate the accuracy of the description at any stage.
The style of description required from the user, and the way it is employed to search the document database, are consequences of the indexing method used for the collection.
The index may take different forms, from storing keywords with links to individual documents, to clustering documents under related topics.

I will consider the problem of helping user’s retrieve information from large diverse collections of textual materials such as bibliographic databases, electronic bulletin boards and online manuals, although I believe that many of my arguments apply to other interactive environments as well.
Information retrieval systems like this must handle a heterogeneous population of users, and even a single user will have a range of information needs including very specific requests as well as more general and often ill-defined topical searches.

With the proliferation of camera phones, new information retrieval applications will emerge.
The image of a scene captured by a camera phone can be a query to a remote server to identify the scene and return relevant information.
But unconstrained scene identification is an open problem.
In this paper, we propose a discriminative measure to rank image patterns sampled from target scene classes.
Support vector classifiers are then trained using top discriminative patterns for scene identification using voting.
We demonstrate our generic approach on two scene databases (ZuBuD and STOIC) with promising results

The major challenge for Question Retrieval (QR) in Community Question Answering (CQA) is the lexical gap between the queried question and the historical questions.
This paper proposes a novel Question-Answer Topic Model (QATM) to learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answer share the same topic distribution.
Experiments conducted on a real world CQA dataset from Yahoo!
Answers show that combining both parts properly can get more knowledge than each part or both parts in a simple mixing way and combining our QATM with the state-of-the-art translation-based language model, where the topic and translation information is learned from the question-answer pairs at two different grained semantic levels respectively, can significantly improve the QR performance.

In this review paper we classified and described measures and approaches for document relatedness evaluation.
For the reviewed measures we pointed out the reasons of their construction and usage limitations.
We concluded this research with a discourse on challenges of the day in estimating document appropriateness in the domain of information retrieval.

We present the innovative visual analytics approach of the VATE system, which eases and makes more effective the experimental evaluation process by introducing the what-if analysis.
The what-if analysis is aimed at estimating the possible effects of a modification to an IR system to select the most promising fixes before implementing them, thus saving a considerable amount of effort.
VATE builds on an analytical framework which models the behavior of the systems in order to make estimations, and integrates this analytical framework into a visual part which, via proper interaction and animations, receives input and provides feedback to the user.

This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation.
Matching algorithms and indexing methods are briefly presented.
The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections.
We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks.

This paper presents an unsupervised method for systematically identifying anomalies in music datasets.
The model integrates categorical regression and robust estimation techniques to infer anomalous scores in music clips.
When applied to a music genre recognition dataset, the new method is able to detect corrupted, distorted, or mislabeled audio samples based on commonly used features in music information retrieval.
The evaluation results show that the algorithm outperforms other anomaly detection methods and is capable of finding problematic samples identified by human experts.
The proposed method introduces a preliminary framework for anomaly detection in music data that can serve as a useful tool to improve data integrity in the future.

The increasing amount of information available for biomedical research has led to issues related to knowledge discovery in large collections of data.
Moreover, Information Retrieval techniques must consider heterogeneities present in databases, initially belonging to different domains-e.g. clinical and genetic data.
One of the goals, among others, of the ACGT European is to provide seamless and homogeneous access to integrated databases.
In this work, we describe an approach to overcome heterogeneities in identifiers inside queries.
We present an ontology classifying the most common identifier semantic heterogeneities, and a service that makes use of it to cope with the problem using the described approach.
Finally, we illustrate the solution by analysing a set of real queries.

This paper describes a NASA-funded project that has provided students in engineering technology, computer science, geographic information sciences, and mathematics at A&amp;M-CC with technical experience and ideas for graduate and undergraduate projects.
The project involves the development of a remotely-controlled, shallow-draft vehicle designed as a supplemental tool for our studies of the South Texas coastal waters.
The system transmits environmental data wirelessly via a radio to a control station in real-time.
The paper will describe several undergraduate projects resulting from students' involvement in this research project

We present the results of a study of user’s perception of relevance of documents.
The aim is to study experimentally how users’ perception varies depending on the form that retrieved documents are presented.
Documents retrieved in response to a query are presented to users in a variety of ways, from full text to a machine spoken query-biased automatically-generated summary, and the difference in users’ perception of relevance is studied.
The experimental results suggest that the effectiveness of advanced multimedia Information Retrieval applications may be affected by the low level of users’ perception of relevance of retrieved documents.

Solid-state quantum emitters, such as artificially engineered quantum dots or naturally occurring defects in solids, are being investigated for applications ranging from quantum information science and optoelectronics to biomedical imaging.
Recently, these same systems have also been studied from the perspective of nanoscale metrology.
In this letter we study the near-field optical properties of a diamond nanocrystal hosting a single nitrogen vacancy center.
We find that the nitrogen vacancy center is a sensitive probe of the surrounding electromagnetic mode structure.
We exploit this sensitivity to demonstrate nanoscale fluorescence lifetime imaging microscopy (FLIM) with a single nitrogen vacancy center by imaging the local density of states of an optical

Reading comprehension (RC or the capability to process document texts and answer questions about them is a difficult task for machines, as human language understanding and real-world knowledge are needed [4 This can serve a wide range of applications, from simplifying information retrieval processes to building more robust artificial intelligence.
Previously, most natural language processing was done with classical probabilistic models.
With the recent progress in deep learning, more researchers are switching to using neural networks as they are proven to produce better results.

Geo-Information Systems (GIS) are software for the handling and analysis of spatial data and are at the heart of Geo-Information Science (GISc Most GIS functions require interaction with human experts.
This is considered problematic since it causes extra complexity and increases the amount of resources required.
Agent Technology has the potential to assist in reducing this problem.
However the current application of software agents to the GIS domain is very limited and fails to take into account the full functionality and advantages of Agent Technology.
In this paper we discuss the application of agents in GIS and argue for the need to produce an agentbased framework for GIS.
By way of context we define a novel agent-based system for one important aspect of GIS, the construction of a variogram.
We quantitatively compare our architecture with two the existing agent-based tools: RePast for ArcGIS and Oracle Agents.

This paper describes the SINAI team participation in the ImageCLEFmed campaign.
The SINAI research group has participated in the multilingual image retrieval subtask.
The experiments accomplished are based on the integration of specific knowledge in the topics.
We have used the MeSH ontology to expand the queries.
The expansion consists in searching terms from the topic query in the MeSH ontology in order to add similar terms.
We have processed the set of collections using Information Gain (IG) in the same way as in ImageCLEFmed 2006.
In our experiments mixing visual and textual information we obtain better results than using only textual information.
The weigth of the textual information is very strong in this mixed strategy.
In the experiments with a low textual weight, the use of IG improves the results obtained.

This literature review provides an overview of the various topics related to using implicit feedback for information retrieval.
Specific topics which are addressed are explicit relevance feedback for query expansion and its advantages and disadvantages, user behaviors that provide implicit information to a system, techniques to implement implicit feedback in systems and the advantages and disadvantages of using implicit feedback in information retrieval systems.
For behaviors of users that were studied by researchers, this study provides a discussion on whether the researchers found them to be useful or not.
A discussion on the techniques with which these behaviors are implemented in systems by researchers is presented to gain an understanding of how these behaviors could be used in a system to interpret the behaviors of users.
It is concluded that information retrieval systems utilizing implicit feedback would help users find relevant documents.

Medical information is accessible from diverse sources including the general web, social media, journal articles, and hospital records; information searchers can be patients and their families, researchers, practitioners and clinicians.
Challenges in medical information retrieval include: diversity of users and user knowledge and expertise; variations in the format, reliability, and quality of biomedical and medical information; the multi-modal nature of much of the data; and the need for accuracy and reliability of medical information.
The aim of the workshop is to bring together researchers interested in medical information search with the goal of identifying specific challenges that need to be addressed to advance the state-of-the-art.

Informativeness measures have been used in interactive information retrieval and automatic summarization evaluation.
Indeed, as opposed to adhoc retrieval, these two tasks cannot rely on the Cranfield evaluation paradigm in which retrieved documents are compared to static query relevance document lists.
In this paper, we explore the use of informativeness measures to evaluate adhoc task.
The advantage of the proposed evaluation framework is that it does not rely on an exhaustive reference and can be used in a changing environment in which new documents occur, and for which relevance has not been assessed.
We show that the correlation between the official system ranking and the informativeness measure is specifically high for most of the TREC adhoc tracks.

Private Information Retrieval (PIR) is a well-known cryptographic primitive: When accessing some database records, the database server (as well as any external observer also seeing requests and replies) should not be able to learn which record has been requested.
In this paper we demonstrate a way to achieve PIR in the context of Information Centric Networking (ICN
Our solution is able to conceal access to single ICN data objects as well as larger collections, leaking only the number of requests while keeping names and the knowledge which information was fetched fully private.

Much of the information in science, engineering and business has been recorded in the form of text.
Traditionally, this information would appear in journals, but in the present decade increasingly it is noticed online in the World-Wide Web.
Tools to support information access and discovery on the Internet are proliferating at an astonishing rate.
Some of this development reflects real progress but there are also many exaggerated claims.
The focus of this paper is to emphasis Research on ontology which is becoming increasingly widespread predominately in the computer science community.
Its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction.

We have produced a test collection for machine translation (MT Our test collection includes approximately 2 000 000 sentence pairs in Japanese and English, which were extracted from patent documents and can be used to train and evaluate MT systems.
Our test collection also includes search topics for crosslingual information retrieval, to evaluate the contribution of MT to retrieving patent documents across languages.
We performed a task for MT at the NTCIR workshop and used our test collection to evaluate participating groups.
This paper describes scientific knowledge obtained through our task.

Since a big change of man-machine interaction mode has occurred in web era, information processing faces the challenge.
That is, information processing has had to deal with the semantic meaning of the information.
Classical information processing approaches only handle the form of information and are irrelevant to its semantic aspects.
The urgent affair is to establish a new strategy that can deal with the content of information based on the classical information processing method.
In the talk, the difficulty we faced recently and the future research direction in information processing are discussed, expressly; the multi-disciplinary study among neural science, cognitive science, psychology and information science is emphasized.

A new method to handle problems of Information Retrieval (IR) and related applications is proposed.
The method is based on Fuzzy Interval Numbers (FINs) introduced in fuzzy system applications.
Definition, interpretation and a computation algorithm of FINs are presented.
The frame of use FINs in IR is given.
An experiment showing the anticipated importance of these techniques in Cross Language Information Retrieval (CLIR) is presented..

Atanassov [1] introduced the concept of intuitionistic fuzzy relations and intuitionistic fuzzy graphs.
The theory of intuitionistic fuzzy sets has an exponential growth in Mathematics and its applications.
This ranges from traditional Mathematics to information sciences.
R.Parvathi and Karunambigai [2] are also studied the concept of intuitionistic fuzzy graphs and its properties
Some of the properties of intuitionistic fuzzy graphs are studied by A.NagoorGani and S.Shajitha begum [3
In this paper we introduce the concept of domination in fuzzy graphs.
For graph theoretic terminologies we refer (Harary 1969)

The generation of massive information makes the development and application of information retrieval technology become inevitable.
The construction of the inverted index can complete the information retrieval effectively, but it didn&#x02019;t consider the accuracy and quickness of the information retrieval.
The paper which based on the inverted technology put forward the aggregate addresses inverted index technology to complete the main design of the index
system;The use of the improved relevance ranking algorithm and the non-back multi-words matching algorithm complete the main design of the query system.
It can adjust the balance of the performance between index and query.
The results of system tested indicate that has better universality, higher performance and better scalability of the scale.

Nowadays, Web Information Retrieval (IR) Systems such as Google and Bing have become an essential part of using the Internet.
One of the main components of an IR system is the indexing module that builds an index file to speed up the searching process.
In this research project we aimed to evaluate the improvements of IR efficiency that results from encoding the terms of the index using Huffman code.
The results showed a reduction of terms size by 40% which reduces the overall index size, and consequently index transfer time.
The results also showed a reduction of the number of comparisons needed to process each query by 36% using binary search.
This approach reduces CPU-time used to process each query which increases the retrieval efficiency.

Retrieving answer containing passages is a challenging task in Question Answering.
In this paper, we describe a novel passage retrieval methodology using answer type profiles.
Our methodology includes two steps: estimation and ranking.
In the estimation step, answer type profiles are constructed from question-answer sentence pairs parallel corpus using a statistical alignment model.
Each answer type profile consists of triples: the query word, the answering sentence word and the probability of translation.
In the ranking step, answer type profiles are incorporated into the Language Modeling framework called Statistical Machine Translation models for Information Retrieval.
Using this framework a set of relevant passages are retrieved, given a question.
We conducted experiments on FACTOID questions from TREC 2002 to 2006 QA tracks.
The experimental results showed significant improvements over different retrieval models including TFIDF, Okapi BM25, Indri and KL-divergence.

In this paper we introduce a formalization of Logical Imaging applied to IR in terms of Quantum Theory through the use of an analogy between states of a quantum system and terms in text documents.
Our formalization relies upon the Schrodinger Picture, creating an analogy between the dynamics of a physical system and the kinematics of probabilities generated by Logical Imaging.
By using Quantum Theory, it is possible to model more precisely contextual information in a seamless and principled fashion within the Logical Imaging process.
While further work is needed to empirically validate this, the foundations for doing so are provided.

In this paper information retrieval system for local databases are discussed.
The approach is to search the web both semantically and syntactically.
The proposal handles the search queries related to the user who is interested in the focused results regarding a product with some specific characteristics.
The objective of the work will be to find and retrieve the accurate information from the available information warehouse which contains related data having common keywords.
This information retrieval system can eventually be used for accessing the internet also.
Accuracy in information retrieval that is achieving both high precision and recall is difficult.
So both semantic and syntactic search engine are compared for information retrieval using two parameters i.e. precision and recall.

Over the past several decades, quantum information science has emerged to seek answers to the question: can we gain some advantage by storing, transmitting and processing information encoded in systems that exhibit unique quantum properties?
Today it is understood that the answer is yes, and many research groups around the world are working towards the highly ambitious technological goal of building a quantum computer, which would dramatically improve computational power for particular tasks.
A number of physical systems, spanning much of modern physics, are being developed for quantum computation.
However, it remains unclear which technology, if any, will ultimately prove successful.
Here we describe the latest developments for each of the leading approaches and explain the major challenges for the future.

Text classification in Information Retrieval can be done by using a linear classifier.
Linear learning algorithms classify documents by learning a linear separator based on the document features.
Littlestone's Winnow is such as linear learning algorithm.
I have described three learning algorithms, based on Littlestone's Winnow, which can be applied to perform this task.
Modifications of the algorithms are described to make them more robust to certain properties of the documents.
These modifications result in robustness against variation in document length and improve the performance.
The variation of the Winnow algorithm with the extensions, performs very good compared to other existing algorithms as experimental results have shown.
In our own experiment, though, the results show differently.

Although several approaches for automated test case generation have been proposed over the last few years, automated test case generation for object oriented application based on their behavior is still in an infant stage.
This thesis proposed a new framework for automated test case generation from UML diagrams for object oriented applications based on the behavior.
This thesis implements this framework with the development of a tool integrates all types of automated test case generation from four UML diagrams (Class Diagram, Sequence diagram and Statechart Diagram and Use case Diagram) in an object oriented application.
Parsing the Petal files to generate the collaborative Test cases from these UML Diagrams using Pattern Discovery and Information Retrieval is discussed.

Domain ontologies have been widely acknowledged as fundamental in supporting knowledge-based applications.
To reduce the bottleneck of knowledge acquisition in the construction of domain ontologies text mining techniques have been used.
This paper proposes a methodology for building domain ontology for information science through extraction of knowledge from related literature and thesaurus.
It involves collecting domain-related vocabularies, developing the domain concept hierarchy, and defining the properties and relationships between concepts.
The core concepts in information science are selected by applying social network theory and the relationships between concepts are discovered by hierarchical clustering method.
Our methodology can be applied to other domain ontologies, such as computer science and management science.

Social bookmark tools are rapidly emerging on the Web.
In such systems users are setting up lightweight conceptual structures called folksonomies.
The reason for their immediate success is the fact that no specific skills are needed for participating.
At the moment, however, there exists no foundational research for these systems.
We present a formal model and a new search algorithm for folksonomies, called FolkRank, that exploits the structure of the folksonomy.
The proposed algorithm is also applied to find communities within the folksonomy and is used to structure search results.
All findings are demonstrated on a large

We propose a principled probabilisitc framework which uses trees over the vocabulary to capture similarities among terms in an information retrieval setting.
This allows the retrieval of documents based not just on occurrences of specific query terms, but also on similarities between terms (an effect similar to query expansion Additionally our principled generative model exhibits an effect similar to inverse document frequency.
We give encouraging experimental evidence of the superiority of the hierarchical Dirichlet tree compared to standard baselines.

Test collections are essential to evaluate Information Retrieval (IR) systems.
The relevance assessment set has been recognized as the key bottleneck in test collection building, especially on very large sized document collections.
This paper addresses the problem of efficiently selecting documents to be included in the assessment set.
We will show how machine learning techniques can fit this task.
This leads to smaller pools than traditional round robin pooling, thus reduces significantly the manual assessment workload.
Experimental results on TREC collections1 consistently demonstrate the effectiveness of our approach according to different evaluation criteria.

File structures and algorithms for multikey searching allow more than a single key to be used in locating a record for use in retrieval or update.
Such algorithms are of use in many different kinds of information systems, including database systems, information retrieval systems, and pattern recognition and image processing systems.
Such algorithms have received increased attention in recent years.
However, they are not as well understood as those which handle single keys.
Multikey algorithms are more difficult to evaluate than those based on the use of single keys.
There are simply more factors to be considered.
The evaluations performed for such algorithms should allow comparisons in order to be useful to a community of researchers and users.
Theoretical analyses should be based on reasonable and clearly stated assumptions.
Experiments should be repeatable and statistically valid whether they are based on "real" data or on randomly generated data.

This paper presents a model for classifying ICD-10 TM using machine learning and information retrieval.
The scope of this research take systematic approach for translating diagnosis from medical records to ICD-10 TM is proposed.
First, an information retrieval is used to find similarity word in Thai and English diagnose.
Then, machine learning approach is applied to classify ICD-10 TM by training models using Na&#x00EF;ve Bayes algorithm.
The result shows that our proposed approach can accurately classify ICD-10 TM in Thai-English diagnose at 81.41%.

A neural model is presented for implementing an associative mapping of highly multidimensional data, as encountered in information retrieval systems, on a structured set of half-axes.
The nonsupervised learning rule uses a local reconstitution of the data by the network and an analytical expression of the gain coefficient.
The author shows first that the model can be configured for a stochastic approximation of principal component analysis and correspondence analysis.
Then he discusses its application, in its associative mapping configuration, to real-world documentary databases.
Finally, an example of such an application is presented ETX>>

Using a multi-agent paradigm in an RFID-based health information system is a very promising approach in many domains such as emergency situations, supervising the medication of elderly people and even automatic diagnosis.
We propose a hybrid multi-agent system, which uses both mobile and static agents for retrieving various medical records of patients who undergo medical investigations in different medical units.
This paper presents an analytical and experimental evaluation of the algorithm applied in the information retrieval process, in order to reveal the parameters that play a role in the performance improvement of the hybrid multi-agent system.

a control target for an automatic control system and estimates feedback is considered.
An adaptive control system (ACS) which on each control step, except the criterion of control optimality, has the criterion to provide more information from human is offered.
The system adapts to combine them for achievement of optimality of several controls.
As an example of the ACS an information retrieval system (IRS) is offered.

This paper presents analyses of the 2006 and 2007 results of the Music Information Retrieval Evaluation eXchange (MIREX) Audio Cover Song Identification (ACS) tasks.
The Music Information Retrieval Evaluation eXchange (MIREX) is a community-based endeavor to scientifically evaluate music information retrieval (MIR) algorithms and techniques.
The ACS task was created to motivate MIR researchers to expand their notions of similarity beyond acoustic similarity to include the important idea that musical works retain their identity notwithstanding variations in style, genre, orchestration, rhythm or melodic ornamentation, etc.
A series of statistical analyses were performed that indicate significant improvements in this domain have been made over the course of 2006-2007.
Post-hoc analyses reveal distinct differences between individual systems and the effects of certain classes of queries on performance.
This paper discusses some of the techniques that show promise in this research domain

This paper first focuses on the studies the status quo of knowledge network based on paper analysis, then analyzes the connotation and extension of the notion of knowledge network put forward, and the authors bring forward its notion with different aspects in information science.
Finally, it makes further discussion and analysis about the characters of knowledge network.

Past research has identified many different types of relevance in information retrieval (IR So far, however, most evaluation of IR systems has been through batch experiments conducted with test collections containing only expert, topical relevance judgements.
Recently, there has been some movement away from this traditional approach towards interactive, more user-centred methods of evaluation.
However, these are expensive for evaluators in terms both of time and of resources.
This paper describes a new evaluation methodology, using a task-oriented test collection, which combines the advantages of traditional non-interactive testing with a more user-centred emphasis.
The main features of a task-oriented test collection are the adoption of the task, rather than the query, as the primary unit of evaluation and the naturalistic character of the relevance judgements.

We propose an approach to help resolve the “battle royale” between the information retrieval and information science communities.
The information retrieval side favors the Cranfield paradigm of batch evaluation, criticized by the information science side for its neglect of the user.
The information science side favors user studies, criticized by the information retrieval side for their scale and repeatability challenges.
Our approach aims to satisfy the primary concerns of both sides.

In vol. 6, 1976, of Advances in Librarianship, I published a review about relevance under the same title, without, of course Part I in the title (Saracevic, 1976 A substantively similar article was published in the Journal of the American Society for Information Science (Saracevic, 1975 I did not plan then to have another related review 30 years later—
but things happen.
The 1976 work attempted to trace the evolution of thinking on relevance, a key notion in information science and] to provide a framework

Web search engines, such as AltaVista and Infoseek, handle tremendous loads by exploiting the parallelism implicit in their tasks and using symmetric multiprocessors to support their services.
The web searching problem that they solve is a special case of the more general information retrieval (IR) problem of locating documents relevant to the information need of users.
In this paper, we investigate how to exploit a symmetric multiprocessor to build high performance IR servers.
Although the problem can be solved by throwing lots of CPU and disk resources at it, the important questions are how much of which hardware and what software structure is needed to effectively exploit hardware resources.
We have found, to our surprise, that in some cases adding hardware degrades performance rather than improves it.
We show that multiple threads are needed to fully utilize hardware resources.
Our investigation is based on InQuery, a state-of-the-art full-text information retrieval engine.

This paper suggests a semi-automatic ontology construction method based on hub words.
To do this, we define the words that are related to many other words as hub words.
We determine the hub words by the statistical analysis of documents.
Additionally, we discuss the base ontology construction process based on hub words and automatic ontology extension method.
The proposed ontology can be used like an index file for traditional document retrieval and can offer more semantic information than simple index files.

Temporal and spatial information in text documents is often expressed in a qualitative way.
Moreover, both are frequently affected by vagueness, calling for appropriate extensions of traditional frameworks for qualitative reasoning about time and space.
Our research aims at defining such extensions based on fuzzy set theory, and applying the resulting frameworks to two important kinds of intelligent information retrieval, viz.
temporal question answering and geographic information retrieval.

During the last decade many efforts for music information retrieval have been made utilizing Computational Intelligence methods.
Here, we examine the information capacity of the Dodecaphonic Trace Vector for composer classification and identification.
To this end, we utilize Probabilistic Neural Networks for the construction of a “similarity matrix” of different composers and analyze the Dodecaphonic Trace Vector’s ability to identify a composer through trained Feedforward Neural Networks.
The training procedure is based on classical gradient-based methods as well as on the Differential Evolution algorithm.
An experimental analysis on the pieces of seven classical composers is presented to gain insight about the most important strengths and weaknesses of the aforementioned approach.

The paper aims to achieve the legal question answering information retrieval (IR) task at Competition on Legal Information Extraction/Entailment (COLIEE) 2017.
Our proposal methodology for the task is to utilize deep neural network, natural language processing and word2vec.
The system was evaluated using training and testing data from the competition on legal information extraction/entailment (COLIEE Our system mainly focuses on giving relevant civil law articles for given bar exams.
The corpus of legal questions is drawn from Japanese Legal Bar exam queries.
We implemented a combined deep neural network with additional features NLP and word2vec to gain the corresponding civil law articles based on a given bar exam ’Yes/No’ questions.
This paper focuses on clustering words-withrelation in order to acquire relevant civil law articles.
All evaluation processes were done on the COLIEE 2017 training and test data set.
The experimental result shows a very promising result.

Requirement tracing is an important activity for its helpfulness to effective system quality assurance, impact analyzing of changes and software maintenance.
In this paper, we propose an automatic approach called LGRTL to recover traceability links between high-level requirements and low-level design elements.
This approach treats the recovery process as Bayesian classification process.
Meanwhile, we add a synonym process to the preprocessing phase, and improve the Bayesian model for performing better.
To evaluate the validity of the method, we perform a case study and the experimental results show that our method can enhance the effect to a certain extent.
Keywords-automatic requirement traceability;Bayesian
classifier;thesaurus;traceability links;information retrieval

Music artist (i.e singer) recognition is a challenging task in Music Information Retrieval (
The presence of different musical instruments, the diversity of music genres and singing techniques make the retrieval of artist-relevant information from a song difficult.
Many authors tried to address this problem by using complex features or hybrid systems.
In this paper, we propose new song-level timbre-related features that are built from frame-level MFCCs via so-called i-vectors.
We report artist recognition results with multiple classifiers such as K-nearest neighbor, Discriminant Analysis and Naive Bayes using these new features.
Our approach yields considerable improvements and outperforms existing methods.
We could achieve an 84.31% accuracy using MFCC features on a 20-classes artist recognition task.

This paper presents the Cross Language Information Retrieval (CLIR) experiments of the Language Technologies Research Centre (LTRC, IIIT-Hyderabad) as part of our participation in the ad-hoc track of CLEF 2007.
We present approaches to improve recall of query translation by handling morphological and spelling variations in source language keywords.
We also present experiments using query expansion in CLIR using a source language monolingual corpus for Hindi, Telugu and English.
We also present the effect of using an Oromo stemmer in Oromo-English CLIR system and report results using the CLEF 2007 dataset.

Interleaving is an online evaluation technique for comparing the relative quality of information retrieval functions by combining their result lists and tracking clicks.
A sequence of such algorithms have been proposed, each being shown to address problems in earlier algorithms.
In this paper, we formalize and generalize this process, while introducing a formal model: We identify a set of desirable properties for interleaving, then show that an interleaving algorithm can be obtained as the solution to an optimization problem within those constraints.
Our approach makes explicit the parameters of the algorithm, as well as assumptions about user behavior.
Further, we show that our approach leads to an unbiased and more efficient interleaving algorithm than any previous approach, using a novel log-based analysis of user search behavior.

Increasing availability of music data via Internet evokes demand for efficient search through music files.
Users' interests include melody tracking, harmonic structure analysis, timbre identification, and so on.
We visualize, in an illustrative example, why content based search is needed for music data and what difficulties must be overcame to build an intelligent music information retrieval system.

Research in Information Retrieval has traditionally focused on serving the best results for a single query.
In practice however users often enter queries in sessions of reformulations.
The Sessions Track at TREC 2010 implements an initial experiment to evaluate the effectiveness of retrieval systems over single query reformulations.

We introduce a system for information retrieval research in the peer-to-peer file-sharing domain.
Our system, IR-Wire, is based on the popular Gnutella protocol, giving us access to a large user base and a large data set.
As a search tool, IR-Wire maintains many statistics and implements a number of information retrieval ranking functions.
As a research tool, our main focus, IR-Wire contains a data logger and analyzer.
The data logger logs both incoming and outgoing queries and query results and provides a way to create a snapshot of the entire data set shared by the users.
The data analyzer provides a simple user interface for data analysis.
We briefly discuss an analysis conducted on a million incoming queries that were collected in our log files.

We make efforts to help the investigator discover the hidden conspirators.
In the criminal cases, the investigators or the police have to make full use of the messages or spoken documents data that they record in files.
Thus, mining the latent information from messages is vital to them.
In Information Retrieval area, Latent Semantic Analysis (LSA) is an important method for query matching which can discover the underlying semantic relation or similarity between words and topics.
We introduce a network hierarchical structure to analyze the original message network, making the analysis conveniently as well as ensuring the connectivity of the inner network connection of all the conspirators.
For this purpose, we use LSA to measure the similarities between topics and Crime Prototype Vector, and the similarities will be used as the weights of the paths in the network hierarchies and calculate the suspicious degrees.

Search engines retrieve relevant information when users know exactly what they are looking for.
The search is usually guided by relevant keywords provided by the user.
But the key words provided might belong to different domains and the search may end up giving results on domains where the users are not interested on.
This paper proposes a novel information retrieval prototype by providing a personalized search interface.
The interface is based on specific domain chosen by the user.
The effectiveness of the search is brought by fragmenting the search criterion into different parameters or the features of that domain.
The user is assisted to enter only domain specific keywords.
This helps to retrieve only the relevant information belonging to the chosen domain.
The user is also given the choice to select more than one search criteria to refine the search.

Today’s large-scale IR systems are not implemented using general-purpose database systems, as the latter tend to be significantly less efficient than custom-built IR engines.
This paper demonstrates how recent developments in hardwareconscious database architecture may however satisfy IR needs.
The advantage is flexibility of experimentation, as implementing a retrieval system on top of a DBMS boils down to relational query formulation, rather than system programming.
We demonstrate in the context of the TeraByte TREC efficiency task that our experimental MonetDB/X100 database system provides highly competitive results both regarding precision and speed.
We analyze the two innovations in MonetDB/X100 that most contributed to this successful application of DB technology in IR, namely vectorized incache processing and the use of two new light-weight compression schemes that work between the RAM and CPU cache memory levels.

In the InfoBeacons system, a peer-to-peer network of beacons cooperates to route queries to the best information sources.
Many internet sources are unwilling to provide more cooperation than simple searching to aid in the query routing.
We adapt techniques from information retrieval to deal with this lack of cooperation.
In particular, beacons determine how to route queries based on information cached from sources&#x00026;#8217; responses to queries.
In this paper, we examine alternative architectures for routing queries between beacons and to data sources.
We also examine how to improve the routing by probing sources in an informed way to learn about their content.
Results of experiments using a beacon network to search 2,500 information sources demonstrates the effectiveness of our system; for example, our techniques require contacting up to 71 percent fewer sources than existing peer-to-peer random walk techniques.

Compared to other aspects of education in information studies, little attention has been paid to the development of curricula that prepare information management and information systems students to work in the e-business and e-commerce sector.1 This paper presents results from a collaborative e-business planning assignment implemented on two e-business and e-commerce modules: an undergraduate Information Management in the Digital Economy module and a postgraduate E-Business and E-Commerce module.
The initiative was part of a teaching and learning development project called Managing Innovation in the Digital Economy (http www.wrce.org.uk/elbidshef.
htm funded by the White Rose Centre for Enterprise
(WRCE http www.wrce.
org.uk One of the anticipated outcomes of WRCE projects is the development of transferable learning materials that can be ABSTRACT

This hands-on ISMIR tutorial focuses on the free, open-source ChucK programming language for music analysis, synthesis, learning, and prototyping.
Our goal is to familiarize the ISMIR audience with ChucK’s new capabilities for MIR prototyping and real-time performance systems, and to stimulate discussion on future directions for language development and toolkit repository contents.built-in classifiers and implement their own.
We will discuss exciting issues in applying classification to realtime performance, including on-the-fly learning.

This paper addresses the problem of automatically analyzing and understanding human actions from video footage.
An x201C;action correlation&#x201D; framework, elastic sequence correlation (ESC is proposed to identify action subsequences from a database of (possibly long) video sequences that are similar to a given query video action clip.
In particular, we show that two well-known algorithms, namely approximate pattern matching in computer and information sciences and dynamic time warping (DTW) method in signal processing, are special cases of our ESC framework.
The proposed framework is applied to two important real-world applications: action pattern retrieval, as well as action segmentation and recognition, where, on average, its run time speed (in matlab) is about 3.3 frames per second.
In addition, comparing with the state-of-the-art algorithms on a number of challenging data sets, our approach is demonstrated to perform competitively.

Concept lattice, the core data structure in formal concept analysis, has been used widely in machine learning, data mining and knowledge discovery, information retrieval, etc.
The main difficulty with concept lattice-based system comes from the lattice construction itself.
In this paper, a parallel algorithm based on the closure search space partition for computing concepts is proposed.
This algorithm divides the closure search space into several subspaces in accordance with criterions prescribed ahead and introduces an efficient scheme to recognize the valid ones, in which the searching for closures is bounded.
An intermediate structure is employed to judge the validity of a subspace and compute closures more efficiently.
Since the searching in subspaces are independent tasks, a parallel algorithm based on search space partition can be directly reached.

Information retrieval is conceptually fundamental in human communication as well as in man-computer communication.
Computing and Information Retrieval professionals have the opportunity to apply information retrieval techniques within the second computer revolution to foster a new potential revolution in education, brought about by the advent of the personal computer.

The Transliterated Search track has been organized for the third year in FIRE-2015.
The track had three subtasks.
Subtask I was on language labeling of words in code-mixed text fragments; it was conducted for 8 Indian languages: Bangla, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, Telugu, mixed with English.
Subtask II was on ad-hoc retrieval of Hindi film lyrics, movie reviews and astrology documents, where both the queries and documents were either in Hindi written in Devanagari or in Roman transliterated form.
Subtask III was on transliterated question answering where the documents as well as questions were in Bangla script or Roman transliterated Bangla.
A total of 24 runs were submitted by 10 teams, of which 14 runs were for subtask
I and 10 runs for subtask II.
There were no participation for Subtask III.
The overview presents a comprehensive report of the subtasks, datasets, runs submitted and performances.

This paper reports on the evaluation of a system that allows the use of spoken queries to retrieve information from a textual document collection.
First, a large vocabulary continuous speech recognizer transcribes the spoken query into text.
Then, an information retrieval engine retrieves the documents relevant to that query.
The system works for Spanish language.
In order to increase performance, we proposed a two-pass approach based on dynamic adaptation of language models.
The system was evaluated using a standard IR test suite from CLEF.
Spoken queries were recorded by 10 di erent speakers.
Results showed that the proposed approach outperforms the baseline system: a relative gain in retrieval precision of 5.74 with a language model of 60,000 words.

This paper presents an approach based on Information Retrieval (IR) techniques for extracting and representing the unstructured information in large software systems such that it can be automatically combined with analysis of program dependencies and execution traces to define new techniques for feature location, impact analysis, and software measurement tasks.
We expect that these new techniques will contribute directly to the improvement of design of incremental changes and thus increased software quality and reduction of software maintenance costs.
The presented results are based on the author's doctoral dissertation [23].

In this work, we address the issue of exploiting the synergy between different modalities to facilitate Internet information retrieval.
It is well-known that information typically exists jointly in different modalities, and this is true for information in the Internet.
We intend to demonstrate that exploiting the synergy between the different modalities of the information may enhance the information retrieval efficiency and effectiveness.
Specifically, we focus on exploiting the cognitive synergy between text and imagery modalities.
Machine learning techniques are used to explicitly exploit this synergy in order to address the effective retrieval of the multimedia information represented in the two modalities.
We will demonstrate the proposed methodology through experimental data.

In fiscal 1994, we have proposed a cooperative information retrieval system as an example of cooperative problem solving system in the real world.
Then, we have implemented several cooperative agents of the cooperative information retrieval system and the proposed system has also been evaluated to establish primitive components of the above novel functions[1,2,3,4
In this report, we explain a basic model of a cooperative information retrieval system and an example of an agent to deal with heterogeneous knowledge in information retrieval.

Because the notion of context is multi-disciplinary
it encompasses lots of issues in Information Retrieval.
In this paper, we define the context as the information surrounding one document that is conveyed via the hypertext links.
We propose different measures depending on the information chosen to enrich a current document, in order to assess the impact of the contextual information on hypertext documents.
Experiments were made over the TREC-9 collections and significant improvement of the precision shows the importance of taking account of the contextual information.

Despite expanding opportunities and substantial financial incentives, women are significantly underrepresented in both the academic and professional environments of the information sciences.
Furthermore, there is much evidence that women experience a cumulative disadvantage, in computer terms, that begins early in life in the grade schools and continues through the college and corporate experience.
A qualitative survey–based approach was conducted in the study of 111 regional corporations.
It focuses on the ongoing strategies employed for the integration and retention of women in the computer information systems discipline at Robert Morris University, with an outlook and survey of the corporate computing environment of Pittsburgh and Southwestern Pennsylvania, and the gender effects of a decline in the occupational and educational dimensions of the discipline in the early years of the

Machine learning techniques have been considered a very promising solution to Web information retrieval, which is based on the ranking of the relevance of samples to a query input.
However, the connotation of labeling in ranking is quite different from that in classification.
Specifically, the labeling of samples for ranking is usually incomplete, i.e. only a part of samples are labeled.
In order to remedy this methodological gap, in this paper we propose a hybrid learning framework, called fuzzy-label learning, which consists of two layers.
First, we utilize a label-propagation algorithm to estimate those labels of unlabeled samples by their neighborhoods.
Second, we adopt RankBoost on the samples with fuzzy labels.
Experiments with five-fold cross-validation using the Letor benchmark datasets show that the proposed hybrid learning framework can definitively improve the search performance achieved by the RankBoost algorithm for Web information retrieval.

The Music Information Retrieval field has acknowledged the need for rigorous scientific evaluations for some time now.
Several efforts were set out to develop and provide the necessary infrastructure, technology and methodologies to carry out these evaluations, out of which the annual Music Information Retrieval Evaluation eXchange emerged.
The community as a whole has enormously gained from this evaluation forum, but very little attention has been paid to reliability and correctness issues.
From the standpoint of the analysis of experimental validity, this paper presents a survey of past meta-evaluation work in the context of Text Information Retrieval, arguing that the music community still needs to address various issues concerning the evaluation of music systems and the IR cycle, pointing out directions for further research and proposals in this line.

The hybridisation of different classification and mining techniques coming from different areas such as the numeric and the symbolic worlds can produce a significant enhancement of the overall classification and retrieval performance in a Data Mining or Information Retrieval context.
This paper introduces an experimental methodology to match an explicative structure issued from a symbolic classification to a numerical classification.
The classification models used in the experiment are a boolean lattice on the symbolic side and a Kohonen Self Organising Map model (SOM) on the numerical side.

Context-aware information is widely available in various ways and is becoming more and more important for enhancing retrieval performance and recommendation results.
The current main issue to cope with is not only recommending or retrieving the most relevant items and content, but defining them ad hoc.
Other relevant issues include personalizing and adapting the information and the way it is displayed to the user's current situation and interests The workshop on Context-awareness in Retrieval and Recommendation is a forum for research on context-awareness in information retrieval, recommender systems and human computer interaction.
The fourth iteration of the workshop was organized in conjunction to the 36th European Conference on Information Retrieval in Amsterdam, The Netherlands.

Text mining is the analysis of data contained in natural language text.
Text mining works by transposing words and phrases in unstructured data into numerical values which can then be linked with structured data in a database and analyzed with traditional data mining techniques.
Text information retrieval and data mining has thus become increasingly important.
In this paper a survey of Text mining have been presented.
Keyword: Information Retrieval, Information Extraction and Indexing Techniques

Vincent Larivière École de bibliothéconomie et des sciences de l’information, Université de Montréal, C.P. 6128, Succ.
Centre-Ville, Montréal, QC.
H3C 3J7, Canada; Cyberinfrastructure for Network Science Center, School of Library and Information Science, Indiana University, 10th St Jordan Ave Wells Library, Bloomington
, IN 47405, USA; Observatoire des sciences et des technologies (OST Centre interuniversitaire
de recherche sur la science et la technologie (CIRST Université du Québec à Montréal.
E-mail: vincent.lariviere@umontreal.ca

The proliferation of government information on local area networks and the Internet creates the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval or federated search A distributed information retrieval system is composed of three components: Resource representation, resource selection and result merging.
Previous research suggested that the CORI algorithm is one of the most effective resource selection algorithms, but its effectiveness in environments containing a wide range of database sizes was not studied thoroughly.
This paper shows that the CORI algorithm does not work well in environments with a skewed distribution of database sizes.
We present a new resource selection algorithm based on estimating the distribution of relevant documents among the online databases.
This new algorithm selects resources more accurately than the CORI algorithm, which can lead to improved document rankings.

Set expansion refers to expanding a given partial set of objects into a more complete set.
A well-known example system that does set expansion using the web is Google Sets.
In this paper, we propose a novel method for expanding sets of named entities.
The approach can be applied to semi-structured documents written in any markup language and in any human language.
We present experimental results on 36 benchmark sets in three languages, showing that our system is superior to Google Sets in terms of mean average precision.

Mobile location service of dynamic geographic information space application that supported school teachers and students is a currently a research hotspot for universities.
The applications of classroom navigation, teacher-student tracking monitoring, registration management, etc.
it will meet the school management which offers convenience for personal position information retrieval, recommend services and individualized maps constructed.
How to use mobile network to organically integrate LBS and WebGIS, and to construct a new generation of mobile information services platform and system environment based on the campus network location perception, this paper presents several key technologies of system framework and implementation.

The TIPSTER Architecture is a software architecture for providing Document Detection (i.e. Information Retrieval and Message Routing) and Information Extraction functions to text handling applications.
The high level architecture is described in an Architecture Design Document.
In May 1996, when the initial architecture design is complete, an Interface Control Document will be provided specifying the form and content of all inputs and outputs to the TIPSTER modules.

A relatively new avenue of Web-based information retrieval research, intended to semantically improve information extraction, is the idea of using geographical information to accurately locate resources.
This paper introduces a technique for locating sound and music files geographically.
It uses information extracted from the Web relating to audio resources and combines it with geospatial location data to provide new information about audio usage in various countries.
The results presented here illustrate the enormous potential for MIR to use the vast amount of audio materials on the Web within a physical and geographical context.
Statistics of audio usage around the world are provided, as well as examples of other applications of these techniques.

HIT-IR-WSD is a word sense disambiguation (WSD) system developed for English lexical sample task (Task 11) of Semeval 2007 by Information Retrieval Lab, Harbin Institute of Technology.
The system is based on a supervised method using an SVM classifier.
Multi-resources including words in the surrounding context, the partof-speech of neighboring words, collocations and syntactic relations are used.
The final micro-avg raw score achieves 81.9% on the test set, the best one among participating runs.

Information Retrieval (IR) technology helps people to search relevant and necessary documents from massive digital database.
In the context of e-law enforcement, it can be used to find verses related to a certain topic and presenting the relationship between one verse to the others.
In this study we propose an intelligent searching system for Indonesian law documents which is enhanced by association analysis to discover the association of law related keywords, thus providing guidance for the user to find related verses.

Participation is today central to many kinds of research and design practice in information studies and beyond.
From user-generated content to crowdsourcing to peer production to fan fiction to citizen science, the concept remains both unexamined and heterogeneous in its definition.
Intuitions about participation are confirmed by some examples, but scandalized by others, and it is difficult to pinpoint why participation seems to be robust in some cases and partial in others.
In this paper we offer an empirically based, comparative analysis of participation that demonstrates its multidimensionality and provides a framework that allows clear distinctions and better analyses of the role of participation.
We derive 7 dimensions of participations from the literature on participation and exemplify those dimensions using a set of 102 cases of contemporary participation that include uses of the Internet and new media.

We adopt the generalized Pareto distribution for the information-based model and show that the parameters can be estimated based on the mean excess function.
The proposed information retrieval model corresponds to the extension of the divergence from independence and is designed to be data-driven.
The proposed model is then applied to the specific object search called the instance search and the effectiveness is experimentally confirmed.

In this paper the IRT project (Internet Information Retrieval Tools) is described.
The basic goal of IRT is to advise users of Internet search engines in retrieving information from the free public access part of the Internet.
In achieving this, IRT has developed a model to evaluate search engines.
This model is described in here.
Evaluation criteria refer to functionality: search options, presentation characteristics and indexing characteristics (which elements of a Web document are indexed
Also evaluated is the consistency of retrieval through search engines.
This model has been tested in the period October December 1998 on six of the major search engines.
We found many differences among Internet indexes in their functionality, as well as in their consistency and reliability.

The mobile search space has witnessed phenomenal growth in recent years.
As a result there has been a growing body of research aimed at understanding why and how mobile users search the Web via their handsets and how their mobile search experiences could be improved.
However, much of this work has focused on addressing the many challenges of the mobile space.
In this short position paper argue the need for more casual, shared, social mobile search experiences.
We outline a number of open and challenging research questions related to shared, social mobile search.
Finally, we present our ideas through a proof-ofconcept mobile paper prototype designed to support causal mobile search and information sharing with co-located groups of friends.

The development of artificial intelligence provides a platform for the enterprise to information management.
As an intelligent cost-based service industry for engineering consulting firms, information management has became the core competitiveness for enterprise development.
This paper analysis the requirements and functions of management information system of engineering cost consulting enterprises, and applied the mind of case-based to information management of engineering cost consulting enterprises, explore how to establish various information retrieval functional systems on the basis of similarity analysis, and combining this method to achieve the function of clustering and retrieval, and analyze problems and countermeasures of the system in practical application.

Based on the rapid development of Chinese agricultures, many English users are interesting on the Chinese agricultural information, and many Chinese users are interesting on English agricultural information too.
This paper is a schema to design an agricultural cross languages engine, the core technology is the mapping between Chinese and English agricultural thesauri.
The paper introduces the all rules of thesauri mapping, and give exact examples for these rules.
With the mapping information, authors design a cross languages engine.
English users can get Chinese agricultural information from web data by English descriptors; Chinese users can get English agricultural information from web data by Chinese descriptors

With the fast booming of online music repositories, the problem of building music recommendation systems is of great importance.
There is an increasing need for content-based automatic indexing to help users find their favorite music objects.
In this work, we propose a new method for automatic classification of musical instruments.
We use a unique set of timbre related descriptors, extracted on polyphonic sounds (multiple distinct instruments) in combination with data mining classifiers.
We report high classification accuracy.

Ohio State's school year is divided into four quarters, each ten weeks long.
It is possible to schedule classes to meet three, four, or five hours per week
The latter with 50 class meetings is similar to a three semester hour course Most students attend classes for three of the four quarters.
Because the amount of material covered in a programming class is proportional to the number and quality of programs completed, we have chosen a two quarter sequence, each course meeting three days a week.
The students receive four quarter hours credit (one hour of laboratory credit) for each course.
The students in this sequence are mostly computer and information science majors, but there are also many mathematics and some science and engineering majors.
There are other courses for administrative science, engineering and other non-technical students.
All the introductory courses are taught by graduate students under the supervision of a faculty member.

With the frequent occurrence of various types of emergencies, construct an emergency GIS system has become a social system project.
A large number of spatial data accumulated is stored in distributed heterogeneous GIS systems, and they are in different formats.
How to deeply retrieve the information resources in the distributed heterogeneous GIS systems, and integrate these multi-source data, is becoming an urgent problem.
In this paper, we propose an emergency GIS system model based on GML and multi-source spatial data.
In which, GML is a simple and effective way to solve multi-source, multi-level, multi-dimensional geospatial information interoperability.
Ontology-based semantic information retrieval technology can improve the recall and precision of information retrieval.
The GIS client can dynamically and transparently read the feature level of spatial data by using the WFS service, and get the results returned in GML format.

The goal of EIIR 2008, the first Workshop on Efficiency Issues in Information Retrieval, was to shed light on efficiency-related issues of modern high-scale information retrieval (IR e.g Web, distributed technologies, peer to peer architectures and also new IR environments such as desktop search, enterprise/expert search, mobile devices, etc.
In addition, the workshop aimed at fostering collaboration between different research groups in this area.

Qinghai Lake Natural Protection Zone is a sanctuary for migratory birds, but also a focal point in global concerns of avian influenza.
In order to support the research about migratory birds, we planned to set up a remote monitoring system in Qinghai Lake.
The development of this system refers to some techniques about GIS(geographic information system J2EE and ActiveX. This web video monitor system will provide an easy platform to monitor the migratory birds and will serve for the launch of bird research, which is an interdisciplinary e-Science innovational project with a combination of information science and biology.
This paper will introduce the construction of the monitoring system network and present a solution about building the web-based video monitor system.

The normalized recall is one of the most popular evaluation measures for information retrieval systems.
In this paper an overview of its development is given.
It is then shown that the normalized recall is closely related to other measures such as the CRE-measure and the expected search length.
Some implications are analysed.

Information retrieval systems offer an attractive alternative to construction of a topic tracking system from scratch.
The freely available PRISE vector space text retrieval system was applied to the TDT-2 topic tracking task.
A simple version of the Rocchio formula was used for profile formulation and a retrieval status value threshold was used in conjunction with a temporal cutoff to make hard decisions.
The results indicate that our simple approach produced a credible system, but comparison with results achieved by other systems indicates that there is room for improvement.
The paper concludes by identifying some promising directions for further work that would be compatible with our approach.

This paper proposes a method to directly facilitate the author's needs during the creation of text documents or source code.
The Information retrieval system is integrated with a text editor in order to find similar documents to the created document during its writing phase.
The Suggested retrieval method consists of text extraction and segmentation from different text document formats and source code documents into logical and semantic segments.
These segments are subsequently used to calculate similarity with the new document using an optimal matching algorithm.

The performance of distributed text document retrieval systems is strongly in uenced by the organization of the inverted index.
This paper compares the performance impact on query processing of various physical organizations for inverted lists.
We present a new probabilistic model of the database and queries.
Simulation experiments determine which variables most strongly inuence response time and throughput.
This leads to a set of design trade-o s over a range of hardware con gurations and new parallel query processing strategies.

(Library and Information Science Abstracts) database records were consulted.
After detailed comparisons, it was found that keywords provided by authors have an important presence in the database descriptors studied; nearly 25% of all the keywords appeared in exactly the same form as descriptors, with another 21% though normalized still detected in the descriptors.
This means that almost 46% of keywords appear in the descriptors, either as such or after normalization.
Elsewhere, three distinct indexing policies appear, one represented by INSPEC and LISA (indexers seem to have freedom to assign the de-scriptors they deem necessary another is represented by CAB (no record has fewer than four descriptors and, in general, a large number of descriptors is employed In contrast, in ISTA, a certain institutional code exists towards economy in indexing because 84% of records contain only four descriptors.

We review the applicability of dictionary-based Cross-Language Information Retrieval (CLIR) from Zulu to English.
Due to the lack of electronic resources and in particular tools for morphological analysis, novel approaches had to be found to deal with the processes of CLIR.
Approximate string matching, combined with a monolingual Zulu word list was used.
The results suggest that the disparate vocabularies of Zulu and English are one of the main causes of the poor results of the experiments performed thus far.
Further research opportunities are outlined.

In this work for CBIR system, all the image feature descriptors including color descriptors, texture descriptors and shape descriptors are used to represent low-level image features.
Implementation of one feature descriptor doesn't give sufficient retrieval accuracy.
For combining of different types of features, there is a need to train these features with different weights to achieve good results.
A real coded chromosome genetic algorithm (GA) and anyone performance evaluation parameter of CBIR like precision or recall are used as fitness function to optimize feature weights.
Meanwhile, a real coded chromosome corresponding to higher precision as fitness function is used to select optimum weights of features.
The optimal weights of features computed by GA have improved significantly all the evaluation measures including average precision and average recall for the combined features method.

Our participation in this year’s robust track aims to 1) test how to improve the effectiveness of IR (according to MAP) using different retrieval methods with different local analysis-based query expansion methods 2) test how to improve the retrieval robustness (according to gMAP) using RankFusion, a novel fusion technique proposed in our experiments.
Our results show that although query expansion can improve the effectiveness of IR significantly, it hurts the robustness of IR seriously.
However, with appropriate parameters setting, using RankFusion for merging multiple retrieval results can improve the robustness significantly while not harming the average precision too much or even increasing it in some cases.

par Ljiljana Dolamic Thèse présentéè a la Faculté des Sciences pour l'obtention du grade de Docteurès Sciences Acceptée sur proposition du jury

Academic libraries are increasingly collecting e-books, but little research has investigated how students use e-books compared to print texts.
This study used a prompted think-aloud method to gain an understanding of the information retrieval behavior of students in both formats.
Qualitative analysis identified themes that will inform instruction and collection practices.

We propose a probabilistic topic model for analyzing and extracting contentrelated annotations from noisy annotated discrete data such as web pages stored in social bookmarking services.
In these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy, i.e. not content-related.
The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition, or can improve information retrieval performance.
The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content.
We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.

In this work we attempted to develop the techniques for co operative client cache and transparent proxy cache managers for effective information retrieval.
As the World Wide Web has grown exponentially in last two decades, every day the search engines process billions of queries.
The web users and web applications are growing in number and this will lead to increase in latency, network congestion and server overloading.
When caching is used, the request for content need not make multiple trips back and forth to the clients and the where the content is stored.
Proxy server caches and personalized client caches are suggested to overcome these problems.
The simulation results show that our proposed hybrid replacement algorithm performs better than other algorithms like LRU, LFU and FIFO.

This paper proposes a new information retrieval method based on speech recognition results.
Nowadays, there is huge speech data like news speech in TV and Radio broadcasts, etc.
However, it is not easy to retrieve necessary information from these data because extra informations for retrieval are not attached to most of these data.
To solve the problems, in this paper a Fuzzy retrieval method based on speech recognition results is proposed.
As speech data is converted into the character data and character-based query is retrieved on the character data, the query can be quickly retrieved.
It is necessary to consider the speech recognition error at the retrieval.
This paper proposes two techniques considering the the speech recognition errors; the first technique is based on the phoneme error tendency in the speech recognition results and the second technique is based on the phoneme spectral distance.
This paper shows the effectiveness of Fuzzy retrieval through evaluation experiments.

This work deals with the implementation of a logical model of Information Retrieval.
Specifically, we present algorithms for document ranking within the Belief Revision framework.
Therefore, the logical model that stands on the basis of our proposal can be efficiently implemented within realistic systems.
Besides the inherent advantages introduced by logic, the expressiveness is extended with respect to classical systems because documents are represented as unrestricted propositional formulas.
As well as representing classical vectors, the model can deal with partial descriptions of documents.
Scenarios that can benefit from these more expressive representations are discussed.

In the context of MIR/MDL evaluation, a key component for evaluation would be the availability to the research community of a large corpus of test data consisting of both audio and structured music data.
This paper proposes a possible path towards this goal by following the basic principles of the SpeechDat projects.
SpeechDat refers to successive EC supported projects of large scale multilingual data collection for developing and testing several classes of speech recognition algorithms.
Even if the domain of speech recognition differs from the domain of Music Information Retrieval, it is suggested that some of the SpeechDat experience could be applied to the collection of large audio/music database that would be suitable for MIR/MDL development and evaluation.

This paper proposes an incremental method that can be used by an intelligent system to learn better descriptions of a thematic context.
The method starts with a small number of terms selected from a simple description of the topic under analysis and uses this description as the initial search context.
Using these terms, a set of queries are built and submitted to a search engine.
New documents and terms are used to refine the learned vocabulary.
Evaluations performed on a large number of topics indicate that the learned vocabulary is much more effective than the original one at the time of constructing queries to retrieve relevant material.

We propose a novel approach to the machine learning of formal word sense, learned in interaction with human users using a new form of Relational Reinforcement Learning.
The envisaged main application area of our framework is humanmachine communication, where a software agent or robot needs to understand concepts used by human users (e.g in Natural Language Processing, HCI or Information Retrieval
In contrast to traditional approaches to the machine learning and disambiguation of word meaning, our framework focuses on the interactive learning of concepts in a dialogue with the user and on the integration of rich formal background knowledge and dynamically adapted policy constraints in the learning process, which makes our approach suitable for dynamic interaction environments with varying word usage contexts.

The IMIRSEL (International Music Information Retrieval Systems Evaluation Laboratory) project provides an unprecedented platform for evaluating Music Information Retrieval (MIR) and Music Digital Library (MDL) techniques, by bringing together large corpora and significant computational resources with the necessary rights management and technical infrastructure to support a variety of MIR/MDL research areas.
The standardized research collection being deployed represents a large and diverse corpus of musical examples, which we are hosting in our secure environment for use in evaluating MIR/MDL algorithms.
Grid services and NCSA's D2K machine learning environment provide a powerful, highperformance, and secure framework for designing, optimising, and executing complex MIR/MDL evaluation applications.
IMIRSEL provides a community resource for researchers who would otherwise not be able to afford the content rights and computational resources to carry out large-scale MIR/MDL evaluations.

Information retrieval systems that support searching of large textual databases are typically accessed by trained search intermediaries who provide assistance to end users in bridging the gap between the languages of authors and inquirers.
We are building a thesaurus in the form of a large semantic network .to
support interactive query expansion and search by end users.
Our lexicon is being built by analyzing and merging data from several large English dictionaries; testing of its value for rea'ieval is with the SMART and CODER systems.

The aim of the project is to understand and describe the information needs and search behavior of a professional design team as a basis for formulation of specifications of an information system that effectively supports the access to a wide network of heterogeneous databases.
This project will supplement a similar project focused on the needs of casual users in public libraries and thus serve to generalize from previous research.
To limit the scope of the study to a realistic project, it will be focused on the activities of an industrial design team.
Below follows a resume of the original project proposal.

The content-based retrieval of Western music has received increasing attention recently.
Much of this research deals with monophonic music.
Polyphonic music is more common, but also more difficult to represent.
Music information retrieval systems must extract viable features before they can define similarity measures.
We summarize and categorize representation features that have been used for polyphonic retrieval with the aim of laying standardized groundwork for future feature extraction research.
Comparisons with and extensions to monophonic approaches are given, and a new feature, an extension of duration-independent pitch slices, is proposed.

The PageRank algorithm is used in Web information retrieval to calculate a single list of popularity scores for each page in the Web.
These popularity scores are used to rank query results when presented to the user.
By using the structure of the entire Web to calculate one score per document, we are calculating a general popularity score, not particular to any community.
Therefore, the PageRank scores are more suited to general queries.
In this paper, we introduce a more general form of PageRank, using Web multi-resolution community-based popularity scores, where each document obtains a popularity score dependent on a given Web community.
When a query is related to a specific community, we choose the associated set of popularity scores and order the query results accordingly.
Using Web-community based popularity scores, we achieved an 11% increase in precision over PageRank.

Information is, and always has been, an elusive concept; nevertheless many philosophers, mathematicians, logicians and computer scientists have felt that it is fundamental.
Many attempts have been made to derive some sensible and intuitively acceptable definition of information; until now, none of these have succeeded.
Authors such as Dretske [4 Barwise, and Devlin claimed that the notion of information starts from the position that given an ontology of objects individuated by a cognitive agent, it makes sense to speak of the information an object (e.g a text, an image, a video) contains about another object (e.g the query This phenomenon is captured by the flow of information between objects.
Its exploitation is the task of an Information Retrieval (IR) system.

Many advanced models have been developed for information retrieval over the last years.
These models are built on various artificial intelligence paradigms to improve the precision of the retrieval.
Most of them exploit some form of term co-occurrences to improve retrieval quality.
In this paper, we compare the retrieval performance of five of these models: the Extended Boolean model, the Generalized Vector Space model, the Frequent Set model, the Rough Set model and a Genetic-Based model.
These models are tested on three sub-collections from TREC
(Text REtrieval Conference We analyze the specificity of the models regarding the form of co-occurrences introduced and report on the retrieval performance and the scalability of each model.

EIREX 2010 is the first in a series of experiments designed to foster new Information Retrieval (IR) education methodologies and resources, with the specific goal of teaching undergraduate IR courses from an experimental perspective.
For an introduction to the motivation behind the EIREX experiments, see the first sections of [Urbano et al 2011 For information on other editions of EIREX and related data, see the website at http ir.kr.inf.uc3m.es/eirex/.

Attention has been focused recently on programs in Computer Graphics within the general Computer Science Curriculum.
A successful graduate program in the Department of Computer and Information Science at The Ohio State University is outlined.
Facilities and laboratories, courses, and staffing are discussed.
Particular attention is given to a software environment, called EDGE (Educational and Development Graphics Environment) which is central to the success of the courses in the curriculum.

Surf Canyon has developed real-time implicit personalization technology for web search and implemented the technology in a browser extension that can dynamically modify search engine results pages (
Google, Yahoo and Live Search A combination of explicit (queries, reformulations) and implicit (clickthroughs, skips, page reads, etc user signals are used to construct a model of instantaneous user intent.
This user intent model is combined with the initial search result rankings in order to present recommended search results to the user as well as to reorder subsequent search engine results pages after the initial page.
This paper will use data from the first three months of Surf Canyon usage to show that a user intent model built from implicit user signals can dramatically improve the relevancy of search results.

In this paper, we present an analysis of the associations between emotion categories and audio features automatically extracted from raw audio data.
This work is based on 110 excerpts from film soundtracks evaluated by 116 listeners.
This data is annotated with 5 basic emotions (fear, anger, happiness, sadness, tenderness) on a 7 points scale.
Exploiting state-of-the-art Music Information Retrieval (MIR) techniques, we extract audio features of different kind: timbral, rhythmic and tonal.
Among others we also compute estimations of dissonance, mode, onset rate and loudness.
We study statistical relations between audio descriptors and emotion categories confirming results from psychological studies.
We also use machine-learning techniques to model the emotion ratings.
We create regression models based on the Support Vector Regression algorithm that can estimate the ratings with a correlation of 0.65 in average.

The ability to provide the right resources in a given context is a key factor for the support of knowledge workers.
The information provided about the resources is crucial for any information retrieval approach, and it should allow multiperspective descriptions of the resources.
Enhancing these descriptions with information about the attention that users spend on such resources in a specific context will provide valuable additional information.
The architecture proposed in this paper will allow to share and distribute contextualized attention metadata gathered with different user observation components to enable the integration of contextaware, personalized information retrieval services in arbitrary contexts and applications.

When collecting subjective human ratings of items, it can be difficult to measure and enforce data quality due to task subjectivity and lack of insight into how judges’ arrive at each rating decision.
To address this, we propose requiring judges to provide a specific type of rationale underlying each rating decision.
We evaluate this approach in the domain of Information Retrieval, where human judges rate the relevance of Webpages to search queries.
Cost-benefit analysis over 10,000 judgments collected on Mechanical Turk suggests a win-win: experienced crowd workers provide rationales with almost no increase in task completion time while providing a multitude of further benefits, including more reliable judgments and greater transparency for evaluating both human raters and their judgments.
Further benefits include reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves.

This paper describes a new Information Retrieval System for pedagogical content.
Each pedagogical resource is explicit thanks to metadata and structure.
We explain how to manipulate the structure of these multimedia resources at an appropriate level of granularity (resources, set of resources Finally we propose combining the structural and the textual analysis of the resources to improve the performance of the

Exhaustive evaluation of ranked queries can be expensive, particularly when only a small subset of the overall ranking is required, or when queries contain common terms.
This concern gives rise to techniques for dynamic query pruning, that is, methods for eliminating redundant parts of the usual exhaustive evaluation, yet still generating a demonstrably "good enough" set of answers to the query.
In this work we propose new pruning methods that make use of impact-sorted indexes.
Compared to exhaustive evaluation, the new methods reduce the amount of computation performed, reduce the amount of memory required for accumulators, reduce the amount of data transferred from disk, and at the same time allow performance guarantees in terms of precision and mean average precision.
These strong claims are backed by experiments using the TREC Terabyte collection and queries.

Search Engines and Classified Directories have become essential tools for locating information on the World Wide Web.
A consequence of increasing demand, as the volume of information on the Web has expanded, has been a vast growth in the number of tools available.
Each one claims to be more comprehensive, more accurate and more intuitive to use than the last.
This paper attempts to organise the available tools into a number of categories, according to their information acquisition and retrieval methods, with the intention of exposing the strengths and weaknesses of the various approaches.
The importance and implications of Information Retrieval (IR) techniques are discussed.
Description of the evolution of automated tools enables an insight into the aims of recent and future implementations

In interactive searching environments, robust linguistic techniques can provide sophisticated search assistance with a reasonable tolerance to errors, because users can easily select relevant items and dismiss the noisy bits.
The general idea is that the combination of Language Engineering and Information Retrieval techniques can be used to suggest complex terms or relevant pieces of information to the user, facilitating query formulation and refinement when the information need is not completely defined a priori or when the user is not familiar with the contents and/or the terminology used in the collection.
In this paper, we describe an interactive search engine that suggests Named Entities extracted automatically from the collection, and related to the initial query terms, helping users to filter and structure relevant information according to the persons, locations or other entities involved.

Traditional information systems design and development methodologies tend to overly focus on the technical details of the system such as memory management, system internals, algorithms and modules.
It is not unusual for system designers and developers to often completely omit from the thought process the human element.
This article offers a new information systems perspective particularly for information retrieval systems with a focus on human computer interaction.

This paper presents a hybrid case-based reasoning (CBR) and information retrieval (IR) system, called SPIRE, that both retrieves documents from a full-text document corpus and from within individual documents, and locates passages likely to contain information about important problem-solving features of cases.
SPIRE uses two case-bases, one containing past precedents, and one containing excerpts from past case texts.
Both are used by SPIRE to automatically generate queries, which are then run by the INQUERY full-text retrieval engine on a large text collection in the case of document retrieval and on individual text documents for passage retrieval.

Ontology mapping is to find semantic correspondences between similar elements of different ontologies.
It is critical to achieve semantic interoperability in the WWW.
This paper proposes a new generic and scalable ontology mapping approach based on propagation theory, information retrieval technique and artificial intelligence model.
The approach utilizes both linguistic and structural information, measures the similarity of different elements of ontologies in a vector space model, and deals with constraints using the interactive activation network.
The results of pilot study, the PRIOR, are promising and scalable.

We introduce a search engine and information retrieval system for providing access to linked opinion data.
Natural language technology of generalization of syntactic parse trees is introduced as a similarity measure between subjects of textual opinions to link them on the fly.
Information extraction algorithm for automatic summarization of web pages in the format of Google sponsored links is presented.
We outline the usability of the implemented system, integrated opinion delivery environment (IODE box.cs.rpi.edu:8080/wise/lancelot.jsp

Most recent document standards like XML rely on structured representations.
On the other hand, current information retrieval systems have been developed for flat document representations and cannot be easily extended to cope with more complex document types.
The design of such systems is still an open problem.
We present a new model for structured document retrieval which allows computing scores of document parts.
This model is based on Bayesian networks whose conditional probabilities are learnt from a labelled collection of structured documents—which is composed of documents, queries and their associated assessments.
Training these models is a complex machine learning task and is not standard.
This is the focus of the paper: we propose here to train the structured Bayesian Network model using a cross-entropy training criterion.
Results are presented on the INEX corpus of XML documents.

The distribution of digital music is one of the most attracting and challenging topics for end users, content owners and musicians these days.
The focus of this paper is the motivation how technological innovations enable new value propositions.
In this paper, the author describes a P2P system music metadata registration and query resolution, that supports attribute value search semantics as well as content-based retrieval.
This study showed the technical feasibility of these ideas in a prototypical P2P environment and described a novel artist recommendation approach using cultural features, which also could be used in both scenarios.

In this paper some specific issues related to Music Information Retrieval (MIR) are presented.
First part is dedicated to introductive notions from this field in the field, and second part is dedicated to giving details about the system we built for MusiCLEF 2011.
An important aspect related to our work is related to searching metrics able to allow us to identify an audio recording in a database with existing songs.
Our system uses chroma features associated to a song and apply on it many types of metrics.
Some of these metrics wants to find more accurate song of which part of that fragment belong to and other metrics are used to enable us to do this as quickly as possible.

Precision at k documents retrieved (P@k a common measure of information retrieval performance, is the fraction of relevant documents in the first k returned by an information retrieval system in response to a query.
It has been observed that P@k increases with collection size, all other variables being equal.
An explanation for this increase derives from the probability ranking principle: information retrieval systems score documents by likelihood of relevance and return the documents in decreasing order of score.
In a larger collection there are more high-scoring documents and therefore the average score of the first k will be higher, resulting in higher P@k.

The logical approach to information retrieval (IR) treats retrieval as uncertain inference.
In advanced applications, we have to deal with a multi-step inference process involving different system components: information needs, search activities, queries, query intermediaries, databases and documents.
In order to provide a better system support for satisfying a user's information need, these components should be changed into active objects (agents) with high-level functions.
Especially, we describe diierent levels of search activities (moves, tactics, stratagems and strategies) as well as diierent types of documents (data only, objects with IR methods, active objects, communicating objects).

One of the main problems facing human analysts dealing with large amounts of dynamic data is that important information may not be assessed in time to aid the decision making process.
We present a novel distributed processing framework called Intelligent Foraging, Gathering and Matching (I-FGM) that addresses this problem by concentrating on resource allocation and adapting to computational needs in real-time.
It serves as an umbrella framework in which the various tools and techniques available in information retrieval can be used effectively and efficiently.
We implement a prototype of I-FGM and validate it through both empirical studies and theoretical performance analysis.

SIM, the Structured Information Manager, is an information retrieval system which is designed to manage multi-gigabyte collections of documents containing text, images, and other forms of data, storing them natively in SGML, XML, MARC, RTF, and ASCII formats.
It is a fully-fledged system that provides integrated support for efficient ranked full text, boolean, and structural querying via a robust database server capable of practically managing multi-gigabyte document collections.

Coverage is an important criterion when evaluating information systems.
This exploratory study investigates this issue by submitting the same query to different databases relevant to the query topic.
Data were retrieved from three databases: ACM Digital Library, Web of Science (with the Proceedings Citation Index) and Scopus.
The search phrase was “information retrieval publication years were between 2013 and 2016.
Altogether 8,699 items were retrieved, out of which 5,306 (61 items were retrieved by a single database only, and only 977 (11 items were located in all three databases.
These 977 items were further analyzed: citation counts retrieved from the three databases were compared.
Citations were also compared to altmetric data of these publications, collected from Mendeley.

In this paper, we propose a cross-language information retrieval (CLIR) method based on estimating for domains of the query using hierarchic structures of Web directories.
To get the most appropriate translation of the queries, we utilize the Web directories written in many different languages as multilingual corpus for disambiguating translation of the query and for estimating the domain of search results using hierarchic structures of Web directories.
From experimental evaluations, we found that there is an advantage in retrieval accuracy using our proposal for disambiguating translation in CLIR system.
We found that it is effective to restrict to target fields of the query using lower level merged categories in order to acquire suited translation of the query.

Considering search process in the evaluation of interactive information retrieval is a challenging issue.
This paper explores tempo of search actions (query, click, and judgement) to measure people’s search process and performance.
When we analyze how people consume their search resource (i.e a total number of search actions taken to complete a task) over the time, it was observed that there was a different pattern in successful sessions and unsuccessful sessions.
Successful sessions tend to have a regular tempo in search actions while poor sessions tend to have uneven distribution of resource usage.
The resource consumption graph also allows us to observe where in the search process was affected by experimental conditions.
Therefore, this paper suggests that tempo of search actions can be exploited to model successful search sessions.

In this report, we describe the creation of a new face image data set, an effort undertaken as part of the MASKS (Maintaining Anonymity by Sequestering Key Statistics) Project.
This set of high-quality, carefully controlled images were taken from April until December 2006 at the GRASP (General Robotics, Automation, Sensing and Perception) Laboratory.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-07-03.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/127 The MASKS Face Image Data Set: Collection Procedures and Progress Report

Re-ranking for Information Retrieval aims to elevate relevant feedbacks and depress negative ones in initial retrieval result list.
Compared to relevance feedback-based re-ranking method widely adopted in the literature, this paper proposes a new method to well use three features in known negative feedbacks to identify and depress unknown negative feedbacks.
The features include: 1) the minor (lower-weighted) terms in negative feedbacks; 2) hierarchical distance (HD) among feedbacks in a hierarchical clustering tree; 3) obstinateness strength of negative feedbacks.
We evaluate the method on the TDT4 corpus, which is made up of news topics and their relevant stories.
And experimental results show that our new scheme substantially outperforms its counterparts.

Nowadays, increasingly, documents are marked-up using XML, the format standard for structured documents.
In contrast to HTML, which is mainly layoutoriented, XML follows the fundamental concept of separating the logical structure of a document from its layout.
This document logical structure can be exploited to allow a focused access to documents, where the aim is to return the most relevant fragments within documents as answers to queries, instead of whole documents.
This article describes approaches developed to query, represent, and rank XML

This is an especially opportune time to be holding this meeting, as we are approaching the close of the first century of quantum theory.
We don’t hear so much about it lately, but at the dawn of the 20th century, there was much anxiety about what became known as the Y(1.9)K problem.
A crisis was averted when that problem was artfully resolved by this distinguished gentleman [Planck By launching quantum theory, Planck set the agenda for the physics of the early 20th century.
Now, as Y2K approaches, we are again in need of visionary scientific leadership to face the challenges that confront 21st century science
[Shor If you contemplate the faces of these two scientific leaders, you may conclude as I did that scientists are much more jolly now than they were 100 years ago.
And why shouldn’t we be happy
this is a great time to be a quantumist!

This paper describes an experiment system framework that enables researchers to design and conduct task-based experiments for Interactive Information Retrieval (IIR
The primary focus is on multidimensional logging to obtain rich behavioral data from participants.
We summarize initial experiences and highlight the benefits of multidimensional data logging within the system framework.

We propose a formal model of Cross-Language Information Retrieval that does not rely on either query translation or document translation.
Our approach leverages recent advances in language modeling to directly estimate an accurate topic model in the target language, starting with a query in the source language.
The model integrates popular techniques of disambiguation and query expansion in a unified formal framework.
We describe how the topic model can be estimated with either a parallel corpus or a dictionary.
We test the framework by constructing Chinese topic models from English queries and using them in the CLIR task of TREC9.
The model achieves performance around 95% of the strong mono-lingual baseline in terms of average precision.
In initial precision, our model outperforms the mono-lingual baseline by 20
The main contribution of this work is the unified formal model which integrates techniques that are essential for effective Cross-Language Retrieval.

An information retrieval system that captures both visual and textual contents from paper documents can derive maximal benefits from DAR techniques while demanding little human assistance to achieve its goals.
This article discusses technical problems, along with solution methods, and their integration into a well-performing system.
The focus of the discussion is very difficult applications, for example, Chinese and Japanese documents.
Solution methods are also highlighted, with the emphasis placed upon some new ideas, including window-based binarization using scale measures, document layout analysis for solving the multiple constraint problem, and full-text searching techniques capable of evading machine recognition errors.

The area of Information Retrieval deals with problems of storage and retrieval within a huge collection of text documents.
In IR models, the semantics of a document is usually characterized using a set of terms.
A common need to various IR models is an efficient term retrieval provided via a term index.
Existing approaches of term indexing, e. g. the inverted list, support efficiently only simple queries asking for a term occurrence.
In practice, we would like to exploit some more sophisticated querying mechanisms, in particular queries based on regular expressions.
In this article we propose a multidimensional approach of term indexing providing efficient term retrieval and supporting regular expression queries.
Since the term lengths are usually different, we also introduce an improvement based on a new data structure, called BUB-forest, providing even more efficient term retrieval.

In the last years Blog Search has been a new exciting task in Information Retrieval.
The presence of user generated information with valuable opinions makes this field of huge interest.
In this poster we use part of this information, the readers' comments, to improve the quality of post snippets with the objective of enhancing the user access to the relevant posts in a result list.
We propose a simple method for snippet generation based on sentence selection, using the comments to guide the selection process.
We evaluated our approach with standard TREC methodology in the Blogs06 collection showing significant improvements up to 32% in terms of MAP over the baseline.

Due to the dramatically increasing amount of available data, effective and scalable solutions for data organization and search are essential.
Distributed solutions naturally provide promising alternatives to standard centralized approaches.
With the computational power of thousands or millions of computers in clusters or peer-to-peer systems, the challenges that arise are manifold, ranging from efficient resource discovery to issues in load balancing and distributed query processing The 2008 edition of the Workshop on Large-Scale Distributed Systems for Information Retrieval (LSDS-IR'08) provided a forum for researchers to discuss these problems and to define new directions for the work on Distributed Information Retrieval.
The Workshop program featured research contributions in the areas of similarity search, resource selection, network organization schemes, issues of data quality, result ranking techniques and query routing algorithms.

Main goal of this work is to show the improvement of using a textual pre-filtering combined with an image re-ranking in a Multimedia Information Retrieval task.
The defined three step-based retrieval processes and a well-selected combination of visual and textual techniques help the developed Multimedia Information Retrieval System to overcome the semantic gap in a given query.
In the paper, five different late semantic fusion approaches are discussed and experimented in a realistic scenario for multimedia retrieval like the one provided by the publicly available ImageCLEF Wikipedia Collection.

This paper describes the development of an Arabic document image collection containing 34,651 documents from 1,378 different books and 25 topics with their relevance judgments.
The books from which the collection is obtained are a part of a larger collection 75,000 books being scanned for archival and retrieval at the Bibliotheca Alexandrina (BA
The documents in the collection vary widely in topics, fonts, and degradation levels.
Initial baseline experiments were performed to examine the effectiveness of different

In the past few years the information retrieval (IR) community has been exploring ways to move further away from the Cranfield style evaluation paradigm, and make evaluations more `realistic more centered on real users, their needs and behaviours
As part of this drive, living labs which involve and integrate users in the research process have been proposed.
The Living Labs for Information Retrieval Evaluation workshop (LL'13) brings together for the first time people interested in progressing the living labs for IR evaluation methodology.

The Semantic Web and Multi-Agent are effective means for constructing information retrieval systems.
Despite a great deal of research, a number of challenges still exist before making Semantic Web and agent-based computing a widely accepted in information retrieval practice.
In order to solve the problem of "difficult to feedback useful information to users the paper developed a new information retrieval system which integrating Semantic Web with Multi-Agent to retrieval relevant documents or information by analyzing semantics contained in the queries and documents.

Collections of sound and music of increasing size and diversity are used both by typical computer users and multimedia designers.
Browsing audio collections poses several challenges to the design of effective user interfaces.
Recent techniques in audio information retrieval allow the automatic extraction of audio content information.
This information can be used to inform and enhance audio browsing tools.
In this paper we describe how audio information retrieval can be utilized to create novel user interfaces for browsing of audio collections.
More specifically we report on recent work on two system prototypes: the Sonic Browser and Marsyas and our current work on merging the two systems in a common flexible system.

There are many attempts to optimize information retrieval using various social concepts such as social annotations and social book marking.
This paper explores the convergence of the information retrieval technique, the social network and the context-aware computing in order to improve information retrieval services.
We propose a new ranking algorithm for the context-aware information retrieval.
To do this, we get ideas from the social network, previous graph theory and the context-aware computing.
Therefore, we investigate the problem of the information retrieval system in ubiquitous computing environment.

We used a custom designed tactor suit to provide full body vibrotactile feedback across the human arm for the purpose of enabling users to perceive a physical sense of collisions in a virtual world.
We constructed a 3-D virtual environment to test arm reach movements.
We present the results of human subject trials that test the benefit of using vibrotactile feedback for this purpose.
Our preliminary results presented here show a small, but distinct, advantage with the use of tactors.
With additional refinements to the system, improved performance results can be obtained.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report No. MSCIS-03-45.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/67 Enhanced Collision Perception Using Tactile Feedback Aaron Bloomfield Norman I. Badler Center for Human Modeling and Simulation University of Pennsylvania {aaronb, badler cis.upenn.edu

At Georgia Tech, we investigated the eeectiveness of a visualization scheme for Information Retrieval systems.
Displayed like a bar-graph, the visualization tool shows the distribution of query words in the set of documents retrieved in response to a query.
We found that end-users use the visualization for two purposes: to gain speciic information about individual documents such as the distribution of diierent query words in that document.
to gain aggregate information about the query result in general such as getting a sense of the direction of the query results.
In general they used the visualization tool as much as the title and full text in the process of deciding if a document addresses the given search topic.
In structured post-session interviews with searchers, we also obtained information about what the searcher liked, what was frustrating to them, and what they wanted in the system.

At present, Vietnamese knowledge base (vnKB) is one of the most important focuses of Vietnamese researchers because of its applications in wide areas such as Information Retrieval (IR Machine Translation (MT) etc.
There have been several separate projects developing vnKB in various domains.
The training in vnBK is the most difficulty because of quantity and quality of training data, and lacking of available Vietnamese corpus with acceptable quality.
This paper introduces an approach, which first extracts semantic information from Vietnamese Wikipedia (vnWK then trains the proposed vnKB by applying support vector machine (SVM) technique.
The experimentation of the proposed approach shows that it is a potential solution because of its good results and proves that it can provide more valuable benefits when applying to our Vietnamese Semantic Information Retrieval system.

The goal of a typical information retrieval system is to satisfy a user's information need e.g by providing an answer or information "nugget while the actual search space of a typical information retrieval system consists of documents i.e collections of nuggets.
In this paper, we characterize this relationship between nuggets and documents and discuss applications to system evaluation
In particular, for the problem of test collection construction for IR system evaluation, we demonstrate a highly efficient algorithm for simultaneously obtaining both relevant <i>documents</i> and relevant <i>
information</i
Our technique exploits the mutually reinforcing relationship between relevant documents and relevant information, yielding document-based test collections whose efficiency and efficacy exceed those of typical Cranfield-style test collections, while also generating sets of highly relevant information.

This study investigates the use of genetic algorithms in information retrieval.
The method is shown to be applicable to three well-known documents collections, where more relevant documents are presented to users in the genetic modification.
In this paper we present a new fitness function for approximate information retrieval which is very fast and very flexible, than cosine similarity fitness function.
Keywords—Cosine similarity, Fitness function, Genetic Algorithm, Information Retrieval, Query learning.

We describe the design and implementation of a system for logic-baaed multimedia retrieval.
As highlevel logic for retrieval of hypermedia documents, we have developed a probabilistic object-oriented logic (POOL) which supports aggregated objects, different kinds of propositions (terms, classifications and attributes) and even rules as being contained in objects.
Based on a probabilistic four-valued logic, POOL uses an implicit open world assumption, allows for closed world assumptions and is able to deal with inconsistent knowledge.
POOL programs and queries are translated into probabilistic Datalog programs which can be interpreted by the HySpirit inference engine.
For storing the multimedia data, we have developed a new basic IR system which yields physical data abstraction.
The overall architecture and the flexibility of each layer supports logic-based methods for multimedia information retrieval.

Social Information Retrieval (SIR) exploits the user's social data in order to refine the retrieval, for instance in the case where users with different backgrounds may express different information needs as a same textual query.
However, this additional source of information is not supported by the classical IR process.
In this article, we propose an approach to generate the user profile from his social data.
This generated profile is integrated within a SIR model allowing to personalize the list of documents returned to the user.

Information retrieval methods represent query results in a ranked, one dimensional list without revealing connections among documents and document groups.
We propose a new model of document representation and extend the notion of similarity to consider document length and word synonyms to organize documents into topically relevant groups.
Matches to a user query are presented in an intuitive, interactive map that facilitates browsing information and finding the most relevant matches within the overall landscape of results.
Current research is aimed at finding the best metric to group documents, and in defining the visual model.
Our system, named GIST, is built using the PREFUSE toolkit for graphical display [1].

The clinical documents stored in a textual and unstructured manner represent a precious source of information that can be gathered by exploiting Information Retrieval techniques.
Classification algorithms can be used for organizing this huge amount of data, but are usually tested on standardized corpora, which significantly differ from actual clinical documents that can be found in a modern hospital.
The result is that observed performance are different from expected ones.
Given such differences, it is unclear how should be the “right” training set, and how its characteristics affects the classification performance.
In this paper we present the results of an experimental analysis, conducted on actual clinical documents from a medical Department, which aims to evaluate the impact of differently sized and assembled training sets on well-known classification techniques.

We present in this paper a combination of Machine Learning based Information Retrieval (IR) techniques and stochastic language modelling in a hierarchical system that extracts surface information from text.
At the lowest level of this hierarchy, documents and paragraphs are successively routed with IR techniques.
At the top level, a stochastic language model extracts the most relevant phrases, and labels the type of information they contain.
The approach and preliminary results are demonstrated on a subset of the MUC-6 Scenario Templates task.

Until now the growing volume of heterogeneous and distributed information on the WWW makes increasingly difficult for the existing tools to retrieve relevant information.
To improve the performance of these tools, we suggest to handle two aspects of the problem: One concerns a better representation and description of WWW pages, we introduce here a new concept of "WWW documents and we describe them thanks to metadata.
We'll use the Dublin Core semantics and the XML syntax to represent these metadata.
We'll suggest how this concept can improve information retrieval on the WWW and reduce the network load generated by robots.
Then, we describe a flexible architecture based on two kinds of robots generalists" and "specialists" that collect and organize these metadata, in order to localize the resources on the WWW.
They will contribute to the overall auto-organizing information process by exchanging their indices.

This introductory paper covers not only the research content of the articles in this special issue of IP&M but attempts to characterize the state-of-the-art in the Cross-Language Information Retrieval (CLIR) domain.
We present our view of some major directions for CLIR research in the future.
In particular, we find that insufficient attention has been given to the Web as a resource for multilingual research, and to languages which are spoken by hundreds of millions of people in the world but have been mainly neglected by the CLIR research community.
In addition, we find that most CLIR evaluation has focussed narrowly on the news genre to the exclusion of other important genres such as scientific and technical literature.
The paper concludes by describing an ambitious five-year research plan proposed by James Mayfield and Paul McNamee.

This paper reports our TREC 2005 QA participation.
Our QA system EagleQA developed last year was expanded and modified for this year’s QA experiments.
Particularly, we used Lemur 4.1 (http www.lemurproject.org as the Information Retrieval (IR) Engine this year to find documents that may contain answers for the test questions from the document collection.
Our result shows Lemur did a reasonable job on finding relevant documents.
But certainly there is room for further improvement.

We investigate the effectiveness of both the standard evaluation measures and the opinion component for topical opinion retrieval.
We analyze how relevance is affected by opinions by perturbing relevance ranking by the outcomes of opinion-only classifiers built by Monte Carlo sampling.
Topical opinion rankings are obtained by either re-ranking or filtering the documents of a first-pass retrieval of topic relevance
The proposed approach establishes the correlation between the accuracy and the precision of the classifier and the performance of the topical opinion retrieval.
Among other results, it is possible to assess the effectiveness of the opinion component by comparing the effectiveness of the relevance baseline with the topical opinion ranking.

Opinion research has been paid much attention by the Information Retrieval community, and opinion holder extraction research is important for discriminating between opinions that are viewed from different perspectives.
In this paper, we describe our experience of participation in the NTCIR-6 Opinion Analysis Pilot Task by focusing on opinion extraction results in Japanese and English.
Our approach to opinion holder extraction was based on the discrimination between author and authority viewpoints in opinionated sentences, and the evaluation results were fair with respect to the Japanese documents.

The plethora of web services available on the World Wide Web presents a great opportunity to users.
It also gives rises to a great challenge: to enable discovery, reuse and interoperation of web services and software components on the web.
To support programmatic service discovery, we have developed a suite of methods that utilize both the semantics of natural language descriptions and identifiers in WSDL (Web Service Description Language) descriptions and the structures of their operations, messages and types to assess the similarity of two WSDL services.
These service discovery methods that combine semantic and structure similarity assessments enable a substantially more precise service-discovery process.

In this paper, an intelligent information retrieval system prototype based on assigning context to documents is proposed.
This system novel prototype has a better retrieval effectiveness than traditional key-word systems.
In this intelligent agent paradigm system, each document is assigned context based on the type of information.
The prototype system offers a new methodology for assigning context of documents in a collection.
This novel intelligent information retrieval system showed promising results.

In present scenario, with the unprecedented development of electronics and tremendous growth of the Internet, World Wide Web (WWW) has become common and popular source of information for varied category of users.
Web search engines provide interface between users and web to extract the desired information from WWW.
The user possess query to search engine and in response the search engine return useful information.
It is observed that typical search engines return the same result for the query submitted by different users irrespective of specific user need.
Generally, each user has specific information requirement.
It is always desirable that search result should satisfy the exact user requirement.
We have proposed approaches to make the search adapting to satisfy the user need.
The approaches discussed here are based on ontology and dynamic user profile.

This paper presents first steps towards building a music information system like last.fm, but with the major difference that the data is automatically retrieved from the WWW using web content mining techniques.
We first review approaches to some major problems of music information retrieval (MIR which are required to achieve the ultimate aim, and we illustrate how these approaches can be put together to create the automatically generated music information system (AGMIS
The problems addressed in this paper are similar and prototypical artist detection, album cover retrieval, band member and instrumentation detection, automatic tagging of artists, and browsing/exploring web pages related to a music artist.
Finally, we elaborate on the currently ongoing work of evaluating the methods on a large dataset of more than 600, 000 music artists and on a first prototypical implementation of AGMIS.

A contextual information system is developed by expanding original query with additional terms identified by global analysis to improve web search results by interactive query expansion.
A context is any type of information which exhibits user’s intention behind search.
This contextual web search is applied to web based learning environment.
In this a contextual web query tool is built to provide better search results than the usual traditional search approach.
The contextual information is obtained from shared content (learning material provided by the teacher subject expertise) which is available in the learning platform and is used in a query expansion approach to get more suitable documents for the learning activities.
This work is a shift from query-centered IR to context-centered IR which aims to close a gap between user queries and resources available on WWW.

Object detection is a fundamental task in computer vision.
Deformable part based model has achieved great success in the past several years, demonstrating very promising performance.
Many papers emerge on part based model such as structure learning, learning more discriminative features.
To help researchers better understand the existing visual features' potential for part based object detection and promote the deep research into part based object representation, we propose an evaluation framework to compare various visual features' performance for part based model.
The evaluation is conducted on challenging PASCAL VOC2007 dataset which is widely recognized as a benchmark database.
We adopt Average Precision (AP) score to measure each detector's performance.
Finally, the full evaluation results are present and discussed.

In the context of the European Union (EU) funded research project PROMISE, a winter school was organized in the small ski resort of Zinal, Valais, Switzerland from January 23-27, 2012.
The title of the winter school was From Information Retrieval to Information Visualization and the goal was to bring together these two research domains that are currently quite separated but have an important potential to help each other in advancing the fields.
Indeed, the school has been attended by participants who came from one domain or the other and offered them the possibility of starting to acquire cross-disciplinary competencies.
Interestingly enough, the school turned out to be a brainstorming and discussion opportunity also for the lecturers, since they had the occasion of meeting colleagues from a quite different field with

In this paper, we describe the Ultima project which aims to construct a platform for evaluating various approaches of music information retrieval.
Three approaches with the corresponding tree-based, list-based, and (n-gram+tree)-based index structures are implemented.
A series of experiments has been carried out.
With the support of the experiment results, we compare the performance of index construction and query processing of the three approaches and give a summary for efficient content-based music information retrieval.

Despite having a large number of speakers, Sorani one of the two principle branches of the Kurdish language is among the less-resourced languages.
This paper reports on the outcomes of a project aimed at providing the essential resources for processing Sorani texts.
The primary output of this project is Pewan, the first standard Test Collection to evaluate Sorani Information Retrieval systems.
The other language resources that we have constructed in this project are i) a light-stemmer ii) a list of affixes, and (iii) a list of stopwords.
We also used these newly-built resources to study the effectiveness of basic IR strategies on Sorani documents.
Our experimental results show that normalization and, to a lesser extent, stemming can greatly improve the performance of Sorani IR systems.

Link analysis is the most important application of web structure mining and serves as a new knowledge source in web information retrieval.
However, the mono-dimensional analysis of links neglects many other aspects.
The results presented in this article show that the structure of a site affects the in-links for its pages.
Link analysis algorithms need to be refined in order to account for that fact and the retrieval of high quality pages cannot solely be based on links as the only parameter.

This position paper for the special session on "Multilingual Information Access" comprises of three parts.
The first part reviews possible demands for Multilingual Information Access (hereafter, MLIA) on the Web, and examines required technical elements.
Among those, we, in the second part, focus on Cross-Language Information Retrieval (hereafter, CLIR particularly a scalable architecture which enables CLIR in a number of language combinations.
Such a distributed architecture developed around XIRCH project (an international joint experimental project currently involves NTT, KRDL, and KAIST) is then described in a certain detail.
The final part discusses some NLP/MT related issues associated with such a CLIR architecture.

Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to the overall semantics of the text.
Rhetorical relations, e.g. contrast, cause, explanation, describe how the parts of a text are linked to each other.
Knowledge about this so-called discourse structure has been applied successfully to several natural language processing tasks.
This work studies the use of rhetorical relations for Information Retrieval (IR Is there a correlation between certain rhetorical relations and retrieval performance?
Can knowledge about a document's rhetorical relations be useful to IR?
We present a language model modification that considers rhetorical relations when estimating the relevance of a document to a query.
Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably gt;10% in mean average precision over a state-of-the-art baseline).

One of the most time consuming and laborious problems facing researchers in Affective Computing is annotation of data, particularly with the recent adoption of multimodal data.
Other fields, such as Computer Vision, Language Processing and Information Retrieval have successfully used crowd sourcing (or human computation) games to label their data sets.
Inspired by their work, we have developed a Facebook game called Guess
for labeling multimodal, affective video data.
This paper describes the game and an initial evaluation of it for social context labeling.
In our experiment, 33 participants used the game to label 154 video/question pairs over the course of a few days, and their overall inter-rater reliability was good
(Krippendorff’s α 70
We believe this game will be a useful resource for other researchers and ultimately plan to make Guess
open source and available to anyone

Zusammenfassung Mit der Verwendung des Attributkonzepts von freeWAIS-sf ist eine Befragung mehrerer WAISDatenbanken nur dann möglich, wenn die einzelnen Datenbanken jeweils die gleichen Anfrageattribute anbieten.
Basierend auf dem Konzept der Datenunabhängigkeit
wird eine externe Sicht auf mehrere freeWAIS-sf-Datenbanken präsentiert, die es zuläßt, auch Datenbanken mit unterschiedlichen Anfrageattributen parallel zu befragen; die Heterogenität der zu befragenden Datenbanken wird vor dem Benutzer verborgen.
SFgate ist ein Gateway
zwischen dem World Wide Web und
freeWAIS-sf, welches den hier vorgestellten Ansatz implementiert.

This decade has seen a great deal of progress in the development of information retrieval systems.
Unfortunately, we still lack a systematic understanding of the behavior of the systems and their relationship with documents.
In this paper we present a completely new approach towards the understanding of the information retrieval systems.
Recently, it has been observed that retrieval systems in TREC 6 show some remarkable patterns in retrieving relevant documents.
Based on the TREC 6 observations, we introduce a geometric linear model of information retrieval systems.
We then apply the model to predict the number of relevant documents by the retrieval systems.
The model is also scalable to a much larger data set.
Although the model is developed based on the TREC 6 routing test data, I believe it can be readily applicable to other information retrieval systems.
In Appendix, we explained a simple and efficient way of making a better system from the existing systems.

Vannevar Bush's 1945 article set a goal of fast access to the contents of the world's libraries which looks like it will be achieved by 2010, sixty-five years later.
Thus, its history is comparable to that of a person.
Information retrieval had its schoolboy phase of research in the 1950s and early 1960s; it then struggled for adoption in the 1970s but has, in the 1980s and 1990s, reached acceptance as free-text search systems are used routinely.
The tension between statistical and intellectual content analysis seemed to be moving towards purelyg statistical methods; now, on the Web, manual linking is coming back.
As we have learned how to handle text, information retrieval is moving on, to projects in sound and image retrieval, along with electronic provision of much of what is now in libraries.
We can look forward to completion of Bush's dream, within a single lifespan.

The Internet has marked this era as the information age.
There is no precedent in the amazing amount of information, especially network news, that can be accessed by Internet users these days.
As a result, the problem of seeking information in online news articles is not the lack of them but being overwhelmed by them.
This brings huge challenges in processing online news feeds, e.g how to determine which news article is important, how to determine the quality of each news article, and how to filter irrelevant and redundant information.
In this paper, we propose a method for filtering redundant and less-informative RSS news articles that solves the problem of excessive number of news feeds observed in RSS news aggregators.
Our filtering approach measures similarity among RSS news entries by using the fuzzy-set information retrieval model and a fuzzy equivalent relation for computing word/sentence similarity to detect redundant and less-informative news articles

This document describes the general methodology used by the SPIRIT consortium to evaluate SPIRIT interim and final prototypes.
SPIRIT project Evaluation Methodology IST-2001-35047 D19 7201
Page 2 of 14

Users who downloaded this article also downloaded: PETER INGWERSEN 1996 COGNITIVE PERSPECTIVES OF INFORMATION RETRIEVAL INTERACTION: ELEMENTS OF A COGNITIVE IR THEORY Journal of Documentation, Vol.
52 Iss 1 pp.
3-50 http dx.doi.org/10.1108/eb026960 Krystyna K. Matusiak 2006 Towards user-centered indexing in digital image collections OCLC Systems &amp; Services: International digital library perspectives, Vol. 22
283-298 http dx.doi.org/10.1108/10650750610706998 N.J. BELKIN, R.N. ODDY, H.M. BROOKS 1982
ASK FOR INFORMATION RETRIEVAL:
PART I. BACKGROUND AND THEORY Journal of Documentation, Vol.
38 Iss 2 pp.
61-71 http dx.doi.org/10.1108/eb026722

Object detection from still images has been among the most active and challenging area in computer vision recently.
In contrast, fully supervised object detection from video has rarely been investigated.
In this paper, we propose an algorithm to improve the performance of object detection from video.
Our proposed method is based on an empirical property that the trajectory of an object is important for detection in videos.
We use object trajectory to filter outliers, determine probable location of object and correct detection errors.
We compare our method with baseline method which regard video frames as still images directly.
Experiments show that our method outperform the compared baseline in terms of average precision while inducing moderate computation overhead.

Most clustering methods for information retrieval application do not work efficiently when dealing with complicated data.
In this paper, we compare the performance of the Topic Mapsbased method with the Clustering-based method.
An experimental test was carried out using 20 volunteer to evaluate and compare the performance of the Topic Maps-based Information Retrieval system and Clustering-based Information Retrieval system in security domain.
The experimental results show that a Topic Maps-based method provides both better recall/precision and shorter search time/search steps.

This paper describes the automatic building of a corpus of short Swedish news texts from the Internet, its application and possible future use.
The corpus is aimed at research on Information Retrieval, Information Extraction, Named Entity Recognition and Multi Text Summarization.
The corpus has been constructed by using an Internet agent, the so called newsAgent, downloading Swedish news text from various sources.
A small part of this corpus has then been manually tagged with keywords and named entities.
The newsAgent is also used as a workbench for processing the abundant flows of news texts for various users in a customized format in the application Nyhetsguiden.

We investigated using the LDC English/Chinese bilingual wordlists for English-Chinese cross language retrieval.
It is shown that the Chinese-to-English wordlist can be considered as both a phrase and word dictionary, and is preferable to the English-to-Chinese version in terms of phrase translation and word translation selection.
Additional techniques such as frequency-based term selection, translation set weighting and term co-occurrence data were employed.
Experiments show that within the TREC 5&6 Chinese corpus and retrieval environment, 74% of monolingual effectiveness is achievable for short queries of a few English words, and 85% for long queries of paragraph sizes.

This paper describes the research design and methodologies we used to assess the usefulness of MeSH (Medical Subject Headings) terms for different types of users in an interactive search environment.
We observed four different kinds of information seekers using an experimental IR system 1) search novices 2) domain experts 3) search experts and (4) medical librarians.
We employed a user-oriented evaluation methodology to assess search effectiveness of automatic and manual indexing methods using TREC Genomics Track 2004 data set.
Our approach demonstrated (1) the reusability of a large test collection originally created for TREC 2) an experimental design that specifically considers types of searchers, system versions and search topic pairs by Graeco-Latin square design and (3) search topic variability can be alleviated by using different sets of equally difficult topics and well-controlled experimental design for contextual information retrieval evaluation.

This work presents an information retrieval architecture developed for the Santa Catarina State Telemedicine System.
This architecture employs DICOM Structured Reporting, controlled vocabularies for data catalogization and a specially developed search engine for data indexing and storing.
Results of our case study show that searches can be performed much faster with the proposed search mechanism and that the precision of results is acceptable in most cases.
In some searches, irrelevant items within the 15 first results were identified.
This occurred partially because search terms found in the additional free text observations inserted into the findings reports were treated with the same relevance as formally diagnostically relevant items of the DICOM SR structure and, partially because the semantics of negations associated to search terms in the findings reports were not taken into consideration.

Information Retrieval in the World Wide Web with general purpose search engines is still a diÆcult and time consuming task.
Specialized search tools have turned out to be much more e ective and useful in certain domains.
However, their ideas are often trimmed to one special domain so they cannot be applied to set up series of specialized tools.
In addition, they tend to induce a tremendous load on the network.
This contribution reports our approach for designing specialized search engines, so-called domain experts.
These experts gather the documents relevant for their domain through mobile lter agents.
We show the e ectiveness of domain experts with a case study in the domain of scienti c articles.
Finally, we demonstrate that the lter agents' mobility can cause a signi cant reduction of the induced network load.

In this paper we suggest a new approach to analysis and design of IR systems.
We argue for design space exploration in constructing IR systems and in analyzing the effects of individual modules and parameters.
We present results of experiments with parametric interpolation, or "homotopy between two systems, and show, incidentally, that the best results are not achieved at the endpoints, and may lie outside the bounding hypercube defined by our choice of parameterization.
Three distinct classes of interpolation are introduced to deal with the complexities of the specific example.

Abstract We cast the ranking problem as (1) multiple classification
Mc 2) multiple ordinal classification, which lead to computationally tractable learning algorithms for relevance ranking in Web search.
We consider the DCG criterion (discounted cumulative gain a standard quality measure in information retrieval.
Our approach is motivated by the fact that perfect classifications result in perfect DCG scores and the DCG errors are bounded by classification errors.
We propose using the Expected Relevance to convert class probabilities into ranking scores.
The class probabilities are learned using a gradient boosting tree algorithm.
Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6 in terms of the (normalized) DCG scores.
An efficient implementation of the boosting tree algorithm is also presented.

Determining semantic similarity between words, concepts and phrases is important in many areas within Artificial Intelligence.
This includes the general areas of information retrieval, data mining, and natural language processing.
Existing approaches have primarily focused on noun to noun synonym comparison.
We propose a new technique for the comparison of general expressions that combines web searching with Latent Semantic Analysis.
This technique is more general than previous approaches, as it is able to match similarities between multiword expressions, abbreviations, and alpha-numeric phrases.
Consequently, it can be applied to more complex comparison problems such as ontology alignment.

Efficient and intelligent music information retrieval is a very important topic of the 21st century.
With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of identifying "similar" artists using both lyrics and acoustic data.
In this paper, we present a clustering algorithm that integrates features from both sources to perform bimodal learning.
The algorithm is tested on a data set consisting of 570 songs from 53 albums of 41 artists using artist similarity provided by All Music Guide.
Experimental results show that the accuracy of artist similarity classifiers can be significantly improved and that artist similarity can be efficiently identified.

In natural language processing morphological analyzer, morphological generator and morphological parser is the essential common tool used in developing language software.
Morphological analyzer is a computer program which provides the grammatical information of a word.
Morphological generator, by giving/providing the root word and grammatical information, it generates all word forms of that word.
On the other hand morphological parsing is the process of determining the morphemes from given word?
In this article I surveys the different work have done about Odia morphology.
Morphology, Morphological Analyzer, Morphological Generator, Morphological Parser, Information Retrieval, Stemmer, IE, Spell Checker, Machine Translation, Question Answering System.

Publications in the life sciences are characterized by a large technical vocabulary, with many lexical and semantic variations for expressing the same concept.
Towards addressing the problem of relevance in biomedical literature search, we introduce a deep learning model for the relevance of a document
’s text to a keyword style query.
Limited by a relatively small amount of training data, the model uses pre-trained word embeddings.
With these, the model first computes a variable-length Delta matrix between the query and document, representing a difference between the two texts, which is then passed through a deep convolution stage followed by a deep feed-forward network to compute a relevance score.
This results in a fast model suitable for use in an online search engine.
The model is robust and outperforms comparable state-of-the-art deep learning approaches.

In this paper we present LocLinkVis (Locate-Link-Visualize a system which supports exploratory information access to a document collection based on geo-referencing and visualization.
It uses a gazetteer which contains representations of places ranging from countries to buildings, and that is used to recognize toponyms, disambiguate them into places, and to visualize the resulting spatial footprints.

submitted to PMAA’06 Giorgos Kollias CEID, Univ.
of Patras GREECE
gdk@hpclab.ceid.upatras.gr Efstratios Gallopoulos CEID, Univ.
of Patras GREECE stratis@ceid.upatras.gr Daniel B. Szyld Dept. Math Temple Univ.
USA szyld@temple.edu
In areas such as Information Retrieval, Pattern Recognition and Image Processing, many data analysis problems contain computational kernels of the form

Access methods are a fundamental tool on Information Retrieval.
However, most of these methods suffer the problem known as the curse of dimensionality when they are applied to objects with very high dimensionality representation spaces, such as text documents.
In this paper we introduce a new parallel access method that uses several graphs as distributed index structure and a kNN search algorithm.
Two parallel versions of the search method are presented, one based on master–slave scheme and the other based on a pipeline.
A thorough experimental analysis on different datasets shows that our method can process efficiently large flows of queries, compete with other parallel algorithms and obtain at the same time very high quality results.

A concept can be linguistically expressed in various syntactic constructions.
Such syntactic variations spoil the effectiveness of incorporating dependencies between words into information retrieval systems.
This paper presents an information retrieval method for normalizing syntactic variations via predicate-argument structures.
We conduct experiments on standard test collections and show the effectiveness of our approach.
Our proposed method significantly outperforms a baseline method based on word dependencies.

The aim of an opinion finding system is not just to retrieve relevant documents, but to also retrieve documents that express an opinion towards the query target entity.
In this work, we propose a way to use and integrate an opinion-identification toolkit, OpinionFinder, into the retrieval process of an Information Retrieval (IR) system, such that opinionated, relevant documents are retrieved in response to a query.
In our experiments, we vary the number of top-ranked documents that must be parsed in response to a query, and investigate the effect on opinion retrieval performance and required parsing time.
We find that opinion finding retrieval performance is improved by integrating OpinionFinder into the retrieval system, and that retrieval performance grows as more posts are parsed by OpinionFinder.
However, the benefit eventually tails off at a deep rank, suggesting that an optimal setting for the system has been achieved.

Until recently, most research on music information retrieval concentrated on monophonic music.
Online Music Retrieval and Searching (OMRAS) is a threeyear project funded under the auspices of the JISC (Joint Information Systems Committee)/NSF (National Science Foundation)
International Digital Library Initiative which began in 1999 and whose remit was to investigate the issues surrounding polyphonic music information retrieval.
Here we outline the work OMRAS has achieved in pattern matching, document retrieval, and audio transcription, as well as some prototype work in how to implement these techniques into library systems.

Statistical Machine Translation (SMT) is often used as a black-box in CLIR tasks.
We propose an adaptation method for an SMT model relying on the monolingual statistics that can be extracted from the document collection (both source and target if available We evaluate our approach on CLEF Domain Specific task (German-English and English-German) and show that very simple document collection statistics integrated in SMT translation model allow to obtain good gains both in terms of IR metrics (MAP, P10) and MT evaluation metrics (BLEU, TER).

Disambiguation is the aim of most translation techniques used in cross-language information retrieval.
In this paper, we present a new method for query translation which only needs a bilingual dictionary and a monolingual corpus.
Unlike the traditional statistical approach, our method uses co-occurrences between pairs of terms as statistical measure.
By adding up all the weights of a k-complete subgraph, we can compare different combinations of target terms.
The output of our method is in the form of probability distribution.
Then the result is converted to the query in the target language.
The method is easy to implement, and experiment shows it performs well.

It has been increasingly recognized that science information systems have need of natural language processing.
F. W. Lancaster, author of the National Library of Medicine Study of the performance of the MEDLARS system, spoke of this at the 1971 annual conference of the ACM, in the panel "Can Present Methods for Library and Information Retrieval Service Survive
He noted that "there is a definite trend away from large carefully controlled vocabularies and toward natural language processing, or at least machine-aided indexing and quoted Klingbiel's remarks to the effect that "highly structured controlled vocabularies are obsolete for indexing and retrieval" and that "the natural language of scientific prose is fully adequate for these purposes."

Capturing semantics in a computable way is desirable for many applications, such as information retrieval, document clustering or classification, etc.
Embedding words or documents in a vector space is a common first-step.
Different types of embedding techniques have their own characteristics which makes it difficult to choose one for an application.
In this paper, we compared a few off-the-shelf word and document embedding methods with our own Ariadne approach in different evaluation tests.
We argue that one needs to take into account the specific requirements from the applications to decide which embedding method is more suitable.
Also, in order to achieve better retrieval performance, it is worth investigating the combination of bibliometric measures with semantic embedding to improve ranking.

Corrigendum to “Identifying consumer consideration set at the purchase time from aggregate purchase data in online retailing Decis.
Support Syst.
Bin Gu Prabhudev Konana, Hsuan-Wei Michelle Chen a Department of Information Systems, W. P. Carey School of Business, Arizona State University, P.O. Box 874606, Tempe, AZ 85287, USA b Department of Information, Risk, and Operations Management, McCombs School of Business, The University of Texas at Austin, CBA 5.202, B6500, Austin, TX 78712, USA c School of Library and Information Science, San Jose State University, One Washington Square, San Jose, CA 95192, USA

Test collections model use cases in ways that facilitate evaluation of information retrieval systems.
This paper describes the use of search-guided relevance assessment to create a test collection for retrieval of spontaneous conversational speech.
Approximately 10,000 thematically coherent segments were manually identified in 625 hours of oral history interviews with 246 individuals.
Automatic speech recognition results, manually prepared summaries, controlled vocabulary indexing, and name authority control are available for every segment.
Those features were leveraged by a team of four relevance assessors to identify topically relevant segments for 28 topics developed from actual user requests.
Search-guided assessment yielded sufficient inter-annotator agreement to support formative evaluation during system development.
Baseline results for ranked retrieval are presented to illustrate use of the collection.

This paper discussed the concept of computer competency and investigated the relationship between students’ computer competency and their perception of enjoyment and difficulty level of web-based distance-learning courses.
Participants were 237 entering graduate students in library and information science from a mid-southwestern state university in the United States from year 2001 to 2003.
Computer competency was estimated by students’ self-report of their prior knowledge of information technology skills in a survey called Computer Skill and Use Assessment.
Statistical significance was found between the correlation of computer competency and students’ perception of enjoyment level (p=.011) and difficulty level (p=.001).

The integrated management of multimedia information, that is of complex information consisting of conventional data, text, graphics, images and voice, is of great interest not only in fields like information Retrieval, Office Automation, Computer Aided Instruction, Computer Aided Design, but also in other emerging fields such as Tourist Applications, Computer generated films, Newspapers and magazines production, and so on.
In this paper the definition of multimedia object, partially derived from the ECMA standard 8220;Office Document Architecture&#8221 is given and an approach to multimedia information management is proposed.
A multilevel environment, where multimedia information can be handled, stored and retrieved and the inner level of which consists of a general purpose Multimedia Data Base Management System (MDBMS is described.
They are outlined the main functionalities that languages which describe and manipulate multimedia objects should provide.

Among the pure sciences, mathematics has had the most important impact on the rest of human knowledge, because it has provided a powerful basis for human reasoning.
Mathematics is building block for all disciplines.
Information Science (IS as an emerging discipline, is included in this principle.
Some mathematical formulae have been frequently used in IS, ranging from theoretical discussions in information theory to applied investigations in information retrieval (Goldfarb, 1997; Kantor, 1983; Kantor, 1984; Ota, 2005; Shibata, 1995).

Locating digital documents in modern organisations with the aid of metadata is a challenging area of research in document management systems.
In order to investigate this, we have built an AWOCADO (adaptive workflow controller and document organiser) prototype system.
AWOCADO provides a framework for defining and managing document attributes as well as looking up documents by using their attributes.
An evaluation study was conducted to observe information retrieval using metadata in our system

In this article, we propose to apply Semantic Web technologies to the field of e -learning.
Today, the resources available on the web increases significantly.
Information Systems Research existing does not allow users to return documents that meet their exact needs expressed by a query on a document collection.
To optimize information retrieval, we propose an approach based on the use of domain ontology for indexing a database and use semantic links between documents or fragments of documents collection, to allow the inference of all relevant documents.
This ontology, in OWL format facilitate the management of learning resources.
The development environment is used protected ontology.
We describe how to semantically index the e-learning resources through semantic annotations.
To test the prototype, we use language SeRQL middleware Sesame.

Recently direct optimization of information retrieval (IR) measures has become a new trend in learning to rank.
In this paper, we propose a general framework for direct optimization of IR measures, which enjoys several theoretical advantages.
The general framework, which can be used to optimize most IR measures, addresses the task by approximating the IR measures and optimizing the approximated surrogate functions.
Theoretical analysis shows that a high approximation accuracy can be achieved by the framework.
We take average precision (AP) and normalized discounted cumulated gains (NDCG) as examples to demonstrate how to realize the proposed framework.
Experiments on benchmark datasets show that the algorithms deduced from our framework are very effective when compared to existing methods.
The empirical results also agree well with the theoretical results obtained in the paper.

This paper describes the methods employed to solve the Author Profiling task at PAN-2015.
The main goal was to test the use of features derived from Information Retrieval to identify the personality traits of the author of a given text.
This paper describes the features, the classification algorithms employed, and how the experiments were run.
Also, I provide a comparative analysis of my results compared to those of other groups.

3D coordinate transformation is usually encountered issue in geodesy, photogrammetry, geographical information science (GIS) and computer vision.
This paper simplifies the mathematical model of 3D similarity transformation by optimization process, which finally only has the parameter of rotation matrix.
According to the orthogonality of rotation matrix, the paper uses Rodrigues matrix to represent it, instead of classical rotation angle form.
Making full use of the properties of Rodrigues matrix, we carry out the linearization of observation equations; further get the least squares solution of the model parameters.
The case study shows that the proposed algorithm is valid and fast, regardless of whether rotation angles are large or small, and almost does not depends on the initial values of the parameter.

Knowledge maps are promising tools for visualizing the structure of large-scale information spaces, but still far away from being applicable for searching.
The first international workshop on “Knowledge Maps and Information Retrieval (KMIR held as part of the International Conference on Digital Libraries 2014 in London, aimed at bringing together experts in Information Retrieval (IR) and knowledge mapping in order to discuss the potential of interactive knowledge maps for information seeking purposes.

In Cross-Language Information Retrieval, finding the appropriate translation of the source language query has always been a difficult problem to solve.
We propose a technique towards solving this problem with the help of multilingual word clusters obtained from multilingual word embeddings.
We use word embeddings of the languages projected to a common vector space on which a community-detection algorithm is applied to find clusters such that words that represent the same concept from different languages fall in the same group.
We utilize these multilingual word clusters to perform query translation for Cross-Language Information Retrieval for three languages English, Hindi and Bengali.
We have experimented with the FIRE 2012 and Wikipedia datasets and have shown improvements over several standard methods like dictionarybased method, a transliteration-based model and Google Translate.

Peer-to-peer technology is widely used for file sharing.
In the past decade a number of prototype peer-to-peer information retrieval systems have been developed.
Unfortunately, none of these has seen widespread real-world adoption and thus, in contrast with file sharing, information retrieval is still dominated by centralized solutions.
In this article we provide an overview of the key challenges for peer-to-peer information retrieval and the work done so far.
We want to stimulate and inspire further research to overcome these challenges.
This will open the door to the development and large-scale deployment of real-world peer-to-peer information retrieval systems that rival existing centralized client-server solutions in terms of scalability, performance, user satisfaction, and freedom.

In this paper we present a new XML-based query language for XML documents denoted XRL.
This language expresses database conditions concerning the attributes and the structure of documents, as well as Information Retrieval conditions over their contents and their relevance.
XRL queries can be stored into a XML repository and manipulated as any other XML document.
In order to illustrate the usefulness of this language, we also describe some general guidelines of its current implementation, and present an example application.
This application consists in a subscription/notification service of news articles, which are periodically retrieved from a digital library of newspapers according to the preferences of each user.

This article proposes a term weighting scheme for measuring query-document similarity that attempts to explicitly model the dependency between separate occurrences of a term in a document.
The assumption is that, if a term appears once in a document, it is more likely to appear again in the same document.
Thus, as the term appears again and again, the information content of the subsequent occurrences decreases gradually, since they are more predictable.
We introduce a parameterized decay function to model this assumption, where the initial contribution of the term can be determined using any reasonable term discrimination factor.
The effectiveness of the proposed model is evaluated on a number of recent web test collections of varying nature.
The experimental results show that the proposed model significantly outperforms a number of well known retrieval models including a recently proposed strong Term Frequency and Inverse Document Frequency (TF-IDF) model.

With the expansion of the Internet, searching for information goes beyond the boundary of physical libraries.
Millions of documents of various media types, such as text, image, video, audio, graphics, and animation, are available around the world and linked by the Internet.
Unfortunately, the state of the art of search engines for media types other than text lags far behind their text counterparts.
To address this situation, we have developed the Multimedia Analysis and Retrieval System (MARS This paper reports some of the progress made over the years towards exploring information retrieval beyond the text domain.
In particular, the following aspects of MARS are addressed in the paper: visual feature extraction, retrieval models, query reformulation techniques, e cient execution speed performance and user interface considerations.
Extensive experimental results are reported to validate the proposed approaches.

This paper presents a case-study of automatic construction of a hypertext from a large full-text document.
The document we used as input of the automatic authoring process is a well-known textbook of Information Retrieval
(IR We explain in detail the steps performed to transform the textbook into a hyper-textbook.
A hyper-textbook is the hypertextual equivalent of a textbook, in the same way as a hyperbook is the hypertextual equivalent of a book.
A hyper-textbook is designed to be used both as a self instruction manual and as a reference source.
Moreover, it can be printed out into its textual form in total or only partially for easy consultation without a computer.
The hyper-textbook is written in HTML and can be accessed and navigated on using a common WWW browser.

Given a set of n points and hyperrectangles in d dimensional space the batched range searching problem is to determine which points each hyperrectangle contains We present two parallel algorithms for this problem on a p n pn mesh connected paral lel computer one average case e cient algorithm based on cell division and one worst case e cient divide and conquer algo rithm Besides the asymptotic analysis of their running times we present an experimental evaluation of the algorithms

Cross Language Information Retrieval (CLIR) between languages of the same origin is an interesting topic of research.
The similarity of the writing systems used for these languages can be used effectively to not only improve CLIR, but to overcome the problems of textual variations, textual errors, and even the lack of linguistic resources like stemmers to an extent.
We have conducted CLIR experiments between three languages which use writing systems (scripts) of Brahmi-origin, namely Hindi, Bengali and Marathi.
We found significant improvements for all the six language pairs using a method for fuzzy text search based on Surface Similarity.
In this paper we report these results and compare them with a baseline CLIR system and a CLIR system that uses Scaled Edit Distance (SED) for fuzzy string matching.

The increase of digital image acquisition devices, combined to the growth of the Web, requires the definition of Information Retrieval (IR) models and systems providing fast access to images searched by users among large amounts of data.

We propose a method for information extraction and presentation using recorded eye gaze data, i.e life-log video data.
We call our method Gaze Cloud, which essentially uses gaze information for the generation of thumbnail images.
One of the usages of wearable computing, personal life-logs are becoming increasingly possible.
However, an aspect that needs to be addressed is information retrieval through different browsing methods.
It is also well known that human memory recall is aided by effective presentation of information.
Our propose method Gaze Cloud calculates the importance of information from gaze data that is consequently used for the generation of thumbnail images.
This method performs the calculation using the eye gaze duration and hot spot information.
Additionally, we construct a prototype daily-use wearable eye tracker system.

This paper describes the development of French–English and English–French machine translation systems for the 2010 WMT shared task evaluation.
These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only.
Most of our efforts were devoted to the choice and extraction of bilingual data used for training.
We filtered out some bilingual corpora and pruned the phrase table.
We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data.
We first collected bilingual data by performing automatic translations of monolingual texts.
The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques.

A progressive application of evolutionary computing to optimize Boolean search queries in crisp and fuzzy information retrieval systems was investigated, evaluated in laboratory environment and presented.
Additionally, WebFusion novel meta- search engine contributing to the effectiveness of web search has been presented.
The system learns the expertness of every particular underlying standalone search engine in a certain category based on the users preferences estimated according to an analysis of the click-through behavior.
An intelligent re-ranking based on ordered weighted averaging is used for fusing the results' scores obtained from the underlying search engines.
In this paper, the two promising web search improvement techniques are merged on the way towards intelligent search application.

This tutorial aims to provide a unifying account of current research on diversity and novelty in different IR domains, namely, in the context of search engines, recommender systems, and data streams.

In this paper, we present our contribution in INEX 2015 Social Book Search Track.
This track aims to exploit social information (users reviews, ratings, etc from LibraryThing and Amazon collections.
We used traditional information retrieval models, namely, InL2 and the Sequential Dependence Model (SDM) and tested their combination.
We integrated tools from natural language processing (NLP) and approaches based on graph analysis to improve the recommendation performances.

This paper describes the experimental combination of traditional Natural Language Processing (NLP) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation (MT) task.
Therefore, we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process.
In the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem.
We conclude with a critical view on the developed approach.

This paper discusses the design process of a multi-index, multisource information retrieval system (SIRS SIRS provides comprehensive visualization of different document types for the JRC working environment.
The interface design is based on elastic window management and on the Focus+Context method to browse large amounts of information without losing its contextual relevance.
Source integration was achieved by mapping techniques, on which we applied methods, degree-of-separation and closure, to provide advanced relational context for objects.

Effective Document Image Retrieval (DIR) requires the use of appropriate Information Retrieval (IR) methods.
Our research indicates that some IR techniques found to be effective for electronic text retrieval do not transfer to DIR in a simple predictable manner, and that modifications must be made to these methods to enable them to work effectively for DIR.

Information Retrieval Systems identify content bearing words, and possibly also assign weights, as part of the process of formulating requests.
For optimal retrieval ee-ciency, it is desirable that this be done automatically.
This paper deenes the notion of serial-clustering of words in text, and explores the value of such clustering as an indicator of a word's bearing content.
This approach is exible in the sense that it is sensitive to context: a term may be assessed as content-bearing within one collection, but not another.
Our approach, being numerical, may also be of value in assigning weights to terms in requests.
Experimental support is obtained from natural text databases in three diierent languages.

This paper evaluates the retrieval effectiveness of distributed information retrieval systems in realistic environments.
We find that when a large number of collections are available, the retrieval effectiveness is significantly worse than that of centralized systems, mainly because typical queries are not adequate for the purpose of choosing the right collections.
We propose two techniques to address the problem.
One is to use phrase information in the collection selection index and the other is query expansion.
Both techniques enhance the discriminatory power of typical queries for choosing the right collections and hence significantly improve retrieval results.
Query expansion, in particular, brings the effectiveness of searching a large set of distributed collections close to that of searching a centralized collection.

This article presents the main features of a novel construction, symbolic analysis, for automatic source code processing.
The method is superior to the known methods, because it uses a semiotic, interpretative approach.
Its most important processes and characteristics are considered here.
We describe symbolic information retrieval and the process of analysis in which it can be used in order to obtain pragmatic information.
This, in turn, is useful in understanding a current Java program version when developing a new version.

A number of coordinate constructions in natural languages conjoin sequences which do not appear to correspond to syntactic constituents in the traditional sense.
One striking instance of the phenomenon is afforded by the "gapping" construction of English, of which the following sentence is a simple example 1)
Harry eats beans, and Fred, potatoes
Since all theories agree that coordination must in fact be an operation upon constituents, most of them have dealt with the apparent paradox presented by such constructions by supposing that such sequences as the right conjunct in the above example, Fred, potatoes, should be treated in the grammar as traditional constituents, of type S, but with pieces missing or "deleted Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-89-76.
This technical report is available at ScholarlyCommons:
repository.upenn.edu/cis_reports/820 Gapping As Constituent Coordination MS-CIS-89-76 LINC LAB 162

In this paper, we reflect on ways to improve the quality of bio-medical information retrieval by drawing implicit negative feedback from negated information in noisy natural language search queries.
We begin by studying the extent to which negations occur in clinical texts and quantify their detrimental effect on retrieval performance.
Subsequently, we present a number of query reformulation and ranking approaches that remedy these shortcomings by resolving natural language negations.
Our experimental results are based on data collected in the course of the TREC Clinical Decision Support Track and show consistent improvements compared to state-of-the-art methods.
Using our novel algorithms, we are able to reduce the negative impact of negations on early precision by up to 65%.

This work shows Information Retrieval experiments performed over handwritten documents produced by a single writer.
The same retrieval task has been performed over both manual (no errors) and automatic (Word Error Rate around 45 transcriptions of 200 handwritten texts.
The results show that the performance loss due to recognition errors is acceptable and that Information Retrieval technologies can be effectively applied to handwritten data.
2005 Elsevier B.V.
All rights reserved.

Textual information is becoming increasingly available in electronic forms.
Users need tools to sift through non-relevant information and retrieve only those pieces relevant to their needs.
The traditional methods such as Boolean operators and key terms have somehow reached their limitations.
An emerging trend is to combine the traditional information retrieval and artificial intelligence techniques.
This paper explores the possibility of extending traditional information retrieval systems with knowledge-based approaches to automatically expand natural language queries.
Two types of knowledge-bases, a domain-specific and a general world knowledge, are used in the expansion process.
Experiments are also conducted using different search strategies and various combinations of the knowledge-bases.
Our results show that an increase in retrieval performance can be obtained using certain knowledge-based approaches.

he complexity of human language makes accessing multilingual information diffi cult.
Most cross-language retrieval systems attempt to address linguistic variation through language-specifi c techniques and resources.
In contrast, APL has developed a multilingual information retrieval system, the Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT which incorporates fi ve language-neutral techniques: character n-gram tokenization, a proprietary term similarity measure, a language model document similarity metric, pre-translation query expansion, and exploitation of parallel corpora.
Through extensive empirical evaluation on multiple internationally developed test sets we have demonstrated that the knowledge-light, language-neutral approach used in HAIRCUT can achieve state-of-the-art retrieval performance.
In this article we discuss the key techniques used by HAIRCUT and report on experiments verifying the effi cacy of these methods.

We propose a conceptual model for semantic search and implement it in security access control.
The model provides security access control to extend the search capabilities.
The scalable model can integrate other ontology providing the general ontology as the transformation interface.
We combine text Information Retrieval (IR) with semantic inference in the model.
So it can not only search the resources and the relationships between them according to the user’s privileges, but also locate the exact resource using text IR.
We build a security ontology based on Role-Based Access Control (RBAC) policy.
A semantic search system Onto-SSSE is implemented based on the model.
The system can perform some complex queries using ontology reasoning, especially about association queries such as the relationships between resources.
The evaluation shows that the new system performs better than exiting methods.

The Problem: Traditional information retrieval systems based on the “bag-of-words” paradigm cannot capture the semantic content of documents.
While these systems are relatively robust and have high recall, they suffer from very poor precision.
On the other hand, it is impossible with current technology to build a practical information access system that fully analyzes and understands unrestricted natural language.
Existing natural language systems, despite their high precision, have low recall and lack robustness.

The large number of potential applications from bridging web data with knowledge bases have led to an increase in the entity linking research.
Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base.
Potential applications include information extraction, information retrieval, and knowledge base population.
However, this task is challenging due to name variations and entity ambiguity.
In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.

This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine.
The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words.
PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL
On both tests, the algorithm obtains a score of 74 PMI-IR is contrasted with Latent Semantic Analysis (LSA which achieves a score of 64% on the same 80 TOEFL questions.
The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).

This extended abstract describes in detail a submission to the task on Audio Cover Song in the Music Information Retrieval eXchange in 2006.
The system uses as feature set a chord sequence identified by an HMM trained with audiofrom-symbolic data, and computes a distance between two chord sequence pair using the Dynamic Time Warping algorithm to find the minimum alignment cost.
The rational behind the system is that cover songs largely preserve harmonic content even if they vary in other musical attributes such as instrumentation, tempo, key, and/or melody.

The ever-increasing information on the web with its heterogeneity and dynamism needs an information retrieval system which serves searcher’s ambiguous, ill-formed, short queries with relevant result in a precise way.
Web search result clustering has been emerged as a method which overcomes these drawbacks of conventional information retrieval (IR) systems.
It is the clustering of results returned by the search engines into meaningful, thematic groups.
This paper gives a succinct overview and categorizes various techniques that have been used in clustering of web search results.

Mass estimation, an alternative to density estimation, has been shown recently to be an effective base modelling mechanism for three data mining tasks of regression, information retrieval and anomaly detection.
This paper advances this work in two directions.
First, we generalise the previously proposed one-dimensional mass estimation to multidimensional mass estimation, and significantly reduce the time complexity to O x03C8;h) from O x03C8;<sup>h</sup making it feasible for a full range of generic problems.
Second, we introduce the first clustering method based on mass-it is unique because it does not employ any distance or density measure.
The structure of the new mass model enables different parts of a cluster to be identified and merged without expensive evaluations.
The characteristics of the new clustering method are i) it can identify arbitrary-shape clusters ii)
it is significantly faster than existing density-based or distance-based methods; and (iii) it is noise-tolerant.

Dynamical Systems Wenbing Huang, Fuchun Sun, Lele Cao, Deli Zhao, Huaping Liu and Mehrtash Harandi 1 Department of Computer Science and Technology, Tsinghua University, State Key Lab. of Intelligent Technology and Systems, Tsinghua National Lab.
for Information Science and Technology
(TNList 3 Australian National University NICTA, Australia huangwb12@mails, fcsun@mail, caoll12@mails, hpliu@mail}.tsinghua.edu.cn
, 2 zhaodeli@gmail.com, 3 Mehrtash.Harandi@nicta.com.au,

This paper is based on a five-day workshop on "Ranked XML Querying" that took place in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people from three different research communities: database systems (DB information retrieval (IR and Web.
The seminar title was interpreted in an IR-style "andish" sense (it covered also subsets of {Ranking, XML, Querying with larger sets being favored) rather than the DB-style strictly conjunctive manner.
So in essence, the seminar really addressed the integration of DB and IR technologies with Web 2.0 being an important target area.

Semantic representation of information has some evident advantages from an integration point of view, but there are many situations in which this approach is not directly applicable, as information is stored in several formats without (implicit or explicit) knowledge of its semantics.
We present a distributed middleware able to unify the access to any arbitrary information source by doing a semantic translation of its contents.

Information Retrieval in the World Wide Web (WWW) using the general search engines is still a difficult and time-consuming task, which creates a demand of new information search and retrieval techniques.
An intelligent retrieval system model based on multi-agent is proposed which assists the user in finding interesting information in the WWW.
This model searches through the popular and special Web search engines, filters their result, and lists to the user a reduced number of information with high probability of being relevant to him.
This filtering is implemented by the agent analyzing the user's behavior on the Web based on user's search intent and preferences.
The design, the architecture, the functional components and the workflow of the model are elaborated.
The model is helpful to solve the shortcomings effectively of low precision and relevant document ranking behind in general search engine.

Approach based on clustering will be described in our paper.
Basic version of our system was given in [5] allows us to expand query through special index.
Hierarchical agglomerative clustering of the whole document collection generates the index.
Retrieving of topic development is specific problem.
Standard methods of IR does not allow us such kind of queries for appropriate solution of information problem.
The goal of presented method is to find list of documents that are bearing on topic, represented by user-selected document, sorted with respect to historical development of the topic.

Clustering is a data mining technique for the analysis of data in various areas such as pattern recognition, image processing, information science, bioinformatics etc.
Hierarchical clustering techniques form the clusters based on top-down and bottom-up approaches.
Hierarchical agglomerative clustering is a bottom-up clustering method.
Ant based clustering methods form clusters by picking and dropping the objects according to surroundings.
This paper proposes an agglomerative clustering algorithm, AGG_ANTS based on ant colonies.
AGG_ANTS clusters the objects by moving ants on the grid and merging their loads according to similarity resulting in bigger clusters.
It avoids the calculation of similarity in the surrounding and pick/drop of objects again and again resulting in a more efficient algorithm.

This article reports the results of Chinese Text Retrieval (CHTR) tasks in NTCIR Workshop 2 and the future plan of NTCIR workshop.
CHTR tasks fall into two categories: Chinese-Chinese IR (CHIR) and English-Chinese IR (ECIR
The definitions, schedules, test collection (CIRB010 search results, evaluation, and initial analyses of search results of CHIR and ECIR are discussed in this article.
The new plan of NTCIR towards multilingual Cross-Language Information Retrieval (CLIR) is also described.

Similar Web pages are easily found on Internet.
The redundancy of information severely slows down internet applications such as crawl module of search engine, and could lead to waste of storage in the indexing procedure.
In this paper, we proposed a content-based approach for detecting webpage duplications.
The algorithm contains three parts: i) pre-processing, excluding HTML tags and unrelated information; ii) use a query-combined fuzzy set information retrieval approach to find out the correlation between every two documents; iii) a threshold is set and duplicate webpages are eliminated.
Original algorithm of duplication detection is revised and focused mainly on performance optimization.
Testing results shows that the performance is greatly improved with an acceptable sacrifice on quality.

For many information retrieval applications, we need to deal with the ranking problem on very large scale graphs.
However, it is non-trivial to perform efficient and effective ranking on them.
On one aspect, we need to design scalable algorithms.
On another aspect, we also need to develop powerful computational infrastructure to support these algorithms.
This tutorial aims at giving a timely introduction to the promising advances in the aforementioned aspects in recent years, and providing the audiences with a comprehensive view on the related literature.

This paper describes a new approach in Cross-Language Information Retrieval that combines query expansion techniques before and after query translation and disambiguation.
Moreover, a new technique based on domain keywords extraction is proposed.
Test results showed the effectiveness of the combined method.

This paper reports on MIMOR which was modelled as an open meta information retrieval system at the handling of diverse information retrieval objects.
It is used to gain further insights concerning the behaviour of information retrieval systems as well as for teaching in the field of Information Science.
The main issue of MIMOR is the exploitation of users’ relevance feedback in order to optimise the fusion of several retrieval engines or resources.
As yet, MIMOR has been applied to a domain specific collection, to a multilingual corpus, and to music data.
Experiments within the CLEF evaluation framework and additional ones are discussed here.

Knowledge acquisition constitutes the bottleneck for the creation of legal expert systems.
A certain degree of formalism of legal language is an inevitable prerequisite.
Our prototype KONTERM deals with that problem by supporting the process of creating a selective thesaurus for a legal information system which can be used for automatic indexing and document classification.
This selectivity is obtained by distinguishing between precise legal terms and words with fuzzy meanings.
The resulting thesaurus can thus be seen as an important step to overcome the "untidiness" of natural language and to represent automatically the expert knowledge of a lawyer about legal terminology.
Therefore, KONTERM can be used as a tool for the semi-automatic creation of a legal knowledge base.

One of the first Multi-Language Information Retrieval (MLIR) systems was implemented in 1969 by Gerard Salton who enhanced his SMART system to retrieve multilingual documents in two languages, English and German.
However, the research field of MLIR is still struggling since the majority of information retrieval systems are monolingual and more precisely English-based, even though only 6&#x025; of the world&#x02019;s population native language have as English [14 This paper presents a Multi-Language Information Retrieval (MLIR) approach that falls into the area of Domain Specific Information Retrieval (E-learning being the domain
The approach we followed is a synergistic approach between (1) Thesaurus-based Approach and (2) Corpus-based Approach.
This research has been implemented on a real platform called HyperManyMedia1 at Western Kentucky University.

The areas of information retrieval(IR) and information filtering(IF)have become very active research domains.
The problems created by the large increase of available online information, of which the vast majority is largely unstructured, have accentuated the need for effective mechanisms to separate the relevant information from the irrelevant.
This paper reviews the main approaches and systems used in IR and in the newer field of IF.
The paper also includes an overview of systems which utilise social or collaborative filtering techniques to deal with the problem of information overload.

Most people have great difficulty in recalling unrelated items.
For example, in free recall experiments, lists of more than a few randomly selected words cannot be accurately repeated.
Here we introduce a phenomenological model of memory retrieval inspired by theories of neuronal population coding of information.
The model predicts nontrivial scaling behaviors for the mean and standard deviation of the number of recalled words for lists of increasing length.
Our results suggest that associative information retrieval is a dominating factor that limits the number of recalled items.

The <i>feature quantity</i a quantitative representation of specificity introduced in this paper, is based on an information theoretic perspective of co-occurrence events between terms and documents.
Mathematically, the feature quantity is defined as a product of probability and information, and maintains a good correspondence with the <i>
tfidf</i>-like measures popularly used in today's IR systems.
In this paper, we present a formal description of the feature quantity, as well as some illustrative examples of applying such a quantity to different types of information retrieval tasks: representative term selection and text categorization.

Semantic similarity measures play vital roles in Information Retrieval (IR) and Natural Language Processing (NLP
Despite the usefulness of semantic similarity measures in various applications, strongly measuring semantic similarity between two words remains a challenging task.
Here, three semantic similarity measures have been proposed, that uses the information available on the web to measure similarity between words and sentences.
The proposed method exploits page counts and text snippets returned by a web search engine.
We develop indirect associations of words, in addition to direct for estimating their similarity.
Evaluation results on different data sets shows that our methods outperform several competing methods.

With the growing significance of digital libraries and the Internet, more and more electronic texts become accessible to a wide and geographically disperse public.
This requires adequate tools to facilitate indexing, storage, and retrieval of documents written in different languages.
We present a method for semi-automatic indexing of electronic documents and construction of a multilingual thesaurus, which can be used for query formulation and information retrieval.
We use special dictionaries and user interaction in order to solve ambiguities and find adequate canonical terms in the language and an adequate abstract language-independent term.
The abstract thesaurus is updated incrementally by new indexed documents is used to search document concerning terms in a query to the document base.

The paper briefly reviews a theory of intonational prosody and its relation syntax, and to certain oppositions of discourse meaning that have variously been called "topic and comment theme and rheme given and new or "presupposition and focus The theory, which is based on Combinatory Categorial Grammar, is presented in full elsewhere.
The present paper examines its implications for the semantics of "focus Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-91-63.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/815 Surface Structure, Intonation, And "Focus" MS-CIS-91-63 LINC LAB 206

In Cross-Language Information Retrieval (CLIR) process, Out-Of-Vocabulary (OOV) or the unknown word translation is a significant and challenging issue.
Specifically, for English-Chinese OOV translation, OOV term detection and extraction of translation pair still remain to be key problems.
In this paper, an English-Chinese OOV translation pattern based on PAT-Tree is proposed.
Web-mining is utilized as the corpus source to collect translation pairs, and translation candidates are acquired by Chinese OOV term extraction based on PAT-Tree.
The experimental results show that the proposed approach can outperform some of the current translation engines, and is especially efficient in English-Chinese OOV translation.

Latent Semantic Analysis (LSA) is a widely used Information Retrieval method based on bag-of-words assumption.
However, according to general conception, syntax plays a role in representing meaning of sentences.
Thus, enhancing LSA with part-of-speech (POS) information to capture the context of word occurrences appears to be theoretically feasible extension.
The approach is tested empirically on a automatic essay grading system using LSA for document similarity comparisons.
A comparison on several POS-enhanced LSA models is reported.
Our findings show that the addition of contextual information in the form of POS tags can raise the accuracy of the LSA-based scoring models up to 10.77 per cent.

Although context-independent word-based approaches remain popular for cross-language information retrieval, many recent studies have shown that integrating insights from modern statistical machine translation systems can lead to substantial improvements in effectiveness.
In this paper, we compare flat and hierarchical phrase-based translation models for query translation.
Both approaches yield significantly better results than either a token-based or a one-best translation baseline on standard test collections.
The choice of model manifests interesting tradeoffs in terms of effectiveness, efficiency, and model compactness.

The medical information sciences program of Amsterdam has been in existence for 15 years now.
Starting in 1987, the program has been modified several times.
Now a full-fledged 4 years master program exists.
Students are taught skills to adequately and systematically apply information and communication technologies in order to optimize health care information processing.
The program is offered within the Faculty of Medicine of the University of Amsterdam.
The structure and contents of the current program will be described.

We describe in this paper a multiagent possibilistic system for web information retrieval, called SARIPOD.
This system is based on Hierarchical Small-Worlds (HSW) and Possibilistic Networks (PN
The first HSW consists in structuring the "Google" search results in dense zones of web pages which strongly depend on each other.
We thus reveal dense clouds of pages which "speak" more or less about the same subject and which all strongly answer the user’s query.
The goal of the second HSW consists in considering the query as multiple in the sense that we don’t seek only the keyword in the web pages but also its synonyms.
The PN generates the mixing of these two HSW in order to organize the searched documents according to user’s preferences.

Online learning to rank for information retrieval has shown great promise in optimization of Web search results based on user interactions.
However, online learning to rank has been used only in the monolingual setting where queries and documents are in the same language.
In this work, we present the first empirical study of optimizing a model for Cross-Language Information Retrieval (CLIR) based on implicit feedback inferred from user interactions.
We show that ranking models for CLIR with acceptable performance can be learned in an online setting although ranking features are noisy because of the language mismatch.

In informetrics, journals have been used as a standard unit to analyze research impact, productivity, and scholarship.
The increasing practice of interdisciplinary research challenges the effectiveness of journalbased assessments.
The goal of this paper is to highlight topics as a valuable unit of analysis.
A set of topic-based approaches is applied to a data set on library and information science publications.
Results show that topic-based approaches are capable of revealing the research dynamics, impact, and dissemination of the selected data set.
The paper also identifies a non-significant relationship between topic popularity and impact, and argues for the need to use both variables in describing topic characteristics.
Additionally, a flow map illustrates critical topic-level knowledge dissemination channels.

This paper describes an ontology-based approach aiming at helping biologists to annotate their documents and at facilitating their information retrieval task.
Our approach, based on semantic web technologies, relies on formalised ontologies, semantic annotations of scientific articles and knowledge extraction from texts.
We propose a method/system for the generation of ontology-based semantic annotations (MeatAnnot) and a system allowing biologists to draw advanced inferences on these annotations (MeatSearch This approach was proposed to support biologists working on DNA microarray experiments in the validation and the interpretation of their results, but it can probably be extended to other massive analyses of biological events (as provided by proteomics, metabolomics

Keyword-based retrieval matches search terms and documents via term co-occurrence.
Such an approach does not allow matching based on the specific plant characteristic descriptions that are often used in botanical text retrieval.
This study applies information extraction techniques to automatically extract plant characteristic information from text and allows users to search using such information in combination with keywords.
An evaluation experiment was conducted using actual users.
The results indicate that this approach enhances task-based retrieval performance.

This paper examines the validity of the User Engagement Scale (UES Originally developed and tested in e-shopping, the scale was administered to users of a multimedia webcast system in an experimental setting.
Factor analysis examined the structure and loadings of 31 items.
As in previous research, a six-factor solution was found.
However, the number of items was reduced and one of the original sub-scales (Felt Involvement) was eliminated.
These results are examined contextually by comparing the current study with previous research.
The findings discuss the feasibility of a universal measure of user engagement in Interactive Information Retrieval (IIR).

The bottleneck for dictionary-based cross-language information retrieval is the lack of comprehensive dictionaries, in particular for many different languages.
We here introduce a methodology by which multilingual dictionaries (for Spanish and Swedish) emerge automatically from simple seed lexicons.
These seed lexicons are automatically generated, by cognate mapping, from (previously manually constructed) Portuguese and German as well as English sources.
Lexical and semantic hypotheses are then validated and new ones iteratively generated by making use of co-occurrence patterns of hypothesized translation synonyms in parallel corpora.
We evaluate these newly derived dictionaries on a large medical document collection within a cross-language retrieval setting.

Learning to rank plays a very important role in information retrieval.
Existing works mainly focus on applying one ranking model to all samples, which may not be suitable for the reality.
In this paper, a new method for learning to rank based on query-level vector extraction is proposed, in which we assume that all samples can be divided into multiple parts, and each part is used to train one set of parameters for the model.
Based on this assumption, we extracted query-level vector and proposed a dataset partition method based on k-means which is used to optimize the ListNet method and RankNet method.
Experimental results show that our assumption is right and our method plays a very important role in improving the performance of ListNet and RankNet, and which is also easy to be extended to other learning to rank methods.

Signed social networks are social networks with edges indicative of both trust and distrust.
We propose novel game theoretic centrality measures for signed networks, which first generalize degree-based centrality from nodes to sets, and then compute individual node centralities using the concept of Shapley Value.
We derive closed form expressions for the Shapley Value for most of these measures.
Moreover, we demonstrate that some of these measures give improved AP (average precision) compared to net positive in-degree for the task of detecting troll users, in the Slashdot signed network.

We propose a Bayesian extension to the ad-hoc Language Model.
Many smoothed estimators used for the multinomial query model in ad-hoc Language Models (including Laplace and Bayes-smoothing) are approximations to the Bayesian predictive distribution.
In this paper we derive the full predictive distribution in a form amenable to implementation by classical IR models, and then compare it to other currently used estimators.
In our experiments the proposed model outperforms Bayes-smoothing, and its combination with linear interpolation smoothing outperforms all other estimators.

The paper presents a uniied approach to intelligent agency that has been developed by the members of the Laboratory for Knowledge Representation in Logic at the Department of Computer and Information Science, University of Linkk oping since 1986.
The approach is based on the theory of Inhabited Dynamical Systems proposed and developed by Sandewall 27 on the integrated layered software architecture concept developed at our lab 24] and on the research on speciication of reactive system behaviour pursued since 1990 15].

This paper firstly studies the features of content-based multimedia information retrieval, then analyzes the system structure of content-based multimedia information retrieval and retrieval procedures, lastly the author discusses the exist limitations and prospects of content-based multimedia information retrieval.

We argue that the advent of large volumes of full-length text, as opposed to short texts like abstracts and newswire, should be accompanied by corresponding new approaches to information access.
Toward this end, we discuss the merits of imposing <italic>structure</italic> on full-length text documents; that is, a partition of the text into coherent multi-paragraph units that represent the pattern of subtopics that comprise the text.
Using this structure, we can make a distinction between the main topics, which occur throughout the length of the text, and the subtopics, which are of only limited extent.
We discuss why recognition of subtopic structure is important and how, to some degree of accuracy, it can be found.
We describe a new way of specifying queries on full-length documents  and then describe an experiment in which making use of the recognition of local structure achieves better results on a typical information retrieval task than does a standard IR measure.

From 01.03.
to 06.03.2009, the Dagstuhl Seminar 09101 Interactive Information Retrieval was held in Schloss Dagstuhl Leibniz Center for Informatics.
During the seminar, several participants presented their current research, and ongoing work and open problems were discussed.
Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper.
The rst section describes the seminar topics and goals in general.
Links to extended abstracts or full papers are provided, if available.

Our participation at ResPubliQA 2010 was based on applying an Information Retrieval (IR) engine of high performance and a validation step for removing incorrect answers.
The IR engine received additional information from the analysis of questions, what produces a slight improvement in results.
However, the validation module discarded sometimes too much correct answers, contributing to reduce the overall performance.
These errors were due to the application of too strict constraints.
Therefore, future work must be focused on reducing the amount of false negatives returned by the validation module.
On the other hand, we observed that IR ranking offers important information for selecting the final answer, but better results could be obtained if additional sources of information were also considered.

This paper follows a formal approach to information retrieval based on statistical language models.
By introducing some simple reformulations of the basic language modeling approach we introduce the notion of importance of a query term.
The importance of a query term is an unknown parameter that explicitly models which of the query terms are generated from the relevant documents (the important terms and which are not (the unimportant terms The new language modeling approach is shown to explain a number of practical facts of today's information retrieval systems that are not very well explained by the current state of information retrieval theory, including stop words, mandatory terms, coordination level ranking and retrieval using phrases.

As an alternative to publish or access information, P2P network is attracting more and more attention because of its advantages in scalability, decentralization and selforganization.
While a lot of research has been focused on semantic query routing in information system over P2P network, little work concerns if semantic query routing can satisfy the requirement of semantic information retrieval, and to what extent the query routing mechanism can provide the information semantically related to users’ query.
This paper studies the application of semantics in traditional information retrieval and information system over P2P network, demonstrates the gap between semantic information retrieval and semantic query routing, and finally present the challenge of semantic information retrieval over P2P network.
MOTS-CLÉS Recherche d’information, réseau P2P, Sémantique, Echelle Sémantique, Routage Sémantique

As examples of manipulation of multipartite entanglement, we demonstrate a quantum teleportation network and telecloning for continuous variables.
Basic studies on quantum information science are being made with quantum optics intensively.
Continuous-variable (CV) approach attracts much interest because of relative easiness to realize the experiments.
Unconditional quantum teleportation was realized in the first time with this approach[1 and various experimental successes of quantum teleportation and other protocols have been reported.[2-6]
In this paper, we will show the progress of this CV approach especially on realization of a quantum teleportation network[7,8] and telecloning[9,10
The heart of these protocols is multipartite entanglement.
In these experiments, we use three entangled light beams (modes A, B, and C) which satisfy the inequalities listed below[10

Traditional information retrieval (IR) systems evaluate user queries and retrieve/rank documents based on matching keywords in user queries with words in documents.
These exact word-matching and ranking approaches ignore too many relevant documents that do not contain the exact keywords as specified in a user query.
Instead of considering these traditional approaches, we propose to retrieve documents using a fuzzy set IR model and rank retrieved documents for any vague query using the "vagueness score" of the documents based on the word senses as defined in WordNet.
Using the vagueness scores, we rank the most highest "relevant" documents of a vague query qas the ones that best cover the different possible senses of keywords in q.
The proposed word-sense ranking method enhances the existing ranking approaches on ordering retrieved documents for vague queries and thus provides a more reliable and elegant tool for information retrieval.

6 1 State Key Laboratory of Intelligent Technology and Systems, Tsinghua National 7 Laboratory for Information Science and Technology, Department of Computer 8 Science and Technology, Tsinghua University, Beijing, China 9 2 Department of Computer Science and Technology, Zhejiang University of 10 Technology, Hangzhou, China 11 3 Institute of Neuroscience, Key Laboratory of Primate Neurobiology, CAS Center for 12 Excellence in Brain Science and Intelligence Technology, Shanghai Institutes for 13 Biological Sciences, Chinese Academy of Sciences 14
15 *Address correspondence to: 16 Tianming Yang (tyang@ion.ac.cn)
17 Institute of Neuroscience 18 320 Yue Yang Rd.
19 Building 23, Room 313 20 Shanghai, China 200031 21 Phone 86-21-54921737, Fax 86-21-54921735 22

An effective retrieval of the most relevant documents from the Web is difficult due to the large amount of information in all types of formats.
Researches have been conducted on ways to improve the efficiency of Information Retrieval (IR) systems.
To arrive to suitable solutions in IR systems, machines need additional semantic information that helps in understanding Web documents.
This is made true by an intelligent web called the Semantic Web, which offers users the ability to work on shared meaningful knowledge representations on the web.
Semantic Web makes the Web content meaningful to computers and it intends to support machine-processing capabilities.
Using Semantic Web is a way to increase the precision of IR systems.
This paper focuses on the various Semantic-based approaches in Web mining research.

Peer-to-peer (P2P) networks have received more and more attention from researchers.
P2P seems to be an interesting architectural paradigm for realizing large-scale information retrieval systems for its scalability, failure resilience and increased autonomy of nodes.
This paper provides a novel peer-to-peer networks system that is based on information retrieval in a large-scale collection of texts, and a semantic similarity model is developed and applied in it, which improves the performance of the system.
Some natural language processing technologies are adopted to increase the accuracy of the system.
Several useful tools are incorporates as external auxiliary resources.
In addition, feedback knowledge such as query information from peers is also widely used to direct querying messages flooding based on a semantic routing mechanism in this system.
Finally, an experimental study is used to verify the advantages of system, and the results are comparatively satisfying.

Searching online databases requires an information retrieval strategy formalized in the EURISKO expert system.
This search strategy is based on different kinds of planning: at the highest level a plan orders a linear and hierarchical planning for the request interrogation and a dynamic planning for the request modification.
The recent development of the system has allowed to supply some new judgements on this approach.

The bulk of clinical data is available in an electronic form.
About 80% of the electronic data, however, is narrative text and therefore limited with respect to machine interpretation.
As a result, the discussion has shifted from "electronic versus paper based data" towards "structured versus unstructured electronic data The XML technology of today paves a way towards more structured clinical data and several XML based standards such as the Clinical Document Architecture (CDA) emerge.
The implementation of XML based applications is yet a challenge.
This paper will focus on XML retrieval issues and describe the difficulties and prospects of such an approach.
The result of our work is a search technique called "topic matching" that exploits structured data in order to provide a search quality that is superior to established text matching methods.
With this solution we are able to utilize large numbers of heterogeneously structured documents with only a minimum of effort.

Users of information retrieval systems employ a variety of strategies when searching for information.
One factor that can directly influence how searchers go about their information finding task is the level of familiarity with a search topic.
We investigate how the search behavior of domain experts changes based on their previous level of familiarity with a search topic, reporting on a user study of biomedical experts searching for a range of domain-specific material.
The results of our study show that topic familiarity can influence the number of queries that are employed to complete a task, the types of queries that are entered, and the overall number of query terms.
Our findings suggest that biomedical search systems should enable searching through a variety of querying modes, to support the different search strategies that users were found to employ depending on their familiarity with the information that they are searching for.

This paper provides a survey of the application of neural network in information retrieval systems.
Various soft computing techniques are being used in information retrieval (IR Neural network, a soft computing technique, is studied to resolve the problems of extracting the keywords and retrieving the relevant from large database.
Literature shows possible applications of variant of neural networks in IR to serve the purpose of retrieving the right documents.
The theoretical implementation of neural network in vector space model of IR shows importance of neural network in IR.

Multimodal information retrieval is a research problem of great interest in all domains, due to the huge collections of multimedia data available in different contexts like text, image, audio and video.
Researchers are trying to incorporate multimodal information retrieval using machine learning, support vector machines, neural network and neuroscience etc.
to provide an efficient retrieval system that fulfills user need.
This paper is an overview of multimodal information retrieval, challenges in the progress of multimodal information retrieval.

We report on the effectiveness of language models for personalization of retrieval results based on a searcher&#146;s preference for document genre.
In principle, such preferences can be obtained via implicit relevance feedback through the observation of the searcher&#146;s actions and behavior during search sessions.
While our approach did not produce significant improvement to retrieval effectiveness, the methodology and experimental setting can and are being used for further work on exploring genre-based personalization.

Nowadays in information retrieval it is generally accepted that if we can better understand the context of searchers then this could help the search process, either at indexing time by including more metadata or at retrieval time by better modelling the user needs.
In this work we explore how activity recognition from tri-axial accelerometers can be employed to model a user's activity as a means of enabling context-aware information retrieval.
In this paper we discuss how we can gather user activity automatically as a context source from a wearable mobile device and we evaluate the accuracy of our proposed user activity recognition algorithm.
Our technique can recognise four kinds of activities which can be used to model part of an individual's current context.
We discuss promising experimental results, possible approaches to improve our algorithms, and the impact of this work in modelling user context toward enhanced search and retrieval.

This paper presents a novel, ranking-style word segmentation approach, called RSVMSeg, which is well tailored to Chinese information
retrieval(CIR
This strategy makes segmentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence.
On the training corpus composed of query items, a ranking model is learned by a widely-used tool Ranking SVM, with some useful statistical features, such as mutual information, difference of t-test, frequency and dictionary information.
Experimental results show that, this method is able to eliminate overlapping ambiguity much more effectively, compared to the current word segmentation methods.
Furthermore, as this strategy naturally generates segmentation results with different granularity, the performance of CIR systems is improved and achieves the state of the art.

Peer-to-Peer (P2P) computing has rapidly evolved into an effective computing paradigm for people to exchange information and share knowledge within a domain.
It has been widely recognized that bibliographic information plays an increasingly important role for scientific research.
This paper presents OBIRE, a P2P network for bibliographic information retrieval.
To improve the accuracy of OBIRE in information retrieval, semantic web technologies are utilized.
For a user query, OBIRE computes the degree of matches to indicate the similarity of a published record to the query.
When searching for information, users can incorporate their domain knowledge into their queries which guides OBIRE to discover the bibliographic records that are of most interest of users.
OBIRE is evaluated from the aspects of precision and recall, and experimental results show the effectiveness of OBIRE in bibliographic information retrieval.

In the light of Quantum Physics, probabilistic technique is said to support a backbone in the domain of Quantum Information Retrieval (QIR
This well known domain is based on the Quantum Probability Theory, which is used to examine the probability of relevance of a document given a user's query.
The Quantum Probability Ranking Principle (QPRP captures dependencies between documents in absolute way via x201C;quantum interference&#x201D; by considering a single vector i.e one-dimension subspace.
The HAL semantic space can be used to capture meaning of the document of information needs by representing the document into a high dimensional semantic space.
This paper presents an investigation of the Quantum Probability Ranking Principle with the concept of HAL semantic space such that which can act as a bridge the gap between Quantum Probability Theory and HAL semantic space, that can results to agreeable performance in an ad-hoc retrieval task of document ranking.

This paper presents a methodology for automatically indexing a large corpus of broadcast baseball games using an unsupervised content-based approach.
The method relies on the learning of a grounded language model which maps query terms to the non-linguistic context to which they refer.
Grounded language models are learned from a large, unlabeled corpus of video events.
Events are represented using a codebook of automatically discovered temporal patterns of low level features extracted from the raw video.
These patterns are associated with words extracted from the closed captioning text using a generalization of Latent Dirichlet Allocation.
We evaluate the benefit of the grounded language model by extending a traditional language model based approach to information retrieval.
Experimental results indicate that using a grounded language model nearly doubles performance on a held out test set.

Recommender systems help customers to choose right product or service from large number of alternatives available on Internet.
In recent time, trust becomes an important issue in designing effective recommender systems.
In this paper we have studied the role of trust and distrust in designing recommender systems.
General Terms E-Commerce, Information Retrieval, Web Mining.

With the rapidly growing amount of available information, an important issue arises of matching the user queries in one language against documents in another language, i.e. cross-language information retrieval (CLIR Based on the work in CLIR evaluation task at the 9<sup>th</sup>
Text Retrieval Conference (TREC-9 an English-Chinese CLIR system has been established.
In this system, English-Chinese query translation is adopted as the dominant strategy; English queries are used as translation objects, and English-Chinese machine readable dictionary is utilized as the important knowledge source to acquire correct translations.
By combining constructed Chinese monolingual IR system, the complete English-Chinese CLIR process is successfully implemented.

Extraction of phrasal knowledge, such as proper names, domain-specific keyphrases and lexical templates from a domain-specific text collection are significant for developing effective information retrieval systems for the Internet.
In this paper, we are going to introduce our ongoing research on automatic phrasal knowledge acquisition for English-Chinese bilingual texts.
The underlying techniques consist of adaptive keyphrase extraction, lexical template extraction, phrase translation extraction and high-order Markov language model construction.
In addition to the increase of retrieval effectiveness, IR systems based on these techniques are expected able to perform much better in many aspects, such as automatic term suggestion, information filtering, text classification and cross-language information retrieval, etc.

The problem of linking syntactic constituents of a sentence to semantic roles is an essential part of many natural language processing tasks.
The research outlined here aims to develop a statistical approach to the problem, extending the methodology that has been very successful in statistical parsing one step closer to language understanding.
Both speci c and more abstract semantic roles are considered in an e ort to generalize from semantic frames for which training data is available to general text.
Possible applications for a statistical semantic analyzer include spoken language dialog systems, information retrieval, machine translation and the emerging eld of text data mining.

The application of virtual reality technology is a trend that holds considerable promise as demonstrated in many applications.
This technology includes the means to visually integrate and interact with diverse multidimensional data in 3D and time context, extend real-time decision-making into mine maintenance, implement virtual mine-planning simulations, training simulations, and risk management.
Virtual mine is the result of integrating various disciplines including mine science, information science, artificial intelligence, computer science and 3S techniques, which will radically change the traditional mine production and our lifestyles.
The functions and main characters of virtual mine are analyzed.
A framework for a virtual mine operation system is provided.
Key techniques concerned in the implementation of virtual mine are discussed in detail.
The future development of virtual mine is prospected.

and retrieval techniques for homology searching of genomic databases are increasingly important as the search tools are facing great challenges of rapid growth in sequence collection size.
Consequently, the indexing and retrieval of possibly gigabytes sequences become expensive.
In this paper, we present two new approaches for indexing genomic databases that can enhance the speed of indexing and retrieval.
We show experimentally that the proposed methods can be more computationally efficient than the existing ones.

Data driven language learning promotes learner autonomy and discovery learning by providing learners with authentic foreign language data for self-directed or guided exploration.
However, the effective use of corpora data requires a certain level of linguistic knowledge.
We propose an information retrieval augmentation to concordance for adapting to self-directed context of independent learners.
The approach involves an expression element model and a retrieving mechanism so as to reduce linguistic threshold and enhance learner empowerment.
Simulation results with English proficiency tests and students’ writing samples support the effectiveness of the approach.

Cognitive informatics is a growing discipline that incorporates elements from cognitive science and information science in the development of information systems.
The approaches used in cognitive informatics largely inform the design of Electronic Medical Record (EMR) systems.
EMR systems have been extensively used by physicians for diagnosis and healthcare data management, ever since 1990s.
Though most of the EMR systems are GUI-based, they are not designed to incorporate the elements of cognitive informatics in operation.
This research aims to propose a DICOM-compliant approach to enhance cognitive informatics in EMR using sequence logo.

The paper addresses some roles of concept-based representations in document clustering to support knowledge discovery.
Computational Intelligence algorithms can benefit from semantic networks in the definition of similarity between pairs of documents.
After analyzing the tuning of semantic networks in a systematic fashion, the research defines and evaluates a novel semantic-based metrics, which integrates both classical and style-related features of texts.
Experimental results confirm the effectiveness of the approach, showing that applying a refined semantic representation into a clustering engine yields consistent structures for information retrieval and knowledge acquisition.

Read more and get great!
That's what the book enPDFd the modern algebra of information retrieval will give for every reader to read this book.
This is an on-line book provided in this website.
Even this book becomes a choice of someone to read, many in the world also loves it so much.
As what we talk, when you read more every page of this the modern algebra of information retrieval, what you will obtain is something great.

Intuitively, words forming phrases are a more precise description of content than words as a sequence of keywords.
Yet, evidence that phrases would be more effective for information retrieval is inconclusive.
This paper isolates a neglected class of phrases, that is abundant in communication, has an established theoretical foundation, and shows promise for an effective expression of the user's information need:
the noun-noun compound
In an experiment, a variety of meaningful <i>
>s were used to isolate relevant passages in a large and varied corpus.
In a first pass, passages were retrieved based on textual proximity of the words or their semantic peers.
A second pass retained only passages containing a syntactically coherent structure equivalent to the original
This second pass showed a dramatic increase in precision.
Preliminary results show the validity of our intuition about phrases in the special but very productive case of <i>NNC</i>s.

A topic of music information retrieval (MIR) field is query-by-example (QBE which searches a popular music dataset using a user-provided query and aims to find the target song.
Since this type of MIR has been generally used in online systems, retrieval time is also as important as accuracy.
In this paper, we propose a QBE-based MIR system and investigate the impact of automatic music genre prediction on the performance of it, specifically on perspective of accuracy-time trade-off, using a score-based genre prediction method as well as similarity measures.
The proposed system is evaluated on a dataset containing 6000 music pieces from six musical genres, and we show that how much improvement on the performance can be achieved in terms of accuracy and retrieval time, compared with a typical QBE-based MIR system that uses only similarity measures to find the user-desired song.

When building vision systems that predict structured objects such as image segmentations or human poses, a crucial concern is performance under task-specific evaluation measures (e.g. Jaccard Index or Average Precision
An ongoing research challenge is to optimize predictions so as to maximize performance on such complex measures.
In this work, we present a simple meta-algorithm that is surprisingly effective Empirical Min Bayes Risk.
EMBR takes as input a pre-trained model that would normally be the final product and learns three additional parameters so as to optimize performance on the complex high-order task-specific measure.
We demonstrate EMBR in several domains, taking existing state-of-the-art algorithms and improving performance up to ~7 simply with three extra parameters.

Search systems have for some time provided users with the ability to request documents similar to a given document.
Interfaces provide this feature via a link or button for each document in the search results.
We call this feature <i>find-similar</i> or <i>
similarity browsing</i
We examined find-similar as a search tool, like relevance feedback, for improving retrieval performance.
Our investigation focused on find-similar's document-to-document similarity, the reexamination of documents during a search, and the user's browsing pattern.
Find-similar with a query-biased similarity, avoiding the reexamination of documents, and a breadth-like browsing pattern achieved a 23% increase in the arithmetic mean average precision and a 66% increase in the geometric mean average precision over our baseline retrieval.
This performance matched that of a more traditionally styled iterative relevance feedback technique.

The Information Retrieval and Advertising Workshop (IRA 2009) was held on July 23, 2009 in Boston, Massachusetts, in conjunction with the 32nd Annual ACM SIGIR Conference.
The workshop covered theoretical and empirical issues in several research areas that span the intersection of computational advertising, information retrieval, and economics.
The workshop consisted of 3 invited talks, 6 refereed paper presentations, 2 positions statement presentations and several discussion sessions.

Applied discourse analysis is a hot topic in Information Retrieval (IR) and the related field of Information Extraction (IE Although interesting observations about discourse can be made "by hand applications require large quantities of data about language data which is rather uninteresting.
This paper investigates using statistical analysis over a body of text to suggest new rules for recognizing named entities.

We propose a simple method for converting many standard measures of retrieval performance into metasearch algorithms.
Our focus is both on the analysis of retrieval measures themselves and on the development of new metasearch algorithms.
Given the conversion method proposed, our experimental results using TREC data indicate that system-oriented measures of overall retrieval performance (such as average precision) yield good metasearch algorithms whose performance equals or exceeds that of benchmark techniques such as CombMNZ and Condorcet.

Development of recommendation systems are now gaining importance in variety of fields ranging from multimedia content on web sites to travel guides or online product buying to help and guide users in finding out relevant information from the vast pool of information generated everyday.
The research in the area of information retrieval, data mining, marketing or e-commerce shows that integration of contextual information into recommendation system would further improve the efficiency and capability of recommendation system for individual users.
In this work the conceptual framework of an user oriented pedestrian navigation system with the capability of integrating context awareness in route recommendation is proposed.
A very small pilot study is undertaken to measure the effectiveness of the recommendation system with incorporation of context awareness.

We summarize the 2013 Symposium on Human-Computer Interaction and Information Retrieval.
The two-day symposium was held October 3-4 in Vancouver, BC.
The event brought together researchers and practitioners from academia and industry for in-depth discussions in an informal setting.
We accepted rigorously peer-reviewed full papers that will be archived and published in the ACM Digital Library, as well as short papers describing work in progress.

Textual data is at the forefront of information management problems today.
Thousands of pages of text data, in many languages, are produced daily: emails, news reports, blog posts, product reviews, discussion forums, academic articles, and business reports.
Computational linguistics interventions have also increased, as we rely more and more on automated language translation, summarization, enhanced information retrieval, and opinion mining.
Managing, exploring, and analysing the flow of linguistic data is becoming both an individual and a societal problem.
Data scale issues are not only a challenge for end users of technology, but are equally challenging for natural language engineers as they develop the systems that allow computers to manipulate and analyze text.

Disambiguating named entities in natural language texts maps ambiguous names to canonical entities registered in a knowledge base such as DBpedia, Freebase, or YAGO.
Knowing the specific entity is an important asset for several other tasks, e.g. entity-based information retrieval or higher-level information extraction.
Our approach to named entity disambiguation makes use of several ingredients: the prior probability of an entity being mentioned, the similarity between the context of the mention in the text and an entity, as well as the coherence among the entities.
Extending this method, we present a novel and highly efficient measure to compute the semantic coherence between entities.
This measure is especially powerful for long-tail entities or such entities that are not yet present in the knowledge base.
Reliably identifying names in the input text that are not part of the knowledge base is the current focus of our work.

This workshop report presents the output of the fifth Bibliometric-enhanced Information Retrieval (BIR) workshop, which has been co-located with the 39th European Conference on Information Retrieval (ECIR 2017) in Aberdeen, UK.
We motivate our workshop and outline the papers (one keynote, six regular papers and five poster papers) presented at BIR 2017.
Finally, we conclude with an outlook and future directions of this workshop activity.

This paper deals with Salton's systems of information retrieval.
In his model, only the correspondence between requests and objects (documents or document descriptions) is described.
The request language is treated as a partially ordered set which may have a lattice structure or even Boolean algebra structure.
Those structures have influence in simplicity or retrieval methods and in mathematical pictoriality of some important properties of information systems like simplicity or selectivity.
This paper investigates such problems and presents a method of designing artificial attributes and their values.

Document representation has been the fundamental issue in the information retrieval (IR
However, the traditional vector space model (VSM) has data sparseness phenomena on the representation of document vectors, and cannot well discriminate the expression competence to the document content of indexing terms in different positions.
This paper proposes an improved term weighting method by introducing information gain of terms while taking above factors into account.
The theoretical analysis and experimental results show that the new scheme improves the performance of VSM in IR in terms of higher recall and precision.

The relationship between information and complexity is analyzed using a detailed literature analysis.
Complexity is a multifaceted concept, with no single agreed definition.
There are numerous approaches to defining and measuring complexity and organization, all involving the idea of information.
Conceptions of complexity, order, organization, and “interesting order” are inextricably intertwined with those of information.
Shannon’s formalism captures information’s unpredictable creative contributions to organized complexity; a full understanding of information’s relation to structure and order is still lacking.
Conceptual investigations of this topic should enrich the theoretical basis of the information science discipline, and create fruitful links with other disciplines that study the concepts of information and complexity.

urgent need to promote Chinese in this paper we will raise the significance of keyword extraction using a new PAT-treebased approach, which is efficient in automatic keyword extraction from a set of relevant Chinese documents.
This approach has been successfully applied in several IR researches, such as document classification, book indexing and relevance feedback.
Many Chinese language processing applications therefore step ahead from character level to word/phrase level,

We deal, in this paper, with the <i>short queries</i containing one or two words) problem.
Short queries have no sufficient information to express their semantics in a non ambiguous way.
Pseudo-relevance feedback (PRF) approach for query expansion is useful in many Information Retrieval (IR) tasks.
However, this approach does not work well in the case of very short queries.
Therefore, we present instead of PRF a semantic query enrichment method based on Wikipedia.
This method expands short queries by semantically related terms extracted from Wikipedia.
Our experiments on cultural heritage corpora show significant improvement in the retrieval performance.

The Medical Coordination, which is the application of logistics techniques to emergency context, is responsible for providing appropriate resources, in appropriate conditions to appropriate patients.
A system for medical coordination of emergency requests was developed in 2009, although some activities related to medical coordination decision making are extremely subjective.
Aiming to decrease subjectivity on activities like prioritization of requests and coordination flow, new technologies of decision support were incorporated to that system.
These technologies include textual and semantic processing of clinical summaries and machine learning tools.
Results indicate that automated tools could support decision on medical coordination process, allowing coordinators to focus attention on critical cases.
These features may streamline the medical coordination, avoiding mistakes and increasing the chances of saving lives.

We propose a method for ranking short information nuggets extracted from a text corpus, using another, reliable reference corpus as a user model.
We argue that the availability and usage of such additional corpora is common in a number of IR tasks, and apply the method to answering a form of definition questions.
The proposed ranking method makes a substantial improvement in the performance of our system.

This paper presents some methodological observations on the measurement of performance in Visual Information Retrieval (VIR) systems.
The paper identifies three different types of measures two of which (Physical Performance and Contextual Evaluation) can be determined with methods inherited from physical and social sciences respectively.
The third model (Decontextualized Evaluation) is more typical of the design and construction of complicated systems, since it allows us to measure the performance of individual modules before their insertion in a particular application.
This paper presents some methodologies for the decontextualized evaluation, anchoring them to a case studies of evaluation of several subsystems of an image database.

The information plays an inevitable role in the handling systems of numerical data existing today.
The amount of information handled by various organizations in the world today is high and its management without the computer is no longer imaginable.
Information Retrieval (IR) is a research area in computer science whose goal is to facilitate access to a set of documents in electronic form (corpus) and allow a user to find the relevant ones for him, that is to say, those whose content best matches the information needs of the user.
We propose in this paper a system called x201C;Neuro-RI&#x201D; putting into practice our model of information retrieval based on neural networks with neighborhood.

In this paper, we present a scalable arrow detection technique for biomedical images to support information retrieval systems under the purview of content-based image retrieval (CBIR) and text information retrieval (TIR The idea primarily follows the criteria based on the geometric properties of the arrow, where we introduce signatures from key points associated with it.
To handle this, images are first binarized via a fuzzy binarization tool and several regions of interest are labeled accordingly.
Each region is used to generate signatures and then compared with the theoretical ones to check their similarity.
Our validation over biomedical images shows the advantage of the technique over the most prominent state-of-the-art methods.

Each day significant numbers of people worldwide have their tist experience in interactive information retrieval using the technologies of the World Wide Web on the Internet and many others return to try again.
Surfing the web can be great entertainment, and even often a source of useful information.
Certainly the ability to connect to known sites, such as company home pages, weather reports, etc. can be extremely helpful.
But attempting to actually search the web for a specific piece of information can often be very frustrating.

The web began its existence around 1989 with a proposal called ‘Information Management: A proposal’ by Tim Berners-Lee, a researcher at CERN the European organization for nuclear research in Switzerland.
The proposal envisaged a network of remote documents linked physically using the internet and logically using hypertext.
The proposal was called the ‘World Wide Web
In response to this proposal, Berners-Lee’s boss had this remark Vague, but exciting A little more than ten years later, this small idea has grown to mammoth proportions beyond imagination.
As of July 2001, the web contained 125,888,197 hosts (Source: Internet Software Consortium http www.isc.org serving almost 1 billion users around the globe.
The vagueness and excitement today lies not just in creating documents and interconnecting them over the web; but in extracting required documents from the web.
Introduction to Web Information Retrieval:
A User Perspective

M-commerce is largely unrealized to date because there still does not exist a single killer application that can attract wireless users to use wireless services.
According to a recent survey by the Gartner, Inc Wong, 2005 besides the importance of coverage of wireless network and pricing issues, the wireless Internet and data services is the next crucial factor that attracts users to wireless service.
As such, there is a need to improve the data services over the wireless network.
One of these services is the information retrieval service.
This article discusses the usage of ontology to create an efficient environment for m-commerce users to form queries.
The establishment of a method that combines keyword searches with using ontology to perform query formation tasks further allows a more flexible m-commerce environment for users.
Also, with the use of genetic algorithm, it is hoped that query effectiveness can be achieved, at the same time saving computational time.

Due to the large volume of data that spreadsheets can store, finding meaningful information can be a difficult task.
Existing tools can be quite complex and difficult to master, especially for novice spreadsheet users.
In the past Natural Language Interfaces have been used to improve the efficiency of using similar tools.
This work aims to look at the use of a Natural Language Interface to provide an easier means of interaction with information retrieval tools for novice spreadsheet users.

Multi-document discoure analysis has emerged with the potential of improving various information retrieval applications.
Based on the newly proposed Cross-document Structure Theory (CST this paper describes an empirical study that uses boosting to classify CST relationships between sentence pairs extracted from topically related documents.
We show that the binary classifier for determining existence of structural relationships significantly outperforms the baseline.
We also achieve promising results on the multi-class case in which the full taxonomy of relationships are considered.

This paper presents a new method for computing a thesaurus from a text corpus.
Each word is rep­ resented as a vector in a multi-dimensional space that captures cooccurrence information.
Words are defined to be similar if they have similar cooccur­ rence patterns.
Two different methods for using these thesaurus vectors in information retrieval are shown to significantly improve performance over the ARPA Tipster evaluation corpus as compared to a tf.idf baseline.

Internet is more and more widely used, which provide a valuable information resource for users.
How retrieve the information users prefers rapidly and accurately become the focus nowadays.
With introducing ant-based clustering and sorting, it makes a more precise and rapid clustering result.
Consequently, it increases the speed and efficiency of information retrieval.

In this paper, two perceptually motivated morphological strategies (PMMS) are proposed to enhance the retrieval performance of common shape matching methods.
Firstly, two human perception customs are introduced, which have important relations to shape retrieval.
Secondly, these two customs are properly modeled by morphological operations.
Finally, the proposed PMMS is applied to improve the retrieval performances of a popular shape matching method named Inner-Distance Shape Contexts (IDSC and then the Locally Constrained Diffusion Process (LCDP) method is exploited to further enhance the retrieval performance.
This combination achieves a retrieval rate of 98.56% on MPEG-7 dataset.
We also conduct the experiments on Swedish Leaf dataset, the ETH-80 dataset and the Natural Silhouette dataset.
The experimental results obtained from four datasets demonstrate clearly the effectiveness of the proposed method 2012 Elsevier Ltd.
All rights reserved.

In this paper, we propose a novel representation of sequences based on the structural information of the sequences.
A sequence is represented by a set of rules, which are derived from its subsequences.
There are two types of subsequences of interest.
One is called frequent pattern, which is a subsequence appearing often enough in the sequence.
The other is called correlative pattern, which is a subsequence composed of highly correlated elements.
The rules derived from the frequent patterns are called frequent rules, while the ones derived from the correlative patterns are called correlative rules.
By considering music objects as sequences, we represent each of them as a set of rules and design a similarity function for effective music retrieval.
The experimental results show that our approaches outperform the approaches based on the Markovmodel on the average precision.

In this paper we survey the notion of Single-Database Private Information Retrieval (PIR
The first Single-Database PIR was constructed in 1997 by Kushilevitz and Ostrovsky and since then Single-Database PIR has emerged as an important cryptographic primitive.
For example, Single-Database PIR turned out to be intimately connected to collision-resistant hash functions, oblivious transfer and public-key encryptions with additional properties.
In this survey, we give an overview of many of the constructions for Single-Database PIR (including an abstract construction based upon homomorphic encryption) and describe some ofconstruction based upon homomorphic encryption) and describe some of the connections of PIR to other primitives.

Within the pages of even this journal, some have interpreted Informing Science as the same as Information Systems.
Others interpret it as the same as Information Science.
It includes both of these and it is more.
The field of Information Systems commonly focuses on the use of information technology to inform business managers, while the field of Information Science focuses on information retrieval, particularly as related to libraries.
Informing Science includes these and all other fields that use information to inform clients.

Most of the web search engines use keyword based approach to search for needed information on the web.
When a query is submitted by the user to the search engine, the web crawler tries to match the keywords with name of file, URL or the meta tags of the documents.
Because of this, user may get many non-relevant documents along with relevant documents.
It can lead to the frustration of information seekers.
This problem can be alleviated, if the search is based on the contents and intents rather than only keywords.
Automatic document understanding focuses on representation of a document in summarized form with its gist containing important contents and the intention of the author.
This paper deals with the framework of a system for automatic document understanding for web information retrieval.
The basic purpose of this work is to enhance the effectiveness of information search on the internet.

It is a common practice among Web 2.0 services to allow users to rate items on their sites.
In this paper, we first point out the flaws of the popular methods for user-rating based ranking of items, and then argue that two well-known Information Retrieval (IR) techniques, namely the Probability Ranking Principle and Statistical Language Modelling, provide simple but effective solutions to this problem.
Furthermore, we examine the existing and proposed methods in an axiomatic framework, and prove that only the score functions given by the Dirichlet Prior smoothing method as well as its special cases can satisfy both of the two axioms borrowed from economics.

Although cited reference studies are common in the library and information science literature, they are rarely performed in non-academic institutions or in the atmospheric and oceanic sciences.
In this paper, we analyze over 400,000 cited references made by authors affiliated with the National Oceanic and Atmospheric Administration between 2009 and 2013.
Our results suggest that these methods can be applied to research libraries in a variety of institutions, that the results of analyses performed at one institution may not be applicable to other institutions, and that cited reference analyses should be periodically updated to reflect changes in authors' referencing behavior.

This paper presents a new two-pass algorithm for Extra Large (more than 1M words) Vocabulary COntinuous Speech recognition based on the Information Retrieval (ELVIRCOS
The principle of this approach is to decompose a recognition process into two passes where the first pass builds the words subset for the second pass recognition by using information retrieval procedure.
Word graph composition for continuous speech is presented.
With this approach a high performances for large vocabulary speech recognition can be obtained.

With the emergence of vast resources of information, it is necessary to develop methods that retrieve most relevant information according to the users needs.
These retrieval methods may benefit from natural language constructs to boost their results by achieving higher precision/recall rates.
In this attempt, we have used part of speech attributes of terms as extra information about document and query terms and have evaluated the impact of such information on the performance of the retrieval algorithms.
Also the effect of stemming has been experimented as a complement to this research.
Our findings indicate that part of speech tags may have small influence on effectiveness of the retrieved results.
However, when this information is combined with stemming it improves the accuracy of the outcomes considerably.

After having been a term of reflection in philosophy as well as psychology for ages, the fascination with human wisdom finally reaches the realms of computer science.
In comparison to the first philosophical definition attempts, that date back to Aristotle (384BC-322BC the efforts in information science during the late 1980s seem quite recent.
Nevertheless have there been astonishing new insights provided by cognitive science since those first formal modelizations were designed findings that strongly suggest a reconsideration of our current formalization of wisdom in the computer science domain.
We suggest to establish a new, integrated view on the concept of wisdom, using the insights of cognitive sciences.
Furthermore, we discuss possible measures that are able to cover at least fundamental properties of wisdom including its vague aspects.

A 2-server Private Information Retrieval (PIR) scheme allows a user to retrieve the ith bit of an n-bit database replicated among two non-communicating servers, while not revealing any information about i to either server.
In this work we construct a 2-server PIR scheme with total communication cost n<sup>O&#8730;(log log n log n sup
This improves over current 2-server protocols which all require 937;(n<sup>1/3</sup communication.
Our construction circumvents the n<sup>1/3</sup> barrier of Razborov and Yekhanin which holds for the restricted model of bilinear group-based schemes (covering all previous 2-server schemes The improvement comes from reducing the number of servers in existing protocols, based on Matching Vector Codes, from 3 or 4 servers to 2.
This is achieved by viewing these protocols in an algebraic way (using polynomial interpolation) and extending them using partial derivatives.

We applied an intelligent multimedia information retrieval technique to devise an algorithm identifying and rating adult images.
Given a query, ten most similar images are retrieved from an adult image database and a non-adult image database in which we store existing images of each class.
If majority of the retrieved are adult images, then the query is determined to be an adult image.
Otherwise, it is determined to be a non-adult class.
Our experiment shows 99% true positives with 23% false positives with a database containing 1,300 non-adult images, and 93.5% correct detections with 8.4% false positives when experimented with a database containing 12,000 non-adult images.
9,900 adult images are used for both experiments.
We also present an adult image rating algorithm which produces results that can be used as a reference for rating images.

Recent years saw an increased interest in the use and the construction of large corpora.
With this increased interest and awareness has come an expansion in the application to knowledge acquisition and bilingual terminology extraction.
The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, combination to linguisticsbased pruning and evaluations on CrossLanguage Information Retrieval.
We propose and explore a two-stages translation model for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives on the basis of their morphological knowledge.
Evaluations using a large-scale test collection on JapaneseEnglish and different weighting schemes of SMART retrieval system confirmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on CrossLanguage Information Retrieval.

In this paper we discuss a hybrid approach combining Case-Based Reasoning (CBR) and Information Retrieval (IR) for the retrieval of full-text documents.
Our hybrid CBR-IR approach takes as input a standard symbolic representation of a problem case and retrieves texts of relevant cases from a document collection dramatically larger than the case base available to the CBR system.
Our system works by first performing a standard HYPO-style CBR analysis and then using the texts associated with certain important classes of cases found in this analysis to “seed” a modified version of INQUERY's relevance feedback mechanism in order to generate a query composed of individual terms or pairs of terms.
Our approach provides two benefits: it extends the reach of CBR (for retrieval purposes) to much larger corpora, and it enables the injection of knowledge-based techniques into traditional IR.
We describe our CBR-IR approach and report on on-going experiments.

We propose a statistical framework for high-level feature extraction that uses SIFT Gaussian mixture models (GMMs) and audio models.
SIFT features were extracted from all the image frames and modeled by a GMM.
In addition, we used mel-frequency cepstral coefficients and ergodic hidden Markov models to detect high-level features in audio streams.
The best result obtained by using SIFT GMMs in terms of mean average precision on the TRECVID 2009 corpus was 0.150 and was improved to 0.164 by using audio information.

In this paper we propose a highly robust audio-fingerprint (AFP We call this AFP the Spectral Entropy Signature (SES To extract the SES of a song, Shannon’s entropy is determined from the spectral coefficients of each one of the first 24 critical bands according to the Bark scale; entropy values are then binary coded to obtain a very compact AFP of only 0.13 kbps.
To put the SES in context we compared it with a spectral flatness signature (SFS) and a Time domain Entropy Signature (TES The SES, TES and SFS were determined for every song in an assorted genre collection of 4 000 elements.
Four hundred songs were severely degraded and searched for using excerpts of five seconds.
The SES showed higher robustness than both the TES and the SFS for the degradations of white noise addition, equalization, lossy compression, re-recording in a noisy environment, low pass filtering, time-shifting and cropping.
EDICS: AUD-CONT Index Terms Audio-Fingerprint, Entropy, Music Information Retrieval.

Together with fast development of different areas of Pattern Analysis, an increasing demand on new models and techniques is observed.
Especially new Information Retrieval tasks, oriented on data meaning rather than layout, prove to be demanding for most known techniques.
Neuronal Group Learning concept presented in this article, together with prototype implementation gives flexibility of utilization of any kind of expert knowledge about the problem to ease classifier inference process.
It can also be used to acquire structural knowledge about an object, which can later be used for solving a segmentation problem- often addressed in semantics-oriented text and image processing.

Current Information Retrieval systems for digital cultural heritage support only the actual search aspect of the information seeking process.
This demonstration presents the second PATHS system which provides the exploration, analysis, and sense-making features to support the full information seeking process.

Wavelet trees are widely used in the representation of sequences, permutations, text collections, binary relations, discrete points, and other succinct data structures.
We show, however, that this still falls short of exploiting all of the virtues of this versatile data structure.
In particular we show how to use wavelet trees to solve fundamental algorithmic problems such as range quantile queries, range next value queries, and range intersection queries.
We explore several applications of these queries in Information Retrieval, in particular document retrieval in hierarchical and temporal documents, and in the representation of inverted lists.

To promote sustainable forest management of production forest and improve sustainability assessment of its management, a research project has been designed and being implemented jointly by the International Institute for Geo-Information Science and Earth Observation “ITC, TROPENBOS International foundation, and a number of local institutions in Indonesia.
The project is aiming at Design and development of an effective monitoring and certification system to support sustainable management of production forest.
This article presents some of the findings of the research program.
In this context special attention is given to the potential role of GIS, Remote Sensing and Decision Science.

With the widespread of the Internet, great research interests are being shown in Chinese language information retrieval in recent years.
The absence of word boundaries in Chinese language makes Chinese information retrieval (IR) different to European IR.
In order to apply traditional IR approaches to Chinese language, sentences have to be segmented into words first.
Word segmentation is playing a key role in Chinese IR.
As word segmentation is not straightforward and the results are sometime ambiguous, n-grams are used as an alternative.
Several experimental studies have been conducted to compare words and n-grams, word segmentation and its effect on information retrieval.
These studies show that using either words or n-grams leads to comparable performances.
Higher word segmentation accuracy does not necessarily result in better retrieval performance.
In this paper we propose a suffix tree based approach for Chinese information retrieval without word segmentation.

Variation in performances of an Information Retrieval system, which merges results from a number of retrieval schemes possessing equal and unequal weights, is studied in this paper.
Weight of the retrieval schemes for a particular document is derived from the relevance scores of that corresponding document.
Since, the relevance scores are varying from document to document and corpus to corpus, the method proposed is dynamic.
A number of weight calculation methods, which are using the error value for computation purpose, are discussed in this paper.
The effectiveness of the weight calculation is tested over three benchmark test collections viz ADI, CISI and MED.
It has been identified that the methods discussed in this paper retrieve articles effectively and they are independent of history or any training data.

Abbreviations are commonly found instances of synonymy in Biomedical journal papers.
Information retrieval systems that index paragraphs rather than full-text articles are more susceptible to term variation of this kind, since abbreviations are typically only defined once at the beginning of the text.
One solution to this problem is to expand the user query automatically with all possible abbreviation instances for each query term.
In this paper, we compare the effectiveness of two abbreviation expansion techniques on the TREC 2006 Genomics Track queries and collection.
Our results show that for highly ambiguous abbreviations the query collocation effect isn’t strong enough to deter the retrieval of erroneous passages.
We conclude that full-text abbreviation resolution prior to passage indexing is the most appropriate approach to this problem.

In this paper, we adopt two methods targeting NTCIR-5 Chinese-English CLIR task.
First, to alleviate problems of unknown query terms, we combine dictionary-based and search-result-based methods to handle query translation for CLIR.
Second, to reduce document retrieval time, we use a Chinese part-of-speech (POS) tagger to extract only nouns, verbs, and foreign words as

The turning point in traditional schemes of cryptography was brought about by the revolutionary work of Pecora and Carroll (Pecora Carroll, 1990
This opened a gateway to varied areas of applications of chaos synchronization namely secure communication, chaos generators design, chemical reactions, biological systems, information science (Hramov Koronovskii, 2005a Garcia Ojalio Roy, 2001; Yang Chu 1997; Bowong, 2004; Kittel et al, 1994) and many more significant applications.

This decade has seen a great deal of progress in the development of information retrieval systems.
Unfortunately, we still lack a systematic understanding of the behavior of the systems and their relationship with documents.
In this paper we present a completely new approach towards the understanding of the information retrieval systems.
Recently, it has been observed that retrieval systems in TREC 6 show some remarkable patterns in retrieving relevant documents.
Based on the TREC 6 observations, we introduce a geometric linear model of information retrieval systems.
We then apply the model to predict the number of relevant documents by the retrieval systems.
The model is also scalable to a much larger data set.
Although the model is developed based on the TREC 6 routing test data, I believe it can be readily applicable to other information retrieval systems.
In Appendix, we explained a simple and efficient way of making a better system from the existing systems.

The difficulty of a user query can affect the performance of Information Retrieval (IR) systems.
This work presents a formal model for quantifying and reasoning about query difficulty as follows:
Query difficulty is considered to be a subjective belief, which is formulated on the basis of various types of evidence.
This allows us to define a belief model and a set of operators for combining evidence of query difficulty.
The belief model uses subjective logic, a type of probabilistic logic for modeling uncertainties.
An application of this model with semantic and pragmatic evidence about 150 TREC queries illustrates the potential flexibility of this framework in expressing and combining evidence.
To our knowledge, this is the first application of subjective logic to IR.

Supervised training of deep neural nets typically relies on minimizing cross-entropy.
However, in many domains, we are interested in performing well on metrics specific to the application.
In this paper we propose a direct loss minimization approach to train deep neural networks, which provably minimizes the application-specific loss function.
This is often non-trivial, since these functions are neither smooth nor decomposable and thus are not amenable to optimization with standard gradient-based methods.
We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems.
Towards this goal, we develop a novel dynamic programming algorithm that can efficiently compute the weight updates.
Our approach proves superior to a variety of baselines in the context of action classification and object detection, especially in the presence of label noise.

Sentiment Analysis has become an indispensible part of product reviews in present scenario.
Sentiment Analysis is a very well studied field, but the scale remains limited to not more than a few hundred researchers.
The problem of analyzing the overall sentiment of a document using Machine learning techniques has been considered.
Results have been improved using multiple kernel approach and compared with previously used techniques..
The present research is a comparison and extension of the work proposed by Mullen and Collier (2003
The system consists of a feature Extraction phase and a learning phase; on the basis of which the overall sentiment of the document is analyzed.
The present work uses the movie review data set used by Pang (
The approach significantly outperforms the previous methods attaining 90% and 92% accuracy using 5 fold cross validation 10 fold cross validation respectively.
General Terms Machine Learning, Information Retrieval.

Retrieving finer grained text units such as passages or sentences as answers for non-factoid Web queries is becoming increasingly important for applications such as mobile Web search.
In this work, we introduce the answer sentence retrieval task for non-factoid Web queries, and investigate how this task can be effectively solved under a learning to rank framework.
We design two types of features, namely semantic and context features, beyond traditional text matching features.
We compare learning to rank methods with multiple baseline methods including query likelihood and the state-of-the-art convolutional neural network based method, using an answer-annotated version of the TREC GOV2 collection.
Results show that features used previously to retrieve topical sentences and factoid answer sentences are not sufficient for retrieving answer sentences for non-factoid queries, but with semantic and context features, we can significantly outperform the baseline methods.

In this paper we prove a lower bound of Ω(n log n) for the common element problem on two sets of size n each.
Two interesting consequences of this lower bound are also discussed.
In particular, we show that linear space neural network models that admit unbalanced rules cannot draw all inferences in time independent of the knowledge base size.
We also show that the join operation in data base applications needs Ω(log n) time given only n processors.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-93-73.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/509 A Lower Bound Result for The Common Element Problem and Its IMplication for Reflexive Reasoning MS-CIS-93-73 G R A S P LAB
356 Paul Dietz Danny Krizanc Sanguthevar Rajasekaran Loke11di-

Privacy remains a major concern when using search engines to find for information on the web due to the fact that search engines own massive resources in preserving search logs of each user and organizations.
However, many of the present query search privacy practices require the very same search engine and third party to collaborate, making privacy even more difficult.
Therefore, as a contribution, we present a heuristic, permutation of web search query types, a noncryptographic heuristic that works by formation of obfuscated search queries via permutation of query keyword categories.
Preliminary results from this study show that web search query and specific user intent privacy might be achievable from the user side without involvement of the search engine or other third parties by the permutation of web search query types.
Keywords—Web search query privacy; user intent privacy; search engines; Information Retrieval

Multivariate loss functions are extensively employed in several prediction tasks arising in Information Retrieval.
Often, the goal in the tasks is to minimize expected loss when retrieving relevant items from a presented set of items, where the expectation is with respect to the joint distribution over item sets.
Our key result is that for most multivariate losses, the expected loss is provably optimized by sorting the items by the conditional probability of label being positive and then selecting top k items.
Such a result was previously known only for the F -measure.
Leveraging on the optimality characterization, we give an algorithm for estimating optimal predictions in practice with runtime quadratic in size of item sets for many losses.
We provide empirical results on benchmark datasets, comparing the proposed algorithm to state-of-the-art methods for optimizing multivariate losses.

In the context of Software Reuse providing techniques to support source code retrieval has been widely experimented.
However, much effort is required in order to find how to match classical Information Retrieval and source code characteristics and implicit information.
Introducing linguistic theories in the software development process, in terms of documentation standardization may produce significant benefits when applying Information Retrieval techniques.
The goal of our research is to provide a tool to improve source code search and retrieval
In order to achieve this goal we apply some linguistic rules to the development process.

We study the problem of efficient dissemination of textual information over wide-area networks.
Our dissemination architecture utilises middle-agents and sophisticated matching algorithms.
The data model and query language is based on the well-known Boolean model from Information Retrieval.
The main focus of this paper is the problem of matching incoming documents with submitted user profiles.
We present four efficient main memory algorithms for this problem and compare them experimentally.

Voice separation is an important component of Music Information Retrieval (MIR
In this paper, we present an HMM which can be used to separate music performance data in the form of MIDI into monophonic voices.
It works on two basic principles: that consecutive notes within a single voice will tend to occur on similar pitches, and that there are short (if any) temporal gaps between them.
We also present an incremental algorithm which can perform inference on the model efficiently.
We show that our approach achieves a significant improvement over existing approaches, when run on a corpus of 78 compositions by J. S. Bach, each of which has been separated into the gold standard voices suggested by the original score.
We also show that it can be used to perform voice separation on live MIDI data without an appreciable loss in accuracy.
The code for the model described here is available at https github.com/apmcleod/voice-

This paper describes our information retrieval system participated in CLIR task of NTCIR3 and reports the results with some observations.
Most previous information retrieval models assume that terms of queries and documents are statistically independent from each another.
However, independence assumption is obviously and openly understood to be wrong.
We presented two method of incorporating the term dependences in probabilistic retrieval model to compensate the weakness of the linked dependence assumption.

Learning systems often describe a target class as a disjunction of conjunctions of conditions.
Recent work has noted that small disjuncts, i.e those supported by few training examples, typically have poor predictive accuracy.
One model of this accuracy is provided by the Bayes-Laplace formula based on the number of training examples covered by the disjunct and the number of them belonging to the target class.
However, experiments show that small disjuncts associated with target classes of different relative frequencies tend to have different error rates.
This note defines the context of a disjunct as the set of training examples that fail to satisfy at most one of its conditions.
An empirical adaptation of the Bayes-Laplace formula is presented that also makes use of the relative frequency of the target class in this context.
Trials are reported comparing the performance of the original formula and the adaptation in six learning tasks.

This paper explores the idea that a concept lattice is an information channel between objects and attributes.
For this purpose we study the behaviour of incidences in L-formal contexts where L is the range of an information-theoretic entropy function.
Examples of such data abound in machine learning and data mining, e.g. confusion matrices of multi-class classifiers or document-term matrices.
We use a wellmotivated information-theoretic heuristic, the maximization of mutual information, that in our conclusions provides a flavour of feature selection providing and information-theory explanation of an established practice in Data Mining, Natural Language Processing and Information Retrieval applications, viz.
stop-wording and frequency thresholding.
We also introduce a post-clustering class identification in the presence of confusions and a flavour of term selection for a multi-label document classification task.

Non-negative Matrix Factorization (NMF) has become increasingly popular in many applications that require data mining techniques such as information retrieval, computer vision, and pattern recognition.
NMF aims at approximating the original data matrix in a high dimensional space with the product of two non-negative matrices in a lower dimensional space.
In many applications with high dimensional data such as text, data often have a global geometric structure, which typically may not be directly derived from the local information.
But the existing literature of NMF completely ignores this problem.
This paper proposes a novel matrix factorization algorithm called Coordinate Ranking regularized NMF (CR-NMF) in order to address this problem.
The idea of the proposed algorithm is to combine NMF and manifold ranking to encode both local and global geometric structures of the data.
Experimental results on two real-world datasets demonstrate the superiority of this algorithm.

Corpus Linguistics is becoming increasingly important.
The most recent conferences of the Association for Computational Linguistics
(ACL and the International Conference on Computational Linguistics (COLING) include a large number of papers about corpus analysis.
Archives are being developed for storing large bodies of text, and lexical databases are beginning to be shared and analyzed.
This panel will discuss some of the recent results in this area, and the bearing they have on Information Retrieval.
Some of the areas of overlap include:

The Automated Teller Machine has become an integral part of our society.
Using the ATM however can often be a frustrating experience.
How often have some of us experienced the people in the queue in front of you reinserting their card for another transaction.
Why does this happen?
Is there a design flaw in the user interface?
It seems that many ATM navigation menus are not as intuitive or as efficient as they could be.
This paper examines a variety of UK Bank ATM navigation menus and proposes a best of breed ATM menu.

The European Commission funds a number of technically-based research and development programs such as ESPRIT and the Telematics Applications Programme (TAP Information Engineering (IE) is one of the areas which receives part-funding for research and development through TAP.
This sector essentially covers techniques to provide access to electronic information in all its forms based upon the notion of the information chain and including the areas of electronic publishing, information dissemination and information retrieval.

In this paper we describe the detailed information of NLP-NITMZ system on the participation of DPIL shared task at Forum for Information Retrieval Evaluation (FIRE 2016
The main aim of DPIL shared task is to detect paraphrases in Indian Languages.
Paraphrase detection is an important part in the field of Information Retrieval, Document Summarization, Question Answering, Plagiarism Detection etc.
In our approach, we used language independent feature-set to detect paraphrases in Indian languages.
Features are mainly based on lexical based similarity.
Our system’s three features are: Jaccard Similarity, length normalized Edit Distance and Cosine Similarity.
Finally, these feature-set are trained using Probabilistic Neural Network (PNN) to detect the paraphrases.
With our feature-set, we achieved 88.13% average accuracy in Sub-Task 1 and 71.98% average accuracy in Sub-Task 2.

Simulation and analysis have shown that selective search can reduce the cost of large-scale distributed information retrieval.
By partitioning the collection into small topical shards, and then using a resource ranking algorithm to choose a subset of shards to search for each query, fewer postings are evaluated.
Here we extend the study of selective search using a fine-grained simulation investigating: selective search efficiency in a parallel query processing environment; the difference in efficiency when term-based and sample-based resource selection algorithms are used; and the effect of two policies for assigning index shards to machines.
Results obtained for two large datasets and four large query logs confirm that selective search is significantly more efficient than conventional distributed search.
In particular, we show that selective search is capable of both higher throughput and lower latency in a parallel environment than is exhaustive search.

This paper presents a generic and domain independent opinion relevance model for a Social Network user.
The Social Opinion Relevance Model (SORM) is able to estimate an opinion's relevance based on twelve different parameters.
Compared to other models, SORM's main distinction is its ability to provide customized results according to whom the opinion relevance is being estimated for.
Due to the lack of opinion relevance corpuses able to properly test our model, we have created a new one called Social Opinion Relevance Corpus (SORC Using SORC, we carried out some experiments on the Electronic Games domain that illustrate the importance of the customizing the opinion relevance in order to achieve better results on typical Information Retrieval metrics, such as NDCG, QMeasure and MAP.
We also performed a statistical significance test that reinforces and corroborates the advantages that SORM offers.

This work deals with Text analysis that involves information retrieval through lexical analysis to learn word occurrence and distributions, pattern recognition, information extraction, data mining techniques and followed by visualization, and predictive analytics.
The primary goal is to turn text into data for analysis, through application of natural language processing (NLP) and analytical tools.
The problem taken for study is to evaluate a trainer through his lecture given in the class applying an innovative algorithm to perform the task.

Information Retrieval is a supporting technique which underpins a broad range of content-based applications including retrieval, filtering, summarisation, browsing, classification, clustering, automatic linking, and others.
Multimedia information retrieval (MMIR) represents those applications when applied to multimedia information such as image, video, music, etc.
In this presentation and extended abstract we are primarily concerned with MMIR as applied to information in digital video format.
We begin with a brief overview of large scale evaluations of IR tasks in areas such as text, image and music, just to illustrate that this phenomenon is not just restricted to MMIR on video.
The main contribution, however, is a set of pointers and a summarisation of the work done as part of TRECVid, the annual benchmarking exercise for video retrieval tasks.

We propose a mechanism for annotating and querying document collections based on the semantic modeling of the context of a search.
We model on the one hand the topics concerned by the content of the document and on the other hand the metadata associated with the documents, by means of two ontologies expressed in the conceptual graph model.
The semantic annotating mechanism is done by automatically building conceptual graphs.

Global medical and epidemic surveillance is an essential function of Public Health agencies, whose mandate is to protect the public from major health threats.
To perform this function effectively one requires timely and accurate medical information from a wide range of sources.
In this work we present a freely accessible system designed to monitor disease epidemics by analysing textual reports, mostly in the form of news gathered from the Web.
The system rests on two major components—MedISys, based on Information Retrieval technology, and PULS, an Information Extraction system.

The aim of this paper is to present a preliminary work about the enhance of information retrieval systems by exploring tags coming from social networks.
We discuss how to obtain folksonomies (social indexing) and their use on a search approach that models tags assigned to documents.
For this, a document is represented by formal concept then we construct a bipartite graph.
From this later, we retrieve emergent tags that describe better a document.

This review examines the basic tenets of qualitative or naturalistic methods in terms of their original grounding in the basic social sciences and their value to library and information science research.
Examination of the five key points provides the understanding needed to move from contemplation to use of these methods: the research problem, data gathering, content analysis, theory development, and validity techniques.

The emergence of immersive documents, which allow the “reader” to perceive unreality as real, is foreseen.
This new type of document will evolve from the combination of contemporary participatory, transmedia storytelling with pervasive computing technologies and multisensory interfaces.
It is argued that a research program within library and information science is needed, to investigate new information behaviors associated with such documents, the new digital literacies needed to make effective use of them, and their place in the information communication chain.

Due to data overload and time-critical nature of information need, automatic summarization of documents plays a significant role in information retrieval and text data mining.
This paper discusses the design of a multi-document summarizer that uses Katz's K-mixture model for term distribution.
The model helps in ranking the sentences by a modified term weight assignment.
The system has been evaluated against the frequently occurring sentences in the summaries generated by a set of human subjects.
Our system outperforms other auto-summarizers at different extraction levels of summarization with respect to the ideal summary, and is close to the ideal summary at 40% extraction level.

Visual Information Retrieval presents many challenges for the computer vision community.
The terabytes of visual information stored in digital image and video libraries will remain inaccessible if the problems of indexing and retrieval are not addressed.
In this paper; we present techniques for content based image retrieval using higher level contextual information.
The content is represented and queried using attributed relational graphs, with colour attributes and relaxation labeling techniques.
We present retrieval examples using both synthetic and real images of national jags.
This, although a simplistic problem, highlights the shortcomings, and diflculties associated with content based retrieval systems.

Nowadays, many factors support a growing production of information.
In modern large environments, for the user's point of view, it is desirable to have Information Retrieval Systems (IRS) that retrieve documents according to their relevance levels.
Relevance levels have been studied in some previous Information Retrieval (IR) works while some others (few) IR research works tackled the questions of IRS e ectiveness and collections size.
These latter works used standard IR measures on collections of increasing size to analyze IRS e ectiveness scalability.
In this work, we bring together these two issues in IR (multigraded relevance and scalability) by designing some new metrics for evaluating the ability of IRS to rank documents according to their relevance levels when collection size increases.

This poster presents results for two approaches using Genetic Programming (GP) to overcome the problem of term mismatch in Information Retrieval
We use automatic query expansion techniques which add terms to a user's initial query in the hope that these words better describe the information need and ultimately return more relevant documents to the user.

Healthcare practitioners are increasingly using search functionality embedded in Electronic Medical Record (EMR) software to search for relevant evidence summaries at point of care.
We introduce a learning to rank approach that exploits information carried in EMR data and UpToDate user accounts to (significantly) improve ranking results, compared to a comparable model that does not exploit such features.

In this paper we introduce <i>
Expressions</i
> as a means for modelling document content.
From an index expression the the Power Index Expression can be derived, which is a powerful instrument for information retrieval.
We describe the characterization of documents in the style of formal logic.
The content of a document is then modelled by a set of axioms, of which the document is a model.
Relating a document to a query is done by proving the query from the axioms of that document.
We introduce three rules of inference.
If such a proof is not possible, the relevance of the document for the query is derived by <i>plausible deduction i>
We introduce two inference rules for plausible deduction.

This paper introduces a particular form of fuzzy-fingerprints—their construction, their interpretation, and their use in the field of information retrieval.
Though the concept of fingerprinting in general is not new, the way of using them within a similarity search as described here is: Instead of computing the similarity between two fingerprints in order to access the similarity between the associated objects, simply the event of a fingerprint collision is used for a similarity assessment.
The main impact of this approach is the small number of comparisons necessary to conduct a similarity search.

Distributed denial of service (DDOS) attacks have emerged as a prevalent way to take down web sites and have imposed financial losses to companies.
The CSI/FBI survey (CSI 2001) shows that 36% of respondents in the last 12-months period have detected denial of service, which imposed more than $4.2 million financial losses.
The effectiveness of DDOS defenses depends on many factors such that the nature of the network’s topology, the specific attack scenario, and various characteristics of the network routers.
However, little research has focused on the tradeoffs inherent in this complex system.
We propose to develop a computational testbed to study security policies and the associated technologies that provide defenses against DDOS attacks.
We will then use this framework to evaluate various policies and technologies.
In this work we draw on research in the areas of computer science, information science, organizational theory and social networks.

The navigation structure of Web sites can be regarded as metadata that can be used for interesting applications in User Interface (UI) design and Human-Computer Interaction (HCI as well as for Information Retrieval (IR) tasks.
However, there currently is no established format for site metadata, which makes it hard for Web sites to publish their structure in a machine-readable way, which could then be used by HCI and/or IR applications.
We propose a model and a format for site metadata that is built on top of an existing format and thus could be deployed with little overhead by publishers as well as consumers.
Making site metadata available as machine-readable data can be used for improving user interfaces (informing user agents about the context of the page they are displaying) and better information retrieval (allowing search engines to use sitemap information for better ranking and display of the results).

This paper reviews literature on dictionary-based cross-language information retrieval (CLIR) and presents CLIR research done at the University of Tampere (UTA The main problems associated with dictionary-based CLIR, as well as appropriate methods to deal with the problems are discussed.
We will present the structured query model by Pirkola and report findings for four different language pairs concerning the effectiveness of query structuring.
The architecture of our automatic query translation and construction system is presented.

Indexing is a crucial technique for dealing with the massive amount of data present on the web.
Indexing can be performed based on words or on phrases.
Our approach aims to efficiently index web documents by employing a hybrid technique in which web documents are indexed in such a way that knowledge available in the Wikipedia and in meta-content is efficiently used.
Our preliminary experiments on the TREC dataset have shown that our indexing scheme is a robust and efficient method for both indexing and for retrieving relevant web pages.
We ranked term queries in different ways, depending if they were found in Wikipedia pages or not.
This paper presents our preliminary algorithm and experiments for the ad-hoc and diversity tasks of the TREC 2011 Web track.
We ran our system on the subset B (50 million web documents) from the ClueWeb09 dataset.
Categories and Subject Description Web Information Retrieval:
Content Analysis, Indexing, and Ranking

Information exists on the Web in a number of languages.
This situation has given rise to new line of research called Cross-Language Information Retrieval (CLIR treating the problem of finding a document written in one language via a query written in another language.
One of the important resources needed for this problem is set of bilingual dictionaries for producing queries in new languages.
The two most important aspects of these bilingual dictionaries for CLIR are the coverage that the dictionary provides for domain-independent corpora, and the adequacy of the translations provided for finding relevant documents in the second language.
In this paper, we present a number of evaluations of these aspects for a bilingual dictionary, available through the ELRA.
These evaluations are run against large corpora used in the TREC information retrieval trials.

Diversified retrieval and online learning are two core research areas in the design of modern information retrieval systems.
In this paper, we propose the linear submodular bandits problem, which is an online learning setting for optimizing a general class of feature-rich submodular utility models for diversified retrieval.
We present an algorithm, called LSBGREEDY, and prove that it efficiently converges to a near-optimal model.
As a case study, we applied our approach to the setting of personalized news recommendation, where the system must recommend small sets of news articles selected from tens of thousands of available articles each day.
In a live user study, we found that LSBGREEDY significantly outperforms existing online learning approaches.

The 20 papers in this special issue focus on quantum communications and information science.

In this paper, we describe the experiments conducted by the <i>
Information Retrieval Group</i> at the Universidad Aut&#243;noma de Madrid (Spain) to tackle the Identifying Ratings
(track 2) task of the CAMRa 2011 Challenge.
The experiments performed include time-frequency probabilistic strategies, heuristic collaborative filtering (CF) and a model-based CF approach.
Results show that probabilistic classifiers based on temporal behavior of users have better performance than traditional recommendation-based strategies, thus reflecting that temporal information is a valuable source for the identification or discrimination of user ratings.

Being able to resolve word senses could improve precision in Information Retrieval, machine translation or other natural language applications.
However, automatic disambiguation rarely provides benefits in proportion to its costs.
In this study we have tried to determine the dominant or ‘most frequent’ sense of an ambiguous word.
By identifying this, future applications could statistically choose a ‘correct’ word sense the majority of the time without computation.
We investigated several ambiguous nouns and the frequency of their cooccurring synonyms from Roget’s thesaurus using the British National Corpus.
This indicates the likelihood of the possible word senses.
The results have been evaluated with some success against varying lexical resources including WordNet and the Concise Oxford Dictionary.

The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach.
This paper considers the fundamental question of how these approaches might be combined.
The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products.
Possible applications of this method for language processing are described.
Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed.

EIREX 2011 is the second in a series of experiments designed to foster new Information Retrieval (IR) education methodologies and resources, with the specific goal of teaching undergraduate IR courses from an experimental perspective.
For an introduction to the motivation behind the EIREX experiments, see the first sections of [Urbano et al 2011a For information on other editions of EIREX and related data, see the website at http ir.kr.inf.uc3m.es/eirex/.

We propose a math-aware search engine that is capable of handling both textual keywords as well as mathematical expressions.
Our math feature extraction and representation framework captures the semantics of math expressions via a Finite State Machine model.
We adapt the passive aggressive online learning binary classifier as the ranking model.
We benchmarked our approach against three classical information retrieval (IR) strategies on math documents crawled from <i>Math
Overflow</i a well-known online math question answering system.
Experimental results show that our proposed approach can perform better than other methods by more than 9%.

Finding a proper distribution of translation probabilities is one of the most important factors impacting the effectiveness of a cross-language information retrieval system.
In this paper we present a new approach that computes translation probabilities for a given query by using only a bilingual dictionary and a monolingual corpus in the target language.
The algorithm combines term association measures with an iterative machine learning approach based on expectation maximization.
Our approach considers only pairs of translation candidates and is therefore less sensitive to data-sparseness issues than approaches using higher n-grams.
The learned translation probabilities are used as query term weights and integrated into a vector-space retrieval system.
Results for English-German cross-lingual retrieval show substantial improvements over a baseline using dictionary lookup without term weighting.

The identification and classification of proper names (named entity recognition) is considered an important task in the area of Information Retrieval and Extraction.
A typical named entity recognition (NER) system mainly consists of a lexicon and a grammar.
When moving to a new domain, these lexical resources should be customised, either manually or exploiting machine learning techniques.
In this paper, we present a NER system based on hand crafted lexical resources.
The system is part of a Greek information extraction system and was tested on a Greek corpus of financial news with satisfactory results.

A Multimedia Cognitive-based Information Retrieval System called AMCIRS which integrates image and text information has been described in [11 12 The AMCIRS query based mechanism is based on multimedia objects content search using the vector model.
The content search process is deduced to the similarity estimation between query and index vectors.
The main objective of this paper is to present an application of AMCIRS in Mineralogy.
The experimental evaluation of AMCIRS retrieval effectiveness is also given.
The retrieval effectiveness is expressed by recall and precision parameters which are the standard measures for the effectivity of the information retrieval systems.
We confirmed our assumption that multiple media retrieval has advantages with respect to single media retrieval italic>

This paper describes a machine induction program (WITT) that attempts to model human categorization.
Properties of categories to which human subjects are sensitive includes best or prototypical members, relative contrasts between putative categories, and polymorphy (neither necessary or sufficient features This approach represents an alternative to usual Artificial Intelligence approaches to generalization and conceptual clustering which tend to focus on necessary and sufficient feature rules, equivalence classes, and simple search and match schemes.
WITT is shown to be more consistent with human categorization while potentially including results produced by more traditional clustering schemes.
Applications of this approach in the domains of expert systems and information retrieval are also discussed.

In this paper the methods we used in the 2005 ImageCLEF content-based image retrieval evaluation are described.
For the medical retrieval task, we combined several low-level image features with textual information retrieval.
Combining these two information sources, clear improvements over the use of one of these sources alone are possible.
Additionally we participated in the automatic annotation task, where our content-based image retrieval system, FIRE, was used as well as a second subimage based method for object classification.
The results we achieved are very convincing.
Our submissions ranked first and the third in the automatic annotation task out of a total of 44 submissions from 12 groups.

When we have an idea of student’s preparation (e.g because of a previous written exam, or a term project, or after having asked the first questions we even do something more.
We ask difficult questions to good students, and we ask easy questions to bad students
What’s the point in asking easy questions to good students?
They will almost certainly answer correctly, as expected, without providing much information about their preparation
What’s the point in asking difficult questions to bad students?
They will almost certainly answer wrongly, without providing much information and incidentally increase examiner’s stress level.

In this paper we study term-based feedback for information retrieval in the language modeling approach.
With term feedback a user <i>directly</i> judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process.
We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback.
Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback.
They are helpful even when there are no relevant documents in the top.

This paper proposes a new framework for Citation Content Analysis (CCA for syntactic and semantic analysis of citation content that can be used to better analyze the rich sociocultural context of research behavior.
The framework could be considered the next generation of citation analysis.
This paper briefly reviews the history and features of content analysis in traditional social sciences, and its previous application in Library and Information Science.
Based on critical discussion of the theoretical necessity of a new method as well as the limits of citation analysis, the nature and purposes of CCA are discussed, and potential procedures to conduct CCA, including principles to identify the reference scope, a two-dimensional (citing and cited) and two-modular (syntactic and

The article will explore the philosophical consequence on the notion of meaning of the adoption of a quantum geometry-based approach in linguistic disciplines such Information Retrieval, which importance grew with Internet and big data.
The paper reconstructs an archaeology of the turn to probabilistic and a quantum perspectives in language research and its reasons, we considers some linguistic features which justify the adoption of a quantum perspective (non-distributiveness, failures of the substitutivity principle, periodicity of the meaning function it underlines two features of quantum models (superposition and entanglement) which seems interesting to represent semantics, we hope to show how quantum models allow a convergence of the interest of different perspectives in philosophy of language.

Mobile agents have attracted considerable interest in recent years.
Mobile agents travel through servers to perform their programs.
Maintaining mobile agent availability in the presence of agent server&#x02019;s crash, during their travel, is a challenging issue, since, developers normally have no control over remote agent servers.
In the context of mobile agents, fault tolerance is crucial to enable the integration of mobile agent technology into today&#x02019;s business applications.
In multi mobile agent deployment, communication among mobile agents is another aspect to concentrate.
When it ensures the aliveness of other agents, agent reliability and data availability, there is a need to locate agents to implement the communication aspect.
Considering the merits of the existing work, this paper proposes an attempt to improve the agent survivability in the aspect of multiple agent deployment and communication among them for information sharing.

This paper evaluates the effectiveness of a query translation and disambiguation as well as expansion techniques on CLEF Collections, using SMART Information Retrieval System.
We focus on the query translation, disambiguation and methods to improve the effectiveness of an information retrieval.
Dictionary-based method with a combination to statistics-based method is used, to avoid the problem of translation ambiguity.
In addition, two expansion strategies are tested on their ability to improve the effectiveness of an information retrieval, an expansion via a relevance feedback before and after translation as well as an expansion via a domain feedback after translation.
This method achieved 85.30% of the monolingual counterpart, in terms of average precision.

The problem of nding documents that are written in a lan guage that the searcher cannot read is perhaps the most challenging application of Cross Language Information Retrieval CLIR technol ogy
The rst Cross Language Evaluation Forum CLEF provided an excellent venue for assessing the performance of automated CLIR tech niques but little is known about how searchers and systems might in teract to achieve better cross language search results than automated systems alone can provide This paper explores the question of how in teractive approaches to CLIR might be evaluated suggesting an initial focus on evaluation of interactive document selection Important evalua tion issues are identi ed the structure of an interactive CLEF evaluation is proposed and the key research communities that could be brought to gether by such an evaluation are introduced

The Vector Space Basis Change (VSBC) is an algebraic operator responsible for change of basis and it is parameterized by a transition matrix.
If we change the vector space basis, then each vector com­ ponent changes depending on this matrix.
The strategy of VSBC has been shown to be effective in separating relevant documents and irrelevant ones.
Recently, using this strategy, some feedback algorithms have been de­ veloped.
To build a transition matrix some optimization methods have been used.
In this paper, we propose to use a simple, convenient and direct method to build a transition matrix.
Based on this method we develop a relevance feedback algorithm.
Experimental results on a TREC collection show that our proposed method is effective and generally superior to known VSBC-based models.
We also show that our proposed method gives a statistically significant improvement over these models.

User generated social annotations provide extra information for describing document contents.
In this paper, we propose an effective method to model the categorization property of social annotations and explore the potential of combining it with classical language models for improving retrieval performance.
Specifically, a novel TR-LDA model is presented to take annotations as an additional source for generating document contents apart from the document itself.
We provide strategies for representing and weighting the categorization property and develop an efficient inference algorithm, where space saving is taken into account.
Experiments are carried out on synthetic datasets, where documents and queries come from the standard evaluation conference TREC and annotations come from the website Delicious.com.
Our results demonstrate the effectiveness of the proposed method on the ad-hoc retrieval task, which significantly outperforms state-of-art baselines.

This paper deals with overlapping clustering, a trade off between crisp and fuzzy clustering.
It has been motivated by recent applications in various domains such as information retrieval or biology.
We show that the problem of finding a suitable coverage of data by overlapping clusters is not a trivial task.
We propose a new objective criterion and the associated algorithm OKM that generalizes the k-means algorithm.
Experiments show that overlapping clustering is a good alternative and indicate that OKM outperforms other existing methods.

A theoretic framework for multimedia information retrieval is introduced which guarantees optimal retrieval eeectiveness.
In particular, a Ranking Principle for Distributed Multimedia-Documents (RPDM) is described together with an algorithm that satisses this principle.
Finally, the RPDM is shown to be a generalization of the Probability Ranking principle (PRP) which guarantees optimal retrieval eeectiveness in the case of text document retrieval.
The PRP justiies theoretically the relevance ranking adopted by modern search engines.
In contrast to the classical PRP, the new RPDM takes into account transmission and inspection time, and most importantly, aspectual recall rather than simple recall.

The term frequency normalisation parameter sensitivity is an important issue in the probabilistic model for Information Retrieval.
A high parameter sensitivity indicates that a slight change of the parameter value may considerably affect the retrieval performance.
Therefore, a weighting model with a high parameter sensitivity is not robust enough to provide a consistent retrieval performance across different collections and queries.
In this paper, we suggest that the parameter sensitivity is due to the fact that the query term weights are not adequate enough to allow informative query terms to differ from non-informative ones.
We show that query term reweighing, which is part of the relevance feedback process, can be successfully used to reduce the parameter sensitivity.
Experiments on five Text REtrieval Conference (TREC) collections show that the parameter sensitivity does remarkably decrease when query terms are reweighed.

Bilinear models are widely used in signal processing, control theory and in many other information sciences.
Although, it is easy to specify simple bilinear time series models using Kumar's third order moment approach, little work has been done on the specification of mixed bilinear time series models.
This paper considers a new class of time series combining Autoregressive Moving Average (ARMA) and bilinear models.
Further, we propose a method for specification of such mixed bilinear time series models based on Pade's approximation and third order moments.

Using Social Network Sites (SNS) as an information source has drawn the attention of the researchers for a while now.
There has been many works that analyzed the types and topics of questions people ask in these networks and why.
Topics like what motivate people to answer such queries, how to integrate the traditional search engines and SNS together are also well investigated.
In this paper, we focus on a relevant but different issue how SNS search varies in developed and developing regions of the world and why.
With established statistics of Internet usage, e-Governance, and our experimental data collection, we have tried to emphasize the differences among them and provided insight that one might require to consider while developing any application for SNS based searching.

This paper presents an analytical analysis of the impulse response function (IRF) of multi-circular SAR (MCSAR) as a function of the system bandwidth and the number of tracks, thereby optimizing the acquisition geometry to get the best 3-D resolution and sidelobe suppression.
Moreover, the wide range of spectrum measurements provided by MCSAR allows the information retrieval over 360 x00B0 thus understanding the anisotropic signature of scatterers, e.g. by their polarimetric signature.
The second part of this paper presents a polarimetric analysis of the resulting holographic SAR tomograms of forested areas by means of the generalized likelihood ratio test (GLRT) algorithm, as well as by coherent and incoherent imaging.
The IRF and the anisotropic analysis are validated with a polarimetric MCSAR campaign conducted at L-band by the DLR's F-SAR sensor over a forested region in Kauf-beuren, Germany.

Due to the historical and cultural reasons, English phases, especially the proper nouns and new words, frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean.
Although these English terms and their equivalences in the Asian languages refer to the same concept, they are erroneously treated as independent index units in traditional Information Retrieval (IR This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it.
Our method firstly extracts an English phrase from Asian language Web pages, and then unifies the extracted phrase and its equivalence(s) in the language as one index unit.
Experimental results show that the high precision of our conceptual unification approach greatly improves the IR performance.

Web Service discovery and selection deal with the retrieval of the most suitable Web Service, given a required functionality.
Addressing an effective solution remains difficult when only functional descriptions of services are available.
In this paper, we propose a solution by applying Case-based Reasoning, in which the resemblance between a pair of cases is quantified through a similarity function.
We show the feasibility of applying Case-based Reasoning for Web Service discovery and selection, by introducing a novel case representation, learning heuristics and three different similarity functions.
We also experimentally validate our proposal with a dataset of 62 real-life Web Services, achieving competitive values in terms of well-known Information Retrieval metrics.

Clinical practice and research are facing a new challenge created by the rapid growth of health information science and technology, and the complexity and volume of biomedical data.
Machine learning from medical data streams is a recent area of research that aims to provide better knowledge extraction and evidence-based clinical decision support in scenarios where data are produced as a continuous flow.
This year’s edition of AIME, the Conference on Artificial Intelligence in Medicine, enabled the sound discussion of this area of research, mainly by the inclusion of a dedicated workshop.
This paper is an introduction to LEMEDS, the Learning from Medical Data Streams workshop, which highlights the contributed papers, the invited talk and expert panel discussion, as well as related papers accepted to the main conference.

In this paper, we show how case-based reasoning (CBR) techniques can be applied to document retrieval.
The fundamental idea is to automatically convert textual documents into appropriate case representations and use these to retrieve relevant documents in a problem situation.
In contrast to Information Retrieval techniques, we assume that a Textual CBR system focuses on a particular domain and thus can employ knowledge from that domain.
We give an overview over our approach to Textual CBR, describe a particular application project, and evaluate the performance of the system.

This paper outlines the development and implementation of CASMIR (Collaborative Agent-based System for Multimedia Information retrieval a Multimedia Information Retrieval (MIR) system that attempts to aid users in searching for multimedia data by using a connectionist IR model within a multi-agent framework.
This IR model allows the system to provide query by reformulation, closest match searching and collaborative user profiling whilst its implementation as a community of Java software agents, communicating in KQML, provides the benefits of parallelism and scalability.
A preliminary evaluation is also described, the results of which indicate that not only is the system able to learn about its users, but that collaborative user profiling can provide a significant increase in the learning rate.

This paper presents the 2005 Miracle’s team approach to Bilingual and Multilingual Information Retrieval.
In the multilingual track, we have concentrated our work on the merging process of the results of monolingual runs to get the multilingual overall result, relying on available translations.
In the bilingual and multilingual tracks, we have used available translation resources, and in some cases we have using a combining approach.

The notion of using many, most likely different, sensory subsystems in a computer object recognition system immediately provokes several questions: How will multiple sensors be used in conjunction?
What object qualities are best described by which sensor, and how is sensor utilization optimized?
To what extent does the information provided by each sensor overlap with that provided by others, and how then is this used?
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-80-22.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/724 University of Pennsylvania
The Moore School of E
lec t r ica l Engineering Computer Architecture
f o r Object Recognition and Sensing

Soft set theory was initiated by Molodtsov in 1999.
In the past years, this theory had been applied to many branches of mathematics, information science and computer science.
In 2003, Maji et al. introduced some operations of soft sets and gave some operational rules.
Recently, some of these operational rules are pointed out to be not true.
Furthermore, Ali et al in their paper, introduced and discussed some new operations of soft sets.
In this paper, we further investigate these operational rules given by Maji et al. and Ali et al
We obtain some sufficient-necessary conditions such that corresponding operational rules hold and give correct forms for some operational rules.
These results will be help for us to use rightly operational rules of soft sets in research and application of soft set theory.
Keywords—Soft sets, union, intersection, complement.

Collecting data from the web is essential for research in social science, like digital marketing and applied economics, and for data research areas like Information Retrieval and Big Data.
However, most of current methods for automatically collecting web data do not take into account principles of data protection policies.
Therefore, it is crucial to realize a method that complies with legal, ethical, and license policies established by governments and associations.
In this paper, we present a general framework for automatically collecting data from social networks which is able to preserve data privacy.
We propose an ontology to capture the nature of social network data, and a procedure for translating web data into an OWL 2 ontology.
Then we propose to use Semantic Web Technologies for formally expressing legal and ethical policies that regulate web data collection and mechanisms for being compliant with those policies.

Networking: Suppose you need to multicast a message from one source node to many other machines on the network.
Along what paths should the message be sent, and how can this set of paths be determined from the network’s structure?
Information Retrieval:
How does a search engine like Google store the contents of the Web so that the few pages that are most relevant to a given query can be extracted quickly?
You need to store a set of variable names along with their associated types.
Given an assignment between two variables we need to look them up in a symbol table, determine their types, and determine whether it is possible to cast from one type to the other (say because one is a subtype of the other Computer Graphics: You are designing a virtual reality system for an architectural building walk-through.
Given the location of the viewer, what portions of the architectural design are visible to the viewer?

1.INTRODUCTION Research in information retrieval (IR) has expanded to take a broader perspective of the information seeking process to explicitly include users, tasks, and contexts in a dynamic setting rather than treating information search as static or as a sequence of unrelated events.
The traditional Cranfield/TREC IR system evaluation paradigm, using document relevance as a criterion, and evaluating single search results, is not appropriate for many circumstances considered in current research.
Several alternatives to relevance have been proposed, including utility, and satisfaction.
We suggest an evaluation model and methodology grounded in the nature of information seeking and centered on usefulness.
We believe this model has broad applicability in current IR research.

This introduction to the second international conference on Formal Ontology and Information Systems presents a brief history of ontology as a discipline spanning the boundaries of philosophy and information science.
We sketch some of the reasons for the growth of ontology in the information science field, and offer a preliminary stocktaking of how the term ‘ontology’ is currently used.
We conclude by suggesting some grounds for optimism as concerns the future collaboration between philosophical ontologists and information scientists.
Categories Descriptors Information Systems, Artificial Intelligence.
General Terms Ontology, conceptual modeling, domain modeling, formal ontology in information systems.

A common information-concept would turn the sciences dealing with certain aspects of real-world information-processing manifestations into a single, though transdisciplinary information science.
The philosophical interpretation of the self-organisation theories is a proper background theory capable of restoring information theory as a theory of evolutionary systems which process information.
A multi-stage (spiral) model of information may be derived from it.

Distributed information retrieval, where a single broker coordinates retrieval from many independent search services, has been extensively studied— but typically without any particular application and sometimes even without any explicit motivation.
There have been a handful of arguments given for distributed IR—coverage, effectiveness, and ease of use for example—
but these are not borne out by experience.
I ask: are there in fact uses for distributed IR?
There are, but generally for organisational, not technical, reasons, and they have not been well-studied.

CMAC is one useful learning technique that was developed two decades ago but yet lacks adequate theoretical foundation.
Most past studies focused on development of algorithms, improvement of the CMAC structure, and applications.
Given a learning problem, very little about the CMAC learning behavior such as the convergence characteristics, effects of hash mapping, effects of memory size, the error bound, etc.
can be analyzed or predicted.
In this paper, we describe the CMAC technique with mathematical formulation and use the formulation to study the CMAC convergence properties.
Both information retrieval and learning rules are described by algebraic equations in matrix form.
Convergence characteristics and learning behaviors for the CMAC with and without hash mapping are investigated with the use of these equations and eigenvalues of some derived matrices.
The formulation and results provide a foundation for further investigation of this technique.

The faculty and student viewpoints of the senior project course in the core curriculum of the Computer and Information Science Department at NJIT are presented.
Each viewpoint is examined along with its impact on the mechanics of the course.
The changes in course mechanics are related to the growing student population in the Department.

On the basis of obtaining the data of mass
WeChat public1, in order to improve the operational efficiency and quality of WeChat public number.
On the basis of the retrieval technology, the quality evaluation model of WeChat public number was established.
A sort learning algorithm based on model retrieval is proposed.
Use the vector space technology based on the weight of the entry position to retrieve the contents of WeChat public number, and then use the WeChat public number quality evaluation model to sort.
The retrieved articles sorted data to recommend to the operator, so that the operator can be faster and more efficient to find their hope to find high quality
WeChat number of public articles.

In evidence-based medicine, clinical questions involve four aspects: Patient/Problem (P Intervention (I Comparison (C) and Outcome (O known as PICO elements.
In this paper we present a method that extends the language modeling approach to incorporate both document structure and PICO query formulation.
We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location-based weighting strategy.
In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60% in MAP and 70% in P@10 as compared to state-of-the-art methods.

We present a very simple Private Information Retrieval (PIR) protocol that is very eecient for streaming data.
The protocol works for privately retrieving 1-out-of-M binary strings of length L with a communication complexity of O(Mk
Lk where k is the security parameter.
It is the rst protocol whose communication complexity depends on M and L only additively.
For the case that L M (which is normally true for streaming data the amortized communication complexity is O(k) per bit transmitted.
We also show how to convert this protocol to one for Symmetric Private Information Retrieval with an additive overhead of O(Mk log 2 k Our protocol is based on the Quadratic Residuosity Assumption and doesn't require the Random Oracle model.

A critical infrastructure (CI) is an array of assets and systems that, if disrupted, would threaten national security, economy, public health and safety, and way of life.
Essential to the practice of critical infrastructure planning and drills are two pieces of knowledge.
One is about the interactions within a CI system, and the other the interdependencies between systems of CI.
In this paper, we present an ontology-driven method that facilitates the learning of the interdependencies among systems of critical infrastructure.
Employing an integrated system of Geographic Information Science and a generic object-modeling tool, it represents and visualizes the two pieces of knowledge both geographically and diagrammatically.

Digital libraries have expanded in the recent years in scope and content to include content in a vast variety of languages.
The development of technologies that enable access to this varied language information regardless of geographic or language barriers are a key factor for true global sharing of knowledge.
Two such technologies that play a major role in success of multilingual digital libraries are Multilingual Information Retrieval and Translation.
We describe our approach and implementation of a Multilingual Information Retrieval system that helps users identify multilingual content and also a customized Reading Assistant service that assists users of a digital library in reading multilingual content via translation.
We discuss how both these approaches make extensive use of a Universal Dictionary, a collection of dictionaries of various languages across the world.
Index Terms —Digital Library, Multilingual Access, Language Technologies, Machine Translation

TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics.
The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution.
The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts.
Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization.

The probabilistic theory of information retrieval involves the construction of mathematical models based on statistical assumptions of various sorts.
One of the hazards inherent in this kind of theory construction is that the assumptions laid down may be inconsistent with the data to which they are applied.
Another hazard is that the stated assumptions may not be the real assumptions on which the derived modelling equations or resulting experiments are actually based.
Both kinds of error have been made repeatedly in research on probabilistic information retrieval.
One consequence of these lapses is that the statistical character of certain probabilistic IR models, including the so-called ‘binary independence’ model, has been seriously misapprehended.

The theme of this symposium The Systems Approach: Key to Successful Computer Applications attracted a significant response on the part of computer professionals, managers, and users.
Twenty-four papers were selected from over 100 competing in traditional technical areas of hardware, software, simulation, information retrieval, and programming techniques.
Increasing numbers of papers were submitted in areas recognized as keys to systems success including &lt;u&gt;user involvement&lt;/u&gt modelling, novel and useful applications, data bases, security, management, communications and networks, social implications, and &lt;u&gt;peopleware&lt;/u&gt;.

The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding.
Relevant research areas include: content-based multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic-web, MPEG-7 and MPEG-21 standards, user interfaces and human factors.
In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented.
These advances will be integrated in the SCHEMA module-based, expandable reference system.

The University of Exeter participated in the CLEF 2001 bilingual task.
The main objectives of our experiments were to compare retrieval performance for different topic languages with similar easily available machine translation resources and to explore the application of new pseudo relevance feedback techniques recently developed at Exeter to Cross-Language Information Retrieval (CLIR
We also report recent experimental results from our investigations of the combination of results from alternative machine translation outputs; specifically we look at the use of data fusion of the output from individual retrieval runs and merging of alternative topic translations.

In the Information Age, career and life-long learning increasingly depend on information-retrieval (IR) skills to utilize digital and on-line information effectively.
Consequently, IR has become a valuable integral element of many disciplines.
To address primary problems in the current college-level IR education, we have proposed a module-based curricular model that facilitates the development of an array of IR course modules that enable flexible adoption and integration.
In addition, these modules can be designed to meet the specific needs of various disciplines and programs.
This paper presents a preliminary integrated evaluation method which combines subjective and objective evaluation in order to assess the effectiveness of the IR modules.
South Central Conference

One of the tasks of ontology in information science is to support the classification of entities according to their kinds and qualities.
We hold that to realize this task as far as entities such as material objects are concerned we need to distinguish four kinds of entities: substance particulars, quality particulars, substance universals, and quality universals.
These form, so to speak, an ontological square.
We present a formal theory of classification based on this idea, including both a semantics for the theory and a provably sound axiomatization.

In this article, we introduce an out-of-the-box automatic term weighting method for information retrieval.
The method is based on measuring the degree of divergence from independence of terms from documents in terms of their frequency of occurrence.
Divergence from independence has a well-establish underling statistical theory.
It provides a plain, mathematically tractable, and nonparametric way of term weighting, and even more it requires no term frequency normalization.
Besides its sound theoretical background, the results of the experiments performed on TREC test collections show that its performance is comparable to that of the state-of-the-art term weighting methods in general.
It is a simple but powerful baseline alternative to the state-of-the-art methods with its theoretical and practical aspects.

In the present work an innovative attempt is being made to develop a novel conflation method that exploits the phonetic quality of words and uses some standard NLP tools like LD (Levenshtein Distance) and LCS (Longest Common Subsequence) for Stemming process.
General Terms Information Retrieval (IR Stemming.

Identifying personal, institutional or product names in a text is an important task for numerous applications.
This paper describes a solution for the Consumer Products Contest organized at International Conference on Data Mining 2012.
The goal of this competition is to determine the state-of-the-art methods to automatically recognize product mentions in a collection of web documents, and to correctly identify the product(s) that each product mention refers to from a large catalog of products.
Our approach combines methods of information retrieval and problem specific heuristics.

In the domain of biomedical publications, synonyms and homonyms are omnipresent and pose a great challenge for document retrieval systems.
For this year’s
TREC Genomics Ad hoc Retrieval Task, we mainly addressed the problem of dealing with synonyms.
We examined the impact of domain-specific knowledge on the effectiveness of query expansion and analyzed the quality of Google as a source of query expansion terms based on pseudo-relevance feedback.
Our results show that automatic acronym expansion, realized by querying the AcroMed database of biomedical acronyms, almost always improves the performance of our document retrieval system.
Google, on the other hand, produced results that were worse than the other, corpus-based feedback techniques we used as well in our experiments.
We believe that the primary reason for Google’s bad performance in this task is its highly restricted query language.

The use of quantum concepts and formalisms in the information sciences is assessed through an analysis of published literature.
Five categories are identified: use of loose analogies and metaphors between concepts in quantum physics and library/information science; use of quantum concepts and formalisms in information retrieval; use of quantum concepts and formalisms in studying meaning and concepts; quantum social science, in areas adjacent to information science; and the qualitative application of quantum concepts in the information disciplines.
Quantum issues have led to demonstrable progress in information retrieval and semantic modelling, with less clear-cut progress elsewhere.
Whether there may be a future “quantum turn
” in the information sciences is debated, the implications of such a turn are considered, and a research agenda outlined.

In Information Retrieval collections are often considered to be relatively dynamic or diverse, but no general definition has been given for these notions and no actual measure has been proposed to quantify them.
We give intuitive definitions of the dynamicity and diversity properties of text collections and present measures for calculating them based on the notion of novelty.
Experimental results show that the proposed measures are consistent with the definitions and can distinguish collections effectively according to their dynamicity and diversity properties.

Solid state disks (SSDs) can potentially eliminate the I/O bottleneck for many conventional applications.
However, they have a very unique characteristic of erase-before-write, which probably makes existing index maintenance methods inapplicable to SSDs.
In this paper, we propose Hybrid Merge, a new online index maintenance strategy for information retrieval systems, which applies SSDs instead of hard disk drives (HDDs) to store inverted indexes.
We analyze the existing indexing methods through experiments, and design a new merge-based indexing method with no random writes.
We try to take the full advantage of the SSD's fast random reads to overcome the defects of existing methods.
Experimental results show that the proposed method improves indexing and query performance with extremely low write traffic compare to existing approaches.

Sentiment classification is an important subject in text mining research, which concerns the application of automatic methods for predicting the orientation of sentiment present on text documents, with many applications on a number of areas including recommender and advertising systems, customer intelligence and information retrieval.
In this paper, we provide a survey and comparative study of existing techniques for opinion mining including machine learning and lexicon-based approaches, together with evaluation metrics.
Also cross-domain and cross-lingual approaches are explored.
Experimental results show that supervised machine learning methods, such as SVM and naive Bayes, have higher precision, while lexicon-based methods are also very competitive because they require few effort in human-labeled document and isn't sensitive to the quantity and quality of the training dataset.

I propose to look at information retrieval applications from the perspective of the data stack infrastructure that is needed in research prototypes and production systems.

The existing bibliographic retrieval systems are too complex to permit direct on-line access by untrained end users.
Expert system approaches have been introduced in the hope of simplifying the document indexing, search and retrieval operations and rendering these operations accessible to end users.
The expert system approach is examined briefly in this note and the conclusion is reached that expert systems are unlikely to provide much relief in ordinary retrieval environments.
Simpler and more effective retrieval systems than those currently in use can be implemented by falling back on methodologies proposed and evaluated over twenty years ago that operate without expert system intervention.

Music is used for people practicing sports, for elderly individuals, and to help train the mind.
Recently in music information science, studies have been conducted on music therapy and on music classification from a therapeutic point of view.
However, most of these studies have classified music based on melody and tempo.
No classification method that is based on lyrics has been performed for music therapy support.
The authors previously propose a music classification method using emotional words included in lyrics toward music therapy support.
As this method was developed for Japanese lyrics, it is necessary to evaluate the method for English lyrics.
In this paper, the results of such evaluation is described.
We also describe an improved method, appropriate for English lyrics.

With the surge in modern research focus towards Pervasive Computing, lot of techniques and challenges needs to be addressed so as to effectively create smart spaces and achieve miniaturization.
In the process of scaling down to compact devices, the real things to ponder upon are the Information Retrieval challenges.
In this work, we discuss the aspects of multimedia which makes information access challenging.
An Example Pattern Recognition scenario is presented and the mathematical techniques that can be used to model uncertainty are also presented for developing a system that can sense, compute and communicate in a way that can make human life easy with smart objects assisting from around his surroundings.

This paper is concerned with the problem of question search.
In question search, given a question as query, we are to return questions semantically equivalent or close to the queried question.
In this paper, we propose to conduct question search by identifying question topic and question focus.
More specifically, we first summarize questions in a data structure consisting of question topic and question focus.
Then we model question topic and question focus in a language modeling framework for search.
We also propose to use the MDLbased tree cut model for identifying question topic and question focus automatically.
Experimental results indicate that our approach of identifying question topic and question focus for search significantly outperforms the baseline methods such as Vector Space Model (VSM) and Language Model for Information Retrieval (LMIR).

Traditional CRM (Customer Relationship Management) contains 3 modules, Marketing, Sales, and Support, which rely on the customer relationship and profiling information.
While the information contained in those 3 modules is input by operator, it will be prudent to gather much more information from the Internet.
We can find relationship between customers and find their profile from the Internet.
This information can be used to enrich and direct the CRM to perform better in supporting the business objectives.
Gathering information from the Internet means that we need Information Retrieval and Information Extraction that involve many sources from Internet, such as social media, net blog, and news.
This research provides the model of data mining utilization in traditional CRM to become social CRM.
This research contributes for CRM enhancement where customer centric application becomes automated.

Simple representations of documents based on the occurrences of terms are ubiquitous in areas like Information Retrieval, and also frequent in Natural Language Processing.
In this work we propose a logical-probabilistic approach to the analysis of natural language text based in the concept of Uncertain Conditional, on top of a formulation of lexical measurements inspired in the theoretical concept of ideal quantum measurements.
The proposed concept can be used for generating topic-specific representations of text, aiming to match in a simple way the perception of a user with a pre-established idea of what the usage of terms in the text should be.
A simple example is developed with two versions of a text in two languages, showing how regularities in the use of terms are detected and easily represented.

Data fusion is the process of combining the output of a number of Information Retrieval (IR) algorithms into a single result set, to achieve greater retrieval performance.
ProbFuse is a data fusion algorithm that uses the history of the underlying IR algorithms to estimate the probability that subsequent result sets include relevant documents in particular positions.
It has been shown to out-perform CombMNZ, the standard data fusion algorithm against which to compare performance, in a number of previous experiments.
This paper builds upon this previous work and applies probFuse to the much larger Web Track document collection from the 2004 Text REtreival Conference.
The performance of probFuse is compared against that of CombMNZ using a number of evaluation measures and is shown to achieve substantial performance improvements.

This paper reviews a selection of international collaborative efforts in the production of information services and attempts to characterize modes of cooperation Information systems specifically discussed include: international nuclear information system (INIS Nuclear Science Abstract (NSA EURATOM AGRIS AGRINDEX Information Retrieval Limited (IRL IFIS (
International Food Information Service Chemical Abstracts Service (CAS MEDLARS and TITUS 3
methods of international information transfer are discussed: commercial transactions negotiated (bilateral) barter arrangements and contribution to internationally managed systems Technical, economic, and professional objectives support the rationale for international cooperation
It is argued that economic and political considerations, as much as improved technology or information transfer, will determine the nature of collaboration in the future.

No wonder you activities are, reading will be always needed.
It is not only to fulfil the duties that you need to finish in deadline time.
Reading will encourage your mind and thoughts.
Of course, reading will greatly develop your experiences about everything.
Reading communities of practice fostering peer to peer learning and informal knowledge sharing in the work place information science and knowledge management is also a way as one of the collective books that gives many advantages.
The advantages are not only for you, but for the other peoples with those meaningful benefits.

Discriminative models have been preferred over generative models in many machine learning problems in the recent past owing to some of their attractive theoretical properties.
In this paper, we explore the applicability of discriminative classifiers for IR.
We have compared the performance of two popular discriminative models, namely the maximum entropy model and support vector machines with that of language modeling, the state-of-the-art generative model for IR.
Our experiments on ad-hoc retrieval indicate that although maximum entropy is significantly worse than language models, support vector machines are on par with language models.
We argue that the main reason to prefer SVMs over language models is their ability to learn arbitrary features automatically as demonstrated by our experiments on the home-page finding task of TREC-10.

Key Lab of Process Industry Automation of Ministry of Education; School of Information Science and Engineering, Northeastern University, Shenyang, 110004, P. R. China (e-mail: sdlqk01@126.com; zhaojun@ise.neu.edu.cn Department of Information Engineering, Research School of Information Sciences and Engineering, The Australian National University, Canberra ACT 0200, Australia Department of Computer Engineering, Dogus University, Kadikoy, TR-34722, Istanbul, Turkey (e-mail: gdimirovski@dogus.edu.tr)

The characteristics of different corpora influence the success of Information Retrieval and NLP methods.
How to best characterise a corpus is still an unexplored research area.
In this paper, we use a model that has so far been applied for user profiling in Information Filtering, to profile the corpora of the TIPSTER collection.
Each corpus profile is a network of terms that allows the extraction of a series of statistical features.
These features can be used to calculate the similarity between the corpora in TIPSTER.
This is part of ongoing work that aims at providing a corpus profiling service that will map corpora to their features and to the corresponding experimental results of various models and techniques.

This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity.
Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests
Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions in this paper we focus on measuring the semantic similarity of short texts.
Through experiments performed on a paraphrase data set, we show that the semantic similarity method outperforms methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric.

This paper describes the first set of experiments defined by the MIRACLE (Multilingual Information RetrievAl for the CLEf campaign) research group for some of the cross language tasks defined by CLEF.
These experiments combine different basic techniques, linguistic-oriented and statistic-oriented, to be applied to the indexing and retrieval processes.

With the growing amount of on-line information and its dynamic nature, users' burden of retrieving information has become heavier and existing passive working is too inefficient to meet users' needs.
An intelligent information retrieval model based on multi-agent paradigm and conceptual graphs is proposed.
The model (IIR agent) has two layers of group agent and personal agent.
Subagents of the IIR agent are also identified so that they can be designed and implemented separately.
The system represents queries and documents in conceptual graph (CG which brings semantic in some functions, such as intelligent search, auto-notification, navigation guide and personal information management.
CG-based model improves ranking of documents returned from WWW search engines and makes passive working mode more efficient

During unexpected events such as natural disasters, individuals rely on the information generated by news outlets to form their understanding of these events.
This information, while often voluminous, is frequently degraded by the inclusion of unimportant, duplicate, or wrong information.
It is important to be able to present users with only the novel, important information about these events as they develop.
We present the problem of updating users about time critical news events, and focus on the task of deciding which information to select for updating users as an event develops.
We propose a solution to this problem which incorporates techniques from information retrieval and multi-document summarization and evaluate this approach on a set of historic events using a large stream of news documents.
We also introduce an evaluation method which is significantly less expensive than traditional approaches to temporal summarization.

This paper presents a case study of the development of an interface to a novel and complex form of document retrieval: searching for texts written in foreign languages based on native language queries.
Although the underlying technology for achieving such a search is relatively well understood, the appropriate interface design is not.
A study involving users (with such searching needs) from the start of the design process is described covering initial examination of user needs and tasks; preliminary design and testing of interface components; building, testing, and further refining an interface; before finally conducting usability tests of the system.
Lessons are learned at every stage of the process leading to a much more informed view of how such an interface should be built.

Newspapers, travel narratives, blogs, books and the Internet hold a huge amount of geographic information that can be extracted in order to provide visual exploration.
Also, the understanding of place references involves knowledge of the document context.
In this way, the study of tools for disambiguation is needed.
For the automatic annotation of time and location, both shared world knowledge and document context needs to be captured.
This paper is centered on analyzing online unstructured documents: travel narratives and online newspapers.
Our approach is based on the exploration of tools able to make automatically the disambiguation of placenames.
In this case, we have used a Geoparsing Web Service to extract geographic coordinates from the online unstructured documents.
Once geographic coordinates are extracted by using eXtensible Markup Language (XML) we draw the geo-positions and link documents into a map image in order to visualize textual information.

Recent advances in commercial conversational services that allow naturally spoken and typed interaction, particularly for well-formulated questions and commands, have increased the need for more human-centric interactions in information retrieval.
The First International Workshop on Conversational Approaches to Information Retrieval (CAIR`17) brings together academic and industrial researchers to create a forum for research on conversational approaches to search.
A specific focus is on techniques that support complex and multi-turn user-machine dialogues for information access and retrieval, and multi-model interfaces for interacting with such systems.
We invite submissions addressing all modalities of conversation, including speech-based, text-based, and multimodal interaction.
We also welcome studies of human-human interaction (e.g collaborative search) that can inform the design of conversational search applications, and work on evaluation of conversational approaches.

The k nearest neighbor (kNN) search on road networks is an important function in web mapping services.
These services are now dealing with rapidly arriving queries, that are issued by a massive amount of users.
While overlay graph-based indices can answer shortest path queries efficiently, there have been no studies on utilizing such indices to answer kNN queries efficiently.
In this paper, we fill this research gap and present two efficient kNN search solutions on overlay graph-based indices.
Experimental results show that our solutions offer very low query latency (0.1 ms) and require only small index sizes, even for 10-million-node networks.

The author of this book has done an excellent job of achieving at least one portion of his two-fold purpose to introduce the computer science student to some of the basic problems of information retrieval and to describe the techniques required to develop suitable computer programs and to describe the general structure of the relevant computer programs so that basic design considerations may be understood by information officers and librarians will find this text palatable."

String Matching is the classical and existing problem, despite the fact that the real world aspects belonging to the research field of computer science.
In this domain one or several strings called “Pattern” is to be searched within a well-built string or “Text String matching strategies or algorithms provide key role in various real world problems or applications.
A few of its imperative applications are Spell Checkers, Spam Filters, Intrusion Detection System, Search Engines, Plagiarism Detection, Bioinformatics, Digital Forensics and Information Retrieval Systems etc.
This paper is inclusive of analyzing nutshells about string matching along with its long-ago contributory details in an assortment of real world applications.

Activity recognition in smart environments and healthcare systems is gaining increasing interest.
Several approaches are proposed to recognize activities namely intrusive and non-intrusive approaches.
This paper presents a new fully non-intrusive approach for recognition of Activities of Daily Living (ADLs) in smart environments.
Our approach treats the activity recognition process as an information retrieval problem in which ADLs are represented as hierarchical models, and patterns associated to these ADLs models are generated.
A search process for these patterns is applied on the sequences of activities recorded when users perform their daily activities.
We show through experiments on real datasets recorded in real smart home how our approach accurately recognizes activities.

Optical music recognition (OMR) serves as one of the key technologies in Music Information Retrieval by mining symbolic knowledge directly from images of scores.
A fullfledged OMR system encompasses both image recognition and music interpretation to convert image data to symbolic representations.
This process has proved to be remarkably challenging, so the state-of-the-art OMR systems still leave much to be desired.
The development of deep learning in recent years has brought great success in different domains, e.g. object recognition and scene understanding, and aroused researcher’s interest to address unsolved problems with this new tool.
Other work has introduced the first attempts of using deep learning to address OMR, but the models apply more appropriately to text than music scores.
In this paper we propose a hybrid model that combines the power of Hidden Markov Models (HMM) and Recurrent Neural Networks (RNN) for end-to-end score recognition.

We propose an information filtering system for documents by a user profile using latent semantics obtained by singular value decomposition (SVD) and independent component analysis (ICA In information filtering systems, it is useful to analyze the latent semantics of documents.
ICA is one method to analyze the latent semantics.
We assume that topics are independent of each other.
Hence, when ICA is applied to documents, we obtain the topics included in the documents.
By using SVD remove noises before applying ICA, we can improve the accuracy of topic extraction.
By representation of the documents with those topics, we improve recommendations by the user profile.
In addition, we construct a user profile with a genetic algorithm (GA) and evaluate it by 11-point average precision.
We carried out an experiment using a test collection to confirm the advantages of the proposed method.

Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical applications they are to be computed on music corpora containing audio files encoded in a variety of lossy formats.
Such encodings distort the original signal and therefore may affect the computation of descriptors.
This raises the question of the robustness of these descriptors across various audio encodings.
We examine this assumption for the case of MFCCs and chroma features.
In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre.
Using two different audio analysis tools over a diverse collection of music tracks, we compute several statistics to quantify the robustness of the resulting descriptors, and then estimate the practical effects for a sample task like genre classification.

The paper proposes an approach to information retrieval based on the use of a structure (ontology) both for document (resp.
query) indexing and query evaluating.
The conceptual structure is hierarchical and it encodes the knowledge of the topical domain of the considered documents.
It is formally represented as a tree.
In this approach, the query evaluation is based on the comparison of minimal sub-trees containing the two sets of nodes corresponding to the concepts expressed in the document and the query respectively.
The comparison is based on the computation of a degree of inclusion of the query tree in the document tree.
Experiments undertaken on MuchMore benchmark showed the effectiveness of the approach.

A simple algorithm is presented for increasing the efficiency of information retrieval searches which are implemented using inverted files.
This optimization algorithm employs knowledge about the methods used for weighting document and query terms in order to examine as few inverted lists as possible.
An extension to the basic algorithm allows greatly increased performance optimization at a modest cost in retrieval effectiveness.
Experimental runs are made examining several different term weighting models and showing the optimization possible with each italic>

A Novel Clustering Method for Patient Stratification Hongfu Liu, Rui Zhao, Hongsheng Fang, Feixiong Cheng, Yun Fu Yang-Yu Liu Department of Electrical and Computer Engineering, Northeastern University, Boston, Massachusetts 02115, USA.
Channing Division of Network Medicine, Brigham and Women’s Hospital, Harvard Medical School, Boston, Massachusetts 02115, USA.
Chu Kochen Honors College, Zhejiang University, Hangzhou, Zhejiang 310058, China.
Department of Statistics, Stanford University, Stanford, California 94305, USA. Center for Complex Network Research and Departments of Physics, Computer Science and Biology, Northeastern University, Boston, Massachusetts 02115, USA.
Center for Cancer Systems Biology, Dana-Farber Cancer Institute, Boston, Massachusetts 02115, USA.
College of Computer and Information Science, Northeastern University, Boston, Massachusetts 02115, USA These authors contributed equally to this work.

This paper analyses the prominent problems emerging in current management of project information system based on the scope definition of information in construction projects, and the information management situation of projects under construction in the new market environment.
It focuses on studying the related strategies of information retrieval in construction projects with the application of information technology.

We built two Information Retrieval systems that were targeted for the TREC-6 “aspect oriented” retrieval track.
The systems were built to test the usefulness of different visualizations in an interactive IR setting —in particular, an “aspect window” for the chosen task, and a 3-D visualization of document inter-relationships.
We studied 24 users of the system and report the usage of different user interface elements, whether experienced users outperformed novices, and whether spatial reasoning ability was a good predictor of effective use of 3-D.

We organized a workshop at SIGIR’01 to explore the area of information retrieval techniques for speech applications.
Here we summarize the results of that workshop

Though lattice-based information representation has the advantage of providing efficient visual interface over textual display, the complexity of a lattice may grow rapidly with the size of the database.
In this paper we formally draw the analogy between Vector Space Model and Concept Lattice, from which we introduce the notion of Term-Document Lattice as a model for information retrieval.
We then propose to use the idea of quotient lattice to reduce the complexity of a Term-Document Lattice.
The equivalence relation required to construct the quotient lattice is obtained by performing a Singular Value Decomposition on the original term-document matrix.

In the extended vector space model, each document vector consists of a set of subvectors representing the multiple concepts or concept classes present in the document.
Typical information concepts, in addition to the usual content terms or descriptors, include author names, bibliographic links italic>etc italic>
The extended vector space model is known to improve retrieval effectiveness.
However, a major impediment to the use of the extended model is the construction of an extended query.
In this paper, we describe a method for automatically extending a query containing only content terms (a single concept class) to a representation containing multiple concept classes.
No relevance feedback is involved.
Experiments using the CACM collection resulted in an average precision 34% better than that obtained using the standard single-concept term vector model.

There has recently been an upsurge of interest in document genres the fusion of form and function that makes a document instantly recognizable to its community of users.
The utility of genre identification is now accepted as a premise, but many interesting and challenging questions remain.
The papers assembled here create a truly interdisciplinary body of work, drawing on literary and communication theories, library and information science, natural language processing, writing, and machine learning, among others.
One of the most challenging aspects of research in genre is to create a rigorous yet flexible way of representing document genres that will reflect their form and properties, as well as their communicative role as embedded in a particular community of discourse.

In this paper, we present our English-Chinese Cross-Language Information Retrieval (CLIR) system.
We focus our attention on finding effective translation equivalents between English and Chinese, and improving the performance of Chinese IR.
On English-Chinese CLIR, we adopt query translation as the dominant strategy, and utilize English-Chinese bilingual dictionary as the important knowledge resource to acquire correct translations.
On Chinese monolingual retrieval, we investigated the use of different entities as indexes and implement our retrieval system based on the Lucene toolkit.
On system evaluation, we present an effective method to generate the sets of relevant documents for query topics.

This paper proposes a method to lead users to appropriate information based on associative information access.
People of nowadays can access Web resources easily.
However it is difficult to reach the information that satisfies users’ requests.
The results obtained from search engines usually contain large amount of unnecessary information.
Our target users are the persons who could not express appropriate keywords for information retrieval.
To support such users, we clarified the several requirements of an associative information access support system.
According to these requirements, we propose Word Colony that visualizes the overview of the contents of results and suggests unexpected words as new keywords for the next retrieval.
To design Word Colony, we introduced the notion of the dependency of term co-occurrence (DTC) in a document.
In this paper, we describe the details of DTC, Word Colony and its demonstration.

This paper addresses the problems that lawyers experience retrieving information from legal-text databases.
Traditional access mechanisms of text databases require users to know how information is stored.
I will propose a method for index organisation which shields lawyers from the internal storage structures and which allows them to address the legal databases in their own legal terms.
The proposed index is based on a model of legal tasks as opposed to traditional database indexes which represent the contents of the database.
I will lay out the architecture of an information system in which this taskmodel is used to determine the information need, to retrieve relevant documents and to give methodical guidance for the legal task itself.
To account for the design of a task-based legal information retrieval system, a substantial part of this paper is devoted to analysis and representation of legal tasks.

The aim of the paper is to describe the information retrieval model which retrieves information from text documents in natural language by neural networks.
This model comes from the linguistic and conceptual approach for the analysis of text documents.
The neural network model accepts the structure of conceptual, lingvistic oriented model, where the problem of document database creation and document indexing for keyword determining is solved.
Query entering uses the same mechanisms as document formalization for example document database creating method.
Proposed structure of neural network model loses the problem of document retrieval on the base of user question.
However, learning algorithms and neural network invariancy is used by usage of neural networks, it is possible to decrease the complexity of language analysis algorithm computatation.

With the development of internet and storage technology, we have got a lot of data.
In order to find the information from these data, Data Mining has become an increasingly important topic in research as well as in industrial application.
Up to now, there are a lot of Data Mining methods and specific tools.
This article mainly talks about a new Data Mining Method called Data Mining based on Lattice.
It has been applied in many research areas, such as: Data Bases, Data Analysis and Machine Learning Technology.
The experiments finished by foreign researcher showed it may be a useful method for information retrieval and machine learning problem domains.
Data Mining based on Lattice is indeed a better method of organization, which is useful for each domain.
The use and application of Data Mining based on Lattice is an area of active and promising research in various fields.
Therefore, it is important for us to study the Data Mining Method based on Lattice.

Finite state machine (FSM) theory has great potentialities in understanding key concepts and analyzing molecular biological systems, especially in the process of gene expression.
Based on the previous research work, this paper extends the study on the control problems in metabolism and gene mutation with FSM.
The goal is to interpret how to apply control technologies to process of metabolism, and how to eliminate the effects to secondary structures of protein caused by gene mutation.
It is hoped that the proposed model-based analysis will provide an exploration of new interdisciplinary theories intersected by information science, control theory, and the Molecular Biology.

This short paper describes the beginnings of a project to digitize some of the older literature in the information retrieval field.
So far 14 of the older reports, such as the Cranfield reports and ISR reports have been scanned, along with Karen Sparck Jones's Information Retrieval Experiment book.
The PDF versions of these are available from the SIGIR web site, which includes a new \museum" of information retrieval that allows searching of this material and provides room for exhibits of historic interest.
The paper finishes with some thoughts for future work on making more of our IR literature available for searching.

This paper proposes an algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence.
It diiers from standard approaches by allowing for as ne grained distinctions as is warranted by the information at hand, rather than supposing a xed number of senses per word, and by allowing for more than one sense to be assigned to a given word occurrence.
The algorithm is applied to the standard vector-space information retrieval model and an evaluation is performed over the Category B TREC-1 corpus (WSJ subcollection Results show that this sense disambiguation algorithm improves performance by between 7% and 14% on average .

"This paper reviews the practices, problems, and prospects of GIS-based urban modelling.
The author argues that current stand-alone and various loose/tight coupling approaches for GIS-based urban modelling are essentially technology-driven without adequate justification and verification for the urban models being implemented.
The absolute view of space and time embodied in the current generation of GIS also imposes constraints on the type of new urban models that can be developed.
By reframing the future research agenda from a geographical information science (GISci) perspective, the author contends that the integration of urban modelling with GIS must proceed with the development of new models for the informational cities, the incorporation of multi-dimensional concepts of space and time in GIS, and the further extension of the feature-based model to implement these new urban models and spatial-temporal concepts according to the emerging interoperable paradigm."

One of the important challenges in today's contact center solutions is to increase the speed at which the agents can find information to respond to customer queries.
In this paper, we study the issues relevant to information retrieval by contact center agents and propose a solution to address those issues.
Our proposed solution is towards a) defining user specific, agent specific, and business specific contexts for contact center b) using simple mechanisms initially to derive current context of a query and pre-fetch the information and appropriately present the pre-fetched information to the agent c) derive contact center specific analyzed context based on past information about customer, agent and business scenarios to understand the context of the query; and (d) using the current and analyzed contexts to retrieve information.

The main objective of Internet users is to find the required information with high efficiency and effectiveness.
Finding information on an object’s visual features is useful when specific keywords for the object are not known.
Since mobile agent technology is expected to be a promising technology for information retrieval, there is a number of mobile agent based-information retrieval approaches have been proposed in recent years.
In this paper a new approach for image-based information retrieval using mobile agents is presented.
Multiple information agents continuously traverse the Internet and collect images that are subsequently indexed based on image information such as the URL location, size, type and the date of indexation.
In the search phase, the mobile agent receives the image of object as a query and searches the set of web pages that contain information about the object by matching the query to images on web pages.

This paper investigates to what extent task-oriented user support based on plan recognition is feasible in a highly situation-driven domain like information retrieval (IR) and discusses requirements for appropriate task models.
It argues that information seeking tasks which are embedded in some higher-level external task context (e.g. travel planning) often exhibit procedural dependences; that these dependences are mainly due to external task; and that they can be exploited for inferring the users' goals and plans.
While there is a clear need for task models in IR to account for situational determinants of user behaviour, what is required are hybrid models that take account of both is 8220;planned&#8221; and 8220;situated&#8221; aspects.
Empirical evidence for the points made is reported from a probabilistic analysis of retrieval sessions with a fact database and from experience with plan-based and state-based methods for user support in an experimental travel planning system

This paper describes our semi-automatic keyword based approach for the four topics of Information Extraction from Microblogs Posted during Disasters task at Forum for Information Retrieval Evaluation (FIRE) 2016.
The approach consists three phases;

This article surveys probablistic approaches to modeling information retrieval.
The basic concepts of probabilistic approaches to information retrieval are outlined and the principles and assumptions upon which the approaches are based are presented.
The various models proposed in the development of IR are described, classified, and compared using a common formalism.
New approaches that constitute the basis of future research are described.

The tagging of Named Entities (NE the names of particular things or classes and numeric expressions, is regarded as an important component technology for many NLP applications.
These applications include Information Extraction, from which it was born, QuestionAnswering, Summarization and Information Retrieval.
However, up to now, the number of NE types has been quite limited, 7 in MUC, 8 in IREX and 5 in the ACE program.
Many more kinds of things have proper names or proper classes of expressions, and also finer distinctions are needed for some applications.
We now propose a Named Entity hierarchy which contains about 150 NE types.
The focus of this paper is the design of the hierarchy and we would like to provide this resource for any application.
We report the design and development procedure of the hierarchy.

For the past many years we are using search engine for image retrieval.
These search engines use shapes, contents, text, and caption based approach for getting relevant image from the web repository.
This image repository contains billions of 2D and 3D images as well as relevant information about those images.
For shape based approach user has to give dimensions of that particular image for getting relevant response.
This paper describes the necessity of an efficient search engine for retrieving information about an image by uploading an image on the search engine or giving image as a query for retrieving information related to that particular image.
It can be proved very helpful for a novice user who is searching information about an

Most data on the Web is in the form of text or image.
Finding desired data on the Web in a timely and cost-effective way is a problem of wide interest.
In the last several years, many search engines have been created to help Web users find desired information.
In this paper we present a new technique to eliminate the affixes and their effects on recognizing similar Persian documents.
Reviewing affixes’ rules and exceptions in Persian language, we extracted about 300 common inflectional suffixes and their combinations.
We evaluate the effectiveness of eliminating the affixes from Persian texts on document similarity using four major document similarity approaches: Latent Semantic Indexing, Shingling, Vector Space Model, and Co-occurrence.
Evaluation results demonstrate improvement in retrieval and detection of similar documents after eliminating affixes.

Over the past few years, HNC has developed a neural network based, vector space approach to text retrieval.
This approach, embodied in a system called MatchPlus, allows the user to retrieve information on the basis of meaning and context of a fi'ee text query.
The MatchPlus system uses a neural network based, constrained sel~organization technique to learn word stem interrelationships directly 3~om a training corpus, thereby eliminating the need for hand crafted linguistic knowledge bases and their often substantial maintenance requirements.
This paper presents results fi'om recent enhancements to the basic MatchPlus concept.
These enhancements include the development of a one step learning law that greatly reduces the amount of time and~or computational resources required to train the system, and the development of a prototype multilingual (English and Spanish) text retrieval system.

This r epor
t i s composed of four s
s 1) a desc r ip t ion scheme for documentation centers and 2) desc
r ip t ions of information
i e v a l systems at three European data archives--a)SSI
~C (Social Science Research Council) Data ~nk, Colchester, Essex, Great Britain; b)
Zentralarchive far enq~irische Socialforschl,n Z.A.R Central Archive for E~pirical Social Research Colc~e, Germany; c) Steinmetz Stichting, Amsterdam, The Netherlands.
One of the more interesting aspects of the "description scheme" presented in the report is that it can also be used, as the author points out, as a "checklist" of areas to be covered in setting up a data archive-a "prescriptive scheme as Soergel calls it.
The functiQns detailed in this part of the report are not confined to information retrieval.
A partial listing of the general table of contents gives an idea of the nature and range of topics considered:

A user-interface architecture, called FireWorks, is described.
It consists of a domain-specific toolkit and frameworks for building information retrieval applications.
The architecture’s exrmessiveness is demonstrated bv first describing an example application, which is designed to help searchers coordinate access to multiple on-line sources.
Second, FireWorks is compared to a similar architecture, called InfoGrid.
The comparison focuses debate on what software abstractions are required for implementing a range of effective environments for information seeking.

This paper describes the experiments of our team for CLEF 2001, which includes both official and post-submission runs.
We took part in the monolingual task, for Dutch, German, and Italian.
The focus of our experiments was on the effects of morphological analyses such as stemming and compound splitting on retrieval effectiveness.
Confirming earlier reports on retrieval in compound splitting languages such as Dutch and German, we found improvements to be around 25% for German and as much as 69% for Dutch.
For Italian, lexiconbased stemming resulted in gains of up to 25%.

In this paper we give a formal characterization of reactive control using action theories.
In the process we formalize the notion of a reactive control program being correct with respect to a given goal, a set of initial states, and action theories about the agent/robot and about the exogenous actions.
We give su ciency conditions that guarantee correctness and use it to give an automatic method for constructing provenly correct control modules.
We then extend our approach to action theories and control modules that have specialized sensing actions and that encode conditional plans.
Finally we brie
y relate our theory with the implementation of our mobile robot Diablo, which successfully competed in AAAI 96 and 97 mobile robot contests.

Mobile navigation is a frequently used application, especially with the increasing proliferation of online geographical data.
However, the origin and destination are often private information closely tied to a user’s personal life.
Sharing thosewith an onlinemap provider greatly increases the chance of the user being profiled.
Contrary to existing location privacy problems, the origin and the destination are essential for finding the shortest path in a realtime traffic setting.
In this paper, we show that the problem can be solved with Private Information Retrieval (PIR) techniques without disclosing the origin or the destination.
We analyze the cost associated with this approach and propose a practical solution with the assumption of a semi-honest third party to improve the efficiency.
The proposed practical solution only introduces encryption overhead over the plain scenario where the path is returned by knowing the origin and destination 2013 Elsevier B.V. All rights reserved.

The prediction of time series is one of the most important scientific fields in information sciences (for example, in the development of AI One of the most difficult tasks is how to reduce the time of building a prediction.
In this paper the efficient method of calculating universal-coding-based predictors is presented.
This approach allows to calculate the Krichevsky predictor and similar ones with linear complexity.

Variability of semantic expression is a fundamental phenomenon of a natural language where same meaning can be expressed by different texts.
Natural Language Processing applications like Question Answering, Summarization, Information Retrieval systems etc.
often demand a generic framework to capture major semantic inferences in order to deal with the challenges created by this phenomenon.
Textual Entailment can provide such framework.
Textual Entailment can be defined as the phenomenon of inferring a text from another.
Entailment is a directional relation between two texts.
This relation holds when the truth of one text fragment follows from the truth of the other.
Conventionally, the entailing fragment is called as text and entailed one is called as hypothesis.
Textual entailment is classically defined as:

This paper describes the MUMIS project, which applies ontology based Information Extraction to improve the results of Information Retrieval in multimedia archives.
It makes use of a domain specific ontology, multilingual lexicons and reasoning algorithms to automatically create a semantic annotation of sources.
The innovative aspect is the use of a cross document merging algorithm that combines the information extracted from separate textual sources to produce an integrated, more complete, annotation of the material.
This merging and unification process uses ontology based reasoning and scenarios which are extracted from annotated sources.
The algorithms presented here have been implemented in a working demonstration prototype and have been applied on material from the Euro 2000 Soccer Championships.

This paper addresses the important problem of finding relevant information in the context of a business process.
It presents an information access solution called EIL (enterprise information leverage) which combines information extraction and semantic search to support information needs of professionals selling IT services.
EIL leverages structured and unstructured data using novel architecture and special purpose algorithms.
Our approach is to organize information around business activities (e.g. a sale and the system supports semantic concept based information retrieval by utilizing both database query and document search where the relevant business activities act as a contextual constraint.
We experimentally show that this approach is promising for reducing noise in search results.
EIL is currently under pilot deployment in one of the IBM services sales units.

Information retrieval techniques play a critical role in the development of the information systems.
Different searches have focused on the way of improving the retrieval effectiveness.
Query expansion via relevance feedback is a good technique that proved to be a good way to improve the retrieval performance.
In this paper, we investigate new methods to improve the query reformulation process.
A two step process is employed to reformulate query.
In a preliminary step, a local set of documents is built from the retrieved result.
In a second step, a co-occurrence analysis is performed on the local document set to deduce the terms to be used for the query expansion.
To build the local set we use firstly a content-based analysis.
It is a similarity study between the retrieved documents and the query.
The second method combines content and hypertext analyses to achieve the local set construction.
The TREC frame is used to evaluate the proposed processes.

As evaluation is an important but difficult part of information retrieval system design and experimentation, evaluation questions have been the subject of much research.
An “evaluation study” is an investigation into some aspect of evaluation.
These types of studies typically experiment on ranked results from actual retrieval systems, most often those that were submitted to TREC tracks.
We argue that the standard of evidence in these types of studies should be increased to the level required of text retrieval studies, by testing on multiple data sets, multiple subsets of data, and comparison to baselines using hypothesis testing.
We demonstrate that baseline performance on the standard data sets is quite high, necessitating strong evidence to support claims.

The integration of database and information retrieval techniques provides users with a wide range of high quality services.
We present a prototype system, called NUITS, for efficiently processing keyword queries on top of a relational database.
Our NUITS allows users to issue simple keyword queries as well as advanced keyword queries with conditions.
The efficiency of keyword query processing and the user-friendly result display will also be addressed in this paper.

Parallel agent-based simulation of individual-level spatial interactions within a multicore computing environment Zhaoya Gong a Wenwu Tang
a b David A. Bennett c JeanClaude Thill a a Department of Geography and Earth Sciences University of North Carolina at Charlotte Charlotte NC USA b Center for Applied Geographic Information Science University of North Carolina at Charlotte Charlotte NC USA c Department of Geography
University of Iowa Iowa City IA USA Published online: 23 Nov 2012.

Query expansion adds related words to a user query in order to improve retrieval results.
It’s an important step in information retrieval.
Most of current query expansion methods pay attention to specific expansion strategies or algorithms, while neglecting the query itself.
In reaction to the phenomenon, a multistrategy query expansion method based on semantics was proposed.
This method started by analyzing the semantic structure of user query, and adopted corresponding strategy to select expansion terms.
The expansion words are derived from three parts: WordNet, massive web page set and search engine performance evaluation data, which were merged semantically in each expansion algorithm later.
The experiment showed this method can improve retrieval results to some extent.
Subject Categories and Descriptors H.3.3
[Information Search and Retrieval Query Formulation: I.2.7 [Natural Language Processing]
General Terms: Information Retrieval, Query Expansion

In this paper, we introduce the second version of Microsoft Research Asia Multimedia (MSRA-MM a dataset that aims to facilitate research in multimedia information retrieval and related areas.
The images and videos in the dataset are collected from a commercial search engine with more than 1000 queries.
It contains about 1 million images and 20,000 videos.
We also provide the surrounding texts that are obtained from more than 1 million web pages.
The images and videos have been comprehensively annotated, including their relevance levels to corresponding queries, semantic concepts of images, and category and quality information of videos.
We define six standard tasks on the dataset 1) image search reranking 2) image annotation 3) query-by-example image search 4) video search reranking 5) video categorization; and (6) video quality assessment.

Nowadays, Internet goes deep into peoplepsilas daily routines, brings human beings mass information, and becomes an increasingly important channel of obtaining information.
Facing such tremendous amount of information, it is a matter of life and death for us to look for a rapid and efficient method of hunting information.
This paper revolves around Search Engine, which is a most efficient tool for information organization and retrieval on the Internet.
The methods of retrieving information on the Internet have been improved greatly in recent years because of the application of search engines.
Nevertheless, for some reasons, search engine does not provide Internet users perfect retrieval service, and often returns unsatisfied results.
Basing on the statistical data from authority, this thesis concentrates on the current developmental situation and its problems found in application of search engines in China, aims at providing search engine developers some references.

Retrieval in a multimedia database usually involves combining information from different modalities of data, such as text and images.
However, all modalities of the data may not be available to form the query.
The retrieval results from such a partial query are often less than satisfactory.
In this paper, we present an approach to complete a partial query by estimating the missing features in the query.
Our experiments with a database of images and their associated captions show that, with an initial text-only query, our completion method has similar performance to a full query with both image and text features.
In addition, when we use relevance feedback, our approach outperforms the results obtained using a full query.

This paper presents a quantitative performance analysis of two different approaches to the lemmatization of the Czech text data.
The first one is based on manually prepared dictionary of lemmas and set of derivation rules while the second one is based on automatic inference of the dictionary and the rules from training data.
The comparison is done by evaluating the mean Generalized Average Precision (mGAP) measure of the lemmatized documents and search queries in the set of information retrieval (IR) experiments.
Such method is suitable for efficient and rather reliable comparison of the lemmatization performance since a correct lemmatization has proven to be crucial for IR effectiveness in highly inflected languages.
Moreover, the proposed indirect comparison of the lemmatizers circumvents the need for manually lemmatized test data which are hard to obtain and also face the problem of incompatible sets of lemmas across different systems.

The Computer and Information Science and Engineering Directorate is encouraging increased participation of researchers in international collaborations.
The IRIS Division is currently involved in three activities to promote international collaboration.

Information retrieval may suggest a document, and information extraction may tell us what it says, but which information sources do we trust and which assertions do we believe when different authors make conflicting claims?
Trust algorithms known as fact-finders attempt to answer these questions, but consider only which source makes which claim, ignoring a wealth of background knowledge and contextual detail such as the uncertainty in the information extraction of claims from documents, attributes of the sources, the degree of similarity among claims, and the degree of certainty expressed by the sources.
We introduce a new, generalized fact-finding framework able to incorporate this additional information into the fact-finding process.
Experiments using several state-of-theart fact-finding algorithms demonstrate that generalized fact-finders achieve significantly better performance than their original variants on both semisynthetic and real-world problems.

Content stored/shared on Web and document repositories has increased greatly leading to problems in locating required information from massive volumes.
Progress in retrieving required information was achieved with search engine technology development that could collect, store and pre-process information globally, responding to users’ needs instantly.
Use of text classification techniques ensures web page classification.
Presently, semantics are the basis for content description and query processing techniques required for Information Retrieval (IR This paper presents an approach for information retrieval from web pages, based on the proposed extraction methods.
AdaBoost algorithm is used to obtain and classify features and BF tree with the proposed feature extraction ensures high classification accuracy.

The principle of polyrepresentation and document clustering are two established methods for Interactive Information Retrieval, which have been used separately so far.
In this paper we discuss a cluster based polyrepresentation approach for information need and document based representations.
In our work we simulate and evaluate two possible cluster browsing strategies a user could apply to explore the polyrepresentative clusters.
In our evaluation we apply information need and bibliographic features on the iSearch collection.
Our results suggest that polyrepresentative cluster browsing may be more effective than exploring a ranked list.

Wireless sensor networks with thousands of tiny sensor nodes, are expected to find wide applicability and increasing deployment in coming years, as they enable reliable monitoring and analysis of the environment.
In this paper, we propose a hybrid routing protocol (APTEEN) which allows for comprehensive information retrieval.
The nodes in such a network not only react to time-critical situations, but also give an overall picture of the network at periodic intervals in a very energy efficient manner.
Such a network enables the user to request past, present and future data from the network in the form of historical, one-time and persistent queries respectively.
We evaluated the performance of these protocols and observe that these protocols are observed to outperform existing protocols in terms of energy consumption and longevity of the network.

Although there have been many prototypes of visualization in support of information retrieval, there has been little systematic evaluation that distinguishes the benefits of the visualization per se from that of various accompanying features.
The current study focuses on such an evaluation of NIRVE, a tool that supports visualization of search results.
Insofar as possible, functionally equivalent 3D, 2D, and text versions of NIRVE were implemented.
Nine novices and six professional users completed a series of information-seeking tasks on a set of retrieved documents.
There were high interface costs for the 3D visualization, although those costs decreased substantially with experience.
Performance was best when the tool’s properties matched task demands; only under the right combination of task, user, and interface did 3D visualization result in performance comparable to functionally matched 2D and textual tools.

Generating query-biased summaries can take up a large part of the response time of interactive information retrieval (IIR) systems.
This paper proposes to use document titles as an alternative to queries in the generation of summaries.
The use of document titles allows us to pre-generate summaries statically, and thus, improve the response speed of IIR systems.
Our experiments suggest that title-biased summaries are a promising alternative to query-biased summaries.

To improve the information retrieval system for user, programmers have to learn a user's preferences accurately.
In order to optimize retrieval accuracy, modeling the users appropriately based on their preferences and personalizing search according to each individual user are important.
Implicit feedback information improves the user modeling process.
The advantage of implicit modeling is effectively improving the user model without extra effort of user.
Several implicit feedback features are used to develop user modeling process.
We present a new model to find a user's preferences from click through behavior and using the exposed preferences to adapt the search engine's ranking function for improving search service.
In this proposed model, the combination of viewed and stored document summaries is used.

Adversarial IR in general, and search engine spam, in particular, are engaging research topics with a real-world impact for Web users, advertisers and publishers.
The AIRWeb workshop will bring researchers and practitioners in these areas together, to present and discuss state-of-the-art techniques as well as real-world experiences.
Given the continued growth in search engine spam creation and detection efforts, we expect interest in this AIRWeb to surpass that of the previous three editions of the workshop (held jointly with WWW 2005, SIGIR 2006, and WWW 2007 respectively).

A large amount of data is present on the web.
It contains huge number of web pages and to find suitable information from them is very cumbersome task.
There is need to organize data in formal manner so that user can easily access and use them.
To retrieve information from documents, we have many Information Retrieval (IR) techniques.
Current IR techniques are not so advanced that they can be able to exploit semantic knowledge within documents and give precise results.
IR technology is major factor responsible for handling annotations in Semantic Web (SW) languages and in the present paper knowledgeable representation languages used for retrieving information are discussed.

Traceability Link Recovery (TLR) is an important software engineering task in which a stakeholder establishes links between related items in two sets of software artifacts.
Most existing approaches leverage Information Retrieval (IR) techniques, and formulate the TLR task as a retrieval problem, where pairs of similar artifacts are retrieved and presented to a user.
These approaches still require significant human effort, as a stakeholder needs to manually inspect the list of recommendations and decide which ones are true links and which ones are false.
In this work, we aim to automate TLR by re-imagining it as a binary classification problem.
More specifically, our machine learning classification approach is able to automatically classify each link in the set of all potential links as either valid or invalid, therefore circumventing the substantial human effort required by existing techniques.

This paper discusses the importance of part-order-preservation in shape matching.
A part descriptor is introduced that supports both preserving and abandoning the order of parts.
The evaluation shows that retrieval results are improved by almost 38% if the original ordering is preserved.

This paper proposes a methodological approach to CLIR applications for the development of a system which improves multi-word processing when specific domain translation is required.
The system is based on a multilingual ontology, which can improve both translation and retrieval accuracy and effectiveness.
The proposed framework allows mapping data and metadata among language-specific ontologies in the Cultural Heritage (CH) domain.
The accessibility of Cultural Heritage resources, as foreseen by recent important initiatives like the European Library and Europeana, is closely related to the development of environments which enable the management of multilingual complexity.
Interoperability between multilingual systems can be achieved only by means of an accurate multi-word processing, which leads to a more effective information extraction and semantic search and an improved translation quality.

SCALIR is a legal information retrieval system which uses a combination of symbolic and connectionist artificial intelligence techniques.
Traditional systems used for automating legal research have many difficulties which make them 8220;brittle&#8221 SCALIR is an attempt to rectify many of these problems.
SCALIR's hybrid nature is especially appropriate for the legal domain, which requires both logical and associative inferences.
The system also benefits from a unique direct-manipulation style interface and the ability to improve its performance based on user feedback.

Simulation of human users engaged in interactive information retrieval (IIR) may be a key resource to enable evaluation of IR systems in interactive settings in ways that are scalable, objective, and cheap to implement.
This paper considers generation of simulated users in an operationalist framework.
It identifies several challenges due to the cognitive nature of IIR and its highly conditionalized interactions with information systems and suggests a program for user simulation must rely on results from user studies and will need to overcome several difficult modeling problems.

Quickly understanding the content of a data source is very useful in several contexts.
In a Peer Data Management System (PDMS peers can be semantically clustered, each cluster being represented by a schema obtained by merging the local schemas of the peers in this cluster.
In this paper, we present a process for summarizing schemas of peers participating in a PDMS.
We assume that all the schemas are represented by ontologies and we propose a summarization algorithm which produces a summary containing the maximum number of relevant concepts and the minimum number of non-relevant concepts of the initial ontology.
The relevance of a concept is determined using the notions of centrality and frequency.
Since several possible candidate summaries can be identified during the summarization process, classical Information Retrieval metrics are employed to determine the best summary.

In NTCIR workshop II, we participated in both the Chinese and English-Chinese Information Retrieval tracks in an automatic way.
In the Chinese Information Retrieval (CHIR) track, to improve the effectiveness of the word-based retrieval system, a new method, named PM-based (PM: Proximity and Mutual information) method, is designed and introduced into the system.
Employing the proximity and mutual information of the term pairs, the effectiveness of the PM-based method is expected to outmatch that of the word-based method in processing dictionary-uncovered words.
In the English-Chinese Information Retrieval (ECIR) track, we mainly focused our efforts on how to disambiguate the Chinese meanings of each English query word when it is translated with a given English-Chinese bilingual dictionary, and then used the word-based Chinese text retrieval system to carry out the ECIR task.

Relevance feedback has a history in information retrieval that dates back well over thirty years (c.f SL96 Relevance feedback is typically used for query expansion during short-term modeling of a user’s immediate information need and for user profiling during long-term modeling of a user’s persistent interests and preferences.
Traditional relevance feedback methods require that users explicitly give feedback by, for example, specifying keywords, selecting and marking documents, or answering questions about their interests.
Such relevance feedback methods force users to engage in additional activities beyond their normal searching behavior.
Since the cost to the user is high and the benefits are not always apparent, it can be difficult to collect the necessary data and the effectiveness of explicit techniques can be limited.

To help expedite the process of constructing use case diagrams, a widely used notation in software engineering, we attempt to develop a generator system that can extract use case diagrams automatically from the input of a software requirements specification.
The use of natural language processing techniques can greatly assist this process, one of which is to use syntax-driven semantic analysis.
Semantic analysis can provide output in the form of semantic representations that can be used to extract appropriate use case elements.
A set of rules have been developed to extract information about the elements of use case diagrams contained in the semantic representation.
Our tests show that the system is able to automatically construct use case diagrams for a wide variety of linguistic variations.
In a test using real-world cases, an average precision of 0.7375 and recall of 0.691 is obtained.

Gerry Stahl directs the Virtual Math Teams research project (mathforum.org/vmt) and teaches HCI and CSCL at the College of Information Science and Technology at Drexel University in Philadelphia, USA, where he is an Associate Professor.
He studied computer science and Artificial Intelligence at the University of Colorado, where he was later a research professor.
He was program chair of CSCL 2002, is founding Executive Editor of the "International Journal of Computer-Supported Collaborative Learning ijCSCL.org) and has published a book on his research and theories Group Cognition:
Computer Support for Building Collaborative Knowledge
His papers and other information are available at his website (www.cis.drexel.edu/faculty/gerry).

iv Acknowledgements vi Table of

An Evidence-based approach is using a best available evidence for making a judicious decision about a given set of problem.
Evidence-based approach is an integration of individually gained expertise with the best possible evidence available from a systematic research.
It started in medicine as evidence-based medicine (EBM) and is now being used in other fields such as nursing, psychology, education, library and information science also.
Its basic principles are that all practical decisions made should 1) be based on research studies and 2) that these research studies are selected and interpreted according to some specific norms characteristic for Evidence Based Practice [EBP Software draws its roots from EBM and does Evidence

The performance of Information Retrieval in the Question Answering system is not satisfactory from our experiences in TREC QA Track.
In this article, we take a comparative study to re-examine IR techniques on document retrieval and sentence level retrieval respectively.
Our study shows: 1) query reformulation should be a necessary step to achieve a better retrieval performance; 2) The techniques for document retrieval are also effective in sentence level retrieval, and single sentence will be the appropriate retrieval granularity.

System combination is an effective strategy to boost retrieval performance, especially in complex applications such as cross-language information retrieval (CLIR) where the aspects of translation and retrieval have to be optimized jointly.
We focus on machine learning-based approaches to CLIR that need large sets of relevance-ranked data to train high-dimensional models.
We compare these models under various measures of orthogonality, and present an experimental evaluation on two different domains (patents, Wikipedia) and two different language pairs (Japanese-English, German-English
We show that gains of over 10 points in MAP/NDCG can be achieved over the best single model by a linear combination of the models that contribute the most orthogonal information, rather than by combining the models with the best standalone retrieval performance.

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.

With the exponential rise in the volume of database there is a demand for faster and timely retrieval of data.
Early data retrieval (IR) systems used straightforward tools of word matching, and the texts to be searched were small.
But in the present situation, due to giant volume of knowledge sources ,which too is in numerous forms, there is a requirement of additional Economical retrieval techniques which might retrieve only that part of the data which has relevancy.
This paper discusses the underlying Principles and challenges of IR, and provides relatives Complexities of tools and techniques for the 2 major jobs performed by any IR system, i.e data illustration and knowledge Retrieval.

In this work we propose a method to identify the raga of a Carnatic music signal.
Raga is analogous to melody in Western music but is much richer and stronger in its representation.
The main motive behind Raga identification is that it can be used as a good basis for music information retrieval of Carnatic music songs or Film songs based on Carnatic music.
The input polyphonic music signal is analyzed and made to pass through a signal separation algorithm to separate the instrument and the vocal signal.
After extracting the vocal signal we segment the vocal signal using our proposed segmentation algorithm.
Using our proposed singer identification algorithm we determine the singer to know the fundamental frequency of the singer.
The frequency components of the signal are then determined and we map these frequency components into the swara sequence and thereby determine the Raga of the particular song, which could be used to index the songs and further for retrieval based on the Raga.

In this paper, we report experimental results of our approach using BM25E model for retrieval large-scale XML collection, to improve the effectiveness of XML Retrieval.
This model is commonly used in the information retrieval community.
We propose new algorithm using Score Sharing that allow to assign parent score by sharing score from leaf node to their parents by a Top-Down Scheme approach.
In order to improve efficiency on response time, The Score Sharing algorithm processing time on 10,000 leaf nodes is around 0.135 ms. per topic after getting the result list from Zettair.
The Zettair is able to process on average time per topic using less than 1 second then the processing time is up to 1 second per topic and our experiment show that the BM25E with Score Sharing improve iP[0.10] by 24.40% and MAiP by 31.89% over the original BM25E.
In addition, our algorithm able to handle both elements level and document level by only setting parameter.

Automatic information retrieval, that is document retrieval, was an early concern in computing.
It might, however, be thought that since modern on-line systems have more than achieved the technological aims of the original workers in the field, there is no further need for research.
I shall argue that this is not the case.

This paper describes the theoretical paradigms of information retrieval in information science and computer science, and constructs the theory framework of information retrieval from three perspectives that are user, information and technology.
It evaluates the research priorities of the two disciplines and crossdomain of information retrieval theory.
Finally, it points-out the theory status and development trend of information retrieval in information science and computer science, and provides exploration direction in information retrieval theory.

In circumstances when a variable in a program has an incorrect value the process of debugging it is often a process about discovering the history of that variable, or rather the ancestry of the value that it contains.
In this paper we propose a new technique for debugging that revolves around being able to trace back through the history of a particular variable or variables that it depends upon.
Our development is in the domain of functional programming as the proposal has particular signi cance in this domain due to the fact that so many standard debugging techniques cannot be used at all.
This paper was presented at the 3rd International Workshop on Automated Debugging (AADEBUG'97 hosted by the Department of Computer and Information Science, Linkoping University, Sweden, 26{27

The paper proposes a novel Information Retrieval technique based on numerical analysis for recovering traceability links between code and software documentation.
The results of a reported case study demonstrate that the proposed approach significantly outperforms two vector-based IR models, i.e the Vector Space Model and Latent Semantic Indexing, and it is comparable and sometimes better than a probabilistic model, i.e the Jensen-Shannon method.
The paper also discusses the influence of each method with the specific artifact type considered and the artifact language.

We propose a new method for cross-lingual information retrieval that avoids direct translation of queries by implicit encoding of translations in a Random Indexing vector space model.
It relies solely on a translation dictionary and source/target language text corpora, without the need for aligned multilingual text.
Initial experiments show promising results for retrieval of related terms in Norwegian-English cross-lingual information retrieval.
A number of possible improvements and extensions to the model are discussed.

for dissemination)
This document describes the research conducted within Clarity addressing the problem of transitive CLIR: cross language retrieval where direct translation from a source language to a target is not possible and translation via an intermediate or pivot language is required.
The report summarizes three pieces of research conducted by Clarity partners.
Two prototype transitive retrieval system are in operation by Clarity partners.

Semantic search has been one of the motivations of the semantic Web since it was envisioned.
We propose a model for the exploitation of ontology-based knowledge bases to improve search over large document repositories.
In our view of information retrieval on the semantic Web, a search engine returns documents rather than, or in addition to, exact values in response to user queries.
For this purpose, our approach includes an ontology-based scheme for the semiautomatic annotation of documents and a retrieval system.
The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm.
Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to knowledge base incompleteness.
Experiments are shown where our approach is tested on corpora of significant scale, showing clear improvements with respect to keyword-based search

This paper addresses some particular issues related to the difficult task of automatic mapping of Web information to help the user to find interesting and unexpected information on the Web.
The approach puts together qualitative and quantitative reasoning.
The qualitative reasoning is done through a spatial mereotopological calculus since we metaphorically see Web sites as space regions.
As a result, documents/sites are grouped together into classes according to the spatial relation they satisfy.
Quantitative reasoning is done through Information Retrieval techniques allowing users to choose a document based on a similarity measure between documents/sites.
Experiments have been undertaken in two directions: to find an intuitive model to display the Web map through the development of a system called HyperMap and to discover interesting and unexpected information from the spatial Web maps metaphor.

Information Retrieval (IR) is an important application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text.
While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes.
Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text.
This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing.
The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection.
The experiment’s
results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance.

Skeletonization of polygons is a technique, which is often applied to problems of cartography and geographic information science.
Especially it is needed for generalization tasks such as the collapse of small or narrow areas, which are negligible for a certain scale.
Different skeleton operators can be used for such tasks.
One of them is the straight skeleton, which was rediscovered by computer scientists several years ago after decades of neglect.
Its full range of practicability and its benefits for cartographic applications have not been revealed yet.
Based on the straight skeleton an area collapse that preserves topological constraints as well as a partial area collapse can be performed.
An automatic method for the derivation of road centerlines from a cadastral dataset, which uses special characteristics of the straight skeleton, is shown.

Blackboard network teaching platform provides teachers with convenient and handy tool modules to create, release, and manage online exams.
It aims to reform the online examination methods.
In this study, an exploration was made on medical information retrieval item bank based on blackboard network teaching platform.
How to use the tool modules of blackboard network teaching platform to build item banks was described.
And the significance of the construction of online test for teachers to conveniently test the students was discussed.
Finally, some suggestions about the construction of item bank based on blackboard network teaching platform were provided.

Bilingual web pages contain abundant term translation knowledge which is crucial for query translation in Cross Language Information Retrieval systems.
But it is a challenging task to extract term translations from bilingual web pages due to the variation in web page layouts and writing styles.
In this paper, based on the observation that translation pairs on the same web page tend to appear following similar patterns, a new extraction model is proposed to adaptively learn extraction patterns and exploit them to facilitate term translation mining from bilingual web pages.
Experiments reflect that this model can significantly improve extraction coverage while maintaining high accuracy.
It improves query translation in cross-language information retrieval, leading to significantly higher retrieval effectiveness on TREC collections.

Hypertext navigation alone is insuficient for eficient Information Retrieval (ZR Previous attempts to combine IR techniques with hypertext have been confined to the pre-authored structure of a document.
In this paper we extend computer-science methods to synthesize a tailor-made hypertext document in response to each user's query.
The synthesis technique can also be used to automatically create a pre-authored hypertext document according to an author's speciJications.

There are so many increasing amount of information in the today’s world-wide web.
For these increasing amounts of information, we need efficient and effective index structure when we have to find needed information.
Most indexing techniques directly matched terms from the document and terms from query.
The role of usage of domain specific ontology is very wide for the specific application area.
Ontology can be defined as a formal explicit specification of a shared conceptualization.
It is a formal and declarative representation which includes vocabulary for referring to the terms in that subject area and logical statements that describe the relationships among the terms.
This system proposes a semantic-based indexing structure that is built on the basic context document by using semantic suffix tree clustering, context ontology and semantic suffix tree clustering with context ontology.
This paper shows the results of these three methods on the same application area.

Xiaonan Luo National Engineering Research Center of Digital Life, State-Province Joint Laboratory of Digital Home Interactive Applications, School of Information Science Technology, Sun Yat-sen University, Guangzhou 510006, China; Shenzhen Digital Home Key Technology Engineering Laboratory, Research Institute of Sun Yat-sen University in Shenzhen, Shenzhen 518057, China.
E-mail: lnslxn@mail.sysu.edu.cn

We present two approaches to the Amharic English bilingual track in CLEF 2004.
Both experiments use a dictionary based approach to translate the Amharic queries into English Bags-of-words, but while one approach removes non-content bearing words from the Amharic queries based on their IDF value, the other uses a list of English stop words to perform the same task.
The resulting translated (English) terms are then submitted to a retrieval engine that supports the Boolean and vector-space models.
In our experiments, the second approach (based on a list of English stop words) performs slightly better than the one based on IDF values for the Amharic terms.

Private Information Retrieval (PIR) protocols enable a client to access data from a server without revealing what data was accessed.
The study of Computational Private Information Retrieval (CPIR) protocols, an area of PIR protocols focusing on computational security, has been a recently reinvigorated area of focus in the study of cryptography.
However, CPIR protocols still have not been utilized in any practical applications.
The aim of this paper is to determine whether the Melchor Gaborit CPIR protocol can be successfully utilized in a practical manner in an anonymous peer-to-peer environment.

Alice is a user.
Clyde is a cloud service provider.
Alice has a bunch of very large files containing many, many words.
Alice wants to store the files in the cloud.
Alice wants to do a particular form of private information retrieval (PIR specifically: Alice wants to determine which file or files contain a given target word.
Alice wants Clyde to do the necessary computations (or most of the computations) in the cloud.
Alice assumes that Clyde is honest but curious: Clyde will faithfully perform any computation
Alice tells him to do and will return the correct answer, but Clyde would like to learn the results for his own purposes.
Alice, on the other hand, has the following goals:

This paper addresses the use of Music Information Retrieval (MIR) techniques in music education and their integration in learning software.
A general overview of systems that are either commercially available or in research stage is presented.
Furthermore, three well-known MIR methods used in music learning systems and their state-of-the-art are described: music transcription, solo and accompaniment track creation, and generation of performance instructions.
As a representative example of a music learning system developed within the MIR community, the Songs See software is outlined.
Finally, challenges and directions for future research are described.
1998 ACM Subject Classification H.5.5 Sound and Music Computing, J.5 Arts and Humanities– Music, H.5.1 Multimedia Information Systems, I.5 Pattern Recognition

The goal of this work is to build an audio information retrieval system which provides users with flexibility in formulating their queries: from audio examples to naı̈ve text.
Specifically, the focus of this paper is on using naı̈ve text to create input queries describing the desired information of the users.
Using naı̈ve text queries, however, raises interoperability issues between annotation and retrieval processes due to the wide variety of available audio descriptions.
In this paper, we propose an intermediate audio description layer (iADL) to solve the interoperability issues between the annotation and retrieval processes.
The iADL comprises two axes corresponding to semantic and onomatopoeic descriptions based on human-to-human communication experiments on how humans express sounds verbally.
Various text modeling schemes, such as latent semantic analysis (LSA) and latent topic model, are utilized to transform the naı̈ve text onto the proposd iADL.

| We present Gibbs Markov random eld models as a powerful and robust descriptor of spatial information in typical remote sensing image data.
This class of stochastic image models provides an intuitive description of the image data using parameters of an energy function.
For the selection among several nested models and the t of the model we proceed in two steps of Bayesian inference.
This procedure yields the most plausible model and its most likely parameters, which together describe the image content in an optimal way.
Its additional application at multiple scales of the image enables us to capture all structures being present in complex remote sensing images.
The calculation of the evidences of various models applied to the resulting quasi-continuous image pyramid automatically detects such structures.
We present examples for both Synthetic Aperture Radar (SAR) and optical data.

In the information society, it is much easier for someone to find relevant data if s/he has an information need because of the availability of databases and electronic information tools.
In information science this topic is usually treated under the topic information behaviour.
In lexicography the term access process is used (Bergenholtz and Gouws 2010 It can be shown that this process beginning with the "origin of the problem" leading to an "information source usage situation" contains different parts, and that each part contains different phases, with the pre-consultation phase and the intra-consultant phase containing different steps.
The most important concepts here are the access route and the access time.
In this paper some experiments in two case studies are described to show how different access processes in different user situations take place.

The Information Retrieval involves the development of computer systems for the storage and retrieval of textual information.
In this paper we present an information retrieval system based on prioritized fuzzy information and triangular uninorms, for dealing with such a problem.
The advantage of this method with respect to related works is the use of the fuzzy prioritized weighting information facilitates the expression of information needs and the ability of trade off between criteria with different priority levels, triangular uninorms are used to deal with the criteria priority in the process.

We propose a hybrid information retrieval (IR) procedure that builds on two well-known IR approaches: data fusion and query expansion via relevance feedback.
This IR procedure is designed to exploit the strengths of data fusion and relevance feedback and to avoid some weaknesses of these approaches.
We show that our IR procedure is built on postulates that can be justified analytically and empirically.
Additionally, we offer an empirical investigation of the procedure, showing that it is superior to relevance feedback on some dimensions and comparable on other dimensions.
The empirical investigation also verifies the conditions under which the use of our IR procedure could be beneficial.

The first Asian Summer School in Information Access (ASSIA 2013) was held between 22nd and 24th June, 2013 in Tsukuba, Japan.
The summer school offered 9 lectures in Information Retrieval, Web Search, and related topics, along with two panel discussions and a poster session.
This reports a successful international summer school in Asia attracting a total of 63 participants from the range of countries in Asia, Europe, and North America.

Alberto Politi, Jonathan C. F. Matthews, Anthony Laing, Alex S. Clark, Jérémie Fulconis, Graham D. Marshall Peter Dekker Martin Ams Michael Withford William J. Wadsworth Terry Rudolph Martin J. Cryan, John G. Rarity, André Stefanov, Siyuan Yu, and Jeremy L. O’Brien Centre for Quantum Photonics, H. H. Wills Physics Laboratory Department of Electrical and Electronic Engineering, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol,
UK www.phy.bris.ac.uk/groups/cqp Jeremy.L.OBrien@gmail.com
∗Centre for Ultrahigh bandwidth Devices for Optical Systems, Macquarie University, Australia †Centre for Photonics and Photonic Materials, Department of Physics, University of Bath, UK ‡QOLS, Blackett Laboratory Institute for Mathematical Sciences, Imperial College London, UK

Neural learning has been used with e6ectiveness in natural language processing tasks.
Particularly, the Widrow–Ho6 and the Kivinen–Warmuth exponentiated gradient (based on neural learning rules) algorithms have been used in text categorization, improving the results obtained by the well-known Rocchio’s algorithm.
The high performance of competitive learning algorithms, recently applied to solve information retrieval problems, leads us to use them in the speci=c text categorization tasks.
This paper presents a multilingual categorization system based on neural learning, using the polyglot Bible as training collection, both in Spanish and English.
The method we suggest is based on using the LVQ algorithm to build a classi=er that learns the training multilingual collection.
We have performed experiments with the four algorithm which show that the ideas we describe are promising and are worth further investigation.
c 2002 Elsevier B.V.
All rights reserved.

This is a general introduction to Information Retrieval con-<lb>centrating on some specific topics.
I will begin by setting the scene for<lb>IR research and introduce its extensive experimental evaluation meth-<lb>odology.
I will highlight some of the related areas of research which are<lb>currently in fashion emphasising the role of IR in each.
For each in-<lb>troductory topic I will illustrate its relevance to IR in the context of a<lb>multimedia and multi-lingual environment where appropriate.
I will also<lb>try and relate these topics to the other papers contained in this volume.<lb
My main purpose will be to introduce some underlying concepts and<lb>ideas essential for the understanding of IR research and techniques.

In social media communication, code switching has become quite a common phenomenon especially for multilingual speakers.
Automatic language identification becomes both a necessary and challenging task in such an environment.
In this work, we describe a CRF based system with voting approach for code-mixed query word labeling at word-level as part of our participation in the shared task on Mixed Script Information Retrieval at Forum for Information Retrieval Evaluation (FIRE) in 2015.
Our method uses character n-gram, simple lexical features and special character features, and therefore, can easily be replicated across languages.
The performance of the system was evaluated against the test sets provided by the FIRE 2015 shared task on mixed script information retrieval.
Experimental results show encouraging performance across the language pairs.
CCS Concepts •Computer systems organization Embedded systems; Redundancy; Robotics Networks Network reliability;

This paper analyzes the computational complexity of finding relevant documents on the Web.
Given a search query that has n significant terms, relevant documents retrieved by search engines will contain at least a number k of the significant terms.
The threshold k chosen will depend on the collection of documents and is determined experimentally upon formation of the collection.
Algorithms are then provided to compute a similarity ranking.
The fundamental analysis is based on combinatorial theory and theorems providing bounds on the runtime complexity of the algorithms are proven.

With the ever increasing amounts of information stored on the Web or archived within computing systems, high performance data processing architectures are required to process this data in real time.
The aim of the work presented in this paper is the development of a hardware text mining IP-Core for use in FPGA based systems.
In this paper we describe the development of our text processing hardware pipeline, with the addition of a complex word stemming and loadable stop list stages.
The performance of this system is then compared to our initial prototype and an equivalent software implementation using the Lucene software library

Measuring similarity between objects is a fundamental task in domains such as data mining, information retrieval, and so on.
Link-based similarity measures have attracted the attention of many researchers and have been widely applied in recent years.
However, most previous works mainly focus on introducing new link-based measures, and seldom provide theoretical as well as experimental comparisons with other measures.
Thus, selecting the suitable measure in different situations and applications is difficult.
In this paper, a comprehensive analysis and critical comparison of various link-based similarity measures and algorithms are presented.
Their strengths and weaknesses are discussed.
Their actual runtime performances are also compared via experiments on benchmark data sets.
Some novel and useful guidelines for users to choose the appropriate link-based measure for their applications are discovered.

Private Information Retrieval (PIR despite being well studied, is computationally costly and arduous to scale.
We explore lower-cost relaxations of information-theoretic PIR, based on dummy queries, sparse vectors, and compositions with an anonymity system.
We prove the security of each scheme using a flexible differentially private definition for private queries that can capture notions of imperfect privacy.
We show that basic schemes are weak, but some of them can be made arbitrarily safe by composing them with large anonymity systems.

Entity Recognition concept came from NER system.
In NER name specifics entities are detected from documents like name of city, state, Country, organization, person, location, sport, river, quantity etc.
Whereas Entity Recognition detects all the entities presents in a documents to help the improvement of the performance of some high level NLP tasks like Question Answering, Auto Summarization, Machine Translation, Information Retrieval.
In this paper our main objective is to perform Entity Recognition in Natural languages CRF.
We will use POS tagging and Named entity Recognition with our own tag set to perform this task.

Structured document retrieval requires different user graphical interfaces from standard Information Retrieval.
An Information Retrieval system dealing with structured documents has to enable a user to query, browse retrieved documents, provide query refinement and relevance feedback based not only on full documents, but also on specific document structural parts.
In this paper, we present a new graphical user interface for structured document retrieval that provides the user with an intuitive and powerful set of tools for structured document searching, retrieved list navigation, and search refinement.
We also present the results of a preliminary evaluation of the interface which highlights strengths and weaknesses of the current implementation and suggests directions of future work.

School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China School of Library and Information Science, Indiana University, Bloomington, IN, USA Department of Computer Science and Technology, Tsinghua University,China School of International Business Administration, Shanghai University of Finance and Economics, Shanghai, China Rawls College of Business, Texas Tech University, TX, USA. ldf3824@yahoo.com.cn binghe, dingying, sugimoto, eyan}@indiana.edu, jietang@tsinghua.edu.cn

This paper describes our participation in the Polarity for Reputation classification task of RepLab 2013.
Our system leveraged on a set of components previously developed for a Twitter message polarity classifier.
Following a supervised approach, a Logistic Regression classifier is trained from annotated data.
A refined language model is used to represent tweets in terms of a vocabulary consisting only of the most informative terms with word features weighted using a measure from the Information Retrieval field.
To help reduce the sparseness of the feature vector, the model is enriched with another, more compact, representation of the words.
Finally, we extract features to capture the use of informal and affective language.
Our approach ranked in the top three for all the metrics, showing that the strategies for Twitter Sentiment Analysis are useful for the task of Polarity for Reputation classification.

The majority of studies in Personalized Information Retrieval (PIR) literature have focused on monolingual IR, and only relatively little work has been done conceming multilingual IR.
In this paper we propose a novel method to represent user models in a multilingual fashion.
We argue that such representation would be more suitable for Personalized Multilingual Information Retrieval (PMIR
Furthermore, we outline two algorithms for query adaptation based on user information from the multilingual user model.

An essential problem in music information retrieval is to determine the similarity between two given melodies; there are several melodic similarity measures that have been proposed, among others, the Mongeau-Sankoff measure.
In this work we implemented a modified version of the Mongeau-Sankoff measure.
We conducted an experimental study to compare the implemented measure with other similarity measures; this evaluation was done in the context of the 2005 edition of the MIREX symbolic melodic similarity competition.
The most relevant result of our work is an implementation of the Mongeau-Sankoff measure that presents greater effectiveness when compared to other current melodic similarity measures.

In recent years, domain-driven data mining (D3M) has received extensive attention in data mining.
Unlike the traditional data-driven data mining, D3M tends to discover actionable knowledge by tightly integrating the data mining methods with the domain-specific business processes.
However, in most cases, the domain specific actionable knowledge cannot be discovered without the support of domain knowledge, mainly provided by human experts.
Thus, the human-machine-cooperated interactive knowledge discovery process is widely applied in realworld applications.
Active learning can integrate the automated learning algorithm with the domain experts.
The main aim of the paper is to get the information from domain experts to the generalized queries with don’t care attribute using data mining with addition off Active Learning with Generalized Queries Algorithm.

We present a new method for information retrieval using hidden Markov models (HMMs) and relate our experience with this system on the TREC-7 ad hoc task.
We develop a general framework for incorporating multiple word generation mechanisms within the same model.
We then demonstrate that an extremely simple realization of this model substantially outperforms tf :idf ranking on both the TREC-6 and TREC7
ad hoc retrieval tasks.
We go on to present several algorithmic re nements, including a novel method for performing blind feedback in the HMM framework.
Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task, and showed extraordinary performance in development experiments on TREC-6.

In this paper we propose a term clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems.
As the search in question answering is conducted over smaller segments of data than in a document retrieval task, the problems of data sparsity and exact matching become more critical.
In this paper we propose Language Modeling (LM) techniques to overcome such problems and improve the sentence retrieval performance Our proposed methods include building class-based models by term clustering, and then employing higher order <i>n-grams</i> with the new class-based model.
We report our experiments on the TREC 2007 questions from QA track.
The results show that the methods investigated here enhanced the mean average precision of sentence retrieval from 23.62% to 29.91%.

This paper proposes a novel method to generate labels for grouping and organizing the search results returned by auxiliary search engines.
It has applied statistical techniques to measure the quantities of co-occurrence keywords for forming the label matrix of them, and then agglomerated them into higher-level clusters by clustering algorithm in order to classify the results which return from the source search engine.
Compared with Lingo, the experimental results show that the labels generated by our algorithm are of more readability and generality.
What's more, F-measure index also shows that our algorithm has improved the quality of text clustering to some extent.

This paper investigates the effectiveness of a state of the art information retrieval (IR) system in the verse retrieval problem for Quranic text.
The evaluation is based on manually indexed topics of the Quran that provides both the queries and the relevance judgments.
Furthermore, the system is evaluated in both Malay and English environment.
The performance of the system is measured based on the MAP, the precision at 1, 5 and 10, and the MRR scores.
The results of the evaluation are promising, showing the IR system has many potential for the Quranic text retrieval.

We present an approach for personalized retrieval in an e-learning platform, that takes advantage of semantic Web standards to represent the learning content and the user/learner profiles as ontologies, and that re-ranks search results/lectures based on how the contained terms map to these ontologies.
One important aspect of our approach is the combination of an authoritatively supplied taxonomy by the colleges, with the data driven extraction (via clustering) of a taxonomy from the documents themselves, thus making it easier to adapt to different learning platforms, and making it easier to evolve with the document/lecture collection.
Our experimental results show that the learner's context can be effectively used for improving the precision and recall in e-learning content retrieval, particularly by re-ranking the search results based on the learner's past activities.

Structure matching has been the focus and strength of standard XML querying.
However, textual content is still an essential component of XML data.
It is therefore important to extend the standard XML database engine to allow for “Information Retrieval” style queries, namely keyword” based retrieval and “result ranking In this paper, we describe our effort in integrating information retrieval techniques into the Timber XML database system being developed at the University of Michigan, and our participation in the INitiative for the Evaluation of XML Retrieval (INEX).

Plenty of contemporary attempts to search exist that are associated with the area of Semantic Web.
But which of them qualify as information retrieval for the Semantic Web?
Do such approaches exist?
To answer these questions we take a look at the nature of the Semantic Web and Semantic Desktop and at definitions for information and data retrieval.
We survey current approaches referred to by their authors as information retrieval for the Semantic Web or that use Semantic Web technology for search.

The main goals of a switch scheme are high utilization low queuing delay and fairness
To achieve high utilization the switch scheme can maintain non zero small queues in steady state which can be used if the sources do not have data to send It is very important to design and analyze the queue control function which is used in such a scheme
In this contribution we study various queue control functions and present analytical explanation of its behavior and simulation results From the study
we conclude that a simple linear queue control function performs satisfactorily Source Bobby Vandalore Raj Jain Rohit Goyal Sonia Fahmy The Ohio State University Department of Computer and Information Science Columbus OH Contact Phone Fax E mail fvandalor jaing cis ohio state
The presentation of this contribution at ATM Forum is sponsored by NASA Lewis Research Center

The application of Word Sense Disambiguation (WSD) is usually determined exclusively by the trust in the disambiguation system used.
In this paper, a study in the Information Retrieval (IR) field is carried out about the impact of others factors in WSD such as the confidence of the WSD tool, the grade of polisemy or granularity and the difference in the discrimination strength between the original term and the disambiguated one.
Thus, a proposal to decide whether a word should be disambiguated or not according to Inverse Document Frequency (IDF) is presented.
Finally, it is shown that a selective disambiguation based on IDF improves slightly the performance of an IR system.

This paper presents a first step toward the formalization of the concept of document reliability in the context of Information Retrieval (and Information Filtering
Our proposal is based on the hypothesis that the evaluation of the relevance of a document can also depend on the concept of reliability of a document.
This concept has the following properties i) it is user-dependent, i.e a document may be reliable for a user and not reliable for another user ii) it is sourcedependent,
i.e the source which a document comes from may influence its reliability for a user; and (iii) it is also author-dependent, i.e the information about who wrote the document may also influence the user when assessing the reliability of a document.

Article history: Received 13 November 2013 Received in revised form 25 January 2014
Accepted 1 February 2014
Available online 25 February 2014

The purpose of this article is to design the basic model for information retrieval based on free-text domain in a particular database environment.
It is an alternative method to the usage of key that is widely used in today database applications.
At the end of this paper, we show the prototype that has been developed to support the freetext model verification.
The result from the testing process is also given.

Many proper names are spelled inconsistently in speech recognizer output, posing a problem for applications where locating mentions of named entities is critical.
We model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel.
The models follow the framework of the IBM translation models.
The model is trained using a parallel text of closed caption and automatic speech recognition output.
We also test a string edit distance based method.
The effectiveness of these models is evaluated on a name query retrieval task.
Our methods result in a 60% improvement in F1.
We also demonstrate why the problem has not been critical in TREC and TDT tasks.

The fundamental difference between standard information retrieval and XML retrieval is the unit of retrieval.
In traditional IR, the unit of retrieval is fixed: it is the complete document.
In XML retrieval, every XML element in a document is a retrievable unit.
This makes XML retrieval more difficult: besides being relevant, a retrieved unit should be neither too large nor too small.
The research presented here, a comparative analysis of two approaches to XML retrieval, aims to shed light on which XML elements should be retrieved.
The experimental evaluation uses data from the Initiative for the Evaluation of XML retrieval (INEX 2002).

This paper considers the type of problem for which the potential for amalgamating Information Retrieval (IR) and Case-Based Reasoning (CBR) technologies is the highest.
IR characterised as a bottom-up approach to retrieval of text within unconstrained domains.
CBR is characterised as a topdown approach to retrieval of formalised information within domain-specific applications.
It is argued that applications that require relatively detailed responses to specific queries of a large-scale, but domain specific text-based archive represent the middle ground between these two disjoint technologies this is illustrated with a worked example.

In what case do you like reading so much?
What about the type of the information retrieval technology second asia information retrieval symposium airs 2005
jeju island korea october 13 15 2005 proceedings lectu book?
The needs to read?
Well, everybody has their own reason why should read some books.
Mostly, it will relate to their necessity to get knowledge from the book and want to read just to get entertainment.
Novels, story book, and other entertaining books become so popular this day.
Besides, the scientific books will also be the best reason to choose, especially for the students, teachers, doctors, businessman, and other professions who are fond of reading.

Currently, a variety of information retrieval systems are availableto potential users 8230; While in many cases these systems areaccessed from personal computers, typically no advantage is taken of thecomputing resources of those machines (such as local processing andstorage
In this paper we explore the possibility of using the user'slocal storage capabilities to cache data at the user's site.
This wouldimprove the response time of user queries albeit at the cost ofincurring the overhead required in maintaining multiple copies.
In orderto reduce this overhead it may be appropriate to allow copies to divergein a controlled fashion 8230; Thus, we introduce the notion of<?Pub
Fmt italic>
quasi-copies<?Pub Fmt /italic which embodies theideas sketched above.
We also define the types of deviations that seemuseful, and discuss the available implementation strategies abstrbyl Pub Fmt italic 8212;From the Authors' Abstract<?Pub Fmt /italic abstrbyl>

Thai Intelligent Tutor with Information Retrieval (TITIR) is a multi-component intelligent tutor system.
TITIR borrows interesting ideas from many fields including natural language processing, information retrieval, and intelligent tutoring system, TITIR facilitates students' learning and communicating with the tutor via their own language in order to allowing student express their requirements or their needs better and easier.
Personalized information retrieval is embedded in the system to retrieve the appropriate contents for each student by taking their personality into account.

Introduction
In the last years several approaches have been defined to soften the classical Boolean Information Retrieval model, These approaches were formalized within different mathematical framework with the following aims:-to improve the description of documents' information content;

In this paper, we present new approach for parallel string matching.
Some known parallel string matching algorithms are considered based on duels by witness which focuses on the strengths and weaknesses of the currently known methods.
This has applications such as string databases, Information Retrieval and computational biology.
The new ‘divide and conquer’ approach has been introduced for parallel string matching, called the W-period, which is used for parallel preprocessing of the pattern and has optimal implementations in a various models of computation.
The idea, common for every parallel string matching algorithm is slightly different from sequential ones as Knuth-Morris-Pratt or Boyer-Moore algorithm.

[Abstract] Majority of the available digital information are associated with some location or regions on the Earth.
Traditional visual interfaces to digital libraries were not designed to deal with the unique geospatial characteristics of data, nor did they take full advantage of the georeferencing as a mechanism for browsing and retrieving information.
This paper highlights the trends of visual interfaces to geospatial information, and suggests major challenges towards effective visualization of multimedia geospatial document space.
Citing recent research efforts, we review the attempts to integrate document visualization (developed in information science) with geovisualization techniques (developed in geographical information science) to support science, information access and decision making.
We conclude by a set of spatial cognitive principles that influence future development of such visual interfaces to geographic information.

To get better performance, Some researchers have proposed relative work to exploit the position and proximity information of query terms in language model.
However these models need large quantity of training data and its computation complexity is comparatively high.
This paper presents an information retrieval model combining sentence level retrieval and use sentence as a unit to compute the relevant degree of the sentence to query.
Experiment results show our model can get better performance than baseline models.

Information retrieval (IR) is a key component of knowledge management systems (KMSs KMSs frequently rely on keyword searches as a primary mechanism for retrieval.
While keyword searches are very helpful to knowledge workers, they have their limitations.
To illustrate some of the limitations of keyword searches, a database containing all of the articles that have appeared in MIS Quarterly was constructed.
Prior research shows that ambiguous, poorly constructed keyword phrases lead to poor information retrieval results.
Knowledge hierarchies have been used to overcome some of the keyword searches limitations.
This research proposes the use of dimensional modeling and multidimensional database technologies to implement knowledge hierarchies.
It suggests that many of the limitations inherent in keyword searches can be eliminated from knowledge management systems by exploiting the benefits of the hierarchical structure that underlies multidimensional databases.

A Euler number-based topological computation model for land parcel database updating Xiao-Guang Zhou, Jun Chen, F. Benjamin Zhan, Zhilin Li, Marguerite Madden, Ren-Liang Zhao Wan-Zeng Liu a School of Geosciences and Info-Physics, Central South University, Changsha, Hunan, PR, China b National Geomatics Center of China, Beijing, PR, China c Texas Center for Geographic Information Science, Department of Geography, Texas State University, San Marcos, TX, USA d Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong e Center for Remote Sensing and Mapping Science (CRMS Department of Geography, The University of Georgia, Athens, GA, USA Published online:
18 Apr 2013.

In Information Retrieval (IR the Dirichlet Priors have been applied to the smoothing technique of the language modeling approach.
In this paper, we apply the Dirichlet Priors to the term frequency normalisation of the classical BM25 probabilistic model and the Divergence from Randomness PL2 model.
The contributions of this paper are twofold.
First, through extensive experiments on four TREC collections, we show that the newly generated models, to which the Dirichlet Priors normalisation is applied, provide robust and effective performance.
Second, we propose a novel theoretically-driven approach to the automatic parameter tuning of the Dirichlet Priors normalisation.
Experiments show that this tuning approach optimises the retrieval performance of the newly generated Dirichlet Priors-based weighting models.

We propose a multimodal retrieval procedure based on latent feature models.
The procedure consists of a Bayesian nonparametric framework for learning underlying semantically meaningful abstract features in a multimodal dataset, a probabilistic retrieval model that allows cross-modal queries and an extension model for relevance feedback.
Experiments on two multimodal datasets, PASCAL-Sentence and SUN-Attribute, demonstrate the effectiveness of the proposed retrieval procedure in comparison to the state-of-the-art algorithms for learning binary codes.

The network technology and the Internet are creating a completely new information era.
It is believed that in the near future numerous of digital libraries and a great variety of multimedia databases, which consist of heterogeneous types of information including text, audio, image, video and so on, will be available worldwide via the Internet.
This paper deals with the problem of Chinese text and Mandarin speech information retrieval with Mandarin speech queries.
Instead of using the syllable-based information alone, the word-based information was also successfully incorporated to further improve the retrieving performance.
A prototype system with an interface supporting some user-friendly functions was successfully implemented and the initial test results verified the feasibility of our approaches.

Words semantic relevance computation which explores semantic relevance/distance between two words is broadly used in many applications, such as word sense disambiguation, information retrieval, structural disambiguation in parsing syntactic, text categorization, etc.
We study the relation among sememes and the functions of dynamic roles in concepts and their relevant sememes set.
We also propose a new approach to compute words semantic relevance which is based on the relation files provided in HowNet.
Experimental results show that this approach can reveal the semantic relevance between words efficiently.

Argument extraction techniques can likely improve legal information retrieval.
Any effort to achieve that goal should take into account key features of legal reasoning such as the importance of legal rules and concepts, support and attack relations among claims, and citation of authoritative sources.
Annotation types reflecting these key features will help identify the roles of textual elements in retrieved legal cases in order to better inform assessments of relevance for users’ queries.
As a result, legal argument models and argument schemes will likely play a central part in the text annotation type system.

IOTA is a prototype of an Information Retrieval System which can manage a corpus made of highly structured, full text documents.
The first version presented here has intelligent capabilities related to heuristic pattern matching procedures for processing natural language queries, which involve an automatically built thesaurus.
The paper emphazises the overall principles of query processing and gives hints about the underlying techniques used while constructing the thesaurus and automatically indexing highly structured documents.

We propose an information retrieval method based on question identification.
We have implemented and integrated a prototype Chinese speech IR system for a commercial PDA with this method for questions concerning mobile phone operations.
A preliminary evaluation showed that our method is effective.
In the evaluation, our method outperformed a similar document search method by ten points or more in both its precision and recall for top-ranked candidates.

With the evolution of internet, users are looking for documents that are accessible on demand.
The growth in volume of stored information affects the performance of retrieval systems.
In any information retrieval system, two main factors to be considered are document representation termed as indexing and retrieval.
Most of the queries to an information retrieval system are casual, ordinary, bag of words that consists of two or three terms.
In this paper, we implemented an index structure for fast phrase querying namely Word pair index using Terrier 3.5, a well-known open source for information retrieval.
In our experiments, we used FIRE 2011 English data set.
As we increased the size of corpus, time to create index structure has also increased linearly.
When the word pair index structure is used for retrieval, the time to retrieve relevant results from the corpus reduced by 94.73 when compared to traditional inverted index structure.

This paper addresses the problem of merging results obtained from different databases and search engines in a distributed information retrieval environment.
The prior research on this problem either assumed the exchange of statistics necessary for normalizing scores (cooperative solutions) or is heuristic.
Both approaches have disadvantages.
We show that the problem in uncooperative environments is simpler when viewed as a component of a distributed IR system that uses query-based sampling to create resource descriptions.
Documents sampled for creating resource descriptions can also be used to create a sample centralized index, and this index is a source of training data for adaptive results merging algorithms.
A variety of experiments demonstrate that this new approach is more effective than a well-known alternative, and that it allows query-by-query tuning of the results merging function.

For peer-to-peer web search engines it is important to keep the delay between receiving a query and providing search results within an acceptable range for the end user.
How to achieve this remains an open challenge.
One way to reduce delays is by caching search results for queries and allowing peers to access each others cache.
In this paper we explore the limitations of search result caching in large-scale peer-topeer information retrieval networks by simulating such networks with increasing levels of realism.
We find that cache hit ratios of at least thirty-three percent are attainable.

In this paper, we propose an efficient peer-to-peer information retrieval system PeerSearch that supports state-of-the-art content and semantic searches.
PeerSearch avoids the scalability problem of existing systems that employ centralized indexing, index flooding, or query flooding.
It also avoids the non-determinism that exhibited by heuristic-based approaches.
PeerSearch achieves both efficiency and determinism through an elegant combination of index placement and query routing.
Given a query, PeerSearch only needs to search a small number of nodes to identify matching documents.

In this paper, we present a suite of flexible UIMA-based components for information retrieval research which have been successfully used (and re-used) in several projects in different application domains.
Implementing the whole system as UIMA components is beneficial for configuration management, component reuse, implementation costs, analysis and visualization.

This report presents algorithms for finding large matchings in the streaming model.
In this model, applicable when dealing with massive graphs, edges are streamed-in in some arbitrary order rather than residing in randomly accessible memory.
For ε 0, we achieve a 1/(1+ε) approximation for maximum cardinality matching and a 1/(2+ε) approximation to maximum weighted matching.
Both algorithms use a constant number of passes.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
No. MSCIS-04-19.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/21 Finding Matchings in the Streaming Model

The concept of using a relational database to perform information retrieval (IR) search functions is well established.
Prior work demonstrates the capability to perform common functions and advanced ranking algorithms using standard, unchanged SQL.
The previous work does not address the preprocessing of unstructured text within the relational model.
In fact, the parsing of the unstructured data into a structured data set was done outside of the database, usually using sequential programming languages such as C.
This work proves that IR preprocessing does not require proprietary application code to build the framework necessary for searching document databases.
Furthermore, the resulting environment is relational and integrates with other data sources within an organization.

Music Information Retrieval (MIR) and Music Digital Library (MDL) are two interrelated, multidisciplinary research areas with a growing community of involved parties MIR/MDL research brings together computer scientists, audio engineers, librarians, musicologists, educators and business executives in a common effort to provide robust mechanisms for organizing, storing and accessing the world's everincreasing volume ofmusic Newcomers to the world ofMIR/MDL are invited to read Downie [8 Futrelle and Downie [10 and Byrd and Crawford [3] for overviews ofissues currently being examined by MIR/MDL researchers .

In this paper we present a formal framework, based on the representational theory of measurement and we define and study the properties of utility-oriented measurements of retrieval effectiveness like AP, RBP, ERR and many other popular IR evaluation measures.

The 38th European Conference on Information Retrieval took place from the 20th to the 23rd of March 2016 in Padua, Italy.
This report summarizes the conference in terms of the presented keynotes, scientific and social programme, industry day, tutorials, workshops and student support.

The author defines the scope of his discussion in his statement The core notion in this book is that an inter-locked triad of user organization, thesauri builder and computer technologist is capable of achieving computer-aided information retrieval systems which make actual a great deal of the potential of automated search p. 169
Wessel offers theoretical and practical grounds for choosing a methodology centered on people supported by computers rather than a fully automated approach.
In particular he advocates development and use of well structured thesauri stored in computers to assist in both indexing and search of documents.

Based on insights from research in information systems, information science, business strategy and organization science, this paper develops the bases for advancing the paradigm of AI and expert systems technologies to account for two related issues a) dynamic radical discontinuous change impacting organizational performance; and (b) human sense-making processes that can complement the machine learning capabilities for designing and implementing more effective knowledge management systems.
q 2001 Elsevier Science Ltd.
All rights reserved.

Opinion mining on Twitter recently attracted research interest in politics using Information Retrieval (IR) and Natural Language Processing (NLP
However, getting domain-specific annotated data still remains a costly manual step.
In addition, the amount and quality of these annotation may be critical regarding the performance of machine learning (ML) based systems.
An alternative solution is to use cross-language and cross-domain sets to simulate training data.
This paper describe a ML approach to automatically annotate Spanish tweets dealing with the online-reputation of politicians.
Our main finding is that a simple statistical NLP classifier without in-domain training can provide as reliable annotation as humans annotators and outperform more specific resources such as lexicon or in-domain data.

In this contribution we investigate the use of description logics (DLs) for information retrieval in a multiagent scenario.
We first describe two advanced DLs and present the relevant reasoning services provided for information retrieval, in particular instance retrieval, instance checking and example-based instance retrieval.
Complete and sound algorithms exist for each of these tasks in both DLs, but it is shown that a combined DL is undecidable.
In order to make use of knowledge bases which use different DLs, a broker-based multiagent information retrieval scheme is presented.
The main idea is to pose transformed queries to individual agents and combine the answers to obtain a correct but not necessarily complete result.
The approach is illustrated with detailed examples.

II CONTENTS III ACKNOWLEDGMENTS V

The syntactic information latent in any coherent text can be exploited to overcome some inadequacies of keyword-based retrieval and make information retrieval more eeective.
We have earlier quantitatively demonstrated how syntactic information is useful in ltering out irrelevant documents.
We have implemented a system which exploits a rich syntactic representation of supertags in a exible manner to lter documents for relevance.
The system has been tested on a large collection of newswire sentences, and achieves recall and precision gures of 88% and 97% for ltering out irrelevant documents.
Its performance and modularity makes it a promising postprocessing addition to any Information Retrieval system.
In this paper we examine how the performance of this system is aaected by varying the context provided to the system and show that the experimentally determined optimal size of the context validates linguistic intuitions .

Forecast is very complex due to many relative factors, and new forecast methods and methodologies have attracted more and more scholars from information science fields.
This paper presents a new interactive human-machine integrated forecast system with PSVF.
First, the framework for integrated forecast system with PSVF is developed base on the thoughts of system science.
Then, some key modules of this integrated forecast system, such as Preprocessing information&#40;P&#41 Selecting forecast method and modeling&#40;S&#41 Verifying forecasting precision&#40;V&#41; and Feeding back forecasting model&#40;F&#41 are expatiated in detail.
Finally, its validity is demonstrated with a case to forecast China's possession of private passenger vehicles.

This paper addresses the integration of XML tags into a term-weighting function for focused XML information retrieval (IR Our model allows us to consider a certain kind of structural information: tags that represent a logical structure (e.g title, section, paragraph, etc as well as other tags (e.g bold, italic, center, etc We take into account the influence of a tag by estimating the probability for this tag to distinguish relevant terms from the others.
Then, these weights are integrated in a term-weighting function.
Experiments on a large collection from the INEX 2008 XML IR evaluation campaign showed improvements on focused XML retrieval.

Video summarization, which has a tremendous usage area that spreads from information retrieval to data compression, plays a crucial role in the multimedia understanding.
In recent years, with the explosion of the number of videos and their area of use, video summarization became a must to signify.
Therefore, this work introduces a novel approach for the summarization problem which is based on human movement understanding.
Proposed system presents efficient video knowledge extraction, especially for surveillance cases.
Human centric videos are analyzed with histogram of oriented gradients as feature extractor and optical flow as motion descriptor.
Above these, a template matching algorithm implemented in a shrinkable and stretchable manner to search for periodicity and thereby extract patterns.
Summarization is built up on the validation of these extracted patterns with a correlation based search-through subsystem.

The 6th issue of the Dutch-Belgian Information Retrieval workshop took place at March 13 and 14, 2006 and was hosted by TNO Information and Communication Technology in Delft, The Netherlands.
The primary aim of the DIR workshops is to provide an international meeting place where researchers from the domain of information retrieval and related disciplines, can exchange information and present new research developments.
This year, there was a special focus on contributions focusing on domain-specific retrieval tasks.

The Informer in collaboration with Springer-Verlag are pleased to announce the launch of the third annual competition for the best student paper in Information Retrieval.
This is an open competition for any student in a European academic institution who has published a paper in a refereed journal/conference/workshop in the period 1st November 1998 1st
November 1999.
Springer-Verlag have kindly donated a prize of £100 worth of Springer-Verlag books (full catalogue at <http www.springer.de/product/index.html

It this section, we provide qualitative examples and plots for the experiment “2D-3D
Matching as an Object Detector Sect.
6.2 in the main paper
To recapitulate, we run our ensemble of NZ-WHO templates on the 3DObject Classes dataset [2 without the finetuning stage.
1 gives the corresponding detection average precision, average viewpoint precision, viewpoint confusion matrix and mean precision in pose estimation results.
Specifically, we followed the detection and viewpoint estimation criteria of [4] where a detection is correct iff intersection over union is at least 0.5 and viewpoint estimation is correct iff detection is correct and azimuth of the viewpoint prediction falls into the correct viewpoint bin.
2 shows successful detection and viewpoint estimation results for car, Fig. 3 for bicycle.
5 show failure cases, which are mostly due to confused front and back views for cars, and slanted bicycle poses.

Text mining is one of the most important tools in Information Retrieval.
Text clustering is the process of classifying documents into predefined categories according to their content.
Existing supervised learning algorithms to automatically classify text requires sufficient documentation to learn exactly.
In this paper, Niching memetic algorithm and Genetic algorithm (GA) is presented in which feature selection an integral part of the global clustering search procedure that attempts to overcome the problem of finding optimal solutions at the local less promising in both clustering and feature selection.
The concept of confusion matrix is then used for derivative works, and finally, hybrid GA is included for the final classification.
Experimental results show benefits by using the proposed method which evaluates F-measure, purity and results better performance in terms of False positive, False negative, True positive and True negative.

Although information retrieval research has always been concerned with improving the effectiveness of search, in some applications, such as information analysis, a more specific requirement exists for <i>high accuracy</i> retrieval.
This means that achieving high precision in the top document ranks is paramount.
In this paper we present work aimed at achieving high accuracy in ad-hoc document retrieval by incorporating approaches from question answering(QA We focus on getting the first relevant result as high as possible in the ranked list and argue that traditional precision and recall are not appropriate measures for evaluatin this task.
We instead use the mean reciprocal rank(MRR) of the first relevant result.
We evaluate three different methods for modifying queries to achieve high accuracy.
The experiments done on TREC data provide support for the approach of using MRR and incorporating QA techniques for getting high accuracy in ad-hoc retrieval task.

Formulating unambiguous queries in the Semantic Web applications is a challenging task for users.
This paper presents a new approach in guiding users to generate clear requests based on their common nature of querying for information.
The approach known as the “front-end approach” gives users an overview about the system data through a “virtual data component” which stores the extracted metadata of the data storage sources in the form of an ontology.
This approach reduces the ambiguities in users’ requests at very early stage; and allows the query refinement process to easily to fulfill users’s demands.
Furthermore, the approach provide a powerful query engine, called “context-based querying that recommends the appropriate query patterns according to the user’s querying context.
These features help the user in generating clear query more easier.

In this paper, we explore the usage of Word Embedding semantic resources for Information Retrieval (IR) task.
This embedding, produced by a shallow neural network, have been shown to catch semantic similarities between words (Mikolov et al 2013
Hence, our goal is to enhance IR Language Models by addressing the term mismatch problem.
To do so, we applied the model presented in the paper
Integrating and Evaluating Neural Word Embedding in Information Retrieval by Zuccon et al 2015) that proposes to estimate the translation probability of a Translation Language Model using the cosine similarity between Word Embedding.
The results we obtained so far did not show a statistically significant improvement compared to classical Language Model.
Keywords— Information Retrieval, Language Model Word Embedding

As XML documents contain both content and structure information, taking advantage of the document structure in the retrieval process can lead to better identify relevant information units.
In this paper, we describe an Information Retrieval (IR) approach dealing with queries composed of content and structure conditions.
The XFIRM model we propose is designed to be as flexible as possible to process such queries.
It is based on a complete query language, derived from XPath and on a relevance values propagation method.
This paper aims at evaluating functions used in the propagation process, and particularly the use of distance between nodes as a parameter.
The proposed method is evaluated thanks to the INEX evaluation initiative.
Results show a relative high precision of our proposal.

In the proposed article a new, ontology-based approach to information retrieval (IR) is presented.
The system is based on a domain knowledge representation schema in form of ontology.
New resources registered within the system are linked to concepts from this ontology.
In such a way resources may be retrieved based on the associations and not only based on partial or exact term matching as the use of vector model presumes
In order to evaluate the quality of this retrieval mechanism, experiments to measure retrieval efficiency have been performed with well-known Cystic Fibrosis collection of medical scientific papers.
The ontology-based retrieval mechanism has been compared with traditional full text search based on vector IR model as well as with the Latent Semantic Indexing method..

White-light interferometry can be used for solving difficult problems in the field of information science.
We give an overview of the method and show examples from ultrafast computing, NP-problems, measurement systems and secure key distribution.

The Australian e-Health Research Centre and Queensland University of Technology recently participated in the TREC 2011 Medical Records Track.
This paper reports on our methods, results and experience using a concept-based information retrieval approach.
Our concept-based approach is intended to overcome specific challenges we identify in searching medical records.
Queries and documents are transformed from their term-based originals into medical concepts as defined by the SNOMED-CT ontology.
Results show our concept-based approach performed above the median in all three performance metrics: bref 12 R-prec 18 and Prec@10 6

We have built a hybrid Case-Based Reasoning (CBR) and Information Retrieval (IR) system that generates a query to the IR system by using information derived from CBR analysis of a problem situation.
The query is automatically formed by submitting in text form a set of highly relevant cases, based on a CBR analysis, to a modified version of INQUERY’s relevance feedback module.
This approach extends the reach of CBR, for retrieval purposes, to much larger corpora and injects knowledge-based techniques into traditional IR.

In recent years, researchers have investigated search result diversification through a variety of approaches.
In such situations, information retrieval systems need to consider both aspects of relevance and diversity for those retrieved documents.
On the other hand, previous research has demonstrated that data fusion is useful for improving performance when we are only concerned with relevance.
However, it is not clear if it helps when both relevance and diversity are both taken into consideration.
In this short paper, we propose a few data fusion methods to try to improve performance when both relevance and diversity are concerned.
Experiments are carried out with 3 groups of top-ranked results submitted to the TREC web diversity task.
We find that data fusion is still a useful approach to performance improvement for diversity as for relevance previously.

I don't have one now
but I'll get one for you.
Adjacency and full-sentence are perhaps the most useful forms of proximity operations.
However, we have found that "adjacent" words are not infrequently separated by "common" words and/or modifiers, e.g the phrases "interactive systems for information retrieval applications interactive retrieval systems and "interactive information retrieval systems At a session with the JURIS system, the queries in the following table were made against the Federal law data base.
Notice that, if column C is the desired result, there is a considerable improvement in

Position Paper Getting it right the first time: Verification of Behavior-based Multirobot Missions Damian Lyons, Ronald Arkin, Shu Jiang, Dagan Harrington and Matthew O'Brien Dept.
Of Computer Information Science, Fordham University, Bronx NY 10458 School of Interactive Computing, Georgia Institute of Technology, Atlanta GA 30332 {dlyons,dharrington5}@fordham.edu arkin, sjiang, mjobrien}@cc.gatech.edu

Internet is one of the main sources of information for millions of people.
One can find information related to practically all matters on internet.
Moreover if we want to retrieve information about some particular topic we may find thousands of Web Pages related to that topic.
But our main concern is to find relevant Web Pages from among that collection.
So in this paper I have discussed that how information is retrieved from the web and the efforts required for retrieving this information in terms of system and users efforts.

Information retrieval test collections are typically built using data from large-scale evaluations in international forums such as TREC, CLEF, and NTCIR.
Previous validation studies on pool-based test collections for <
ad hoc</i> retrieval have examined their reusability to accurately assess the effectiveness of systems that did not participate in the original evaluation.
To our knowledge, the reusability of test collections derived from "living labs" evaluations, based on logs of user activity, has not been explored.
In this paper, we performed a "leave-one-out" analysis of human judgment data derived from the TREC 2016 Real-Time Summarization Track and show that those judgments do not appear to be reusable.
While this finding is limited to one specific evaluation, it does call into question the reusability of test collections built from living labs in general, and at the very least suggests the need for additional work in validating such experimental instruments.

In practice we are often faced with random experiments whose outcomes are not numbers (or vectors in &I but are expressed in inexact linguistic terms.
As an example, consider a group of individuals chosen at random who are questioned about the weather on a particular city on a particular winter day.
Some possible answers would be “cold more or less cold very cold extremely cold and so on.
A natural question which arises with reference to this example is: What is the average opinion about the weather in that particular city?
A possible way of handling situations like this is by using the concepts of fuzzy sets and fuzzy functions [24] found useful in many applications, notably in pattern recognition, clustering, information retrieval, and systems analysis (
cf 161 Motivated by examples of the type given above and related problems (especially concerning group opinions we introduce fuzzy random variables and their expectations, and we investigate some of their properties.

A linguistic model of the query subsystem of an information retrieval systems based on the concept of linguistic variables 12] is presented.
Queries are weighted by means of the linguistic weights expressing a semantic of importance.
A weighted query evaluation mechanism based on the Linguistic Weighted Disjunction operator and the Linguistic Weighted Conjunction operator 7] is given.

It is a well known observation for about last two decades that when exponential growth in storage and processing capacities were combined with rapid developments in computer network technologies, many applications which were impossible to envision before have been either flourished or conceived.
Of them, IR (Information Retrieval) techniques for Turkish have become critical since, until second half of 90's, there were no systematic researches on retrieval systems and tools especially focusing on electronic resources.
As part of such a systematic research movement, we can list works on metadata [1 stemming [2,3 search engines [4 and statistical nature of Turkish language, such as zipf law and vocabulary growth rate [5].

This article brings an interesting comparison of two different methods, which were implemented on GPU and help us to detect system intrusions.
Generally, both of them can be widely used in the area of information retrieval.
The modern trends of parallel computation have a significant influence on performance of implemented methods (Non-negative Matrix Factorization (NMF) and Self-Organizing Maps (SOM Both methods were compared on real data.

Similarity measures have been used widely in information retrieval research.
Most research has been done on query-document or document-document similarity without much attention to the user's perception of similarity in the context of the information need.
In this study, we collect user preference judgements of web document similarity in order to investigate 1) the correlation between similarity measures and users' perception of similarity 2) the correlation between the web document features plus document-query features and users' similarity judgements.
We analyze the performance of various similarity methods at predicting user preferences, in both unsupervised and supervised settings.
We show that a supervised approach using many features is able to predict user preferences close to the level of agreement between users, and moreover achieve a 15% improvement in AUC over an unsupervised approach.

The study on XML keyword search gradually becomes the focus of information retrieval.
Most previous XML keyword search algorithms are based on SLCA (Smallest Lowest Common Ancestor but in the process of keyword search, we discover that some weakness or flaw exists in SLCA, it is summarized as follows 1)
the query result is absolutely accurate but it is meaningless 2)
The return information can&#x02019;t satisfy user&#x02019;s search needs 3)
A huge number of trivial results are returned, make user difficult to distinguish them.
In order to solve the problems given above, we propose a simplified algorithm for meaningful SLCA, experiment result shows that the algorithm is efficient, and it achieves a good balance in the precision and the recall.

Over the past decades, significant progress has been made in Information Retrieval (IR ranging from efficiency and scalability to theoretical modeling and evaluation.
However, many grand challenges remain.
Recently, more and more attention has been paid to the research in domain specific IR applications, as evidenced by the organization of Genomics and Legal tracks in the Text REtrieval Conference
Now it is the right time to carry out large scale evaluations on chemical datasets in order to promote the research in chemical IR in general and chemical Patent IR in particular.
Accordingly, we organize a chemical IR track in TREC (TREC-CHEM) in order to address the challenges in chemical and patent IR.
This paper describes these challenges and the accomplishments of the first year and opens up the discussions for the next year.

The Reliable Information Access (RIA)
Workshop was held in the summer of 2003, with a goal of improved understanding of information retrieval systems, in particular with regard to the variability of retrieval performance across topics.
The workshop ran massive cross-system failure analysis on 45 of the TREC topics and also performed cross-system experiments on pseudo-relevance feedback.
This paper presents an overview of that workshop, along with some preliminary conclusions from these experiments.
Even if this workshop was held 6 years ago, the issues of improving system performance across all topics is still critical to the field and this paper, along with the others in this issue, are the first widely published full papers for the workshop.

Nowadays the utility of domain ontologies is widely acknowledged in many area, such as information systems¿software engineer¿natrue language processing¿artificial intelligence¿electronic commerce and so on.
Ontologies are enable to fulfill knowledge appearance, information retrieval and search.
However, there still exists several drawbacks that must be resolved before ontologies become practical and useful tools.
A critical issue is the ontology mapping.
This matching process will directly influence the precision and recall of information retrieval among several different ontologies.
In this paper, we propose an approach of ontology matching and an ontology matching algorithm based on description logic system through analyzing the characteristic of description logics, including definitions of syntax, semantics and basic reasoning services.
and finally demonstrate the practicality of this ontology matching method between two concrete Book-System ontologies.

With the explosive growth of the amount of digital music available on the Internet, Musical Information Retrieval (MIR) has become a topic that has attracted the attention of researchers in a wide range of disciplines.
The quality of a content-based retrieval system depends heavily on how well the individual components for representation and matching of the data perform.
Most existing commercial music databases use text as the main supplier of meta-data of music, such as the name of the artist/performer and the title of the song.
For text, rapid matching methods are available and are applied extensively in search engines on the World Wide Web.
However, once such meta-data is incomplete or not available, all of the existing commercial systems will fail to deliver.

Is it possible to discover semantic term relations useful for thesauri without any semantic information?
Yes, it is.
A recent approach for automatic thesaurus construction is based on explicit linguistic knowledge i.e. a domain independent parser without any semantic component, and implicit linguistic knowledge contained in large amounts of real world texts.
Such texts include implicitly the linguistic, especially semantic, knowledge that the authors needed for formulating their texts.
This article explains how implicit semantic knowledge can be transformed to an explicit one.
Evaluations of quality and performance of the approach are very encouraging.

This paper describes an approach for Mixed-script Ad hoc retrieval, a subtask as part of FIRE 2015 Shared Task on Mixed Script Information Retrieval.
We participated in subtask 2 of the shared task, where a statistical model was used to carry out back transliteration to Devanagari script.
To perform the search, bigram based index of the documents were used and search was performed using pivot terms in the query.

In this paper we introduce evaluation results of Cross-language information retrieval for two small languages, Finnish and Swedish.
Our approach is based on machine translation of topics and usage of the Frequent Case Generation method for management of query term variation in translated topics.
Retrieval results of more standard query term variation management approaches, such as stemming and lemmatization of translated topics, are also shown.

In the proposed PhD thesis, it will be examined how attention data from the user, especially generated by an eye tracker, can be exploited in order to enhance and personalize information retrieval methods.

This work improves a novel Service Selection Method for the development of Service-Oriented Applications in the context of the Service-Oriented Computing (SOC) paradigm.
We have defined a Semantic-Structural Scheme to assess Web Services on Interface Compatibility exploring the available information from WSDL documents.
The structural information involves data types from return, parameters and exceptions.
The semantic information concerns identifiers from parameters and operation names.
The lexical database WordNet is used as a semantic basis.
Two appraisal values were defined: compatibility gap and adaptability gap.
The former is centered on functional aspects.
The latter explains the adaptation effort to a successful integration.
We validated those appraisals values through different experiments with a data-set of 465 real-life Web Services and measured the results using three metrics from the Information Retrieval field.

rdered, labeled trees are trees in which each node has a label and the left-to-right order of its children (if it has any) is fixed.
Although similarity search on textual data has been extensively studied, searching for similar trees is still an open problem due to the high complexity of computing the similarity between trees, especially for large numbers of tress Tree-structured data are becoming ubiquitous nowadays and manipulating them based on similarity is essential for many applications.
A weighted tree similarity algorithm has been developed earlier which combines matching and missing values between two taxonomy trees.
It is shown in this paper that this algorithm has some limitations when the same sub-tree appears at different positions in a pair of trees.
In this paper, we introduce a generalized formula to combine matching and missing values.
Subsequently, two generalized weighted tree similarity algorithms are proposed.

Biology is rapidly turning into an information science, thanks to enormous advances in the ability to observe the molecular properties of cells, organs and individuals.
This wealth of data allows us to model molecular systems at an unprecedented level of detail and to start to understand the underlying biological mechanisms.
This field of systems biology creates a huge need for methods from machine learning, which find statistical dependencies and patterns in these large-scale datasets and that use them to establish models of complex molecular systems.
MLSB is a scientific forum for the exchange between researchers from Systems Biology and Machine Learning, to promote the exchange of ideas, interactions and collaborations between these communities.

Nearest neighbor search methods based on hashing have attracted considerable attention for effective and efficient large-scale similarity search in computer vision and information retrieval community.
In this paper, we study the problems of learning hash functions in the context of multimodal data for cross-view similarity search.
We put forward a novel hashing method, which is referred to Collective Matrix Factorization Hashing (CMFH CMFH learns unified hash codes by collective matrix factorization with latent factor model from different modalities of one instance, which can not only supports cross-view search but also increases the search accuracy by merging multiple view information sources.
We also prove that CMFH, a similarity-preserving hashing learning method, has upper and lower boundaries.
Extensive experiments verify that CMFH significantly outperforms several state-of-the-art methods on three different datasets.

There have been significant advances in Cross-Language Information Retrieval (CLIR) in recent years.
One of the major remaining reasons that CLIR does not perform as well as monolingual retrieval is the presence of out of vocabulary (OOV) terms.
Previous work has either relied on manual intervention or has only been partially successful in solving this problem.
We use a method that extends earlier work in this area by augmenting this with statistical analysis, and corpus-based translation disambiguation to dynamically discover translations of OOV terms.
The method can be applied to both Chinese-English and English-Chinese CLIR, correctly extracting translations of OOV terms from the Web automatically, and thus is a significant improvement on earlier work.

This report describes the methods that our Information Retrieval Group at Purdue University used for the TREC Microblog 2011 track.
The first method is the pseudo-relevance feedback, a traditional algorithm to reformulate the query by adding expanded terms to the query.
The second method is the affinity propagation, a non parametric clustering algorithm that can group the top tweets according to their similarities.
The final score of a tweet is based on its relevance score and the relevance score of its representative in the group.
We found that query expansion is a very useful technique for microblog retrieval, while affinity propagation could achieve a comparable performance when combining with other techniques.

This paper discusses possibilities of extending functions in information retrieval (IR) systems.
The standard usage of IR systems (described for example in [Po98 concentrates on obtaining some information such as papers, books etc or descriptions how and where it is possible to obtain these desired documents.
Users often do not need complete informatio n, but they want to know where to find it.
In IR systems we usually do not physically manipulate with the actual documents, but we take advantage of knowledge about them.
This knowledge (identification and kind of document, main keywords characterising the document, etc is called metadata (or secondary information
In [Fr92 Po92 Po98 several models and query languages are described, which provide for users methods, how to obtain some information with usage specific secondary information in a very large collection of documents.
However, the possibility of manipulation with documents (structuring, sorting etc is usually suppressed.

This paper discusses the combination of context injection, metadata enrichment and information broking with current semantic web and community oriented concepts in order to optimize the search and retrieval process of relevant information.
The presented approach focuses the shift from information pull to information push, and thus reduces the effort on finding the needed information for a specific issue in a concrete context.

In this paper, we propose an agent-centric approach to resource description and selection in a multiagent information retrieval (IR
In the multiagent system, each agent learns from its experience through its interactions with other agents their capabilities and qualifications.
Based on a distributed ontology learning framework, our methodology allows an agent to profile other agents in a dynamic translation table and a neighborhood profile, which together help determine resource description and selection process.
Further, we report on the experiments and results of the first phase of our research, which focuses on the operational issues (e.g real-time constraints, frequency of queries, number of threads, narrowness in ontology) on how the agents handle queries collaboratively.

Music Information Retrieval has received increasing attention from both the industrial and the research communities in recent years.
Many audio extraction techniques providing content-based music information have been developed, sparking the need for intelligent storage and retrieval facilities.
This paper proposes to satisfy this need by extending technology from business-oriented data warehouses to so-called music warehouses that integrate a large variety of music-related information, including both low-level features and high-level musical information.
Music warehouses thus help to close the “semantic gap” by supporting integrated querying of these two kinds of music data.
This paper presents a number of new challenges for the database community that must be taken up to meet the particular demands of music warehouses.

Information retrieval is the process of evaluating a user's query, or information need, against a set of documents (books, journal articles, web pages, etc to determine which of the documents satisses the query.
With the advent of the World Wide Web, there is suddenly a need to query enormous sets of documents both eeciently and accurately.
In the vector space model of information retrieval, documents are represented by sparse vectors each component of which corresponds to a term, usually a word, in the documents set.
In the simplest case, the components of these vectors are the raw frequency counts of each term in each document.
More sophisticated term weighting schemes are used to improve information retrieval accuracy.
We study a speciic term weighting scheme (log-entropy weighting) to determine its eeectiveness on diierent aspects of retrieval.
New approaches to term weighting are also examined.
In addition, we describe our workshop experience and some of our technical work.

Using content-specific models to guide information retrieval can provide richer interfaces to end-users in both navigating news articles and learning the context of news events.
We present Brussell, a system that uses semantic models of news event situations to perform anticipatory information retrieval, organize extraction results and present a novel interface for navigating among the milestone events of a situation.

Mind maps are used by millions of people.
In this paper we present how information retrieval on mind maps could be used to enhance expert search, document summarization, keyword based search engines, document recommender systems and determining word relatedness.
For instance, words in a mind map could be used for creating a skill profile of the mind maps’ author and hence enhance expert search.
This paper is a research-in-progress paper which means no research results are presented but only ideas.
Keywords-data mining, information retrieval, mind maps, expert seach, document clustering, document classification

Taylor Francis makes every effort to ensure the accuracy of all the information (the “Content contained in the publications on our platform.
However, Taylor Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content.
Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor Francis.
The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information.
Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.

Despite their predominant application in robotics, the utility of methods for information fusion is not limited to sensor-based fusion tasks.
The paper presents an information retrieval (IR) system for multimedia weather documents which makes use of linguistic fusion methods and a semantically rich retrieval model based on methods from fuzzy set theory.
The computational problem of how to efficiently organize the query evaluation process is solved by object-based mediation and asynchronous parallel invocations both of the document evaluation and fusion methods.

Determining the material category of a surface from an image is a demanding task in perception that is drawing increasing attention.
Following the recent remarkable results achieved for image classification and object detection utilising Convolutional Neural Networks (CNNs we empirically study material classification of everyday objects employing these techniques.
More specifically, we conduct a rigorous evaluation of how state-of-the art CNN architectures compare on a common ground over widely used material databases.
Experimental results on three challenging material databases show that the best performing CNN architectures can achieve up to 94.99% mean average precision when classifying materials.

In Private Information Retrieval (PIR one wants to download a file from a database without revealing to the database which file is being downloaded.
Much attention has been paid to the case of the database being encoded across several servers, subsets of which can collude to attempt to deduce the requested file.
With the goal of studying the achievable PIR rates in realistic scenarios, we generalize results for coded data from the case of all subsets of servers of size t colluding, to arbitrary subsets of the servers.
We investigate the effectiveness of previous strategies in this new scenario, and present new results in the case where the servers are partitioned into disjoint colluding groups.

We present a hybrid method to turn off-the-shelf information retrieval (IR) systems into future event predictors.
Given a query, a time series model is trained on the publication dates of the retrieved documents to capture trends and periodicity of the associated events.
The periodicity of historic data is used to estimate a probabilistic model to predict future bursts.
Finally, a hybrid model is obtained by intertwining the probabilistic and the time-series model.
Our empirical results on the New York Times corpus show that autocorrelation functions of time-series suffice to classify queries accurately and that our hybrid models lead to more accurate future event predictions than baseline competitors.

We describe our progress extending the undergraduate Computer Science (CS) curriculum to include a deep understanding of information retrieval (IR) and data mining (DM Instead of simply understanding how to build applications using tools involving IR and DM, students build these tools and learn the relevant algorithms implemented in these tools.
Some novel approaches exist in our work.
We include a hands-on lab setting where students use the tools they have built to perform experiments that could ultimately extend the field.
Hence, undergraduates have firsthand knowledge of performing research in Computer Science using a scientific method.
Secondly, we have a rigorous set of evaluation criteria developed by our Psychology department that will evaluate how well students learn using our novel approaches.
Ultimately, we believe these two courses warrant consideration into standards developed for the undergraduate CS curriculum.

The gathering of information, exploration of information resources etc. is a longstanding problem and a well known agent task (from personal search agents [4] over distributed information retrieval systems [5] to autonomous agents exploring dynamic document networks [3 An information retrieving agent not only faces the problem of acquiring information but also of selecting the interesting documents.

Histogram construction or sequence segmentation is a basic task with applications in database systems, information retrieval, and knowledge management.
Its aim is to approximate a sequence by line segments.
Unfortunately, the quadratic algorithm that derives an optimal histogram for Euclidean error lacks the desired scalability.
Therefore, sophisticated approximation algorithms have been recently proposed, while several simple heuristics are used in practice.
Still, these solutions fail to resolve the efficiency-quality tradeoff in a satisfactory manner.
In this paper we take a fresh view on the problem.
We propose conceptually clear and scalable algorithms that efficiently derive high-quality histograms.
We experimentally demonstrate that existing approximation schemes fail to deliver the desired efficiency and conventional heuristics do not fare well on the side of quality.
On the other hand, our schemes match or exceed the quality of the former and the efficiency of the latter.

Thomas Kuhn’s ideas, particularly of paradigm, are used with some frequency in information science.
The usages of paradigm and the problematic nature of Kuhn’s thought are explored.
Alternatives to Kuhn are suggested as a way out of the confusion his thought leads to.
Résumé Les idées de Thomas Kuhn, plus particulièrement le paradigme, sont utilisées assez fréquemment en science de l’information.
Les utilisations
du paradigme et la nature problématique de la pensée de Kuhn sont explorées.
Afin de contrer la confusion entourant la pensée de Kuhn, des alternatives sont proposées.

Time is an important dimension of any information space.
It can be very useful for a wide range of information retrieval tasks such as document exploration, similarity search, summarization, and clustering.
Traditionally, information retrieval applications do not take full advantage of all the temporal information embedded in documents to provide alternative search features and user experience.
However, in the last few years there has been exciting work on analyzing and exploiting temporal information for the presentation, organization, and in particular the exploration of search results.
In this paper, we review the current research trends and present a number of interesting applications along with open problems.
The goal is to discuss interesting areas and future work for this exciting field of information management.

The novel RTPD/MACSC model proposed in this paper tunes the cache size adoptively on the fly.
It combines the extant MACSC (model for adaptive cache size control) and the novel RTPD (real-time traffic pattern detection) capability.
Similar to its MACSC predecessor, the new RTPD/MACSC tuner maintains the given cache hit ratio consistently and persistently.
It, however, differs from its predecessor because it adapts itself to compensate for the ill effects on the cache hit ratio by the Internet traffic, which embeds various patterns over time.
The automatic adaptation (self-tuning) capability in response to traffic pattern changes detected by the RTPD is called self-reconfiguration in the RTPD/MACSC context

Based on a new framework for capturing dynamic areas of interest in eye-tracking, we model the user search process as a Markov-chain.
The analysis indicates possible system improvements and yields parameter estimates for the Interactive Probability Ranking Principle (IPRP).

We propose a new approach to querying hypermedia documents on the Web based on information retrieval (IR browsing, and database techniques so as to provide maximum exibility to the user.
We present a model based on object representation where an identity does not correspond to a source HTML page but to a fragment of it.
A fragment is identiied using the explicit structure provided by the HTML tags as well as the implicit structure extracted using IR techniques.
Our fragmentation provides access to diierent heterogeneous components (text, image, audio, video, etc of a given document, and to their relationships (implicit or explicit through hyperlinks Our language expresses browsing and restructuring based on IR techniques in a uniied framework.
All these are integral components of the AKIRA system, currently under development.

Australian Business Deans Council (ABDC Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; Gale Directory of Publications Broadcast Media; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Thomson Reuters; Ulrich’s Periodicals Directory;
Web of Science Research Articles

This paper is devoted to the interactive visualization of search results obtained by the search engine using the concept lattices.
We provide a tool in which the process is realized from the query input, the creation of the concept lattice and then its visualization.
The concept lattices are created using Formal Concept Analysis which hierarchically organizes the results in the form of clusters of particular objects composed of the documents with the shared attributes.
The resulted concept lattice is able to provide a structured view on the domain of query to the user.
This work uses Generalized One-Sided Concept Lattice (GOSCL) for building a hierarchy of concepts.
This model is able to create concept lattice from input data tables that contain different types of attributes representing fuzzy sets.
Thus, the concept lattice is then shown as an interactive graph on which reductions can be applied, increasing the clarity of the output visualization.

This paper describes issues surrounding the planning and design of GermanFrameNet (GFN a counterpart to the English-based FrameNet project.
The goals of GFN are (a) to create lexical entries for German nouns, verbs, and adjectives that correspond to existing FrameNet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms.
GFN will take a fine-grained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis.
The parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation.

This paper proposes a new approach for Indonesian medicinal plant image retrieval by combining leaf image and text features.
Fuzzy Local Binary Patterns were used to extract texture features based on leaf image of medicinal plants.
To improve the image similarity, we proposed Probabiistic Neural Network to calculate the weight of image features.
The text features were extracted from medicinal plant documents in Indonesian language.
Experiments result show that combining image and texture features in medicinal plant image retrieval improves the performance.
The Average Precision (AVP) has increased from 0.3138 to 0.7081.

Due to the increasing number of available biomedical data repositories, providing a comprehensive and intuitive access to information is still a demanding task for Information Retrieval systems.
In this work we present an interactive data exploration system that retrieves relevant information by propagating the user's interest within a network.
The developed techniques have been applied to two different retrieval tasks useful for biomedical research: the prioritization of proteins related to a disease of interest and the search of publications in the literature.
The method relies on a network of biomedical entities, scoring of entities of interest by the user, and score propagation.
The assessment of the relevance of the retrieved information confirmed a high accuracy of the presented algorithms for both the domains considered.

Ranking is the key problem for information retrieval and other text applications.
Recently, the ranking methods based on machine learning approaches, called learning to rank, become the focus for researchers and practitioners.
The main idea of these methods is to apply the various existing and effective algorithms on machine learning to ranking.
However, as a learning problem, ranking is different from other classical ones such as classification and regression.
In this paper, we investigate the important papers in this direction; the cons and pros of the recent-proposed framework and algorithms for ranking are analyzed, and the relationships among them are discussed.
Finally, the promising directions in practice are also pointed out.

With our participation in TREC2004, we test Terrier, a modular and scalable Information Retrieval framework, in three tracks.
For the mixed query task of the Web track, we employ a decision mechanism for selecting appropriate retrieval approaches on a per-query basis.
For the robust track, in order to cope with the poorlyperforming queries, we use two pre-retrieval performance predictors and a weighting function recommender mechanism.
We also test a new training approach for the automatic tuning of the term frequency normalisation parameters.
In the Terabyte track, we employ a distributed version of Terrier and test the effectiveness of techniques, such as using the anchor text, query expansion and selecting an optimal weighting model for each query.
Overall, in all three tracks we participated, Terrier and the tested Divergence From Randomness models were shown to be stable and effective.

CHIH-LUNG LIN HSU-YUNG CHENG, KUO-CHIN FAN, CHUN-WEI LU, CHANG-JUNG JUAN* and CHIH-WEI KUO Department of Electronic Engineering, Hwa Hsia Institute of Technology New Taipei City 23568, Taiwan Institute of Computer Science and Information Engineering National Central University, Chung-Li City 32054, Taiwan Institute of Information Science, Academia Sinica Taipei 11529, Taiwan §
Materials and Electro-Optics Research Division Chung-Shan Institute of Science and Technology Taoyuan County, Taiwan
linclr@gmail.com

Previous applications of the Markov random field model for information retrieval have used manually chosen features.
However, it is often difficult or impossible to know i>a priori</i
the best set of features to use for a given task or data set.
Therefore, there is a need to develop automatic feature selection techniques.
In this paper we describe a greedy procedure for automatically selecting features to use within the Markov random field model for information retrieval.
We also propose a novel, robust method for describing classes of textual information retrieval features.
Experimental results, evaluated on standard TREC test collections, show that our feature selection algorithm produces models that are either significantly more effective than, or equally effective as, models with manually selected features, such as those used in the past.

When the disaster occurs, the social network site such as Twitter is increasingly being used for helping direct rescue operations.
This article describes the methods we used in the Fire2017.
We regarded the distinction of need-tweets and availability-tweets as classification tasks, and the logistic regression and Support Vector Machine are used to decide the type of the tweets.
In the need and availability matching, we regard it as an information retrieval task, using the retrieval model to complete the task.

In this paper we will present a MIR (Music Information Retrieval) system using natural language as input for human-oriented queries to large-scale music collections, applicable in web databases, cd-chargers for cars, or mobile services.
The outlined system is a full-fledged architecture combining state-of-the-art approaches from the fields of natural language understanding including phonetic matching, automatic analysis of audio data for the purpose of meta tag construction, content-based classification of audio and music ontologies as a backbone for the representation of musical knowledge.
On top of this architecture different prototypes for industrial applications are described including first results of real-life field tests.
This work has been performed at the German Research Center for AI and the authors spin-off company, the sonicson GmbH.

Private Information Retrieval (PIR despite being well studied, is computationally costly and arduous to scale.
We explore lower-cost relaxations of information-theoretic PIR, based on dummy queries, sparse vectors, and compositions with an anonymity system.
We prove the security of each scheme using a flexible differentially private definition for private queries that can capture notions of imperfect privacy.
We show that basic schemes are weak, but some of them can be made arbitrarily safe by composing them with large anonymity systems.

Extracting valuable information from source code automatically was the subject of many research papers.
Such information can be used for document traceability, concept or feature extraction, etc.
In this paper, we used an Information Retrieval (IR) technique: Latent Semantic Indexing (LSI) for the automatic extraction of source code concepts for the purpose of test cases’ reduction.
We used and updated the open source FLAT Eclipse add on to try several code stemming approaches.
The goal is to check the best approach to extract code concepts that can improve the process of test cases’ selection or reduction.

Automatic data extraction from Web pages is a challenging yet significant problem in the fields of Information Retrieval and Data Mining.
The problem arises particularly on the World-Wide Web, because search engines wrap up the results of user queries on web response pages.
These response pages are often decorated with side bars, branding banners and advertisements.
Automatic data extraction therefore has to deal with extracting relevant data from these pages
Though many automated and manual text analysis solutions to this problem exist, most of them are heavily dependent on the specifics of HTML and they have to be changed according to the changes in markup language.
This paper proposes, a novel and language independent technique to solve the data extraction problem using a combined approach that make use of features of DOM tree and also the visual features of html elements.

Usability is a combination of factors that affect the user’s experience with a system.
These factors include but are not limited to: the ease of learning, the efficiency of use, memorability of a system (e.g. how effective a returning user can perform an old task without re-learning the system error prevention (i.e. how effective a system prevents problems from occurring in the first place and aesthetic and minimalist design.
A problem with many of today’s online systems is that they do no meet the quality requirements of user.
The literatures presented below attempt to examine factors and possibilities that can further improve the quality of MIR systems.

It has been recognized that single words extracted from natural language texts are not always useful for the representation of information content.
Associated or related terms, and complex content identifiers derived from thesauruses and knowledge bases, or constructed by automatic word grouping techniques, have therefore been proposed for text identification purposes.
The area of associative content analysis and information retrieval is reviewed in this study.
The available experimental evidence shows that none of the existing or proposed methodologies are guaranteed to improve retrieval performance in a replicable manner for document collections in different subject areas.
The associative techniques are most valuable for restricted environments covering narrow subject areas, or in iterative search situations where user inputs are available to refine previously available query formulations and search output.

The second SIGIR workshop on neural information retrieval (Neu-IR?17) took place on August 11, 2017, in Tokyo, Japan.
Following the successful 2016 edition, the workshop continued to serve as a forum for academic and industrial researchers to present new work on neural methods for retrieval.
In addition, a special track was organized focusing on resources for evaluation and reproducibility, including proposals for public benchmarking datasets and shared model repositories.
A total of 19 papers?which included five special track papers? were presented in the form of oral or poster presentations.
Organizers of four of the TREC 2017 tracks were invited to present at the workshop on how these IR tasks may be suitable for evaluating recent data-hungry neural approaches.
The full-day workshop?with more than 170 registrants?concluded with an engaging panel discussion.

MIREX (MapReduce Information Retrieval Experiments) is a software library initially developed by the Database Group of the University of Twente for running large scale information retrieval experiments on clusters of machines.
MIREX has been tested on web crawls of up to half a billion web pages, totaling about 12.5 TB of data uncompressed.
MIREX shows that the execution of test queries by a brute force linear scan of pages, is a viable alternative to running the test queries on a search engine's inverted index.
MIREX is open source and available for others.

With the exponential growth of documents available to us on the web, the requirement for an effective technique to retrieve the most relevant document matching a given search query has become critical.
The field of Information Retrieval deals with the problem of document similarity to retrieve desired information from a large amount of data.
Various models and similarity measures have been proposed to determine the extent of similarity between two objects.
The objective of this paper is to summarize the entire process, looking into some of the most well-known algorithms and approaches to match a query text against a set of indexed documents.

This review summarizes the main topics covered by the presentations given at the RIAO 2007 Conference.
The theme of this year's conference Large-scale Semantic Access to Content (Text, Image, Video and Sound encompasses two current, and recurring, challenges in Information Retrieval: inclusion of semantics in retrieval and adaptation to different types of media.
The use of the term "large-scale" underlines the need for scalable methods in real-world applications.
RIAO 2007, hosting both innovative applications and research papers, presented a panorama of current trends in semantics based information retrieval.

Conceptual graphs allow for powerful and computationally affordable representation of the semantic contents of natural language texts.
We propose a method of comparison (approximate matching) of conceptual graphs.
The method takes into account synonymy and subtype/supertype relationships between the concepts and relations used in the conceptual graphs, thus allowing for greater flexibility of approximate matching.
The method also allows the user to choose the desirable aspect of similarity in the cases when the two graphs can be generalized in different ways.
The algorithm and examples of its application are presented.
The results are potentially useful in a range of tasks requiring approximate semantic or another structural matching among them, information retrieval and text mining.

While a number of commercial patent retrieval systems and services have long been operated, patent retrieval has not been paid much attention in the information retrieval community.
One of the reasons is the lack of test collection targeting patent information.
Although TREC test collection includes patent documents, the proportion of those documents is significantly small.
Because patent documents are associated with a number of interesting characteristics from a scientific point of view, such as document length, document structures, and classifications, it is important to provide a test collection consisting of patent documents and promote research and development on patent information retrieval.

This paper describes the work done as part of the shared task on Detecting Paraphrases in Indian Languages(DPIL) in Forum for Information Retrieval and Evaluation(FIRE
2016 Paraphrase identification is the task of deciding whether two given text fragments have the same meaning.
Our detection system is for Malayalam language and makes use of the cosine similarity measure, an existing state of the art method for determining the similarity between sentences.
The experiments were done on the standard data set and the results showed that the system was able to give performance comparable to methods employing more sophisticated procedures.

Traditional Content-based Information Retrieval Sys- tems (CBIR) use low level characteristics, this is, primary characteristics such as the color, shape, texture and also in textual attributes related to images.
Although, users make queries based on semantics, which are not representatives just by such low level characteristics.
Recent works on content-based image retrieval have demonstrated that re-
searchers have been trying to map visual low level charac- teristics and high level semantics.
These work have moti- vated this paper which proposes a model for automatic text classification and categorization for image searching by us- ing an self-organizing neural network architecture.
mental results confirm this text-based model is complemen- tary to image-driven techniques such as Retin.

The majority of research into Collaborative Information Retrieval (CIR) has assumed a uniformity of information access and visibility between collaborators.
However in a number of real world scenarios, information access is not uniform between all collaborators in a team e.g. security, health etc.
This can be referred to as Multi-Level Collaborative Information Retrieval (MLCIR To the best of our knowledge, there has not yet been any systematic investigation of the effect of MLCIR on search outcomes.
To address this shortcoming, in this paper, we present the results of a simulated evaluation conducted over 4 different non-uniform information access scenarios and 3 different collaborative search strategies.
Results indicate that there is some tolerance to removing access to the collection and that there may not always be a negative impact on performance.
We also highlight how different access scenarios and search strategies impact on search outcomes.

Oja, M 2007 Methods for exploring genomic data sets: application to human endogenous retroviruses.
Doctoral thesis, Helsinki University of Technology, Dissertations in Computer and Information Science, Report D23, Espoo, Finland.

Traditional tools for information retrieval (IR) evaluation, such as TREC's trec_eval, have outdated command-line interfaces with many unused features, or 'switches accumulated over the years.
They are usually seen as cumbersome applications by new IR researchers, steepening the learning curve.
We introduce a platform-independent application for IR evaluation with a graphical easy-to-use interface: the TREC_Files Evaluator.
The application supports most of the standard measures used for evaluation in TREC, CLEF, and elsewhere, such as MAP, P10, P20, and bpref, as well as the Averaged Normalized Modified Retrieval Rank (ANMRR) proposed by MPEG for image retrieval evaluation.
Additional features include a batch mode and statistical significance testing of the results against a pre-selected baseline.

A new means of evaluating the cluster hypothesis is introduced and the results of such an evaluation are presented for four collections.
The results of retrieval experiments comparing a sequential search, a cluster-based search, and a search of the clustered collection in which individual documents are scored against the query are also presented.
These results indicate that while the absolute performance of a search on a particular collection is dependent on the palrwise similarity of the relevant documents, the relative effectiveness of clustered retrieval versus sequential retrieval is independent of this factor.
However, retrieval of entire clusters in response to a query usually results in a poorer performance than retrieval of individual documents from clusters.

We introduce a mechanism that provides key words which can make human-computer interaction increase in the course of information retrieval, by using natural language processing technology and mathematic measure for calculating degree of inclusion.
We show what type of words should be added to the current query, i.e. keywords which previously had been input, in order to make humancomputer interaction more creative.
We try to extract related word sets from documents by employing casemarking particles derived from syntactic analysis.
Then, we verify which kind of related words is more useful as an additional word for retrieval support.

Towards restructuring Inforrnatiori Science and Technology education and research in the UniversiQ of Tokyo, the Graduate School of Irlforniatiori Science and Technology has been newly established since April 2001.
This article describes this new graduate school to explain our plan for computer science education and research for the Information Technology era.
Establishment of new Graduate School of Information Science and Technology
In April 2001, the University of Tokyo established a new Graduate School of “Information Science and Technology with the goal of improving the foundations and developing new axes for information sciences and technologies in the 21st century.
In this article, based on the information which were originally put on the web of this new graduate school in Japanese, the author tries to explain it in some part, from the personal viewpoint.

When transcribing Broadcast News data in highly inflected languages, the vocabulary growth leads to high out-of-vocabulary rates.
To address this problem, we propose a daily and unsupervised adaptation approach which dynamically adapts the active vocabulary and LM to the topic of the current news segment during a multi-pass speech recognition process.
Based on texts daily available on the Web, a story-based vocabulary is selected using a morpho-syntatic technique.
Using an Information Retrieval engine, relevant documents are extracted from a large corpus to generate a story-based LM.
Experiments were carried out for a European Portuguese BN transcription system.
Preliminary results yield a relative reduction of 65.2% in OOV and 6.6% in WER.

Quantum information science relies on superpositions of quantum states with a definite phase relation, but such superpositions are inherently fragile against interactions with their environment.
Fortunately, if some kind of common property bounds these interactions, it is possible, though by no means easy in a practical setting, to come up with states that are essentially immune to their environment.
In papers appearing in Physical Review A and Physical Review Letters, Magnus Rådmark and Mohamed Bourennane at Stockholm University in Sweden and Marcin Wiśniak and Marek Żukowski at Uniwesytet Gdański in Poland report that they have prepared a six-photon entangled state that is immune to the environmental disturbances commonly occurring in optical fibers [1].

We propose and evaluate a system for content-based visualization and exploration of music collections.
The system is based on a modification of Kohonen’s Self-Organizing Map algorithm and allows users to choose the locations of clusters containing acoustically similar tracks on the music space.
A user study conducted to evaluate the system shows that the possibility of personalizing the music space was perceived as difficult.
Conversely, the user study and objective metrics derived from users’ interactions with the interface demonstrate that the proposed system helped individuals create playlists faster and, under some circumstances, more effectively.
We believe that personalized browsing interfaces are an important area of research in Multimedia Information Retrieval, and both the system and user study contribute to the growing work in this field.

The evaluation of effectiveness in Information Retrieval systems has been developed in parallel to its evolution, generating a great amount of proposals to achieve this process.
This paper focuses on a particular task of Music Information Retrieval: a system for Cover Song Identification.
We present a concrete example and then try to elucidate which metrics work best to evaluate such a system.
We end up with two evaluation measures suitable for this problem: bpref and Normalized Lift Curves.

Thank you very much for downloading new directions in cognitive information retrieval.
Maybe you have knowledge that, people have search numerous times for their favorite readings like this new directions in cognitive information retrieval, but end up in malicious downloads.
Rather than enjoying a good book with a cup of tea in the afternoon, instead they cope with some infectious virus inside their computer.

Information use is an understudied area within information science thus strategies pertinent to using information remains understudied.
However, research implicates strategically using information as a performance booster, especially within academic contexts.
This paper reports on an ongoing research on information use strategies of graduate students as they attend to an identified academic task.
Résumé L'utilisation et les stratégies pertinentes à l'utilisation de l'information demeurent un domaine sous-étudié en science de l'information.
Cependant, la recherche implique l'utilisation stratégique de l'information comme stimulant du rendement, particulièrement en contexte scolaire.
Cette communication porte sur une recherche
en cours sur les stratégies d'utilisation de l'information des étudiants universitaires de 2 e et 3 e cycle lors d'une tâche scolaire prédéfinie.

Music Information Retrieval has received increasing attention from both the industrial and the research communities in recent years.
Many audio extraction techniques providing content-based music information have been developed, sparking the need for intelligent storage and retrieval facilities.
This paper proposes to satisfy this need by extending technology from business-oriented data warehouses to so-called music warehouses that integrate a large variety of music-related information, including both low-level features and high-level musical information.
Music warehouses thus help to close the “semantic gap” by supporting integrated querying of these two kinds of music data.
This paper presents a number of new challenges for the database community that must be taken up to meet the particular demands of music warehouses.

We analyzed transaction logs of a set of 51,473 queries posed by 18,113 users of <i>
i> a major Internet search service.
We provide data on i
queries</b the number of search terms, and the use of logic and modifiers ii b>sessions</b changes in queries during a session, number of pages viewed, and use of relevance feedback, and (iii b>terms</b their rank/frequency distribution and the most highly used search terms.
Common mistakes are also observed.
Implications are discussed.

Tree structures play an important role in computer science.
For instance, the binary tree is a fundamental data structure for rapidly storing sorted data and rapidly retrieving stored data.
In this paper, we establish the structures of k-noncrossing trees and k-proper trees.
Moreover, the relations between these structures and k-ary trees are also constructed.
It exposes that such structures may be employed as efficient data structures for computer and information science.

This paper describes some new ideas on developing a logical algebra for databases that manage textual data and support information retrieval functionality.
We describe a first prototype of such a system.

This paper reviews and compares theories of fuzzy sets and rough sets applying information retrieval.
Vagueness and uncertainty have attracted the attention of philosophers and logicians for many years.
The aim of this paper is to synthetically present the rough set and fuzzy set approach to the modeling of flexibility with respect to vagueness and uncertainty in the specification of users’ information needs.
The two theories model different types of uncertainty .The
rough set theory takes into consideration the indiscernibility between objects; typically characterized by an equivalence relation.
Rough sets are the results of approximating crisp sets using equivalence classes.
The fuzzy set theory deals with the ill-definition of the boundary of a class through a continuous generalization of set characteristic functions.
The indiscernibility between objects is not used in fuzzy set theory.
So the membership relation is best theory for categorization of two theories.

The term relevance weighting method has been shown to produce optimal information retrieval queries under well-defined conditions.
The parameters needed to generate the term relevance factors cannot unfortunately be estimated accurately in practice; futhermore, in realistic test situations, it appears difficult to obtain improved retrieval results using the term relevance weights over much simpler term weighting systems such as, for example, the inverse document frequency weights.
It is shown in this study that the inverse document frequency weights and the term relevance weights are closely related over a wide range of the frequency spectrum.
Methods are introduced for estimating the term relevance weights, and experimental results are given comparing the inverse document frequency with the estimated term relevance weights.

Information retrieval is attracting significant attention due to the exponential growth of the amount of information available in digital format.
The proliferation of information retrieval objects, including algorithms, methods, technologies, and tools, makes it difficult to assess their capabilities and features and to understand the relationships that exist among them.
In addition, the terminology is often confusing and misleading, as different terms are used to denote the same, or similar, tasks.

As opposed to traditional Information Retrieval (IR) which views whole documents as atomic units of retrieval, XML IR processes XML elements as possible units of retrieval.
Many open issues appear when considering Relevance Feedback (RF) in XML documents.
They are mainly related to the form of XML documents that mix content and structure and to the new granularity of information processed by the Information Retrieval Systems (IRS Most of the RF approaches proposed in XML retrieval are simple adaptations of traditional RF to the new granularity of information.
They enrich queries by adding terms extracted from relevant elements instead of terms extracted from whole documents.
In this paper, we propose to extend the initial query by adding both content and structural constraints.
Experiments are carried out with the INEX evaluation campaign and results show the interest of our method.

In this paper, we propose an unsupervised method for discovering inference rules from text, such as “X is author of Y X wrote Y X solved Y X found a solution to Y and “X
caused Y Y is triggered by X Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general.
Our algorithm is based on an extended version of Harris’ Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar.
Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.

We present a novel adaptation technique for search engines to better support information-seeking activities that include both lookup and exploratory tasks.
Building on previous findings, we describe (1) a classifier that recognizes task type (lookup vs. exploratory) as a user is searching and (2) a reinforcement learning based search engine that adapts accordingly the balance of exploration/exploitation in ranking the documents.
This allows supporting both task types surreptitiously without changing the familiar list-based interface.
Search results include more diverse results when users are exploring and more precise results for lookup tasks.
Users found more useful results in exploratory tasks when compared to a base-line system, which is specifically tuned for lookup tasks.

We present an analysis of word senses that provides a fresh insight into the impact of word ambiguity on retrieval effectiveness with potential broader implications for other processes of information retrieval.
Using a methodology of forming artifically ambiguous words, known as pseudowords, and through reference to other researchers' work, the analysis illustrates that the distribution of the frequency of occurrance of the senses of a word plays a strong role in ambiguity's impact of effectiveness.
Further investigation shows that this analysis may also be applicable to other processes of retrieval, such as Cross Language Information Retrieval, query expansion, retrieval of OCR'ed texts, and stemming.
The analysis appears to provide a means of explaining, at least in part, reasons  for the processes' impact (or lack of it) on effectiveness.

Query disambiguation is considered as one of the most important methods in improving the effectiveness of information retrieval.
In the present paper, we focus on query terms disambiguation via, a combined statistical method both before and after translation, in order to avoid source language ambiguity as well as incorrect selection of target translations.
By combining query expansion with dictionary-based translation and statistics-based disambiguation, in order to overcome query terms ambiguity, information retrieval should become much more efficient.
Thus, query expansion techniques through relevance feedback were performed prior to either the first or the second disambiguation processes.
We tested the effectiveness of the proposed combined method, by an application to a French-English Information Retrieval.
Experiments involving TREC data collection revealed the proposed disambiguation and expansion methods to be highly effective.

Information retrieval (IR) systems are traditionally developed using the objective relevance approach based on the “best match” principle assuming that users can specify their needs in queries and that the documents retrieved are relevant to them.
This paper advocates a subjective relevance (SR) approach to value-add objective relevance and address its limitations by considering relevance in terms of users’ needs and contexts.
A pilot study was conducted to elicit features on SR from experts and novices.
Elicited features were then analyzed using characteristics of SR types and stages in information seeking to inform the design of an IR interface supporting SR.
The paper presents initial work towards the design and development of user-centered IR systems that prompt features supporting the four main types of SR.

This paper proposes a method of creating a web document representation using a web ontology concepts instead of ‘bag-ofwords
However, since the web domain has a very small vocabulary, we are unable to transform all or most of the keywords of the web document into web ontology concepts.
This particular problem is solved by creating an extended part of the web ontology with words obtained from an external linguistics knowledgebase.
The promising outcome as the result of Natural Language Processing (NLP) and Information Retrieval (IR) fields being merged together convinces us to create the extended ontology using NLP technique.

The concepts of information increment and demand information value in supply chains are defined, and then the measurement methods of them are established basing on the theory of information science.
The mechanism of demand information influencing the ordering process and the wholesaler's expected benefits is clarified by building a supply chain ordering model.
The relationship between information increment and information value is studied, when assuming the demands are uniformly and normally distributed.
The results of the analysis and the simulation experiment prove that there is a positive correlation between information increment and information value, and the information increment can influence the information user's benefits.
The results suggest that it is necessary to establish an effective information evaluation system for members in supply chains to increase their benefits by improving the quality of decision information.

This study uses a novel simulation framework to evaluate whether the time and effort necessary to achieve high recall using active learning is reduced by presenting the reviewer with isolated sentences, as opposed to full documents, for relevance feedback.
Under the weak assumption that more time and effort is required to review an entire document than a single sentence, simulation results indicate that the use of isolated sentences for relevance feedback can yield comparable accuracy and higher efficiency, relative to the state-of-the-art Baseline Model Implementation (BMI) of the AutoTAR Continuous Active Learning CAL method employed in the TREC 2015 and 2016 Total Recall Track.

Recent developments in Information Retrieval diversity are based on the consideration of a space of information need aspects, a notion which takes different forms in the literature.
The choice of a suitable aspect space for diversification is a critical issue when designing an IR diversification strategy, which has not been explicitly addressed to some depth in the literature.
This paper aims to identify relevant properties of the aspect space which may help the system designer in making a suitable choice in selecting and configuring this space, and diagnosing malfunctions of the diversification algorithms.
In particular, we identify the mutual information between aspects and documents as a meaningful magnitude, in terms of which anomalous cases can be characterized.
We further seek to discern favorable cases through a combination of theoretic and empirical analysis.

This minitrack covers the broad theory and application issues related to data mining, machine learning, knowledge acquisition, knowledge discovery, information retrieval, database, and inductive decision-making.
Both structured and unstructured data repositories including human expert decisions, environmental/normative datasets, large document collections, and web databases are considered.
Theoretical and methodological exploration in the previous years motivates us to further investigate the various and richer data and knowledge representation schemes such as Web, multimedia, and geographic data applied to science as well as management domains.

M. Mohseni, A. T. Rezakhani, and D. A. Lidar Department of Chemistry and Chemical Biology, Harvard University, 12 Oxford Street, Cambridge, Massachusetts 02138, USA Department of Chemistry, University of Southern California, Los Angeles, California 90089, USA Center for Quantum Information Science and Technology, and Departments of Chemistry and Physics, University of Southern California, Los Angeles, California 90089, USA Institute for Quantum Information Science, University of Calgary, Alberta, Canada
T2N 1N4 Departments of Physics and Electrical Engineering, University of Southern California, Los Angeles, California 90089
, USA Received 21 February 2007; published 13 March 2008

Separating singing voice from music accompaniment is very useful in many applications, such as lyrics recognition and alignment, singer identification, and music information retrieval.
Although speech separation has been extensively studied for decades, singing voice separation has been little investigated.
We propose a system to separate singing voice from music accompaniment for monaural recordings.
Our system consists of three stages.
The singing voice detection stage partitions and classifies an input into vocal and nonvocal portions.
For vocal portions, the predominant pitch detection stage detects the pitch of the singing voice and then the separation stage uses the detected pitch to group the time-frequency segments of the singing voice.
Quantitative results show that the system performs the separation task successfully

In this study, we present the analysis of the interconnection network of a distributed Information Retrieval (IR) system, by simulating a switched network versus a shared access network.
The results show that the use of a switched network improves the performance, especially in a replicated system because the switched network prevents the saturation of the network, particularly when using a large number of query servers.

This paper describes a study of Turkish-English cross language information retrieval (CLIR) system.
One of the biggest issues with CLIR studies is to access to bi-lingual parallel corpus.
So, the first step of this study was to construct a parallel Turkish-English corpus.
We have constructed a corpus that has 1801 parallel documents.
The corpus has been divided in to two parts, first one for training the system and second one for testing the system.
Latent semantic indexing (LSI) techniques applied to the training set to obtain the language relations.
After the training, we have performed set of tests (queries) to measure the effectiveness of LSI based retrieval on Turkish-English parallel corpus.
Our experimental results show that, LSI based CLIR outperforms the non-LSI based retrieval where their retrieval successes are %69 and %26 respectively.

In the field of Music Information Retrieval (MIR multi-label genre classification is the problem of assigning one or more genre labels to a music piece.
In this work, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification.
Our goal is to enhance classification performance by combining multiple classifiers.
In addition, we also investigate some existing ensemble techniques from machine learning.
The effectiveness of these techniques is demonstrated through a set of empirical experiments and various related issues are discussed.
To the best of our knowledge, there has been limited work on applying ensemble techniques to multi-label genre classification in the literature and we consider the results in this work as our initial efforts toward this end.
The significance of our work has two folds 1) proposing a set of ensemble techniques specific to music genre classification and (2) shedding light on further research along this direction.

We produce photon-number squeezed light pulses efficiently using an all-fibre asymmetric Sagnac interferometer.
Our source is very compact, flexible and stable, which makes it highly suitable for applications in quantum information science.

Prediction query performance (PQP) has recently been recognized by the IR (information retrieval) community as an important capability for IR systems.
Put forward pseudo prediction query performance in the base of analyzing PQP.Design a new IR method.
The experiment adopted testing data of TREC Web Track shows the method can improve the effectiveness of query.

Elicitation of hidden knowledgeable information from voluminous data is the most prompting technique in data mining (DM
This is proved by DM algorithms for knowledge discovery.
Data mining incorporated with Cloud computing technology helps to achieve maximize profit and minimum cost with different possible ways through shared Cloud resource.
This paper examines the development framework for information retrieval using DM techniques in Cloud computing environment.
Here, the proposed work involves the four different approaches like Boosted K-NN, SVM, Fuzzy based ant colony optimization and Fuzzy artificial bee colony optimization for handling the cloud computing dataset.
The main aim of this work is to deploy these approaches in Google Cloud using Google App Engine with Cloud SQL.
The performance results show better outcomes compared to real world applications and also reduce the computing time, cost and infrastructure, to produce the results with mean time.

Previous research on stemming has shown both positive and negative effects on retrieval performance.
This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection.
Experiments especially focus on the measurement of Recall.
Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linguistic stemming, both in average Precision and R-Recall.
Best results are obtained with a linguistic stemmer which is enhanced with compound analysis.
This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.

Peer-to-peer (P2P) networks have emerged as a popular way to build large scale information systems by using the principle of resource sharing.
The P2P paradigm holds many promises, e.g. scalability, failure resilience and increased autonomy of nodes.
For these reasons P2P seems also to be an interesting architectural paradigm for realizing large scale information retrieval systems.
However, search methods in P2P networks are still mostly limited to simple keyword queries and the use of advanced retrieval models is in its infancy.

The Semantic Web seems to be evolving into a property-linked web of RDF data, conceptually divorced from (but physically housed in) the hyperlinked web of HTML documents.
We discuss the Unified Web model that integrates the two webs and formalizes the structure and the semantics of interconnections between them.
We also discuss the Hybrid Query Language which combines the Data and Information Retrieval techniques to provide a convenient and uniform way to retrieve data and documents from the Unified Web.
We present the retrieval system SITAR and some preliminary results.

With the rapid development of e-commerce, online shopping has become an important part in people's lives, in order to support the smooth development of e-commerce activities, how to provide users with an efficient and practical product information search method has become an urgent and critical problem.
This paper presents a framework for an ontology-based e-commerce product information retrieval system and proposes an ontology-based adaptation of the classical Vector Space Model with the consideration of the weight of product attribute.
A computer and components related ontology has been built, which is adopted to annotate the html documents and construct concept vectors of the documents.
Then the system test is done and the experimental result indicates that our proposal is better than the traditional keywords based search.

Music Genre Classification is one of the most active tasks in Music Information Retrieval (MIR Many successful approaches can be found in literature.
Most of them are based on Machine Learning algorithms applied to different audio features automatically computed for a specific database.
But there is no computational model that explains how musical features are combined in order to yield genre decision in humans.
In this work we present a listening experiment where audio has been altered in order to preserve some properties of music (rhythm, harmony, etc) but at the same time degrading other ones.
Results are compared with a series of state-of-the-art genre classifiers based on these musical properties and we draw some lessons from that comparison.

В статті запропоновано метод семантичної трансформації пошукового запиту користувача у вигляді ключових слів в запит, що складається із релевантних концепцій онтології заданої предметної області. Метод трансформації використовує створення профайлів користувачів. Запропоновано правила семантичної трансформації.
Ці правила базуються на використанні семантичних відношень субклас–суперклас частина–ціле синонімія та відношення
“екземпляр–клас Для оцінки методу була використана предметна область наукових публікації ACM, онтологія якої представлена на мові DAML+OIL.
Алгоритм трансформації реалізовано у прототипі, щ складається з агенту трансформації запитів і агенту онтології в рамках інтелектуального мультиагентного медіатора для отримання інформації.

Today research data and output from nearly every disciplinary and interdisciplinary domain exist also (and sometimes only) in digital form.
Consequently, to ensure the data’s efficient retrieval as well as long term usage and relevance, Knowledge Organisation and its tools/systems, i.e. lexical resources like thesauri and ontologies, play a major role in the Digital Libraries world.
In this paper we discuss our approach of pursuing the conversion and mapping of two of the most promising lexical resources, the Information Coding Classification and the MultiWordNet, after having converted them into the EuroWordNet RDF/OWL format.
In a second step we show how to integrate this additional knowledge into a domain Knowledge Organisation System (KOS
In the end we will have a presentation of the method and mapping as well as a use case for its evaluation for Information Retrieval purposes.

World Wide Web (WWW) is a mine of information for most people.
Due to the huge amount of ‎information and documents available on the internet, the process ‎of retrieving documents that are most relevant to user needs become a tremendous problem.
In addition to ‎that, the time needed for retrieving what a user searches for ‎increases dramatically.
In this paper, island genetic algorithm (IGA) is applied to achieve parallelism and speed up the web information retrieval ‎process
To retrieve pages most relevant to user needs, four different islands are developed.
Each island has different selection method, and different ‎fitness function.
These islands are executed ‎independently on different servers to achieve the parallel ‎behavior.
Finally, the results obtained by the four islands are combined and passed to a decision making phase to choose the documents most relevant to user query.
Cosine similarity measure is used to evaluate the performance of the proposed technique.

Today, XML is used in three different ways.
First, XML is used as a markup language, where documents are considered to be trees (with the occasional hyper link added) which represent the document structure.
Secondly, XML is used as an interchange format for structured data.
Here, a document is considered as a set of fields, each of which has a specific data type.
The third aspect is to use XML to represent text, where a document consists of words which can be stemmed and phrases and the like.
Obviously, a single XML document may represent more than one of these aspects, so for querying XML we need a query language which takes into account all these aspects.
[Robie et al.
98] is a query language for XML documents which is a natural extension of the W3C standard XPath [Clark DeRose 99
Therefore, XQL is a promising start for designing an XML query language for Information Retrieval systems.
However, the following features are desirable for IR systems, yet not available with XQL:

In general, ranking entities (resources) on the Semantic Web (SW) is subject to importance, relevance, and query length.
Few existing SW search systems cover all of these aspects.
Moreover, many existing efforts simply reuse the technologies from conventional Information Retrieval (IR which are not designed for SW data.
This paper proposes a ranking mechanism, which includes all three categories of rankings and are tailored to SW data reading.ac.uk

We explore the use of dictionary-based approaches for cross-lingual information retrieval tasks and propose a novel Coupled Dictionary Learning (CDL) algorithm to learn two separate representations simultaneously for documents in a parallel corpus alongside learning mappings from one representation to the other.
We evaluate the performance of the proposed algorithm for the task of comparable document retrieval and compare with existing baselines.

Collaborative filtering as a classical method of information retrieval has been widely used in helping people to deal with information overload.
In this paper, we introduce the concept of local user similarity and global user similarity, based on surprisal-based vector similarity and the application of the concept of maximin distance in graph theory.
Surprisal-based vector similarity expresses the relationship between any two users based on the quantities of information (called surprisal) contained in their ratings.
Global user similarity defines two users being similar if they can be connected through their locally similar neighbors.
Based on both of Local User Similarity and Global User Similarity, we develop a collaborative filtering framework called LS&GS.
An empirical study using the MovieLens dataset shows that our proposed framework outperforms other state-of-the-art collaborative filtering algorithms.

An efficient variant of an optimal algorithm is presented, which, in the context of a large dynamic fulltext information retrieval system, reorganizes data that has been compressed by an on-the-fly compression method based on LZ77, into a more compact form, without changing the decoding procedure.
The algorithm accelerates a known technique based on a reduction to a graph-theoretic problem, by reducing the size of the graph, without affecting the optimality of the solution.
The new method can thus effectively improve any dictionary compression scheme using a static encoding method.

This article studies a paper cutting platform in a comprehensive approach to folk art, folklore, communication, computer and information science focusing on paper-cut in Hebei China.
The main content includes 9312;the data collecting and recording of the scattered Chinese folk paper-cut work in Hebei 9313;the digitization, optimization, compression, classification, icon and pattern extraction, vectorization and work analysis of the first-hand material 9314;the demonstration, dissemination, database construction and retrieval of the classified material, icons and patterns
9315; the demonstration of the reconstruction and application of icon and pattern database, and 9316;the design and development of immersive virtual gaming platform of the multi-media scenes and production process of folk paper-cut.
In this article, a virtual multi-media system framework of paper cutting is constructed.

The goal of the project is to implement a citation analysis system with graphical interface for papers in artificial intelligence and machine learning area.
The main functionality is topic clustering and tracking of references between papers.
The idea of the interface is to represent search result in cluster diagrams, so that user can pick one cluster and either see documents in it, sorted with some relevance measure, or enter the cluster recursively to see internal structure of the cluster.

Language modeling is an important part for both speech recognition and machine translation systems.
Adaptation has been successfully applied to language models for speech recognition.
In this paper we present experiments concerning language model adaptation for statistical machine translation.
We develop a method to adapt language models using information retrieval methods.
The adapted language models drastically reduce perplexity over a general language model and we can show that it is possible to improve the translation quality of a statistical machine translation using those adapted language models instead of a general language model.

In logistics of stocks, items/parts information are embodied in product nomenclatures.
They may change due the variety of production growth in conditions which the codification of parts used can remain unchanged.
Also, can occur in nomenclature, various items/parts with incomplete information.
This dynamic can generate a number of issues related to the assessment inventory and flows of equipment.
To reduce these anomalies can use the techniques in the category Information Retrieval, which allows combining traditional search, with one of the category x201C;full-text searchings&#x201D
This technique allows the construction of dictionaries, in order to search.
Maintaining updated nomenclature is a process that depends on the accuracy of the entire logistics system.
This paper attempts to outline software architecture enabling the implementation of such techniques.

A novel information structure and its use for query expansion is presented.
The information structure, called a similarity thesaurus, consists of term-term similarities that are based on how the terms of a collection \are indexed" by the documents.
It reeects domain knowledge about the collection and is used to select and weight additional query terms when expanding a query.
This is in contrast to conventional query expansion methods as the similarity between candidate terms and the concept of the entire query is taken into account.
Experiments on the TREC collection show that the retrieval eeectiveness is considerably higher when this method is applied.
The hope is that this concept-based query expansion can also be used to produce better results in large-scale operational IR environments.

The main goals of a switch scheme are high utilization low queuing delay and fairness
To achieve high utilization the switch scheme can maintain non zero small queues in steady state which can be used if the sources do not have data to send It is very important to design and analyze the queue control function which is used in such a scheme
In this contribution we study various queue control functions and present analytical explanation of its behavior and simulation results From the study
we conclude that a simple linear queue control function performs satisfactorily Source Bobby Vandalore Raj Jain Rohit Goyal Sonia Fahmy The Ohio State University Department of Computer and Information Science Columbus OH Contact Phone Fax E mail fvandalor jaing cis ohio state
The presentation of this contribution at ATM Forum is sponsored by NASA Lewis Research Center

With the increase of textual information available electronically, we assist to a great diversification of the demands on Information Retrieval (IR) and Information Extraction (IE) systems.
In this paper we apply Machine Learning techniques of sequence analysis to the tasks of highlighting and labeling text with respect to an information extraction task.
Specifically, dynamic probability models are used.
Like IR systems, they use little semantics, are fully trainable and do not require any knowledge representation of the domain.
Unlike IR approaches, documents are considered as a dynamic sequence of words.
Furthermore, additional word information is naturally included in the representation.
Models are evaluated on a sub-task of the MUC6 Scenario Template corpus.
When morpho-syntactic word information is introduced into the representation, an increase in performances is observed.

Extracting a singing voice from its music accompaniment can significantly facilitate certain applications of Music Information Retrieval including singer identification and singing melody extraction.
In this paper, we present a hybrid approach for this purpose, which combines properties of the Azimuth Discrimination and Resynthesis (ADRess) method with Independent Component Analysis (
ICA Our proposed approach is developed specifically for the case of singing voice separation from stereophonic recordings.
The paper presents the characteristics of the proposed method and details an objective evaluation of its effectiveness.

A large portion of data exchanged in today’s Peer-to-Peer (P2P) networks consists of music stored as MP3 compressed audio.
Existing P2P systems typically are not scalable and only support primitive methods for the searching of music files, e.g by looking up exact filenames or using simple metadata information such as artist or album name.
In this paper, we present the design and evaluation of a scalable P2P system that uses Rendezvous Points (RPs) for music file registration and query resolution, and supports contentbased Music Information Retrieval (MIR) of audio signals.

Folk song research (FSR) often deals with large collections of tunes that have various types of relations to each other.
Computational methods can support the study of the contents of these collections.
Music Information Retrieval (MIR) research provides such methods.
Yet a fruitful cooperation of both disciplines is difficult to achieve.
We present a role-model to structure this cooperation in which tasks and responsibilities are distributed among the roles of MIR, Computational Musicology (CM) and FSR.

In this paper we describe and evaluate a learning model for information ltering which is an adaptation of the generalised probabilistic model of Information Retrieval.
The model is based on the concept of \uncertainty sampling" a technique that allows for relevance feedback both on relevant and non relevant documents.
The proposed learning model is the core of a prototype information ltering system called ProFile.
Previously at Dipartimento di
Elettronica e Informatica, Universit a di Padova, Padova, Italy.

In the paper we deal with automatic detection of anchoring segments in a collection of TV programmes.
The anchoring segments are intended to be further used as a basis for subsequent hyperlinking to another related video segments.
The anchoring segments are therefore supposed to be fetching for the users of the collection.
Using the hyperlinks, the users can easily navigate through the collection and find more information about the topic of their interest.
We present two approaches, one based on metadata, the second one based on frequencies of proper names and numbers contained in the segments.
Both approaches proved to be helpful for different aspects of anchoring problem: the segments which contain a large number of proper names and numbers are interesting for the users, while the segments most similar to the video description are highly informative.

This paper proposes the use of local context as a way to semantic information retrieval.
In our model, rather than trying to formalize the contents of the documents among which the search is done (e.g. by formally annotating them we try to automatically build a representation of the context in which the search is done.
We consider that search is always done as part of an activity, and that the search context is determined by the activity that is carried out at a particular moment.
Many activities that a person is engaged in are carried out with the help of a computer, and leave a digital trace in the files that are created in connection to it.
We take these files, and the structural relations between the directories in which they are stored, as a starting point for the representation of context.

THE RATES OF ASSIGNMENT OF THESAURUS TERMS IN THE ERIC INFORMATION RETRIEVAL SYSTEM:
AN ANALYSIS OF HIERARCHIES AND LEVELS GEORGIANNA HENRY VIRGIL DIODATO Article information:
To cite this document: GEORGIANNA HENRY VIRGIL DIODATO 1991 THE RATES OF ASSIGNMENT OF THESAURUS TERMS IN THE ERIC INFORMATION RETRIEVAL SYSTEM: AN ANALYSIS OF HIERARCHIES AND LEVELS Journal of Documentation, Vol.
47 Iss 3 pp.
276 283 Permanent link to this document: http dx.doi.org/10.1108/eb026881

Dept. of Biology, Oncology and Genetics, Univ.
Genova, Largo R. Benzi 10, 16132 Genova Italy Advanced Biotechnologies Center, Largo R. Benzi 10, 16132 Genova Italy Dept. of Computer Science, Univ.
Pisa, Corso Italia 40, 56125 Pisa Italy National Institute for the Physics of Matter,
Via Dodecaneso 33, 16146 Genova Italy Dept. of Computer and Information Science, Univ.
Genova, Via Dodecaneso 35, 16146 Genova Italy

We give an overview of some algorithmic problems arising in the representation of text/image/multimedia objects in a form amenable to automated searching, and in conducting these searches efficiently.
These operations are central to information retrieval and digital library systems.
tional linguistics 3) user interfaces and user models 4) network and distributed retrieval issues including server/network performance and load balancing 5) security, access control and rights management.

In this paper, we present a novel Probabilistic Latent Semantic Analysis-based (PLSA-based) aspect model and turn cross-media retrieval into two parts of multi-modal integration and correlation propagation.
We first use multivariate Gaussian distributions to model continuous quantity in PLSA, avoiding information loss between feature-instance versus real-world matching.
Multi-modal correlations are learned in an asymmetrical manner, giving a better control of the respective influence of each modality in the latent space.
Then we propose a new propagation pattern to refine multi-modal correlations by efficiently taking the complementary from multi-modalities.
Experimental results demonstrate that our method is accurate and robust for cross-media information retrieval.

As more and more collections of data are becoming available on the web to everyone, non expert users demand easy ways to retrieve data from these collections.
One solution is the so called Visual Query Systems (VQS) where queries are represented visually and users do not have to understand query languages such as SQL or XQuery.
In 1996, a paper by Catarci reviewed the Visual Query Systems available until that year.
In this paper, we review VQSs from 1997 until now and try to determine whether they have been the solution for non expert users.
The short answer is no because very few systems have in fact been used in real environments or as commercial tools.
We have also gathered basic features of VQSs such as the visual representation adopted to present the reality of interest or the visual representation adopted to express queries.

The objective of this paper is to present a textual similarity model for Information Retrieval (IR) based on the Distributional Semantic (DS) model.
This model is an extension of the standard Vector Space model, which further takes into account the co-frequencies between the terms in a given reference corpus, that are considered to provide a distributional representation of the "semantics" of the terms.
Practical retrieval experiments using DSbased similarity models have been conducted in the framework of the AMARYLLIS evaluation campaign.
The results obtained are presented, and indicate significant improvement of the performance in comparison with the standard approach.
Textual similarity, Information Retrieval, Distributional Semantics.

Paraphrasing means expressing or conveying the same meaning or essence of a sentence or text using different words or rearrangement of words.
Paraphrase detection is a challenge, especially in Indian languages like Hindi, because it is very essential to understand the semantics of the language.
Detecting paraphrases is very relevant in real life because it has a lot of importance in applications like Information Retrieval, Extraction and Text Summarization.
This paper focuses on using Machine Learning classification techniques for detecting paraphrases in Hindi language for the DPIL Task in Fire 2016.
A feature vector based approach has been used for detecting paraphrases.
The task involves checking whether a given pair of sentences conveys the same information and meaning even if they are written in different forms.
Given a pair of sentences in Hindi, the proposed technique labels whether the pair of sentences are Paraphrases (P Semi-Paraphrases (SP) or Not Paraphrases (NP).

The basic classification techniques for organizing information are thesauri, taxonomy and faceted classification.
Topic map is relatively a new entrant to this information space.
Topic map standard describes how complex relationships between abstract concepts and real world resources can be represented using XML syntax.
This paper explores how topic map incorporates the traditional techniques and what are its advantages and disadvantages in several dimensions such as content management, indexing, knowledge representation, constraint specification and query languages in the context of information retrieval.
The constructs of topic maps are illustrated with a use-case implemented in XTM.

In this work we propose a novel approach to anomaly detection in streaming communication data.
We first build a stochastic model for the system based on temporal communication patterns across each edge, which we call the REWARDS (REneWal theory Approach for Real-time Data Streams) model.
We then define a measure of anomaly for an arbitrary subgraph based on the likelihood of its recent activity given past behavior.
Finally, we develop an algorithm to efficiently identify subgraphs with the most anomalous activity.
Although our work has until now focused on the cybersecurity domain, the model we present is more broadly applicable to information retrieval in data streams and information networks.

Although recently crowd sourcing has become popular, the reliability of crowdsourced results has been questioned because of workers' varied degrees of attention, skill, and accuracy.
Therefore, understanding the factors that affect the reliability of crowd sourcing is crucial.
Using crowdsourcing has recently proposed as an alternative to create relevance judgments for the evaluation of information retrieval systems as traditional methods are expensive and scale poorly.
The aim of this study is to measure selected cognitive ability (general reasoning skill) of crowdsourced workers, and investigate the effect that this characteristic has upon judgment reliability, as measured against a human gold standard.
A significant correlation was found between judgment reliability and measured general reasoning skill.
Our findings show that general reasoning skill influences the accuracy of crowdsourced workers who create the relevance judgments set.

As an important linguistic resource, collocation represents a significant relation between words.
Automatic collocation extraction is very important for many natural language processing applications, such as word sense disambiguation, machine translation and information retrieval etc.
While traditional collocation extraction approaches use only one single statistical measure, they may not be optimal in that they can not take advantage of multiple statistical measures.
In this paper, we propose a logistic linear regression model (LLRM) that combines five classical lexical association measures: x<sup>2</sup>-test, t-test, co-occurrence frequency, log-likelihood ratio and mutual information.
Experiments show that our approach leads to a significant performance improvement in comparison with individual basic methods in both precision and recall.

In this paper, we describe the design and development of personal information assistant (PIA a system aiming to meet individual needs of the searchers.
The system’s goal is to provide more up-to-date and relevant information to users with respect to their needs and interests.
The main component of the system is a profile learner for capturing temporal user needs, based on implicit feedback gathering techniques.
It monitors the system usage, the documents viewed and other user actions in order to infer users’ changing needs.

We present in this article a summary of the main research results in the last years in the area of Web Information Retrieval produced by two Brazilian groups, from the Federal University of Amazon as and the Federal University of Minas Gerais.
The two groups have worked in several common research projects in the last five years.
The research results described here are concentrated in three main topics related to Web research: information retrieval models for Web search, efficiency issues in Web search, and noise removal.
The results obtained by the two groups in the last years represent significant contributions to improve both the quality and the efficiency of Web search systems.

In our formal runs, we have experimented with hybrid-term indexing and bigram indexing because hybrid-term indexing is a more distinct type of indexing strategy for better pooling and because bigram indexing usually gives robust (near) good results.
We have also used our pseudo-relevance feedback (PRF) methods.
In the informal runs, we have experimented with our previous re-ranking strategy, called title re-ranking.
This strategy rewards documents which title terms match with the terms in the title query.
Title re-ranking is able to improve the effectiveness performance for both short and long queries when bigram indexing is used.
For formal runs, our best relax MAP achieved was 36% and 51% using PRF, for title queries and long queries respectively.
For informal runs, our best relax MAP achieved was 43% for title queries and 50% for long queries using both PRF and merging retrieval lists.

Over recent years, many Private Information Retrieval (PIR) schemes have been designed aiming for computational efficiency and overall real-world practicality.
In particular, some preprocessing techniques have been studied to reach those goals.
Our main contribution is a new preprocessing technique that reduces overall computation and communication and allows the client and server to share some of the computational burden without significantly reducing the security of the scheme and requires little to no additional space on the server's side.
We show how this technique is naturally compatible with at least two schemes.
One based on the Approximate GCD assumption and the other on the Ring-LWE problem.
We provide theoretical complexities and show that in some cases we can achieve a less-than-n complexity in a single server PIR scheme for the first time through the combination of multiple optimization techniques.

This paper, introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks.
We investigate for the first time the challenges posed by the wireless medium and recognise the factors that require optimisation.
We propose novel techniques, which attain a significant reduction in both response times and traffic, compared to naive approaches.
Extensive experimental results illustrate the appropriateness and efficiency of the proposed method in this bandwidth-starving and volatile, due to mobility, environment.

This paper presents an information retrieval system for the NTCIR-7 information retrieval for question answering task.
This system is composed by three parts 1)
Query processing (2) Retrieval model (3) Re-rank module.
Query processing filters stop-words and selects query terms to generate a required term set for further retrieval.
Threes retrieval models from two famous retrieval systems are adopted.
They retrieved relevant documents based on the generated required set.
Finally, Re-rank module gives documents scores according to the distributions of their possessive terms.
The performance of our system achieves a mean average precision (MAP) of 0.4635, a Q-measure of 0.4811, and an MSn-DCG of 0.6831 on NTCIR-7 IR4QA testing set.

In metadata-based databases, stored information is identified by associated metadata.
The more such data that is associated, the better the content can be uniquely described.
However, a large number of types of metadata prevent users from specifying appropriate queries.
In this thesis, a model for representing the metadata of a system to users that easier facilitates information retrieval is presented, based on taking under consideration specific subsets of metadata and then arranging them in a tree structure with constraints.
Using this model, the interface for an existing metadata-based database is implemented.

This article presents the analytical and retrieval potential of visualization maps.
Obtained maps were tested as information retrieval (IR) interface.
The collection of documents derived from the ACM Digital Library was mapped on the sphere surface.
Proposed approach uses nonlinear similarity of documents by comparing ascribed thematic categories and thereby development of semantic connections between them.
For domain analysis the newest IT trend Cloud Computing was monitored across time period 2007-2009.
Visualization reflects evolution, dynamics and relational fields of cloud technology as well as its paradigmatic property.

This paper describes some aspects of a project with the aim of developing a user-friendly interface to a classical Information Retrieval (IR) System in order to improve the effectiveness of retrieval.
The character by character approach to IR has been abandoned in favor of an approach based on the meaning of both the queries and the texts containing the information to be sought.
The concept space, locally derived from a thesaurus, is used to represent a query as well as documents retrieved in atomic concept units.
Dependencies between the search terms are taken into account.
The meanings of the query and the retrieved documents (results of Elementary Logical Conjuncts (ELCs are compared.
The ranking method on the semantical level is used in connection with existing data of a classical IR system.
The user enters queries without using complex Boolean expressions italic>

In this paper we describe an Information Retrieval problem called collection fusion.
The collection fusion problem is to maximize the number of relevant natural language documents retrieved given: a natural language query, multiple collections of documents, and a fixed total number of documents to retrieve.
We describe two algorithms that use past queries to learn collection fusion strategies.
Tests of these algorithms on a corpus of 742,000 documents indicate that they can learn good fusion strategies.
Moreover, the strategies learned by our methods are consistently superior to those learned by a standard learning algorithm.

It is increasingly a difficult problem how to decrease the usage of network resources by network management systems.
Otherwise, the performance of network management is affected by the scheme to collect MIB (management information base) information.
A novel algorithm DGP (distributed group- prefetching algorithm) is put forward, which is used for retrieving MIB information from the managed devices in the management of CERNET2
(China Education and Research Network based on IPv6 DGP can adjust many little retrieving objects into one prefetching group to access, decrease the frequency of retrieval, and thus can use fewer network resources and decrease the network overhead without alteration of existing network management protocols.

The primary objective of IIiX is to investigate how the concept of context can be understood and exploited to make information systems truly interactive.
Knowledge about context can be used effectively to constrain retrieval and seeking of information and to understand ensuing information behaviours including relevance and use, thereby reducing the complexity of the interaction processes.
As in the case of the first IIiX symposium, the aim of IIiX’08 was to bring together researchers from the related areas of information seeking behaviour, laboratory IR, and interactive IR to provide a forum for discussion and collaboration on context issues with respect to information seeking and retrieval.
This was successfully achieved in IIiX 2008, as a variety of angles to the concept of context, as well as suggestions as to how to consider and treat context in information seeking and retrieval, were presented.

The SyncPlayer is a prototypical software framework that integrates various Music Information Retrieval (MIR) techniques such as music synchronization, audio structure analysis and content-based retrieval into a powerful, multimodal system [1
, 2 The SyncPlayer system basically consists of three software components: a server component, a client component, and some tools for data administration.
These three components can be summarized as follows:

A challenging problem in machine learning, information retrieval and computer vision research is how to recover a low-rank representation of the given data in the presence of outliers and missing entries.
The L1-norm low-rank matrix factorization (LRMF) has been a popular approach to solving this problem.
However, L1-norm LRMF is difficult to achieve due to its non-convexity and non-smoothness, and existing methods are often inefficient and fail to converge to a desired solution.
In this paper we propose a novel cyclic weighted median (CWM) method, which is intrinsically a coordinate decent algorithm, for L1-norm LRMF.
The CWM method minimizes the objective by solving a sequence of scalar minimization sub-problems, each of which is convex and can be easily solved by the weighted median filter.
The extensive experimental results validate that the CWM method outperforms state-of-the-arts in terms of both accuracy and computational efficiency.

It has long been recognised that interactivity improves the effectiveness of Information Retrieval systems.
Speech is the most natural and interactive medium of communication and recent progress in speech recognition is making it possible to build systems that interact with the user via speech.
However, given the typical length of queries submitted to Information Retrieval systems, it is easy to imagine that the effects of word recognition errors in spoken queries must be severely destructive on the system’s effectiveness.
The experimental work reported in this paper shows that the use of classical Information Retrieval techniques for spoken query processing is robust to considerably high levels of word recognition errors, in particular for long queries.
Moreover, in the case of short queries, both standard relevance feedback and pseudo relevance feedback can be effectively employed to improve the effectiveness of spoken query processing.

The rapid growth in information science and technology has lead to generation of huge amount of valuable data in many areas.
In finance for example, over the past five years, many banks have experienced exceptional growth in service and have built up bank’s Group Data Warehouse.
In order to realize faster, more effective decisions and provide more excellent customer services, new technologies to handle or extract fully the latent knowledge within the data are urgently required.
The finance company that donated the data for 2007 PAKDD competition would like to build a cross-selling model to predict the potential take-ups of cross-selling home loans to its credit card customers (Qiu, Wang Bi, 2008 abstract

Information dissemination by means of computer networks uses information systems to store, retrieve and transmit knowledge, which are considered very important tools for society nowadays.
The concepts of Information Science, their application to the solution of real situations regarding life cycle inventory and the procedures used in Information Technology provide a new direction to Information Science.
This article presents an approach regarding life cycle inventories in Brazil and aspects for the concept of information management that gives support to the initiatives of life cycle assessment (LCA) methodology, contributing to the competitiveness of Brazilian industries in a globalized world.

In this work, we propose a hybrid method for improving recall in electronic discovery proceedings.
This approach takes ideas from Natural Language Processing (Word sense disambiguation) and Information Retrieval in enhancing retrieval of responsive documents using the semantics of query terms instead of direct text matching.
Preliminary results from disambiguation of user queries show that this approach is promising to improve recall at the same time maintaining high degree of precision in the retrieval of relevant documents to help lawyers and their clients during litigations.

A data model, called the entity-relationship model, which incorporates the semantic information in the real world is proposed.
A special diagramatic technique is introduced for exhibiting entities and relationships.
An example of data base design and description using the model and the diagramatic technique is given.
The implications on data integrity, information retrieval, and data manipulation are discussed.

This paper presents a survey of existing techniques for achieving mobile code security, as well as a representative sampling of systems which use them.
In particular, the problem domain is divided into two portions: protecting hosts from malicious code; and protecting mobile code from malicious hosts.
The discussion of the malicious code problem includes a more in-depth study of the Java security model, as well as touching upon several other systems.
The malicious host problem, however, is much more difficult to solve, so our discussion is mostly restricted to ongoing research in that area.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-98-28.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/168 Mobile Code Security Techniques

We are presenting the construction of a Swedish corpus aimed at research on Information Retrieval, Information Extraction, Named Entity Recognition and Multi Text Summarization, we will also present the results on evaluating our Swedish text summarizer SweSum with this corpus.
The corpus has been constructed by using Internet agents downloading Swedish newspaper text from various sources.
A small part of this corpus has then been manually annotated.
To evaluate our text summarizer SweSum
we let ten students execute our text summarizer with increasing compression rate on the 100 manually annotated texts to find answers to questions.
The results showed that at 40 percent summarization/compression rate the correct answer rate was 84 percent.

Proper names are problematic for cross language information retrieval.
Standard bilingual dictionaries typically have poor coverage of proper names.
On the other hand, IR tasks involving news corpora, like TDT and TREC cross language IR, have proper names at their core.
In this study, we demonstrate the importance of proper names in one such task, the TREC 2002 (Arabic-English) cross language track, by showing that performance degrades a tremendous amount when the bilingual lexicons do not have proper names.
We then examine several different sources of proper name translations from English to Arabic, both static and generative (transliteration) and explore their effectiveness in the context of the TREC 2002 cross language IR task.
We support a conclusion that a combination of static translation resources plus transliteration provides a successful solution.

In the biomedical field, using of specialized terms is key to access information.
However, in most Indo-European languages, these terms are complex morphological structures.
The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (
IR We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms.
These morphological analysis are used to improve the indexing of medical documents.
The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system.
MOTS-CLÉS morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d’information biomédicale.

In the work at hand, we combine a Private Information Retrieval (PIR) protocol with Somewhat Homomorphic Encryption (SHE) and use Searchable Encryption (SE) with the objective to provide security and confidentiality features for a third party cloud security audit.
During the auditing process, a third party auditor will act on behalf of a cloud service user to validate the security requirements performed by a cloud service provider.
Our concrete contribution consists of developing a PIR protocol which is proceeding directly on a log database of encrypted data and allowing to retrieve a sum or a product of multiple encrypted elements.
Subsequently, we concretely apply our new form of PIR protocol to a cloud audit use case where searchable encryption is employed to allow additional confidentiality requirements to the privacy of the user.
Exemplarily we are considering and evaluating an audit of client accesses to a controlled resource provided by a cloud service provider.

Most search engines have to face the dynamic nature of the web, and it becomes a big problem that how to offer near real-time query service while the underlying document collection increases dramatically every day.
As a result, the online indexing approaches become one of the kernel research problems of information retrieval.
In this paper, we first present a detailed classification of various online indexing strategies, from the classics to the state-of-the-arts.
We then perform an evaluation on selected strategies.
A new evaluation metric is introduced in this paper to characterize the dynamic performance when queries interact with online indexing concurrently.
Evaluation results characterize the performance differences among strategies and indicate the future improvements on update and query performance.

A panel held at the SIGIR99 conference, reflecting the mandate of the conference to focus on user interface issues, raised some interesting questions regarding applying results of user studies and observations to the design of information retrieval systems and interfaces.
In part, the panel echoed a problem well-known to the IR community: how can we respect the tradition and methodology of search engine evaluation and at the same time recognize and adapt to the emerging role of interactive information retrieval?
The community, it seems, has a few choices:

GeoCLEF ran as a regular track for the second time within the Cross Language Evaluation Forum (CLEF) 2007.
The purpose of GeoCLEF is to test and evaluate cross-language geographic information retrieval (GIR retrieval for topics with a geographic specification.
GeoCLEF 2007 consisted of two sub tasks.
A search task ran for the third time and a query classification task was organized for the first.
For the GeoCLEF 2007 search task, twenty-five search topics were defined by the organizing groups for searching English, German, Portuguese and Spanish document collections.
Topics were translated into English, German and Spanish.
Several topics in 2007 were geographically challenging.
Thirteen groups submitted 108 runs.
The groups used a variety of approaches.
For the classification task, a query log from a search engine was provided and the groups needed to identify the queries with a geographic scope and the geographic components within the local queries.

Economists have investigated models of optimal search behavior that may be relevant to information retrieval.
This talk will describe some of the reults of this research and speculate on how they might be useful in the IR context.
Professor Varian has published numerous papers in economic theory, industrial organization, financial economics, econometrics and information economics.
He is the author of two major economics textbooks which have been translated into 9 languages.
His recent work has been concerned with the economics of information technology and the information economy.
He recently co-authored Information Rules (with Carl Shapiro, Harvard Business School Press 1998 a book which distills the economic principles of information and networks into practical business strategies.
Information Rules was named Editor's Choice as one of the best business books of the year at Amazon.com

We review here the results of one of the experiments performed at the 2003 Reliable Information Access (RIA) Workshop, hosted by Mitre Corporation and the Northeast Regional Research Center (NRRC The experiment concentrates on query expansion using relevance feedback and explores the behaviour of several information retrieval systems using variable numbers of relevant documents.

The World Wide Web is a rich source of digital information.
Web search engines allow users to locate resources on the Internet thus helping them to satisfy their information needs.
The advent of the Wireless Application Protocol (WAP) has made wireless Internet access possible.
However, WAP users do not have access to the same quality of online Information Retrieval systems as 'wired' users.
We present a system, named WapSearch, that enables WAP enabled mobile phones and PDAs to search for and browse Internet content written in HTML.
The WapSearch system is based around an HTML/WML conversion tool and a popular Web search engine and employs a novel document navigation functionality designed to aid the Information Retrieval process on these devices.
In short, we support sophisticated document browsing by bypassing the (typically small) cache memory of the client device, instead exploiting the server's much greater memory and processing capabilities.

Artificial intelligence models may be used to improve performance of information retrieval (IR) systems and the genetic algorithms (GAs) are an example of such a model.
This paper presents an application of GAs as a relevance feedback method aiming to improve the document representation and indexing.
In this particular form of GAs, various document descriptions compete with each other and a better collection indexing is sought through reproduction, crossover and mutation operations.
In this paradigm, we are searching for the optimal balance between two genetic parameters: the population size and the number of generations.
We try to discover the optimal parameter choice both by experiments using the CACM and CISI collections, and by a theoretical analysis providing explanation of the experimental results.
The general conclusion tends to be that larger populations have better chance of significantly improving the effectiveness of retrieval.

Data citation has a profound impact on the reproducibility of science, a hot topic in many disciplines such as as astronomy, biology, physics, computer science and more.
Lately, several authoritative journals have been requesting the sharing of data and the provision of validation methodologies for experiments (e.g Nature Scientific Data and Nature Physics these publications and the publishing industry in general see data citation as the means to provide new, reliable and usable means for sharing and referring to scientific data.
In this paper, we present the state of the art of data citation and we discuss open issues and research directions with a specific focus on reproducibility.
Furthermore, we investigate reproducibility issues by using experimental evaluation in Information Retrieval (IR) as a test case.

The massive grow of the modern information retrieval system (IRS especially in natural languages becomes more difficult.
The search in Arabic languages, as natural language, is not good enough yet.
This paper will try to build similar thesaurus based on Arabic language in two mechanisms, the first one is full word mechanisms and the other is stemmed mechanisms, and then to compare between them.
The comparison made by this study proves that the similar thesaurus using stemmed mechanisms get more better results than using traditional in the same mechanisms and similar thesaurus improved more the recall and precision than traditional information retrieval system at recall and precision levels.

This paper presents two approaches for Information Retrieval (IR) from a collection of documents: Bayesian theory of probability and Dempster-Shafer theory of belief functions.
Each method has been supported with essential derivations to prove their suitability for IR.
The conclusions of derivations have been applied in illustrative examples.
Finally, comparison of both the methods suggests the suitability of each for specific domains.

With the success of the World-Wide Web new solutions for very fast document access have emerged.
ELEKTRA is an article delivery system that supports full-text and relational information retrieval as well as electronic article ordering and delivery.
Full-length articles can be both viewed by standard-web browsers and printed by PostScript printers in a high resolution quality.
ELEKTRA has been realized by using the general multimedia digital library tool Omnis 4.0.

Technical documents for multilateral agreements or international business transactions are normally produced in a bilingual or multilingual form.
Being mostly of legal nature, these documents require especially accurate and speedy translations by expert translators.
In order to aid these experts, automatic ways of checking translation results (such as a spelling checker) would be highly desirable.
This paper describes the MIRAC system for Multilingual Information Retrieval And Checking.
It is designed to find translation errors in multilingual documents, and to evaluate the overall results of translation.
Unlike a machine translation or a translation memory system [Volk98, Webb98 the primary function of the MIRAC system is to evaluate previously translated and aligned documents in source and target languages, while dynamically building a database that consists of aligned multilingual texts carrying semantically equivalent content.

In this paper the changing context of information retrieval is discussed by examining the role of the Okapi text retrieval system in the “Tools for Innovative Publishing in Science TIPS) project.
TIPS project poses a number of new challenges for end-user probabilistic retrieval, including fieldbased searching, Web access, and integration with other software.
These and some other problems involved in designing a sophisticated Web-based best match retrieval system are highlighted.
The architecture of the implemented Okapi Web system, its integration with the other systems that comprise the TIPS portal and design and evaluation of the user interface are discussed.

This paper focuses on information retrieval from relational databases on biodiversity content of flora and fauna in Malaysia.
The retrieval process was done considering factors such as the databases are distributed, located in Windows as well as Linux platforms, developed using a variety of Database Management Systems and the structure of the tables are not standardized.
Structured Query Language (SQL) was used to generate query, ASP and PHP were used as scripting languages, database drivers such as ODBC, Oledb,
DataDirect32 bit SequeLink 5.4 were used for communication with the DBMSs and finally XML for generating standard retrieval results from the databases queried.

Currently geospatial information (GI) technology can be characterized by two important developments 1) it goes beyond the application stages towards more conceptual and theoretical developments, dealing with more profound issues around geographic information science, and (2) it continues to be application or service oriented in serving our modern society, hence location-based services (LBS) arisen out of convergence of GI technology, the Internet, mobile wireless telecommunication and positioning technologies.
The convergence of GIS and various technologies has been existing since the widespread availability of the Internet, and it is reflected in a series of GIS terms such as Internet GIS, web GIS, wireless GIS, and mobile GIS.
Nowadays it appears that all the “GIS” come to a pot with a more fashionable term LBS.

Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.

This paper describes development of meta databases for geospatial data in the WWW in general as well as efforts/activities in the context of developing InGeoMDB (InG eoForum Meta Data Base
Hence, especially existing metadata standards wiIl be analysed and evaluated.
Furthermore possibilities of integrating meta databases into metadata information systems (MIS) resp.
catalogue systems (CS) will be pointed out as well as apsects, which should be considered with regard to ‘Information Retrieval’ via MIS/C S in the world wide web.

Current information retrieval systems focus on the use of keywords to respond to user queries.
We propose the additional use of surface level knowledge in order to improve the accuracy of information retrieval.
Our approach is based on the database concept of semantic modeling (particularly entities and relationships among entities We extend the concept of query-document similarity by recognizing basic entity properties (attributes) which appear in text.
We also extend query-document similarity using the linguistic concept of thematic roles.
Thematic roles allow us to recognize relationship properties which appear in text.
We include several examples to illustrate our approach.
Test results which support our approach are reported.
The test results concern searching documents and using their contents to perform the intelligent task of answering a question.

The ability to discover and access available ocean data and information is a major challenge for the ocean science community.
This challenge has been recognized by the United States government policies that require science data be current, accessible and consistent with certain standards.
This paper describes how the Bureau of Ocean Energy Management designed and developed a system that utilizes geospatial and information science to allows scientific data and information from its Environmental Studies program dating back to 1973 to be easily accessible and discoverable via the Internet.

Text clustering could be very useful both as an intermediate step in a large natural language processing system and as a tool in its own right.
The result of a clustering algorithm is dependent on the text representation that is used.
Swedish has a fairly rich morphology and a large number of homographs.
This possibly leads to problems in Information Retrieval in general.
We investigate the impact on text clustering of adding the part-of-speech-tag to all words in the the common term-bydocument matrix.
The experiments are carried out on a few different text sets.
None of them give any evidence that part-of-speech tags improve results.
However, to represent texts using only nouns and proper names gives a smaller representation without worsen results.
We also investigate the effect of lemmatization and the use of a stoplist, both of which improves results significantly in some cases.

Recent availability of commercial online machine translation (MT) systems makes it possible for layman Web users to utilize the MT capability for cross-language information retrieval (CLIR To study the effectiveness of using MT for query translation, we conducted a set of experiments using Google Translate, an online MT system provided by Google, for translating queries in CLIR.
The experiments show that MT is an excellent tool for the query translation task, and with the help of relevance feedback, it can achieve significant improvement over the monolingual baseline.
The MT based query translation not only works for long queries, but is also effective for the short Web queries.

Term weighting is a core idea behind any information retrieval technique which has crucial importance in document ranking.
In graph based ranking algorithm, terms within a document are represented as a graph of that document.
Term weights for information retrieval are estimated using term&#x02019;s co-occurrence as a measure of term dependency between them.
The weight of vertex in the document graph is calculated based on both local and global information of that vertex.
This paper introduces a method of information retrieval using random walk model considering positional values of a term in the document for computing its inverse document frequency and assigning trained weight to terms in the user provided query
x0A0 x0A0;Experiments on standard datasets have shown that our approach provides improvement in recall and precision of information retrieval system.

While there are many textual and image retrieval systems, few have explored the granularity of the retrieval unit and the use of all available information for retrieval.
This paper presents our work on using textual and image retrieval, fusing the results and providing document retrieval that uses visual and textual information from documents.
A query re nement technique is also shown that blurs the line between browsing and searching and integrates both into the same framework.

This paper is concerned with the generalization ability of learning to rank algorithms for information retrieval (IR We point out that the key for addressing the learning problem is to look at it from the viewpoint of <
We define a number of new concepts, including query-level loss, query-level risk, and query-level stability.
We then analyze the generalization ability of learning to rank algorithms by giving query-level generalization bounds to them using query-level stability as a tool.
Such an analysis is very helpful for us to derive more advanced algorithms for IR.
We apply the proposed theory to the existing algorithms of Ranking SVM and IRSVM.
Experimental results on the two algorithms verify the correctness of the theoretical analysis.

We present a way of estimating term weights for Information Retrieval (IR using term co-occurrence as a measure of dependency between terms.
We use the random walk graph-based ranking algorithm on a graph that encodes terms and co-occurrence dependencies in text, from which we derive term weights that represent a quantification of how a term contributes to its context.
Evaluation on two TREC collections and 350 topics shows that the random walk-based term weights perform at least comparably to the traditional tf-idf term weighting, while they outperform it when the distance between co-occurring terms is between 6 and 30 terms.

Information Retrieval in structured documents (and particularly XML) requires the user to have a good knowledge of the document structure and of some query language.
This article discusses the advantages that could be brought by a system allowing natural language queries, and presents a technique to translate such requests into a formal query language.

In this article, we report on our participation in the JRS Data-Mining Challenge.
The approach used by our system is a lazylearning one, based on a simple k-nearest-neighbors technique.
We more specifically addressed this challenge as an opportunity to test Information Retrieval (IR) inspired techniques in such a data-mining framework.
In particular, we tested different similarity measures, including one called vectorization that we have proposed and tested in IR and Natural Language Processing frameworks.
The resulting system is simple and efficient while offering good performance.

Dual-wing harmoniums is a promising technique for modeling the relationship between heterogeneous data sources, like associated text and images or genetic variations and observed traits.
Unsatisfied with contrastive divergence (CD) and maximum likelihood learning, we implemented Bayesian learning in DWH using brief Langevin MCMC approach.
We proposed three different types of priors for both information retrieval and quatitative trait loci (QTL) mapping and examined the results.
Bayesian learning and different priors are evaluated by classification and retrieval task for TRECVID’03 video clips and cross validation of predicting morphological shapes of two Drosophila species.
Experiments shows that Bayesian learning gives close to CD accuracy and average precision in information retrieval, but fails on QTL prediction.
We discussed possible causes and future directions.

In many fields, for example in business, engineering, and law there is interest in the search and the classification of text documents in large databases.
To information retrieval purposes there exist methods.
They are mainly based on keywords.
In cases where keywords are lacking the information retrieval is problematic.
One approach is to use the whole text document as a search key.
Neural networks offer an adaptive tool for this purpose.
This paper suggests a new adaptive approach to the problem of clustering and search in large text document databases.
The approach is a multilevel one based on word, sentence, and paragraph level maps.
Here only the word map level is reported.
The reported approach is based on smart encoding, on Self-Organizing Maps, and on document histograms.
The results are very promising.

Query terms are used for document retrieval in information retrieval systems.
Query reweighting techniques can be used to improve the performance of information retrieval systems.
In this paper, we present a new query reweighting method for document retrieval based on neural networks for dealing with document retrieval based on the user’s relevant feedback.
The proposed query reweighting method uses the vector space model for representing documents and queries and uses neural network techniques to let the vector formed by the weights of the query terms as close as possible to the “cluster center vector” formed by document vectors of relevant feedback documents.
The proposed query reweighting method can increase the precision rate and the recall rate of information retrieval systems for dealing with document retrieval.

This article describes and evaluates various information retrieval models used to search document collections written in English through submitting queries written in various other languages, either members of the IndoEuropean family (English, French, German, and Spanish) or radically different language groups such as Chinese.
This evaluation method involves searching a rather large number of topics (around 300) and using two commercial machine translation systems to translate across the language barriers.
In this study, mean average precision is used to measure variances in retrieval effectiveness when a query language differs from the document language.
Although performance differences are rather large for certain languages pairs, this does not mean that bilingual search methods are not commercially viable.
Causes of the difficulties incurredwhen searching or during translation are analyzed and the results of concrete examples are explained.

Evaluation of retrieval performance is a crucial problem in content-based image retrieval (CBIR Many di erent methods for measuring the performance of a system have been created and used by researchers.
This article discusses the advantages and shortcomings of the performance measures currently used.
Problems such as de ning a common image database for performance comparisons and a means of getting relevance judgments (or ground truth) for queries are explained.
The relationship between CBIR and information retrieval (IR) is made clear, since IR researchers have decades of experience with the evaluation problem.
Many of their solutions can be used for CBIR, despite the di erences between the elds.
Several methods used in text retrieval are explained.
Proposals for performance measures and means of developing a standard test suite for CBIR, similar to that used in IR at the annual Text REtrieval Conference (TREC are presented.

Incorporating syntactic features in a retrieval model has had very limited success in the past, with the exception of term dependencies.
This paper presents a new term dependency modeling approach based on a dependency parsing technique used for both queries and documents.
Our model is inspired by a quasi-synchronous stochastic process for machine translation [21 It describes four different types of syntactic relationships between dependent terms and allows inexact matching between documents and queries to deal with possible syntactic transformations.
We also propose a machine learning technique for predicting optimal parameter settings for a retrieval model incorporating the syntactic relationships.
The results on TREC collections show that the quasi-synchronous dependence model can improve retrieval performance and outperform a strong state-of-art baseline when we use predicted optimal parameters.

Document re-ranking is a middle module in information retrieval system.
It&#x02019;s expected that more relevant documents with query appear in higher rankings, from which automatic query expansion can benefit, and it aims at improving the performance of the entire information retrieval.
In this paper, we construct a pseudo labeled document based on pseudo-relevance feedback principle, and discuss about the relationship between performance of document re-ranking and the number of top documents in initial retrieval, the number of key terms from the top documents when constructing a pseudo labeled document.
Experiment shows our approach of a pseudo labeled document constructed is greatly helpful to document re-ranking.
It is the main contribution in the paper.
Moreover, experiment shows the performance of document re-ranking is decreasing as the number of top documents increases; and increasing as the number of key terms from these documents increases.

This work proposes to adapt an existing general SMT model for the task of translating queries that are subsequently going to be used to retrieve information from a target language collection.
In the scenario that we focus on access to the document collection itself is not available and changes to the IR model are not possible.
We propose two ways to achieve the adaptation effect and both of them are aimed at tuning parameter weights on a set of parallel queries.
The first approach is via a standard tuning procedure optimizing for BLEU score and the second one is via a reranking approach optimizing for MAP score.
We also extend the second approach by using syntax-based features.
Our experiments show improvements of 1-2.5 in terms of MAP score over the retrieval with the non-adapted translation.
We show that these improvements are due both to the integration of the adaptation and syntax-features for the query translation task.

Digital Library; Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; GetCited; Google Scholar; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCIRUS; SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Guest Editorial Preface

The study is tailored towards investigating the level and extent of ICT based access to digital content and information by the Ugandan agricultural and development community.
The target community includes the researchers, policy makers and selected groups of farmers who have attained the level of recognising their information needs.
This very particular paper is focusing on the current ICT based digital content access systems and methods among Ugandan agricultural research and development sector.
Attempts are made, using business modelling methods and other information sciences methodologies, such as bibliometrics, to evaluate the current access models, content usability etc.
Preliminary findings indicate skewed development strategies and action interventions in the country.

One of the major problems of modern Information Retrieval (IR) systems is the vocabulary Problem that concerns with the discrepancies between terms used for describing documents and the terms used by the researcher to describe their information need.
We have implemented an automatic thesurs, the system was built using Vector Space Model (VSM
In this model, we used Cosine measure similarity.
In this paper we use selected 242 Arabic abstract documents.
All these abstracts involve computer science and information system.
The main goal of this paper is to design and build automatic Arabic thesauri using term-term similarity that can be used in any special field or domain to improve the expansion process and to get more relevance documents for the user's query.
The study concluded that the similarl thesaurus improved the recall and precision more than traditional information retrieval system in terms of recall and precision level.

In this paper a fuzzy-based recommendation method is presented.
Its main goal is to improve the recommendation recall maintaining high recommendation precision.
The formal model has been built to describe the method and to analyze how the measures used in traditional Information Retrieval may be adapted to evaluate the effectiveness of recommendation process.
The original contributions consist among others of proving several properties which show that the method is able to adapt to changing user’s needs and achieving the maximum effectiveness if the component methods work properly.

Content based image retrieval system is the technique which uses visual contents to search images from large scale image databases according to the user’s interest.
The term content refers to color, shape, texture that can be derived from the image.
In this paper an image retrieval system using artificial neural network (ANN) in MATLAB with the help of Gabor filter features is contemplated.
In the proposed system, mean and standard deviation of the images are calculated later to the filtering process of the images using Gabor filter.
Using the neural network classifier the system is trained and tested and classifies the images from a vast database relevant to the requirement.
A database having 1000 images spread across ten categories is taken for the implementation purpose.
Net average precision and recall values are computed for the database query.
The obtained results show the performance improvement with higher precision and recall values.

In this paper, we propose a retrieval method using textual information retrieval techniques, such as vector space model, for images.
Many image retrieval systems are proposed.
However, these systems are mainly based on pattern recognition techniques.
Therefore, the features of images are also based on these recognition techniques, such as color histogram, and shape of the object in images.
Generally, these systems do not consider weight of features, which means how important these features are, which are generally used in textual information retrieval systems.
In this paper, we propose a method considering weight, such as TFIDF, to identify the importance degree of features.
Using our proposed method, the system can retrieve intuitively similar retrieval target images to user's query images.

The paper summarizes the essential properties of document retrieval and reviews both conventional practice and research findings, the latter suggesting that simple statistical techniques can be effective.
It then considers the new opportunities and challenges presented by the user’s ability to search full text directly (rather than e.g. titles and abstracts and suggests appropriate approaches to doing this, with a focus on the potential role of natural language processing.
The paper also comments on possible connections with data and knowledge retrieval, and concludes by emphasizing the importance of rigorous performance testing.

Long-distance education is a very important teaching method now.
The foundation of the long distance is the construction of educational resources database.
The key work of constructing resources database is to structure an appropriate index.
The paper studies the representation of the characters of multimedia educational resources and the relations between the hierarchy characters.
A hierarchy index is built to satisfy all kinds of queries to the educational resources database system.
The subject ontology is used to get a standard annotation of the resources and a standard description of the query requirement.
The subject ontology is also used to extend the semantics of the query requirement.
And the mapping rules of the hierarchy characters provide a way to represent the semantics of the resource automatically.

a Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China b Centre for Quantum Computation Intelligent Systems, University of Technology Sydney, Broadway, NSW 2007, Australia c Research Center on Fictitious Economy and Data Science, Chinese Academy of Sciences, Beijing, China d College of Information Science Technology, Univ.
of Nebraska at Omaha, Omaha, NE 68182, USA e School of Computer Science Information Eng Hefei University of Technology, Hefei 230009, China f Department of Computer Science, University of Vermont, Burlington, VT

In this paper, we propose four peer-to-peer models for contentbased music information retrieval (CBMIR) and carefully evaluate them on network load, retrieval time, system update and robustness qualitatively and quantitatively.
And we bring forward an algorithm to improve the speed of CBP2PMIR and a simple but effective method to filter out the replica in the final results.
And we present the architecture of QUIND, a content-based peer-topeer music information retrieval system, which can implement CBMIR.
QUIND combines content-based music information retrieval technologies and peer-to-peer environments, and has strong robustness and good expansibility.
Music stored and shared on each PC makes up of the whole available music resource.
When a user puts forward a music request, e.g. a song or a melody, QUIND can retrieve a lot of similar music quickly and accurately according to the content of music.
After the user selects his favorite ones, he can download and enjoy them.

Communication and collaboration with other people is a major theme in the information seeking process.
Collaborative querying addresses this issue by sharing other users’ search experiences to help users formulate appropriate queries to a search engine.
This paper describes a collaborative querying system that helps users with query formulation by finding previously submitted similar queries through mining web logs.
The system operates by clustering and recommending related queries to users using a hybrid query similarity identification approach.
The system employs a graph-based approach to visualize the query recommendations.

Whenever digital libraries or knowledge management systems are to be automatically filled with web pages from the internet, document classification of the web pages is one of the major challenges.
We present an approach which uses HTML tags in order to improve the quality of the hypertext document classification.
Our approach uses weighting of HTML tags for separating relevant information in hypertext documents from the noise.
We have evaluated our approach on the basis of a document classification algorithm.
The results show that our weighting approach yields a classification which is approximately 35% better than a classification without the use of the HTML tagging information.

*School of Computer Science and Information Technology
Northeast Normal University Changchun 130117, P. R. China Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education Jilin University, Changchun 130012, P. R. China School of Natural and Computing Sciences University of Aberdeen, Aberdeen, AB24 3UE, UK §School of Computer and Software Nanjing University of Information Science and Technology Nanjing 210044, P. R. China College of Computer Science and Technology, Jilin University
Changchun 130012, P. R. China jinchao0374@mail.com
pangwei@abdn.ac.uk
yanlinzheng@nenu.edu.cn wz2000@jlv.edu.cn mazq0431@gmail.com

AUTOCORRELATION AND REGULARIZATION OF QUERY-BASED INFORMATION RETRIEVAL SCORES

Statistical language models have been proposed recently for several information retrieval tasks, including the resource selection task in distributed information retrieval.
This paper extends the language modeling approach to integrate resource selection, ad-hoc searching, and merging of results from different text databases into a single probabilistic retrieval model.
This new approach is designed primarily for Intranet environments, where it is reasonable to assume that resource providers are relatively homogeneous and can adopt the same kind of search engine.
Experiments demonstrate that this new, integrated approach is at least as effective as the prior state-of-the-art in distributed IR.

In these days WEKA has become one of the most important data mining and machine learning tools.
Despite the fact that it incorporates many algorithms, on the classification area there are still some unimplemented features.
In this paper we cover some of the missing features that may be useful to researchers and developers when working with decision tree classifiers.
The rest of the paper presents the design of a package compatible with the WEKA Package Manager, which is now under development.
The functionalities provided by the tool include instance loading, successor/predecessor computation and an alternative visualization feature of an enhanced decision tree, using the J48 algorithm.
The paper presents how a new data mining/machine learning classification algorithm can be adapted to be used integrated in the workbench of WEKA.

We have investigated the potential use of question answering systems and participated in the Question Answering Challenge (QAC) of National institute of informatics Test Collection for Information Retrieval systems (NTCIR
In this paper, we describe our question answering system, the preliminary results of our experiments to the contest and some possible improvement to question answering systems.

Desktop Search, the search across local storage such as a personal computer, is a common practice among computer users.
There has been much activity in Web-related Information Retrieval, but Desktop Search has only recently increased in popularity.
As the structure and accessibility of data in a local environment is different to the Web, new algorithmic possibilities arise for Desktop Search.
We apply a connectivity analysis approach to the local environment— a filesystem.
We describe how it can be used in parallel with existing tools to provide “more useful” ranked results.
Our evaluation reveals that such an approach has promise, and we conclude that exploiting the organization of a filesystem is beneficial for Desktop Search.

The task of document retrieval systems is to match one natural language query against a large number of natural language documents.
Neural networks are known to be good pattern matchers.
This paper reports our investigations in implementing a document retrieval system based on a neural network model.
It shows that many of the standard strategies of information retrieval are applicable in a neural network model.
Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery.
To copy otherwise, or to republish, requires a fee and/or specific permission.
n 1991 ACM 0-89791 -448 -1/91

Session search is an information retrieval task that involves a sequence of queries for a complex information need.
It is characterized by rich user-system interactions and temporal dependency between queries and between consecutive user behaviors.
Recent efforts have been made in modeling session search using the Partially Observable Markov Decision Process (POMDP To best utilize the POMDP model, it is crucial to find suitable definitions for its fundamental elements States, Actions and Rewards.
This paper investigates the best ways to design the states, actions, and rewards within a POMDP framework.
We lay out available design options of these major components based on a variety of related work and experiment on combinations of these options over the TREC 2012 2013 Session datasets.
We report our findings based on two evaluation aspects, retrieval accuracy and efficiency, and recommend practical design choices for using POMDP in session search.

Overview and Phase
The IFRI database with 15 years of time series data about forests worldwide, together with the SES ontological framework that Elinor Ostrom is pioneering offers a rich application for computer and information science research investigation in two areas.
The first investigates the in-situ archive as a software framework that brings archive functionality to the research lab.
An in-situ archive operates hand in hand with a relational database to provide scientific databases with a solution to archiving that preserves as much of the application domain model as possible in order to allow familiar discovery and access processes over the archived copy.
The second investigates how to provide semantically rich and feature based access to a legacy relational database.

In this work, we propose a method to identify and transcript the note of a Carnatic music signal.
The main motive behind note transcription is that, it can be used as a good basis for music note information retrieval of Carnatic music songs or Film songs based on Carnatic music.
The input monophonic music signal is analysed and made to pass through a signal frequency extracting algorithm.
The frequency components of the signal are then mapped into the swara sequence, which could be used to determine the Raga of the particular song and can be used in Carnatic music training institutes to verify the correctness of the Carnatic music note.
General Terms Artificial Neural Network, Note Identification, Digital Signal Processing.

Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora.
Two kinds of methods are proposed: offline data optimization and online model optimization.
The offline method adapts the training data by redistributing the weight of each training sentence pairs.
The online method adapts the translation model by redistributing the weight of each predefined submodels.
Information retrieval model is used for the weighting scheme in both methods.
Experimental results show that without using any additional resource, both methods can improve SMT performance significantly.

The work presented in this paper focus on Knowledge Management services enabling CSCW (Computer Supported Cooperative Work) applications to provide an appropriate adaptation to the user and the situation in which the user is working.
In this paper, we explain how a knowledge management system can be designed to support users in different situations exploiting contextual data, users’ preferences, and profiles of involved artifacts (e.g documents, multimedia files, mockups
The presented work roots in the experience we had in the MILK project and early steps made in the MAIS project.
Keywords—Information Management Systems, Information Retrieval, Knowledge Management, Mobile Communication Systems.

In our fourth participation in the CLEF evaluation campaigns, our objective was to verify whether our combined query translation approach would work well with new requests and new languages (Russian and Portuguese in this case As a second objective, we were to suggest a selection procedure able to extract a smaller number of documents from collections that seemed to contain no or only a few relevant items for the current request.
We also applied different merging strategies in order to obtain more evidence about their respective relative merits.

Search engines process queries conjunctively to restrict the size of the answer set.
Further, it is not rare to observe a mismatch between the vocabulary used in the text of Web pages and the terms used to compose the Web queries.
The combination of these two features might lead to irrelevant query results, particularly in the case of more specific queries composed of three or more terms.
To deal with this problem we propose a new technique for automatically structuring Web queries as a set of smaller subqueries.
To select representative subqueries we use information on their distributions in the document collection.
This can be adequately modeled using the concept of maximal termsets derived from the formalism of association rules theory.
Experimentation shows that our technique leads to improved results.
For the TREC-8 test collection, for instance, our technique led to gains in average precision of roughly 28% with regard to a BM25 ranking formula.

Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains.
In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms.
We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance.
We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques.
Our preliminary experimental results prove the viability of our approach.

We examine various computational accounts of aspects of music understanding.
These accounts involve programs which can notate melodies based on pitch and duration information.
It is argued that this task involves significant musical intelligence.
In particular, it requires an understanding of basic metric and harmonic relations implicit in the melody.
We deal only with single-voice, tonal melodies.
While the task is a limited one, and the programs give only partial solutions to this task, we argue that this represents a first step towards a computational realization of significant aspects of musical intelligence.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-91-66.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/435 Computational Accounts of Music Understanding MS-CIS-91-66
LINC LAB-207

There are many challenges concerning online privacy.
Private information retrieval (PIR) tries to find solution to following problem.
Server has a database of n records and client wants to retrieve w-th record without revealing w to the server.
PIR schemes can be described for single-server and multi-server case (where each server holds identical copy of database Current report reviews a multi-server scheme proposed by Fanti and Ramchandran [1
They adapt a previously described scheme [2] by adding ability to handle some number of unsynchronized records.
Scheme also allows some servers to collaborate without discovering w.

“Cases on Global E-Learning Practices: Successes and Pitfalls” look into the global practices of e-learning which has assumed a considerable role in the education sector recently.
It is edited by Ramesh C. Sharma and Sanjaya Mishra from Indira Gandhi National Open University in New Delhi, India.
It is published by Information Science Publishing, which is an imprint of Idea Group, an international publishing company specializing in research publications in the fields of technology, management and information science.

Information Retrieval has become common-place with the advent of the Internet with most Internet users being familiar with the use of search engines such as Google, Yahoo, and the like.
Indeed, with the amount of information on the Internet growing so rapidly, search engines are crucial in finding what we want.
The problem is the interface used for performing such searches has changed very little from those proposed in the 1950s, despite many alternative interfaces being proposed and developed by various researchers.
Users still provide a list of keywords, and are presented with a list of pages that contain those keywords, and are faced with the task of scanning the result list one-byone for the information they seek.
This paper enumerates some of the main alternatives to ranked list result displays, discusses their advantages and disadvantages, and discusses how researchers have evaluated their interfaces.

In translingual information retrieval (TIR ad hoc queries in any of a set of languages can be used to retrieve documents in any of a set of languages.
Classical informationretrieval methods such as the vector-space model cannot be applied to TIR because they base similarity on the overlap of terms between queries and documents|this is typically zero in TIR.
The generalized vector-space model (GVSM) and latent semantic indexing (LSI) are two variations of the vector-space model that make comparisons outside of term space.
For this reason, both can be and have been applied to TIR.
In this paper, we report on a series of experiments comparing the performance of GVSM and LSI on monolingual and translingual retrieval tasks.
We nd that the performance of both methods depends crucially on parameter settings, that LSI performs better, and that GVSM runs more quickly.

Fekadu Yadetie,1 Astrid Laegreid,1 Ingunn Bakke,1 Waclaw Kusnierczyk,2 Jan Komorowski,3 Helge L. Waldum,1 and Arne K.
1Department of Cancer Research and Molecular Medicine, Faculty of Medicine, Norwegian University of Science and Technology, N-7489 Trondheim, Norway;
2Department of Computer and Information Science, Norwegian University of Science and Technology, N-7491 Trondheim, Norway; 3The Linnaeus Center for Bioinformatics, Uppsala University, SE-7551 24 Uppsala, Sweden

In Information Retrieval (IR whether implicitly or explicitly, queries and documents are often represented as vectors.
However, it may be more beneficial to consider documents and/or queries as multidimensional objects.
Our belief is this would allow building “truly” interactive IR systems, i.e where interaction is fully incorporated in the IR framework.
The probabilistic formalism of quantum physics represents events and densities as multidimensional objects.
This paper presents our first step towards building an interactive IR framework upon this formalism, by stating how the first interaction of the retrieval process, when the user types a query, can be formalised.

Manifold alignment has been found to be useful in many areas of machine learning and data mining.
In this paper we introduce a novel manifold alignment approach, which differs from “semisupervised alignment” and “Procrustes alignment” in that it does not require predetermining correspondences.
Our approach learns a projection that maps data instances (from two different spaces) to a lower dimensional space simultaneously matching the local geometry and preserving the neighborhood relationship within each set.
This approach also builds connections between spaces defined by different features and makes direct knowledge transfer possible.
The performance of our algorithm is demonstrated and validated in a series of carefully designed experiments in information retrieval and bioinformatics.

(8) Patentblatt, 1958-1966.
Baltimore, Md 1966 9) Tokkyo Koho, 1958-1966 1 3 Information Retrieval Among Patent Offices-
Tokyo Meeting BIRPI, Geneva, Switzerland,
) INPADOC Patent Family Service 1 5 Patents-
A Source of Technical Information British Patent Office, Department of Commerce, 1974 1 6 International Patent Classification 2nd ed, WIPO.
Morgan-Grampian Ltd London, England, 1974 10)
Official Gazette of the United States Patent and Trademark Office 1 1 Third Annual Meeting of the Committee for International Retrieval Among Examining Patent Offices-
ICIREPAT Spartan Books, Baltimore, Md 1964
12 Fourth Annual Meeting of the Committee for International Retrieval Among Examining Patent Offices-ICIREPAT Spartan Books,

This thesis discusses information retrieval from multimedia archives, focusing on documents containing visual material.
We investigate search and retrieval in collections of images and video, where video is defined as a sequence of still images.
No assumptions are made with respect to the content of the documents; we concentrate on retrieval from generic, heterogeneous multimedia collections.
In this research area a user's query typically consists of one or more example images and the implicit request is Find images similar to this one
In addition the query may contain a textual description of the information need.
The research presented here addresses three issues within this area.

Intelligent information retrieval tools can help intelligence and security agencies to retrieve and exploit relevant information from unstructured information sources and give them insight into the criminal behavior and networks, in order to fight crime more efficiently and effectively.
This article aims at analysing off-the-shelf information extraction tools on their applicability and competency for such applications.

The e-book reader revolution is here already.
The questions we asked ourselves were: what are the reading preferences of information science students at the beginning of the second decade of the 21 century?
How do different variables such as relative advantage, comprehension and learning strategies affect students' reading preferences?
The research was conducted in Israel during the first semester of the 2015 academic year and encompassed 177 LIS students in the Information Science Department in Israel.
Three questionnaires were used: personal details, relative advantage, and learning strategies, and two further questions that focus on reading habits.
The study showed students' preferences of printed materials.
In addition, it emphasizes the importance of personal variables that may affect students' will to read electronic materials: relative advantage and comprehension.

The paper presents a formal approach to the semantic interpretation of scientific and technical texts.
Semantic proximity expression by means of a linguistic variable and its application to the information retrieval and documents classification are shown.

We describe Amharic-English cross lingual information retrieval experiments in the adhoc bilingual tracs of the CLEF 2006.
The query analysis is supported by morphological analysis and part of speech tagging while we used different machine readable dictionaries for term lookup in the translation process.
Out of dictionary terms were handled using fuzzy matching and Lucene[4] was used for indexing and searching.
Four experiments that differed in terms of utilized fields in the topic set, fuzzy matching, and term weighting, were conducted.
The results obtained are reported and discussed.

PC stands for path-conjunctive, the name of a class of queries and dependencies that we define over complex values with dictionaries.
This class includes the relational conjunctive queries and embedded dependencies, as well as many interesting examples of complex value and oodb queries and integrity constraints.
We show that some important classical results on containment, dependency implication, and chasing extend and generalize to this class.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-98-24.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/135

Recently, the geolocalisation of tweets has become an important feature for a wide range of tasks in Information Retrieval and other domains, such as real-time event detection, topic detection or disaster and emergency analysis.
However, the number of relevant geo-tagged tweets available remains insufficient to reliably perform such tasks.
Thus, predicting the location of non-geotagged tweets is an important yet challenging task, which can increase the sample of geo-tagged data and help to a wide range of tasks.
In this paper, we propose a location inference method that utilises a ranking approach combined with a majority voting of tweets weighted based on the credibility of its source (Twitter user Using geo-tagged tweets from two cities, Chicago and New York
(USA our experimental results demonstrate that our method (statistically) significantly outperforms our baselines in terms of accuracy, and error distance, in both cities, with the cost of decrease in recall.

Annie Dai, David Palensky, Alex Piatski, Kendall Queen, Gina Vockeroth, Graduate assistant: Teresa Lebair, Faculty mentor: Bradford E. Peercy, Clients: Margaret Watts and Arthur Sherman Department of Mathematics, Northeastern University Department of Mathematics, University of Maryland, College Park Department of Computer and Information Science, University of Pennsylvania Department of Computer Science and Electrical Engineering, UMBC Department of Mathematics and Statistics, Loyola University Chicago Department of Mathematics and Statistics, UMBC Laboratory of Biological Modeling, National Institutes of Health

Information Retrieval, Summarization and Question Answering need accurate linguistic information with a much higher coverage than what is being recently offered by currently available parsers, so we assume that the starting point of any interesting application in those fields must necessarily be a good syntactic -semantic parser.
The system presented in the paper has undergone extensive testing and the parser has been trained on available testsuites and the Remedia corpus texts, one of which will be commented in some detail.

This book is one of the Information Sciences series published by John Wiley and is meant for "Users, Managers, and Technologists
The objective is to give a set of guidelines for implementing "complex" information systems.
Complex information systems are defined as those "functioning within a multipurpose, multiuser environment whether they are relatively simple data banks, more involved accounting or banking/credit systems, documentation systems, or middle or higher level management decision systems
Though the definition includes "management decision systems the book focuses upon information storage and retrieval systems.
Four of the five case studies presented are on the computerization of documentation systems.
No mention has been made of simulation, modelling, optimization packages, etc which are essential components of any information support system for managerial decision making, nor is the general problem of management information system design covered.

Detecting the spatial objects is an important research agenda for geospatial information science.
Appling object-oriented image classification to extract GIS features, which fulfills the needs of updating the geospatial databases with remote sensing imagery will greatly enhance the ongoing digital city construction and national condition monitoring.
This paper describes the key technology for high spatial resolution image feature extraction.
Utilizing object-oriented feature extraction and classification with ADS40 aerial imagery, various surface features are obtained satisfactorily.

This paper reports the use of a document distance-based approach to automatically expand the number of available relevance judgements when these are limited and reduced to only positive judgements.
This may happen, for example, when the only available judgements are extracted from a list of references in a published review paper.
We compare the results on two document sets: OHSUMED, based on medical research publications, and TREC-8, based on news feeds.
We show that evaluations based on these expanded relevance judgements are more reliable than those using only the initially available judgements, especially when the number of available judgements is very limited.

Edith A. Scarletto is Assistant Professor, Head Map Library, and Subject Librarian for Geography and Geology in the University Library at Kent State University; e-mail: escarlet@kent.edu 2014 Edith A. Scarletto, Attribution-NonCommercial (http creativecommons.org/licenses/by-nc/3.0 CC BY-NC
This study analyzed citations in four journals, Annals of the Association of American Geographers, Cartography and Geographic Information Science, International Journal of Geographical Information Science, and Cartographic Journal, using Bradford’s Law of Scattering to identify three influence zones indicating core and peripheral titles in the study areas of GIS.
Journals were ranked resulting in twenty-three core journals and 187 secondary journals.
Scores for relevant indexing/abstracting services are also given to describe access points and coverage.
The results can assist librarians and collection managers to support research in their institutions where GIS is both used and studied.

The associative Hopfield memory is a form of recurrent Artificial Neural Network (ANN) that can be used in applications such as pattern recognition, noise removal, information retrieval, and combinatorial optimization problems.
In general, ANNs are considered as intrinsically fault-tolerant.
A study of the capability of this algorithm to tolerate transient faults such as bit-flips provoked by the radiation environment is presented.
Two software versions of the Hopfield Neural Network (HNN one original and one fault-tolerant were implemented and executed by a LEON3 processor.
Experimental results show the efficiency of the adopted strategy to tolerate faults that were injected at hardware level.

Evaluating the quality of ranking functions is a core task in web search and other information retrieval domains.
Because query distributions and item relevance change over time, ranking models often cannot be evaluated accurately on held-out training data.
Instead, considerable effort is spent on manually labeling the relevance of query results for test queries in order to track ranking performance.
We address the problem of estimating ranking performance as accurately as possible on a fixed labeling budget.
Estimates are based on a set of most informative test queries selected by an active sampling distribution.
Query labeling costs depend on the number of result items as well as item-specific attributes such as document length.
We derive cost-optimal sampling distributions for the commonly used performance measures Discounted Cumulative Gain and Expected Reciprocal Rank.
Experiments on web search engine data illustrate significant reductions in labeling costs.

The important growth of available multimedia information shows the limits of traditional information retrieval systems.
In this paper, we describe the design of a metadata engine dedicated to retrieve annotated images using meta-information, and part of the more general MediaSys Image Search Engine.
As a case study, we use medicinal plant images and their detailed description.
We propose a flexible approach based on fuzzy concepts such as the fuzzy subset theory, the possibility theory and fuzzy thesauri Keywords]
Metadata Engine, Medicinal Plants, Information Retrieval, Fuzzy Logic, Possibility Theory, Thesaurus

Features Ranked List Similarity Ranker Abstract Features Database

The Z39.50 Information Retrieval Protocol is a proposed ANSI standard for the retrieval of bibliographic information that originates from the library community in the United States.
Unlike previous interlibrary loan protocols designed by the library community, Z39.50 is intended to be the basis of an end-user service.
This paper provides a brief introduction to Z39.50, then discusses the shortcomings of the protocol that were discovered as a result of implementation experience.

Case-Based Reasoning (CBR) and Information Retrieval (IR) are two historically disjoint technologies.
This paper contends that the time has now come for CBR and IR to be conjoined in order to collaboratively solve many of the text retrieval problems faced by modern companies.
In particular, it will be argued that the two technologies fundamentally differ in terms of the type of information retrieval queries they allow to be answered.
An attempt will be made to define what types of query remain problematic for both techniques, yet may benefit from a combined approach.

Prefix search is a fundamental operation in information retrieval, but it is difficult to implement in a peer-to-peer (P2P) network.
Existing techniques for prefix search suffer from several problems: increased storage/index costs, unbalanced load, fault tolerance, hot spot, and lack of some ranking mechanism.
In this paper, we present KISS (Keytokenbased Index and Search Scheme
a simple and novel approach for prefix search to overcome the above problems.

Based on previous experience from working on a task-based search engine, we present a list of suggestions and ideas for an Information Retrieval (IR) framework that could inform the development of next generation professional search systems.
The specific task that we start from is the clinicians’ information need in finding rare disease diagnostic hypotheses at the time and place where medical decisions are made.
Our experience from the development of a search engine focused on supporting clinicians in completing this task has provided us valuable insights in what aspects should be considered by the developers of vertical search engines.

In this paper we present results of experiments with Chinese word segmentation and information retrieval.
Our experiments with three different word segmentation algorithms indicate that accurate segmentation measurably improves retrieval performance.
We discuss the evaluation of word segmentation algorithms for the purpose of better indexing segmented texts for retrieval.

This paper reports progress in the development of free text retrieval systems for the Fujitsu AP1000.
Work is now focussed on the classical information retrieval problem, that of retrieving documents or articles relevant to a user's query.
The current version of ftr permits use of the AP1000's DDV options for storing text bases, resulting in signi cant decreases in loading times.
A new graphical user interface (called retrieve and based on tcl) which provides a user-friendly mechanism for invoking the ftr system from remote workstations, specifying and carrying out searches, and selecting, retrieving, viewing, and storing whole entries from the text base being searched.
A range of useful tools is being developed for text base administration purposes.
Initial performance results are presented, and likely future directions for the work are outlined.

Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text.
This paper reports on the application of a few simple, yet robust and efficient nounphrase analysis techniques to create better indexing phrases for information retrieval.
In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics.
Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system.
The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction.

Random Forest (RF) classification algorithm is widely used in the area of information retrieval and became a basis for some extended branches of classification and/or regression algorithms.
Cluster Forest (CF) represents a particular branch, and brings usually better results than individual clustering algorithms.
This article describes a new ensemble clustering algorithm based on CF that internally uses a probabilistic model called Gaussian Mixture Model (GMM Finally, Expectation-maximization algorithm is used for estimation of GMM parameters.
The proposed ensemble clustering algorithm will be compared with several different approaches and tested on eight datasets.

In this paper we discuss the integration of different GIR systems by means of a fuzzy Borda method for result fusion.
Two of the systems, the one by the Universidad Politécnica de Valencia and the one of the Universidad of Jaén participated to the GeoCLEF task under the name TextMess.
The proposed result fusion method takes as input the document lists returned by the different systems and returns a document list where the documents are ranked according to the fuzzy Borda voting scheme.
The obtained results show that the fusion method allows to improve the results of the component systems, although the fusion is not optimal, because it is effective only if the components return a similar set of relevant documents.

With the widespread adoption of online social networks as a crucial means for communication, social information retrieval is becoming one of the most interesting areas of research in terms of the large number of–theoretical and practical–issues that it encompasses.
We argue that agent technology is central in supporting the decentralization of next generation online social networks and the synergistic pairing of agents and social networks is evident, if nothing else, because members of a social network interact as agents do in a multiagent system.
In this paper we investigate the possibilities that agent technology can offer to social information retrieval and we emphasize the role that agents and multi-agent systems can play by presenting Blogracy, an agent-based online social network

This paper reports an experiment to evaluate a Cross Language Information Retrieval (CLIR) system that uses a multilingual ontology to improve query translation in the travel domain.
The ontology-based approach significantly outperformed the Machine Readable Dictionary translation baseline using Mean Average Precision as a metric in a user-centered experiment.

Reflecting the rapid growth in the utilization of large test collections for information retrieval since the 1990s, extensive comparative experiments have been performed to explore the effectiveness of various retrieval models.
However, most collections were intended for retrieving newspaper articles and technical abstracts.
In this paper, we describe the process of producing a test collection for patent retrieval, the NTCIR-3 Patent Retrieval Collection, which includes two years of Japanese patent applications and 31 topics produced by professional patent searchers.
We also report experimental results obtained by using this collection to re-examine the effectiveness of existing retrieval models in the context of patent retrieval.
The relative superiority among existing retrieval models did not significantly differ depending on the document genre, that is, patents and newspaper articles.
Issues related to patent retrieval are also discussed.

For querying structured and semistructured data, data retrieval and document retrieval are two valuable and complementary techniques that have not yet been fully integrated.
In this paper, we introduce integrated information retrieval (IIR an XML-based retrieval approach that closes this gap.
We introduce the syntax and semantics of an extension of the XQuery language called XQuery/IR.
The extended language realizes IIR and thereby allows users to formulate new kinds of queries by nesting ranked document retrieval and precise data retrieval queries.
Furthermore, we detail index structures and efficient query processing approaches for implementing XQuery/IR.
Based on a new identification scheme for nodes in node-labeled tree structures, the extended index structures require only a fraction of the space of comparable index structures that only support data retrieval.

This paper briefly examines certain of the Intelligent Information Retrieval (IIR) mechanisms used in the RESEDA system, a system equipped with 8220;reasoning&#8221; capabilities in the field of complex biographical data management.
Particular attention is paid to a description of the different 8220;levels&#8221; of inference procedure which can be executed by the system.
The intention is to show that the technical solutions to IIR problems implemented in RESEDA are of an equivalent level to those now proposed in the same field by the Japanese project for Fifth Generation Computer Systems.

Currently, the success of data services used through digital mobile phone networks is very limited.
Different reasons can be identified for this: At first, the costs for data connections through these wireless networks are extremely high.
Secondly, the user handling of the physically constrained handheld terminals appears as very uncomfortable.
Here, a concept for customer-centred information services is proposed, which meets the limited capabilities of the terminal devices.
An adequate UI is presumed to make the use of data services on mobile digital phones as also on PDAs more convenient.
Furthermore, the information access speed is increased and the costs for the information retrieval are reduced by the described concept.

While neural network approaches are achieving breakthrough performance in the natural language related elds, there have been few similar aempts at mathematical language related tasks.
In this study, we explore the potential of applying neural representation techniques to Mathematical Information Retrieval (MIR) tasks.
In more detail, we rst briey analyze the characteristic dierences between natural language and mathematical language en we design a “symbol2vec” method to learn the vector representations of formula symbols (numbers, variables, operators, functions, etc
Finally, we propose a “formula2vec” based MIR approach and evaluate its performance.
Preliminary experiment results show that there is a promising potential for applying formula embedding models to mathematical language representation and MIR tasks.

There has been a great deal of research into the use of Information Retrieval (IR)-based techniques to support concept location in source code.
Much of this research has been focused on determining how to use various IR techniques to support concept location.
Very little attention has been given to the effect of different configurations of corpus building and indexing on query results.
In this paper, we propose a tool designed to support large-scale studies of IR techniques in varying configurations of parameters with the intention of automatically calibrating these parameters.
We also discuss preliminary efforts to create the benchmark data such studies require.

This paper analyzes the properties, structures and limitations of vector-based models for information retrieval from the computational geometry point of view.
It is shown that both the pseudo-cosine and the standard vector space models can be viewed as special cases of a generalized linear model.
More importantly, both the necessary and sufficient conditions have been identified, under which ranking functions such as the inner-product, cosine, pseudo-cosine, Dice, covariance and product-moment correlation measures can be used to rank the documents.
The structure of the solution region for acceptable ranking is analyzed and an algorithm for finding all the solution vectors is suggested.

We enhance and suppress the excitonic lifetime of an InAs quantum dot (QD) by modifying its alignment with a resonance cavity in a photonic crystal slab.
The QD/cavity system has applications in quantum information science.

Schema matching is the task of finding semantic correspondences between elements of two schemas.
It takes two schemas as input and returns a mapping that identifies corresponding elements in the two schemas.
Schema matching is an important and vital step in many schema and data translation and integration applications, such as integration of web data sources, data warehousing, XML message mapping etc.
In this paper, we describe different characteristics exhibited by matching element pairs at various levels in the XML schema and propose a system which uses these characteristics to perform the matching process.
The novelty in the system is the architecture of the system which comprises of a linguistic matcher in combination with a set of filters operating based on the level of the element pairs to be matched in the source and target XML schemas.
General Terms Datamining, Datawarehousing, Information Retrieval, XML Message Mapping

The PIKM workshop gives Ph.D. students an opportunity to present their dissertation proposals at a global stage.
Similarly to the CIKM, the PIKM workshop covers a wide range of topics in the areas of databases, information retrieval and knowledge management.
Interdisciplinary work across these tracks is particularly encouraged.

The goal of many natural language processing platforms is to be able to someday correctly treat all languages.
Each new language, especially one from a new language family, provokes some modification and design changes.
Here we present the changes that we had to introduce into our platform designed for European languages in order to handle a Semitic language.
Treatment of Arabic was successfully integrated into our cross language information retrieval system, which is visible online.

The Internet and World Wide Web are becoming more and more dynamic in terms of their content and use.
Information retrieval (IR) efforts aim to keep up with this dynamic environment by designing intelligent systems which can deliver Web content in real time to various wired or wireless devices.
Evolutionary and adaptive systems (EASs) are emerging as typical examples of such systems.
This paper contains one of the first attempts to gather and evaluate the nature of current research on Web-based IR using EAS and proposes future research directions in parallel to developments on the Web environments.

Information Retrieval (IR) in the Electronic Health Record (EHR) should provide healthcare professionals with the right information to the right person at the right time and place and should reduce the hard tasks of manual information retrieval from papers or from computer.
In this context, the objective of this study was to describe the features of a semantic search engine implemented in an EHR.
In this paper, we describe a flexible and scalable object-oriented query language designed for retrieving and viewing data which support any data model.
This search engine deals with structured and unstructured data, on a unique patient in the context of care, and on N patients in the context of epidemiology.
In this study, we tested several types of queries on a test databases containing 2,000 anonymized patients and about 200,000 records.
Dossier du patient informatisé recherche d’information indexation automatique

— Several techniques are proposed to retrieve the most relevant HTML documents to user query.
Among these techniques is the genetic algorithm which iteratively creates several generations using selection, crossover and mutation before producing the final result.
In this paper, a new hybrid crossover technique is proposed to enhance the quality of the retrieved results.
This technique is applied to HTML documents and evaluated using recall, precision and recall-precision measures.
Its performance is compared to three well known techniques of crossover.
The results show high improvement in the quality of the retrieved documents in terms of these measures.

Multi-project is an emerging field of specialization in a number of professions, including Information Science (IS This paper deals with the development of a new approach for supporting the improvement of information management and the overall information systems infrastructure of multi-project management, in particular, the paper discusses the application of lean thinking to information management.

Zusammenfassung
In diesem Beitrag wird ein neuer Ansatz für die Modellierung und den Entwurf interaktiver Information-Retrieval-Systeme vorgestellt, der einerseits eine enge Integration von Syntax, Semantik und Layout der verwalteten Informationsobjekte verfolgt und andererseits den interaktiven Informationsdialog mit Methoden der Informationsvisualisierung unterstützt.

GeoFEM is a parallel nite element analysis system intended for multi-physics/multi-scale problems and is being developed at RIST.
Within \Earth Simulator" project, the GeoFEM group will deal with the modeling and simulation of solid earth eld phenomena, and the development of large-scale parallel software for the \Earth Simulator Since there are models which are not completely established in the solid earth eld, the simulation must be carried out on a trial and error basis.
Therefore, a joint venture with the geo-science modeling research group is crucial for the development of a targeted simulation software system.
This project is expected to be a breakthrough in bridging the geoscience and information science elds.
When complete, this software system will be able to solve problems in the scale of 100 million degree of freedoms.

Perusal of text documents and articles is a central process of research in many fields and disciplines.
However, there are often vast amounts of literature for any given area of discourse.
Thus, the process of extracting useful information from textual sources encompasses not only the review of a large numbers of texts in a general area, but also the filtering and selection of texts relevant to one's research from this multitude of documents.
As online electronic document repositories become increasingly prevalent it is important that tools be developed to to support their use so that researchers can and extract and filter the information they need from these document repositories.
This scenario of finding relevant documents in a large corpus is a specific example of the kinds of issues dealt with in the field of information retrieval.

In this paper, we describe Terrier, a high performance and scalable search engine that allows the rapid development of large-scale retrieval applications.
We focus on the opensource version of the software, which provides a comprehensive, flexible, robust, and transparent test-bed platform for research and experimentation in Information Retrieval (IR).

Latent semantic indexing (LSI) has been shown to be extremely useful in information retrieval, but it is not an optimal representation for text classification.
It always drops the text classification performance when being applied to the whole training set (global LSI) because this completely unsupervised method ignores class discrimination while only concentrating on representation.
Some local LSI methods have been proposed to improve the classification by utilizing class discrimination information.
However, their performance improvements over original term vectors are still very limited.
In this paper, we propose a new local LSI method called "local relevancy weighted LSI" to improve text classification by performing a separate single value decomposition (SVD) on the transformed local region of each class.
Experimental results show that our method is much better than global LSI and traditional local LSI methods on classification within a much smaller LSI dimension.

The article presents a model of the structural properties of virtual communities and the information they can access.
It argues that a large part of the information and actually knowledge present in virtual communities can be identified by a graph structure that consists of three node types actors, media and qualities as well as the relations that connect them.
Based on these relations, information retrieval and other inference mechanisms can be mapped into the model.

The presence of traceability links between software artefacts is very important to achieve high comprehensibility and maintainability.
This is confirmed by several researches and tools aiming at support traceability link maintenance and recovery.
We propose to use traceability information combined with Information Retrieval techniques within an Eclipse plug-in to show the software engineer the similarity between source code components being developed and the high level artefacts they should be traced on.
Such a similarity suggests actions aiming at improving the correct usage of identifiers and comments in source code and, as a consequence, the traceability and the comprehensibility level.
The approach and tool have been assessed with a controlled experiment performed with master students

In this paper we study cross-language information retrieval using a bilingual topic model trained on comparable corpora such as Wikipedia articles.
The bilingual Latent Dirichlet Allocation model (BiLDA) creates an interlingual representation, which can be used as a translation resource in many different multilingual settings as comparable corpora are available for many language pairs.
The probabilistic interlingual representation is incorporated in a statistical language model for information retrieval.
Experiments performed on the English and Dutch test datasets of the CLEF 2001-2003 CLIR campaigns show the competitive performance of our approach compared to cross-language retrieval methods that rely on pre-existing translation dictionaries that are hand-built or constructed based on parallel corpora.

As lack of semantic for information description and semantic support for the query processing, traditional Blog systems are unable to satisfying users in the performance of information organization and retrieval.
Through analyzing the existing technologies in Blog systems and focusing on semantic retrieval of Blog information resource, this paper proposes Blog information retrieval model based on semantic.
In order to implement semantic description for Blog information resource, we design a Blog ontology and a domain subject classification ontology.
With predefined rules, we put forward a class hierarchy tree generating algorithm, expand Blog classification item semantic retrieval, and finally implement semantic retrieval by SPARQL query.

Indexing allows converting raw document collection into easily searchable representation.
Bigger scale indexing poses some challenges such as how to distribute indexing computation efficiently on a cluster of nodes.
MapReduce framework can be an effective tool for parallelizing such tasks as inverted index construction.
We propose SciPDFindexer, distributed information retrieval system for scientific articles in PDF.
For given large collection of scientific articles in PDF our system parses and extracts metadata from articles, and then indexes extracted content using our proposed scheme.
Our contribution is the design of distributed IR system and indexing scheme that improve the overall indexing performance.

Aural queries, often called “query by humming are a popular input to music information-retrieval systems.
These systems perform a complex series of operations, from transcribing input to retrieving pieces.
We suggest a test-bed needs to represent each data-processing stage, from initial transcription of an audio query, to the relevancy ranking of each piece in the database compared to a particular query.
Such a multi-stage dataset would let each research group concentrate on the portion of the task most interesting to that group, while at the same time providing training and testing data for each component of the system.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.

HTML anchors are often surrounded by text that seems to describe the destination page appropriately.
The text surrounding a link or the <i>
link-context</i> is used for a variety of tasks associated with Web information retrieval.
These tasks can benefit by identifying regularities in the manner in which "good" contexts appear around links.
In this paper, we describe a framework for conducting such a study.
The framework serves as an evaluation platform for comparing various link-context derivation methods.
We apply the framework to a sample of Web pages obtained from more than 10,000 different categories of the ODP.
Our focus is on understanding the potential merits of using a Web page's tag tree structure, for deriving link-contexts.
We find that good link-context can be associated with tag tree hierarchy.
Our results show that climbing up the tag tree when the link-context provided by greater depths is too short can provide better performance than some of the traditional techniques.

Our interest is in developing techniques for constructing a semantic lexicon of broad coverage from machine tractable resources.
We work within a paradigm in which each word sense is represented as a vector in an ndimensional feature space.
So far our experiments have encompassed the MerriamWebster Compact Electronic Dictionary, the Irish An Focl6ir Beag and the Princeton WordNet.
Our main uses for the results are in full text information retrieval, machine assisted translation and lexical alignment.
In this paper we explain the background to the approach, outline the algorithms with which we have been experimenting and report on the results which we have obtained.

Viet Ha Nguyen, Gene Cooperman, Nina Menenzes, Chloe J. Lopez, Christopher Melinosky, Ona Wu, Hakan Ay, Yawu Liu, Juho Nuutinen, Hannu J. Aronen, Jari Karonen, A. Gregory Sorensen, Walter J. Koroshetz, Homer H. Pien A.N.Lab Inc, Hanoi, Vietnam (2) Department of Radiology, Massachusetts General Hospital, Boston, MA 02129 (3) College of Computer and Information Science, Northeastern University, Boston, MA 02115 (4) Department of Neurology, Massachusetts General Hospital, Boston, MA 02129 (5) Helsinki University Central Hospital, Helsinki, Finland
National Institutes of Health NINDS, Bethesda, MD 20892

In this short paper we describe the goals and questions associated with our project Cairsweb.
In this project we aim at the construction of a framework of agents for personalized context-sensitive Information Retrieval in a massively distributed information landscape, in which questions of privacy and access control on the one hand and semantic crawling explicification” on the other hand are the main focus.
By “explicification” we mean the process of generating semantically rich interfaces to information sources on the web with the help of Semantic Web standards.

Latent Semantic Indexing (LSI) is a technique used in Information Retrieval (IR) as an effective tool in correlating and retrieving relevant documents.
The authors presented a new philosophy for LSI analysis and evaluation based on the use of image processing tools.
In this new approach the Term Document Matrix (TDM) generated in the LSI process is visualized and treated as an image enabling techniques from image processing to be applied.
This paper presents a novel extension to this work in which various features of the target databases can be used to predict, and pre-select, search criteria.
This latest approach has been evaluated and validated by applying it to a range of sample databases.

In this paper we propose a search approach that can process large volumes of textual data efficiently and effectively even in environments where computational resources are limited.
The traditional search solution for large collections assumes availability of practically unlimited computational resources.
For many applications and organization this assumption is not realistic.
Empirical evaluation of the proposed approach using some of the largest available datasets demonstrates that the proposed search approach is substantially more efficient than the existing approach, is on par if not better in terms of effectiveness, and can operate using very few computational resources.
Information Retrieval/Large-scale Search Text Processing Advanced technical talk]

This paper focuses on the problem of representing, in a meaningful way, the knowledge involved in the HealthAgents project.
Our work is motivated by the complexity of representing electronic healthcare records in a consistent manner.
We present HADOM (HealthAgents domain ontology) which conceptualises the required HealthAgents information and propose describing the sources knowledge by the means of conceptual graphs (CGs
This allows to build upon the existing ontology permitting for modularity and flexibility.
The novelty of our approach lies in the ease with which CGs can be placed above other formalisms and their potential for optimised querying and retrieval.

The increase number of manuscripts and their diversity add the difficulty of searching and arranging for relevant manuscripts.
The quality of search results provided by search engines has not been maximized in response to user requests because it does not involve semantic elements in the search process.
It is necessary to build a information retrieval system for manuscript that makes it easier for researchers finding the title of the manuscript accordance with the topic of their research.

The purpose of this paper is to compare a certain number of well known models used in the fields of Mechanical Translation (M.T and Information Retrieval (I.R Different surveys of this type exist
(Bar-Hillel
[1 Hays [2 Lecerf [3 Sestier and Dupuis [4 where models have been compared from the point of view of practical and linguistic adequacy.
We wish here to compare certain formal characteristics of these models, in fact to show that they are strictly equivalent to a well studied model.
The notion of equivalence will be defined formally, the model to which the other models are equivalent is Chomsky's model of context-free languages (c.f.
languages The equivalences discussed here have not only an abstract character; several practical problems which arise naturally are clarified.

This is a case study in the design and analysis of a g-site TREC-6 experiment aimed at comparing the performance of 12 interactive information retrieval (IR) systems on a shared problem: a question-answering task, 6 statements of information need, and a collection of 210,158 articles from the Financial Times of London 1991-1994.
The study discusses the application of experimental design principles and the use of a shared control IR system in addressing the problems of comparing experimental interactive IR systems across sites: isolating the effects of topics, human searchers, and other site-specific factors within an affordable design.
The results confirm the dominance of the topic effect, show the searcher effect is almost as often absent as present, and indicate that for several sites the a-factor interactions are negligible.
An analysis of variance found the system effect to be significant, but a multiple comparisons test found no significant pairwise differences.

8:30 am 9:00 am
Coffee break 9:00 am 9:30 am Conference opening and introduction Derek Raine (Physics Peter Jackson and Emmanuel Haven
(Management)
9:30 am 10:30 am Plenary Talk Professor Edward Nelson Department of Mathematics Princeton University Title of Talk:
Stochastic mechanics of particles and fields 10:30 am 11:00am
Coffee break 11:00 am 12:30 pm
Paper session
Session Chair: Harald Atmanspacher Meaning-focused and Quantum-inspired Information Retrieval Diederik Aerts, Jan Broekaert, Sandro Sozzo and Tomas Veloz

Document retrieval is the task of returning relevant textual resources for a given user query.
In this paper, we investigate whether the semantic analysis of the query and the documents, obtained exploiting state-of-the-art Natural Language Processing techniques (e.g Entity Linking, Frame Detection) and Semantic Web resources (e.g YAGO, DBpedia can improve the performances of the traditional term-based similarity approach.
Our experiments, conducted on a recently released document collection, show that Mean Average Precision (MAP) increases of 3.5 percentage points when combining textual and semantic analysis, thus suggesting that semantic content can effectively improve the performances of Information Retrieval systems.

This paper describes the development of a structured document collection containing user-generated text and numerical metadata for exploring the exploitation of metadata in information retrieval (IR The collection consists of more than 61,000 documents extracted from YouTube video pages on basketball in general and NBA (National Basketball Association) in particular, together with a set of 40 topics and their relevance judgements.
In addition, a collection of nearly 250,000 user profiles related to the NBA collection is available.
Several baseline IR experiments report the effect of using video-associated metadata on retrieval effectiveness.
The results surprisingly show that searching the videos titles only performs significantly better than searching additional metadata text fields of the videos such as the tags or the description.

The concept of ‘relevance’ is crucial to legal information retrieval, but because of its intuitive understanding it goes undefined too easily and unexplored too often.
We discuss a conceptual framework on relevance within legal information retrieval, based on a typology of relevance dimensions used within general information retrieval science, but tailored to the specific features of legal information.
This framework can be used for the development and improvement of legal information retrieval systems.

1 Graduate Institute of Biomedical Electronic and Bioinformatics, National Taiwan University, Taipei 106, Taiwan 2 Institute of Information Science, Academia Sinica, Taipei 115, Taiwan 3 Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292, USA 4 Bioinformatics Center, Institute for Chemical Research, Kyoto University, Uji, Kyoto 611-0011, Japan 5 Institute of Clinical Medicine, National Yang-Ming University, Taipei 112, Taiwan

Note onset detection and instrument recognition are two of the most investigated tasks in Music Information Retrieval (MIR Various detection methods have been proposed in previous research for western music, with less focus on other music cultures of the world.
In this paper, we focus on onset detection for percussion instruments in Beijing Opera, a major genre of Chinese traditional music.
A dataset of individual audio samples of four primary percussion instruments is used to obtain the spectral bases for each instrument.
With these bases, we separate the input percussion ensemble recordings into its spectral sources and their activations using a Non-negative Matrix Factorization (NMF) based algorithm.
A simple onset detection conducted on each NMF activation presents satisfactory overall detection rates, and provides us valuable implications and suggestions for future development of drum transcription and percussion pattern analysis in Beijing Opera.

This paper discusses the evaluation of automatic speech recognition (ASR) systems developed for practical applications, suggesting a set of criteria for application-oriented performance measures.
The commonly used word error rate (WER which poses ASR evaluation as a string editing process, is shown to have a number of limitations with respect to these criteria, motivating alternative or additional measures.
This paper suggests that posing speech recognition evaluation as an information retrieval problem, where each word is one unit of information, offers a flexible framework for application-oriented performance analysis based on the concepts of recall and precision.

The paper discusses the role of grammars in sentence processing, and explores some consequences of the "Strong Competence Hypothesis" of Bresnan and Kaplan for combinatory theories of grammar.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report
No. MSCIS-92-53.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/475 Grammars and Processors MS-CIS-92-53 LINC LAB 231

One fundamental issue of learning to rank is the choice of loss function to be optimized.
Although the evaluation measures used in Information Retrieval (IR) are ideal ones, in many cases they can't be used directly because they do not satisfy the smooth property needed in conventional machine learning algorithms.
In this paper a new method named RankCSA is proposed, which tries to use IR evaluation measure directly.
It employs the clonal selection algorithm to learn an effective ranking function by combining various evidences in IR.
Experimental results on the LETOR benchmarh datasets demonstrate that RankCSA outperforms the baseline methods in terms of P@n, MAP and NDCG@n.

This article explores the use of metatheory as an integrative conceptual tool that can help analyze, direct, and enhance theory building, professional practice, and professional preparation in LIS.
The field's historic under-examination of metatheories is addressed, the nature of metatheory is explicated, and an emergent social justice metatheory for LIS is introduced, with the intention of encouraging discussion and increasing awareness of both metatheoretical approaches and social justice in LIS.

The interdisciplinary interaction of bibliography with sociology, history, pedagogics, information science, and cultural studies is considered based on the information approach.
Bibliography is shown to act as an indicator of social development as a part of the metainformation “layer” of society.

In this design case we describe the experience of designing a resuable class library for information retrieval user interfaces.
The &sign process is described with reflections on how the processwas organisedand the design problem on the process.

The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past, with an argument that the next big leap in search and retrieval will come through incorporating social and collaborative aspects of information seeking.
This half-day tutorial will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration.
The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques.
The course is intended for those interested in social and collaborative aspects of IR (from both academia and industry and requires only a general understanding of IR systems and evaluation.

Description Logics (DLs) are suitable, well-known, logics for managing structured knowledge.
They allow reasoning about individuals and well defined concepts, i.e. set of individuals with common properties.
The experience in using DLs in applications has shown that in many cases we would like to extend their capabilities.
In particular, their use in the context of Multimedia Information Retrieval (MIR) leads to the convincement that such DLs should allow the treatment of the inherent imprecision in multimedia object content representation and retrieval.
In this paper we will present a fuzzy extension of ALC, combining Zadeh’s fuzzy logic with a classical DL.
In particular, concepts becomes fuzzy and, thus, reasoning about imprecise concepts is supported.
We will define its syntax, its semantics, describe its properties and present a constraint propagation calculus for reasoning in it.

Over a series of evaluation experiments conducted using naive judges recruited and managed via Amazon’s Mechanical Turk facility using a task from information retrieval (IR we show that a SVM shows itself to have a very high accuracy when the machine-learner is trained and tested on a single task and that the method was portable from more complex tasks to simpler tasks, but not vice versa.

Algorithm visualization is an efficient way to teach programming.
Several different visualization techniques have been developed in the past decades.
The Concretization Environment Framework, CEF, combines algorithm visualization with concrete objects (e.g. Lego Mindstorms robots CELM, Concretization Environment for Lego Mindstorms is an application of this framework.
By using the framework, the user can turn the mental model the user has into a concrete one.
User feedback on the framework and its application has confirmed the functionality of the concept and the usefulness of the approach.
ACM-classification (ACM Computing Classification System, 1998 version K.3.2 [
Computer and Education Computer and Information Science Education Computer Science Education; I.6.8 [Simulation and Modeling Types of Simulation Distributed, Parallel

© 2005 Edward Arnold (Publishers) Ltd 10.1191/0309132505ph581pr

retrieval models within music and music/audio formats that makes use of XML documents as content descriptors.
In this article, it is described how music/audio semantics can be actually represented within the Structural Layer of IEEE 1599, thanks to the introduction of novel Music Information Retrieval (MIR) objects that can be exploited by music search engines.
A complete description of MIR objects is provided and it is shown how they can be used to embed metadata relative to specific music retrieval models, thus allowing for the description of music content in different retrieval contexts.
To this aim, a new concept for MIR Model is introduced together with its formalization and tools provided by category theory.
The role of MIR objects and morphisms in music content description and retrieval is explained.
Furthermore, a concrete example is given with the implementation of a graph-based model within the IEEE 1599 framework.

Reversible logic has become one in all the promising analysis directions in low power dissipating circuit style within the past few years and has found its applications in low power CMOS style, cryptography, digital signal process, optical information science and technology.
This paper presents a quantum value economical reversible full adder gate and Reversible Decoder in technology.
This gate will work severally as a reversible full adder &amp; Decoder unit and needs only 1 clock cycle.
The projected gate may be a universal gate within the sense that it may be wont to synthesize any absolute mathematician functions.
To implement reversible logic gates we tend to square measure planning Full adder and Decoder circuit by mistreatment the T-Spice simulation and calculate the facility consumption with TSMC018 metric linear unit Technology.

To enhance the retrieval accuracy of information search engine, this paper proposes a information retrieval system based on semantics and document refinement that realized by employing the semantic description and relevance of ontology to the information system.
We describe the using of LSI (latent semantic indexing) approach to replace the traditional VSM (vector-space model) approach in detail in the results of sorting process and have a comparative experiment.
Various experiments demonstrate the feasibility and effectiveness of our approach, LSI approach proposed in this paper is more effective and able to query the most relevant results in the top of the returnee for semantic retrieval than VSM and about 10.7%~22.2% increase of the performance.

We survey the major techniques for information retrieval
In the rst part we provide an overview of the traditional ones full text scanning inversion signature les and clustering
In the second part we discuss attempts to include semantic information natural language processing latent semantic indexing and neural networks

We introduce query-free information retrieval, a paradigm in which queries are constructed autonomously and information relevant to a user is offered without explicit request.
Query-free methods offer an apparently new approach for integrating knowledge-based applications with legacy databases.
We describe a prototype system, F IXIT which integrates an expert diagnostic system with a preexisting full-text database of maintenance manuals.
The reported results suggest that query-free information retrieval can liberate the user from burdensome information retrieval activities while incurring only modest system development costs and minimal run-

Goal of conferences like TREC, TIPSTER, NTCIR, CLEF is to judge the performance of different algorithms.
Most of these conferences have tracks that deal with new and innovative information retrieval problems, but none has tackled to work with Urdu data, primarily because of the lack of resources.
In this paper we present a baseline for Urdu IR evaluation along with resources necessary to do the task.
The goal of this paper is to explore the strategy for creation of the test reference collection for Urdu Information Retrieval.
A small test reference collection is presented composed of news articles from the Web along with some experiments to show its effectiveness.

In this paper we study the performance of linguistically-motivated connation techniques for Information Retrieval in Spanish.
In particular, we have studied the application of productive derivational morphology for single word term connation and the extraction of syntactic dependency pairs for multi-word term connation.
These techniques have been tested on several search engines implementing diierent indexing models.
The aim of this study is to nd the strong and weak points of each technique in order to develop heuristics for automatic query expansion .

Information Retrieval (IR) systems require input corpora to be indexed.
The advent of terabyte-scale Web corpora has reinvigorated the need for efficient indexing.
In this work, we investigate distributed indexing paradigms, in particular within the auspices of the MapReduce programming framework.
In particular, we describe two indexing approaches based on the original MapReduce paper, and compare these with a standard distributed IR system, the MapReduce indexing strategy used by the Nutch IR platform, and a more advanced MapReduce indexing implementation that we propose.
Experiments using the Hadoop MapReduce implementation and a large standard TREC corpus show our proposed MapReduce indexing implementation to be more efficient than those proposed in the original paper.

Practical information retrieval systems must manage large volumes of data, often divided into several collections that may be held on separate machines.
Techniques for locating matches to queries must therefore consider identiication of probable collections as well as identiication of documents that are probable answers.
Furthermore the large amounts of data involved motivates the use of compression, but in a dynamic environment compression is problematic, because as new text is added the compression model slowly becomes inappropriate.
In this paper we describe solutions to both of these problems.
We show that use of centralised blocked indexes can reduce overall query processing costs in a multi-collection environment, and that careful application of text compression techniques allow collections to grow by several orders of magnitude without recompres-sion becoming necessary.

A simple modification of Horn's circle drawing procedure yields a disk generator for a class of graphic devices capable of drawing rectangular areas.
Another variation produces a disk a scan-line at a time allowing it to be drawn at the refresh rate of the display.
The calculations involve only additions and binary shifts.
Disciplines Computer Engineering Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-77-47.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/997
I Appeared in Computer Graphics and Image Processlng, 1977 UP-MS-CIS-77-47 DISK GENERATORS FOR A RASTER DISPLAY
DEVICE Movement Project Report No. 2 Norman I. Badler

In the context of the Scientific Information Retrieval, this experiment aims to observe dependences between user characteristics (the profile) and pertinent document characteristics (metadata We know and use the relation between the topic of the information need and the topic of the document, but we think other less intuitive dependences exist.
We built possible assumption of dependences between a profile and metadata.
We confirmed them by an experimentation based on the observation of user carrying out an information retrieval on the Web.
Finally, we think that integration of these dependences in a SRI will decrease the documentary noise.
MOTS-CLÉS recherche d’information, contexte, caractéristiques utilisateur, métadonnées.

We present the Private Digital Library (PDL) project that represents a service of the Corporate Digital Library (CDL) prototype.
The main ideas underlying this project are the following.
When a user is looking for documents that he already retrieved in the past, he has to repeat the search procedure and solve the same problems he encountered in the past access.
On the contrary, consider the possibility for the user to store documents in a private library.
In this case, he will have the chance to retrieve the documents of interest more easily and quickly.
It may also be the case that the user has not a clear understanding of what he is looking for.
Moreover, he might not know exactly the library content and organization.
Thus, he needs for assistance to suggest him what to look for and how to query the system.
A method to store and catalogue documents according to personal criteria might help to overcome these problems.

There is an explosive growth of regulatory and related information now available online.
This paper reviews the current state of practice in accessing patent-related documents in the form of patents, government regulations, court cases, scientific publications, etc.
This paper proposes an ontology-based framework to retrieve documents from the multiple heterogeneous databases.
A use case, erythropoietin, is developed to test the framework.
A corpus of 135 patents and 30 court cases closely related to the use case is built.
Methods are discussed to improve the results obtained through the use of bio-ontologies for document retrieval in the two databases.
The challenges faced with respect to the integration of these documents and future plans are briefly discussed.

This paper presents a boosting based algorithm for learning a bipartite ranking function (BRF) with partially labeled data.
Until now different attempts had been made to build a BRF in a <
i>transductive</i> setting, in which the test points are given to the methods in advance as unlabeled data.
The proposed approach is a semi-supervised <i>inductive</i> ranking algorithm which, as opposed to transductive algorithms, is able to infer an ordering on new examples that were not used for its training.
We evaluate our approach using the TREC-9 Ohsumed and the Reuters-21578 data collections, comparing against two semi-supervised classification algorithms for ROCArea (AUC uninterpolated average precision (AUP mean precision@50 (TP) and Precision-Recall (PR) curves.
In the most interesting cases where there are an unbalanced number of irrelevant examples over relevant ones, we show our method to produce statistically significant improvements with respect to these ranking measures.

In this paper, we reflect on ways to improve the quality of bio-medical information retrieval by drawing implicit negative feedback from negated information in noisy natural language search queries.
We begin by studying the extent to which negations occur in clinical texts and quantify their detrimental effect on retrieval performance.
Subsequently, we present a number of query reformulation and ranking approaches that remedy these shortcomings by resolving natural language negations.
Our experimental results are based on data collected in the course of the TREC Clinical Decision Support Track and show consistent improvements compared to state-of-the-art methods.
Using our novel algorithms, we are able to reduce the negative impact of negations on early precision by up to 65%.

This article investigates whether a fuzzy set approach to the natural language syntactic analysis can support information retrieval systems.
It concentrates on a web search since the internet becomes a vast resource of information.
In addition, this article presents a module of syntax analysis of TORCH project where the fuzzy set disambiguation has been implemented and tested.

INTRODUCTION
In recent years “big data” has become something of a buzzword in business, computer science, information studies, information systems, statistics, and many other fields.
As technology continues to advance, we constantly generate an ever-increasing amount of data.
This growth does not differentiate between individuals and businesses, private or public sectors, institutions of learning and commercial entities.
It is nigh universal and therefore warrants further study.

Stemming is a technique used to reduce inflected and derived words to their basic forms (stem or root
It is a very important step of pre-processing in text mining, and generally used in many areas of research such as: Natural language Processing NLP, Text Categorization TC, Text Summarizing TS, Information Retrieval IR, and other tasks in text mining.
Stemming is frequently useful in text categorization to reduce the size of terms vocabulary, and in information retrieval to improve the search effectiveness and then gives us relevant results.
In this paper, we propose a new multilingual stemmer based on the extraction of word root and in which we use the technique of n-grams.
We validated our stemmer on three languages which are: Arabic, French and English.

In this paper, we deal with information retrieval approach based on language model paradigm, which has been intensively investigated in recent years.
We propose, implement, and evaluate an enrichment of language model employing syntactic dependency information acquired automatically from both documents and queries.
By testing our model on the Czech test collection from Cross Language Evaluation Forum 2007 Ad-Hoc track, we show positive contribution of using dependency syntax in this context.

Work on the statistical validity of experimental results in retrieval tests has concentrated on treating the topics as a sample from a population, but regarding the collection of documents as fixed.
This paper raises the argument that we should also consider the documents as having been sampled from a population.
It follows that we should regard a per-topic measurement as also having a per-topic noise or error associated with it, which may depend critically on the number of relevant documents for that topic.
Some of the common measures used in retrieval testing are re-examined from this point of view.
The examination is essentially theoretical, supported by limited simulation experiments.
Reprinted from: Studies in Theory of Information Retrieval.
Budapest: Foundation for Information Science, 2007

As digital collections expand, the importance of the temporal aspect of information has become increasingly apparent.
The aim of this paper is to investigate the effect of using long-term temporal profiles of terms in information retrieval by enhancing the term selection process of pseudo-relevance feedback (PRF For this purpose,
two temporal PRF approaches were introduced considering only temporal aspect and temporal along with textual aspect.
Experiments used the AP88-89 and WSJ87-92 test collections with
TREC Ad-Hoc Topics 51-100.
Term temporal profiles are extracted from the Google Books n-grams dataset.
The results show that the long-term temporal aspects of terms are capable of enhancing retrieval effectiveness.

As web is the largest collection of information and plenty of pages or documents are newly added and deleted on frequent basis due to the dynamic nature of the web.
The information present on the web is of great need, the world is full of questions and the web is serving as the major source of gaining information about specific query made by the user.
Search engines generally return a large number of pages in response to user queries.
To assist the users to navigate in the result list, ranking methods are applied on the search results.
Most of the ranking algorithms proposed in the literature are either link or content oriented.
Proposed methodology uses, a page ranking mechanism based on frequency of keywords found in the query made by user.
This approach helps to rank most valuable and relevant search results on the top of the result list.
Therefore tries to enhance the search engine results.
Keywords— Information Retrieval; PageRank; Search Engine; Web Mining; World Wide Web

The goal of this work is to study how individual learning helps adaptive agents, living in an Artificial Life world, to develop correct behaviors when seeking for information in distributed and dynamic environments.
We developed a prototype of an information retrieval system where a population of adaptive organisms browses through the environment, trying to find relevant information for a specific query.
Two learning procedures are proposed and their effect is studied in different situations.
Experimental results show that changes induced by learning provide an important advantage.

This paper is intended to serve as a springboard for a panel discussion with audience participation on information l-tering and retrieval.
Medical informatics is an emerging specialty which links medicine and information technology.
With the unprecedented availability of digital information, retrieval and ltering are becoming important aspects of medical informatics.
An overview of medical applications for ltering and retrieval is provided, and important state-of-the-art techniques are introduced in a way that does not presuppose prior knowledge in this eld.
With this basis for understanding issues and research developments, the ensuing discussion will examine challenges and opportunities for workers in this eld.

This report describes AIR 2008, the second international workshop on Adaptive Information Retrieval (AIR held in October 2008 at London, UK.
The workshop attracted over 30 participants across the world and the largest workshop held in conjunction with IIiX 2008.
The presentation ranged widely from theories to practices on Adaptive Information Retrieval.
The workshop consisted of two keynote presentations, three oral presentations, breakout sessions, and poster session.

Based on the study of linguistics, Information Science and Library and Information Science, we research on the real-time news posted on the authority sites in the world's major countries.
By analyzing the massive news of different information sources and language origins, we come up with a basic theory model and its algorithm on news, which is capable of intelligent collection, quick access, deduplication, correction and integration with news' backgrounds.
Furthermore, we can find out connections between news and readers' interest.
So we can achieve a real-time and on-demand news feed as well as provide a theoretical basis and verification of scientific problems on real-time processing of massive information.
Finally, the simulation experiment shows that the multilingual news matching technology could give more help to distinguish the similar news in different languages than the traditional method.

This article addresses a question regarding relevant information in a social media such as a wiki that can contain huge amount of text, written in slang or in natural language, without necessarily observing a fixed terminology set.
This text could not always be adherent to the discussed subject.
The main motivation leads to the need of developing methods that would allow the extraction of relevant information in such scenario.
A result system was designed upon ideas from the semantic Web combined with an adaptation of the classic vector model for information retrieval.
The semantic information is not embedded in the media but within a structurally independent ontology.
It was implemented using Java and a MySQL database.
The objective was the achievement of, at least, 80&#x025; for recall and precision on the system results.
The system was considered successful by achieving rates of 100&#x025; of recall and approximately 93&#x025; of precision.

The purpose of the presentation is generalization of teacher’s work with students at Kharkiv G. S. Skovoroda Pedagogical University, faculty of physics and mathematics, specialty «mathematics and informatics (information science
The aims of studies are formalization and representation of educational material on the topics "Symmetry" and "Polyhedra Further use of this material in the system of distance education in Ukraine is supposed.

The 30th European Conference on Information Retrieval Research (ECIR '08) took place in Glasgow, Scotland, on 30<sup>th</sup>
March 3<sup>rd</sup> April, 2008 and was organised by the Department of Computing Science, University of Glasgow.

Nowadays information retrieval based on specific queries is already used in computer system.
One of the popular methods is document ranking using Vector Space Model (SVM) based on TF.IDF term-weighting.
In this paper TF.IDF.ICS<inf x03B4 inf>F term-weighting based class-indexing is proposed, afterward comparing its effectiveness to TF.IDF and TF.IDF.ICF term weighting.
Each method is investigated through Al-Qur'an dataset.
Al-Qur'an consist many verses, each verse of the Al-Qur'an is a single document which is ranked based on user query.
The experimental show that the proposed method can be implemented on document ranking and the performance is better than previous methods with accurate value 93%.

This paper describes the 8220;conceptual&#8221; Knowledge Representation Language (KRL) proper to an environment for the construction and use of large Knowledge Bases and/or 8220;Intelligent&#8221; Information Retrieval Systems.
In the KRL, we separate the treatment of the episodic memory (extensional, assertional data 8220;Snoopy is Charlie Brown's beagle&#8221 from the treatment of the semantic memory (intensional, terminological data
A beagle is a sort of hound a hound is a dog 8230
A compromise between an 8220;object-oriented approach&#8221; and a 8220;logic-oriented approach&#8221; is proposed for implementation purposes.

S. K. Misra, J. T. Kirby, M. Brocchini, F. Veron, M. Thomas, and C. Kambhamettu Center for Applied Coastal Research, University of Delaware, Newark, Delaware 19716, USA Istituto di
Idraulica e Infrastrutture, Universit Politecnica delle Marche,
Via Brecce Bianche,
60131 Ancona, Italy Graduate College of Marine Studies, University of Delaware, Newark, Delaware 19716, USA Department of Computer and Information Science, University of Delaware, Newark, Delaware 19716, USA

Social Information Retrieval can be interpreted as querying the private information spaces of others within one's social network.
One of the crucial steps in such a search approach is to identify the set of potential information providers to route the query to.
In this experiment, we compare various routing mechanisms based on topic models (Latent Dirichlet Allocation, LDA Explicit Semantic Analysis (ESA and traditional metrics like Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) to identify expertise using a publicly available data collection with 1, 400 scientific abstracts including author information, queries, and relevance judgments.
The abstracts are interpreted as knowledge profile in a social information retrieval scenario.
Our results suggest that both LDA and ESA can solve the routing problem, whereas the LDA-based approach and a new ESA approach considering links between semantic concepts perform best on the tested dataset.

We have designed a representation scheme, which is based on the <i>discrete</i> representation of a document ranking function, which is capable of reproducing and enhancing the properties of such popular ranking functions as <i>tf.idf, BM25</i> or those based on language models.
Our tests have demonstrated the capability of our approach to <
i>achieve the performance of the best known scoring functions</i> solely through training, without using any known heuristic or analytic formulas.

In this paper we report the results of an independent experimental evaluation of an information retrieval (IR) system developed at the Illinois Institute of Technology (IIT The system, which is called the
Advanced Information Retrieval Engine</i
AIRE consists of a set of tools and utilities providing indexing, extraction, searching and visualization.
We evaluated AIRE on three data sets from the Text REtrieval Conference (TREC TREC 8, 9 and 10.
Overall, our results indicate that AIRE is a highly accurate IR system.
Compared with results published by IIT, in our experiments AIRE consistently scored higher in recall.
AIRE also scored higher in precision, but only for automatic tasks.
In manual tasks, AIRE scored lower in precision in our experiments, but we attributed that to factors external to AIRE.
Our final conclusion is that AIRE is a highly accurate IR system.

1School of Information Science Technology, Nanjing Forestry University, 210037 Nanjing, China.
2Danzhou Investigation Experiment Station of Tropical Crops, Ministry of Agriculture/Rubber Research Institute, Chinese Academy of Tropical Agricultural Sciences, Danzhou 571737, China 3Advanced Analysis and Testing Centre, Nanjing Forestry University, 210037 Nanjing, China.
4College of Forestry, Nanjing Forestry University, 210037 Nanjing, China.
Center Sustainable Forestry Studies in Southern China, Nanjing Forestry University, 210037 Nanjing, China

Distributed Information Retrieval (DIR) is a generic area of research that brings together techniques, such as resource selection and results aggregation, dealing with data that, for organizational or technical reasons, cannot be managed centrally.
Existing and potential applications of DIR methods vary from blog retrieval to aggregated search and from multimedia and multilingual retrieval to distributed Web search.
In this tutorial we briefly discuss main DIR phases, that are resource description, resource selection, results merging and results presentation.
The main focus is made on applications of DIR techniques: blog, expert and desktop search, aggregated search and personal meta-search, multimedia and multilingual retrieval.
We also discuss a number of potential applications of DIR techniques, such as distributed Web search, enterprise search and aggregated mobile search.

The paper proposes a Vector Space Model over the Cayley-Klein Hyperbolic Geometry (referred to as Hyperbolic Information Retrieval HIR) using a similarity measure derived from the hyperbolic distance.
It is shown that the proposed model is equivalent with the classical Vector Space Model using Cosine measure with normalized weighting scheme.
It is also shown that the categoricity of the new retrieval system can be varied by only modifying the radius of the hyperbolic space and without using a different weighting scheme and similarity measure, which is not the case in the VSM, where the same effect can only be obtained by both changing the weighting scheme and similarity measure at the expense of a more costly computation.
Experiments are also reported to demonstrate and support the ideas, and they show that categoricity in HIR can be varied more than O(n) faster, where n is the number of index terms, than in the VSM.

Information Retrieval systems normally have to work with rather heterogeneous sources, such as Web sites or documents from Optical Character Recognition tools.
The correct conversion of these sources into flat text files is not a trivial task since noise may easily be introduced as a result of spelling or typeset errors.
Interestingly, this is not a great drawback when the size of the corpus is sufficiently large, since redundancy helps to overcome noise problems.
However, noise becomes a serious problem in restricted-domain Information Retrieval specially when the corpus is small and has little or no redundancy.
This paper devises an approach which adds noise-tolerance to Information Retrieval systems.
A set of experiments carried out in the agricultural domain proves the effectiveness of the approach presented.

This paper gives an overview of tools and methods for CrossLanguage Information Retrieval (CLIR) that are developed within the Twenty-One project.
The tools and methods are evaluated with the TREC CLIR task document collection using Dutch queries on the English document base.
The main issue addressed here is an evaluation of two approaches to disambiguation.
The underlying question is whether a lot of effort should be put in finding the correct translation for each query term before searching, or whether searching with more than one possible translation leads to better results?
The experimental study suggests that the quality of search methods is more important than the quality of disambiguation methods.
Good retrieval methods are able to disambiguate translated queries implicitly during searching.

In this note, we discuss a very nice and important application of modular arithmetic: the RSA public-key cryptosystem, named after its inventors Ronald Rivest, Adi Shamir and Leonard Adleman.
Cryptography is an ancient subject that really blossomed into its modern form at the same time1 as the other great revolutions in the general fields of information science/engineering.
The basic setting for basic cryptography is typically described via a cast of three characters: Alice and Bob, who with to communicate confidentially over some (insecure) link, and Eve, an eavesdropper who is listening in and trying to discover what they are saying.

With the intensive and large scale application of IETM in equipment integrated support, information retrieval technology becomes one of the most key technologies.
This article discusses the full-text search technology and Lucene full-text retrieval engine, and combines them to develop a highperformance scalable IETM full-text retrieval system, this system can effectively deal with IETM unstructured data and structured data, significantly improving search efficiency and it is convenient to extend and easy to maintain.
First, it briefly introduces information retrieval theory, and discusses the information retrieval requirement in IETM.
Then, it thoroughly surveys the Lucene full-text retrieval engine, including its framework, retrieval process and index mechanism.
Next this paper designs Lucene based IETM information retrieval process.
Finally, the realization of Lucene based IETM information

The aim of our participation in the topic distillation and the named page finding tasks of the Web track is the evaluation of a well-founded modular probabilistic framework for Web Information Retrieval, which integrates content and link analyses.
The link analysis component of the framework employs a new probabilistic approach, called the Absorbing Model, for calculating a measure of popularity for documents induced from the Web graph.

Information retrieval systems are currently being developed to represent and manipulate complex multimedia objects.
Such objects contain two types of components, i.e. structured components (e.g. of the type integer, real, fixed-length string, etc and unstructured components (e.g. text, images, sounds Relational database systems (RDBS) are often used to store structured data and retrieve it via exact matching, while unstructured data is organized into inverted files, and accessed by Information storage and Retrieval Systems (IRS utilizing indeterminate matching.
The difficulty is how to fill the gap between the RDBs and inverted files, as well as the gap between the RDBMSs and IRSs based on inverted files.
We describe an approach to integrating the two types of systems and the two different types of data.

Automatic query expansion has been known to be the most important method in overcoming the word mismatch problem in information retrieval.
Thesauri have long been used by many researchers as a tool for query expansion.
However only one type of thesaurus has generally been used.
In this paper we analyze the characteristics of di erent thesaurus types and propose a method to combine them for query expansion.
Experiments using the TREC collection proved the e ectiveness of our method over those using one type of thesaurus.

A new definition of complex objects is introduced which provides a denotation for incomplete tuples as well as partially described sets.
Set values are 8220;sandwiched&#8221; between 8220;complete&#8221; and 8220;consistent&#8221; descriptions (representing the Smyth and Hoare powerdomains respectively allowing the maximal values to be arbitrary subsets of maximal elements in the domain of the set.
We also examine the use of rules in defining queries over such objects.

This chapter initially defines what characterizes and distinguishes research frameworks from research models.
The Laboratory Research Framework for IR illustrates the case.
We define briefly what is meant by the concept of research design, including research questions, and what this chapter regards as central IIR evaluation research settings and variables.
This is followed by a description of IIR components, pointing to the elements of the Integrated Cognitive Research Framework for IR that incorporates the Laboratory Framework in a contextual manner.
The following sections describe and exemplify 1)
Request types, test persons, task-based simulations of search situations and relevance or performance measures in IIR 2)
UltraLight Interactive IR experiments 3) Interactive-Light IR studies; and (4) Naturalistic field investigations of IIR.
The chapter concludes with a summary section, a reference list and a thematically classified bibliography.

The debate on the effectiveness of ontology in solving semantic problems has increased recently in many domains of information technology.
One side of the debate accepts the inclusion of ontology as a suitable solution.
The other side of the debate argues that ontology is far from an ideal solution to the semantic problem.
This article explores this debate in the area of information retrieval.
Several past approaches were explored and a new approach was investigated to test the effectiveness of a generic ontology such as WordNet in improving the performance of information retrieval systems.
The test and the analysis of the experiments suggest that WordNet is far from the ideal solution in solving semantic problems in the information retrieval.
However, several observations have been made and reported in this article that allow research in ontology for the information retrieval to move towards the right direction.

This paper reports on the Large Scale Hierarchical Classification workshop (http kmi.open.ac.uk/events/ecir2010/workshops-tutorials held in conjunction with the European Conference on Information Retrieval (ECIR)
The workshop was associated with the PASCAL 2 Large-Scale Hierarchical Text Classification Challenge (http lshtc.iit.demokritos.gr which took place in 2009.
We first provide information about the challenge, presenting the data used, the tasks and the evaluation measures and then we provide an overview of the approaches proposed by the participants of the workshop, together with a summary of the results of the challenge.

The following paper describes a set of methods that is currently used in a study of the task performance process of patent engineers within the Swedish Patentand Registration Office (SPRO
The focus of the study is to investigate the relationship between the user’s work-task and the information seeking and retrieval process.
The study is performed within a real life work setting where patent engineers are performing real work tasks involving real information needs.
This paper will focus on and describe a set of data collection methods used in our study.
Generally, IR studies are performed within a controlled laboratory environment with controlled variables and design or simulated information need.
We argue that we need to take a broader perspective on the information seeking and retrieval in order to understand the task performance process and elicit requirements for information systems design.

Speech recognition can be used in music retrieval systems to identify the words in users’ sung queries.
Our aim was to determine which of several techniques is most suitable for retrieving songs given a sung query with words.
We used Sphinx for speech recognition, and tested several retrieval techniques on the output of the recognition system.
The most effective retrieval technique was a combination of Edit Distance and Okapi, which persistently retrieved the correct song at the top one ranked results given that the queries were at least 50% correct.
However, techniques performed differently when the queries were split into four buckets with varying level of correctness in the range of 0 to 73%.

This thesis is protected by copyright, with all rights reserved.
By reading and using the thesis, the reader understands and agrees to the following terms:
The reader will abide by the rules and legal ordinances governing copyright regarding the use of the thesis.
The reader will use the thesis for the purpose of research or private study only and not for distribution or further reproduction or any other purpose.
The reader agrees to indemnify and hold the University harmless from and against any loss, damage, cost, liability or expenses arising from copyright infringement or unauthorized usage.

We propose a novel language model for sentence retrieval in Question Answering (QA) systems called trained trigger language model.
This model addresses the word mismatch problem in information retrieval.
The proposed model captures pairs of trigger and target words while training on a large corpus.
The word pairs are extracted based on both unsupervised and supervised approaches while different notions of triggering are used.
In addition, we study the impact of corpus size and domain for a supervised model.
All notions of the trained trigger model are finally used in a language model-based sentence retrieval framework.
Our experiments on TREC QA collection verify that the proposed model significantly improves the sentence retrieval performance compared to the state-of-the-art translation model and class model which address the same problem.

In large web search engines the performance of Information Retrieval systems is a key issue.
Block-based compression methods are often used to improve the search performance, but current self-indexing techniques are not adapted to such data structure and provide suboptimal performance.
In this paper, we present SkipBlock, a self-indexing model for block-based inverted lists.
Based on a cost model, we show that it is possible to achieve significant improvements on both search performance and structure’s space storage.

The rapid development of the Internet brings a new problem, which is how to rapidly and effectively retrieve needed information from vast number of web pages.
In this paper, we propose a valid approach to find hub sites using hill-climbing algorithms .It uses a fuzzy clustering algorithm based on web pages&#x02019; contents and link analysis in order to help uses find relevant web information more easily and precisely.
This paper focuses on improving the effectiveness and efficiency of web search.

The Rolf Nevanlinna Prize is awarded once every four years at the International Congress of Mathematicians by the International Mathematical Union for outstanding contributions in mathematical aspects of information sciences.
The 2010 recipient was Daniel Spielman, who was cited for “smoothed analysis of Linear Programming, algorithms for graph-based codes, and applications of graph theory to Numerical Computing
In this article, we summarize some of Spielman’s seminal contributions in these areas.
Unfortunately, because of space constraints, we can barely scratch the surface and have to leave out many of his impressive results (and their interconnections) over the last two decades.

We present an evaluation of an information retrieval system designed for the 1997 TREC-6 Interactive Track; that is, Aspect Oriented Retrieval, or finding documents that cover all aspects of relevance to a given topic.
Our system includes a basic search system, a task-specific "aspect window and a 3-D visualization of document and aspect relationships.
We compare two versions of our system against ZPRISE, a baseline system provided by NIST.
A study of 20 searchers shows significant differences between two classes of searchers, and supports several hypotheses about the design of an aspect oriented system.
An interesting result is a likely correlation between structural visualization ability and facility with a 3-D visualization.

This article introduces the design principle and implementation method of the automatic judgment software of terminal in sports competition.
Image recognition technology makes the competition management level achieve equality, justice, precision and high efficiency, This actualizing automatic judgment by means of recognition model for sports imaging, which is based on the principle of radio frequency identification (RFID).

This paper shows how a free text, Boolean-type retrieval system can be used to provide a wide range of information processes.
The paper describes the STATUS retrieval system, concentrating on the recent developments and improvements to the system.
In particular, the use of preand post-processors is discussed.
These give much greater flexibility to the user interface and allow STATUS to be integrated into existing information systems.
This is illustrated with examples from a library loans system.

In the course of anti-structuralist criticism, the main thrust of LéviStrauss’s epistemological approach seems to have been lost, to the collective detriment of social sciences and anthropology.
By its monumental character, Lévi-Strauss’s work evokes that of the founders of anthropology, whereas, by the way in which it puts in relation the cultural and the mental, it anticipates a theoretical anthropology to come, with the ambition of providing a rigorous method that comes close to scientific knowledge.
The fundamental point remains the emancipation of the structural approach from the linguistic model and its orientation toward a new context of science and technology, as exemplified in mathematics, information science, cybernetics and game theory, which made it possible for structural anthropology to innovatively account for the social systems and praxis of competitive and strategic practices.

Digital libraries relating to particular subject domains have invested a great deal of human e ort in developing metadata in the form of subject area thesauri.
This e ort has emerged more recently in arti cial intelligence as ontologies or knowledge bases which organize particular subject areas.
The purpose of subject area thesauri is to provide organization of the subject into logical, semantic divisions as well as to index document collections for e ective browsing and retrieval.
Prior to free-text indexing (i.e. the bag-of-words approach to information retrieval subject area thesauri provided the only point of entry (or 'entry vocabulary to retrieve documents.
A debate began over thirty years ago about the relative utility of the two approaches to retrieval:

Information retrieval (IR) system plays an important role in many web applications such as digital library, digital museum and so on.
On the aspect of querying process, most of the existing IR systems are based on keyword query.
This type of query requires users to describe their query request by using keywords.
But, common users prefer using natural language to express their request.
So, natural language-based (especially Chinese natural language-based) IR system is proposed in this paper.
At present, most traditional IR systems are single data source oriented and always centralized.
Grid is chosen to enable the coordinated use of distributed resources.
All the algorithms are encapsulated into services and are effectively shared.

Most information retrieval systems are comprised of a focused set of domain-specific documents located within a single logical repository.
A mechanism is developed by which user queries against such a system are used to generate a concept hierarchy pertinent to the domain.
First, an algorithm is described which extracts terms from documents matching user queries, and then reduces this set of terms to a manageable length.
The resulting terms are used to generate a feature vector for each query, and the queries are clustered using a Hierarchical Agglomerative Clustering (HAC) algorithm.
The HAC algorithm generates a binary tree of clusters, which is not particularly amenable to use by humans and which is slow to search due to its depth, so a subsequent processing step applies min-max partitioning to form a shallower, bushier tree that is a more natural representation of the concept hierarchy inherent in the system.

In this paper, we describe our participation in the Adhoc Information Retrieval, Cross Lingual Information Retrieval (CLIR) tasks of FIRE 2010.
We use a discriminative approach to IR.
Besides the basic term based matching features proposed in literature, we include novel features which model certain crucial elements of relevance like named entities, document length and semantic distance between query and document.
We also investigate the effect of including Pseudo-Relevance Feedback (PRF) based features in the above framework.
In CLIR, we use a query translation approach using bilingual dictionaries.
We use an iterative query disambiguation algorithm for query disambiguation.
In the disambiguation approach, in lieu of regular term-term co-occurrence measures proposed in literature, we use the similarity between terms in LSI space.

A new web content structure based on visual representation is proposed in this paper.
Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure.
This paper presents an automatic top-down, tag-tree independent approach to detect web content structure.
It simulates how a user understands web layout structure based on his visual perception.
Comparing to other existing techniques, our approach is independent to underlying documentation representation such as HTML and works well even when the HTML structure is far different from layout structure.
Experiments show satisfactory results.

This paper proposes a new Information Retrieval Model based on possibility and necessity measures.
This model encodes relationship dependencies existing between terms, documents and query by possibilistic networks.
The user’s query triggers a propagation process to retrieve necessarily or at least possibly relevant documents.

Exploratory search is a complex, iterative information seeking activity that involves running multiple queries and finding and examining many documents.
We designed a query preview control that visualizes the distribution of newly-retrieved and re-retrieved documents prior to running the query.
When evaluating the preview control with a control condition, we found effects on both people's information seeking behavior and improved retrieval performance.
People spent more time formulating a query and were more likely to explore search results more deeply, retrieved a more diverse set of documents, and found more different relevant documents when using the preview.

This article aims to review the main characteristics of natural language processing techniques, focusing on its application in information retrieval and related topics Specifically, in the second section we will study the different problems in automatic natural language processing; in the third section we will describe the key methodologies of NLP applied in information retrieval; and in the fourth section we will state several fields of research related to information retrieval and natural language processing; finally we present the conclusions and an annexe (Annexe 1) showing some of the particular aspects of NLP in Spanish.

Computer-based information retrieval has been a primary area of study in information processing for the past eighteen years.
Today, our concern extends far beyond conventional information retrieval.
One of the major problems in information processing is automated knowledge transfer and utilization, which is a key factor in productivity improvement.
Knowledge engineering is a field of scientific endeavor in which the primary goal is to develop techniques and methodologies for achieving automated knowledge transfer and utilization.
The objective of this paper is to stimulate research and development in this important field by clarifying the problem concepts and proposing some approaches.
The paper concludes with a discussion of the MEDIKS, which is an operational computer system demonstrating certain aspects of automated knowledge transfer and utilization, and proposes the organization of a decision support system which is designed on the basis of knowledge engineering principles.

With the development of more and more sophisticated Music Information Retrieval approaches, aspects of adaptivity are becoming an increasingly important research topic.
Even though, adaptive techniques have already found their way into Music Information Retrieval systems and contribute to robustness or user satisfaction they are not always identified as such.
This paper attempts a structured view on the last decade of Music Information Retrieval research from the perspective of adaptivity in order to increase awareness and promote the application and further development of adaptive techniques.
To this end, different approaches from a wide range of application areas that share the common aspect of adaptivity are identified and systematically categorized.

Content Based Multimedia Information Retrieval (CBMIR) is characterised by the combination of noisy sources of information which, in unison, are able to achieve strong performance.
In this thesis we focus on the combination of ranked results from the independent retrieval experts which comprise a CBMIR system through linearly weighted data fusion.
The independent retrieval experts are low-level multimedia features, each of which contains an indexing function and ranking algorithm.
This thesis is comprised of two halves.
In the first half, we perform a rigorous empirical investigation into the factors which impact upon performance in linearly weighted data fusion.
In the second half, we leverage these finding to create a new class of weight generation algorithms for data fusion which are capable of determining weights at query-time, such that the weights are topic dependent.

Cross-Language Multimedia Information Retrieval Sharon Flank emotion, Inc.
2600 Park Tower Dr Vienna, VA 22180 USA sharon.flank@emotion.com
Simple measures can achieve high-accuracy cross-language retrieval in carefully chosen applications.
Image retrieval is one of those applications, with results ranging from 68% of human translator performance for German, to 100% for French.

In this paper an original soft hierarchical Fuzzy Clustering algorithm is proposed, named Hierarchical Hyper-spherical Divisive Fuzzy C-Means (H2D-FCM with the following characteristics:
it generates a 8220;soft&#8221; hierarchy in which a document can belong to several child clusters of a node, and the clusters in the same hierarchical level are more specific (general) than the clusters in the upper (lower) level.
The proposed algorithm is a divisive algorithm based on a modified bisective K-Means, applying a modified probabilistic Fuzzy C Means algorithm to divide each node into child-nodes.
The algorithm determines the proper number of cluster to generate at the first level based on an entropy measure and decides if a node can be further split based on a 8220;density&#8221; measure.
The paper presents the algorithm and its evaluations on two standard collections.

As part of research on modeling user needs and behavior, we describe an approach to select web Information Retrieval Services (IRS) adapted to user’s needs.
An experimental system integrating a model of the user by a user profile representing these interests, modeling the behavior by a mechanism for retrieving user interactions and a database of general and vertical SRI is presented.
Our research focus on the construction of a selection model with features from the literature and the user profile.
We propose the use of reinforcement learning using the theory of Markov decision process for learning our model.
MOTS-CLÉS Recherche d’Information, Modélisation de l’utilisateur, Selection de Services de Recherche d’Information

The origin of route of text mining is the process of stemming.
It is usually used in several types of applications such as Natural Language Processing (NLP Information Retrieval (IR) and Text Mining (TM) including Text Categorization (TC Text Summarization
(TS Establish a stemmer effective for the language of Gujarati has been always a search domain hot since the Gujarati has a very different structure and difficult
that the other language due to the rich morphology.

More research in web and Information Retrieval is turning towards session-based retrieval rather than single item or query investigation.
However, most of the session detection attempts only used simplistic rules (e.g 30 mins inactivity creates a new session Up to this point, there are various fuzzy definitions of session, but no general consensus about it in the literature [3 Whilst comparably little work has involved the mental model about the "web session" from real users.
In response to these, my research focuses on web session detection involving real users with a comprehensive set of factors identified by them rather than the "simple fixed timeout My objective is to develop a session detection model with corresponding rules for each factor, and then embedded them into a Chrome Extension to automatically detect more accurate web sessions from log data.

The Third International Conference of "Perspecti~ves of Information Systems" was held in the honor of Academician Andrei Petrovich Ershov, the leader of the national programming institute and one of the outstanding Russian scientists in the field of information science from July 6 to 9, 1999, in Akademgorodok, Novosibirsk.
The first two conferences were held in 1991 and 1996 and given wide international recognition.
In Russia and abroad, they have become known as the Ershov conferences.
The proceedings of the Second Conference were published by Springer (Germany) in the Lecture Notes in Computer Science series (volume 1181).

This paper presents a machine learning method for resolving place references in text, i.e. linking character strings in documents to locations on the surface of the Earth.
This is a fundamental task in the area of Geographic Information Retrieval, supporting access through geography to large document collections.
The proposed method is an instance of stacked learning, in which a first learner based on a Hidden Markov Model is used to annotate place references, and then a second learner implementing a regression through a Support Vector Machine is used to rank the possible disabiguations for the references that were initially annotated.
The proposed method was evaluated through gold-standard document collections in three different languages, having place references annotated by humans.
Results show that the proposed method compares favorably against commercial state-of-the-art systems such as the Metacarta geo-tagger and Yahoo!
Placemaker.

Nonlinear properties of quantum states, such as entropy or entanglement, quantify important physical resources and are frequently used in quantum-information science.
They are usually calculated from a full description of a quantum state, even though they depend only on a small number of parameters that specify the state.
Here we extract a nonlocal and a nonlinear quantity, namely, the Renyi entropy, from local measurements on two pairs of polarization-entangled photons.
We also introduce a "phase marking" technique which allows the selection of uncorrupted outcomes even with nondeterministic sources of entangled photons.
We use our experimental data to demonstrate the violation of entropic inequalities.
They are examples of nonlinear entanglement witnesses and their power exceeds all linear tests for quantum entanglement based on all possible Bell-Clauser-Horne-Shimony-Holt inequalities.

One of the bottlenecks in Natural Language Processing for a given language is creating a lexicon that covers the language.
The morphological lexicon provides two important pieces of information for NLP applications: 1) the normalization of a word, its lemmatization, which allows the application to recognize two variants of the same word; and 2) the part-of-speech roles that the word can play, which allows the application to parse the text, creating relations between the words in a text.
Many NLP applications, e.g. Information Retrieval, Classification, Terminology Extraction, etc depend upon the normalization and parsing information found in lexicons.
When words are not present in these lexicons, it is difficult to predict what their proper lemmatizations and parts-of-speech are.
In this paper we present a technique for updating a lexicon given an unknown word via induction of paradigms from an existing, but incomplete, lexicon and validation of the paradigm using corpus evidence.

Fuzzy similarity measure is very useful in fuzzy information retrieval.
This study presents a new method based on fuzzy-number similarity measure to deal with fuzzy information retrieval problems.
The proposed method can obtain the suitable retrieval results for the user because it translate the degree of satisfaction F~ into a linguistic term, and user can easily determine whether the retrieved document di satisfies his/her requirement by the linguistic term.

Multimedia is any combination of text, art, sound, animation, and video delivered by computer or other electronic or digitally manipulated means (Vaughan, 2006 Through the rapid growth of multimedia technology, multimedia content can be created, shared and distributed easily.
The amount of available digital music is continuously increasing, promoted by a growing interest of users and by the development ABSTRACT

Husband-and-wife structural biologists Jamie Cate and Jennifer Doudna will take up posts at the University of California, Berkeley this year.
The moves will allow them to live and work in the same city; their labs are currently a two-hour drive apart.
Cate, an assistant professor at the Massachusetts Institute of Technology, is moving to California in July.
Doudna, who has a larger lab at Yale which will take longer to move will join him later
We’re going to be bi-coastal for a little while Doudna says.
At Berkeley both will maintain separate labs and projects.
Cate works on protein-synthesis mechanisms, including the structure of the ribosome at different states.
Doudna studies catalytic RNAs and RNAs that interact with the ribosome.
In addition to allowing them to work closer to each other, the move also means they will be closer to synchrotron beamlines, needed for X-ray crystallography elucidation of their molecules of interest.

With the consolidation of the Internet and other computer networks, the need of sophisticated systems to retrieve information increased enormously.
In these networks, information is distributed on several machines and sometimes in different formats.
In this context, we propose an architecture that besides retrieving information can classify it according to some criteria.
To implement the proposed architecture, we used some existing technologies such as mobile agents to transport information between data sources; the Naive Bayes model for classification; the Java language and the XML model for development and storage of the required information.

The objective of this paper is to show what the current techniques in image processing, artificial intelligence, and computer graphics can do in computed tomography.
More concretely, we wish to show that given the tomographic data what can be done in order to: a) improve the spatial resolution b) improve the visualisation of the data c) improve the identification of anatomic structures
Thus, we shall not deal with different hardware, nor with various reconstruction algorithms.
We shall assume that the data is given and ask what can be done from there on.
Examples, documenting each of the above points, will be presented.
Disciplines Computer Engineering Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report
MSCIS-80-15.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/1002

The team from Long Island University (LIU) participated for the first time in the TREC 2007 Legal Track Interactive Task.
We received a call for participation in mid-March 2007 while a doctoral seminar titled Information Retrieval was in session.
All nine students, evenly divided into three groups, performed this task till early May when the semester ended.
Each group worked on one topic, taken from the first three on the priority list.
The three topics are:

In this paper, we investigate the problem of automatic singer identification, detection and tracking in popular music recordings with one or multiple singers.
This problem reflects an important issue in multimedia applications that require the transcription and indexing of music data to meet the increasing demand for content-based information retrieval.
The major challenges for this study arise from the fact that a singer's voice tends to be arbitrarily altered from time to time and is inextricably intertwined with the signal of the background accompaniment.
To determine who is singing, or whether or when a particular singer is present in a music recording, methods are presented for separating vocal from nonvocal regions, for isolating singers' vocal characteristics from background music, and for distinguishing singers from one another.
Experimental evaluations conducted on a pop music database consisting of solo and duet tracks confirm the validity of the proposed methods.

The query translation of Out of Vocabulary (OOV) is one of the key factors that affect the performance of Cross-Language Information Retrieval (CLIR Based on Wikipedia data structure and language features, the paper divides translation environment into target-existence and target-deficit environment.
To overcome the difficulty of translation mining in the target-deficit environment, the frequency change information and adjacency information is used to realize the extraction of candidate units, and establish the strategy of mixed translation mining based on the frequency-distance model, surface pattern matching model and summary-score model.
Search engine based OOV translation mining is taken as baseline to test the performance on TOP1 results.
It is verified that the mixed translation mining method based on Wikipedia can achieve the precision rate of 0.6279, and the improvement is 6.98% better than the baseline.

Aim to the actual needs of marine remote sensing in China, MRSMDVS (Marine Remote
Sensing Multidimensional Dynamic Visualization System) is designed and developed to support assistant decision for reasonable exploitation of marine natural resource.
This paper starts with the introduction of COM basic principle, presents characteristics of marine remote sensing data, sets up multidimensional dynamic data structure, then brings out design framework, later develops Image Access Component and Information Retrieval Component based on COM.
By now the prototype system is already accomplished.
Based on result, it is pointed out that COM technique can expand dynamically new satellite data formats and custom remote sensing information retrieval algorithms in MRSMDVS conveniently.
Its application has good future in other fields such as module management.

We describe a method for proactive information retrieval targeted at retrieving relevant information during a writing task.
In our method, the current task and the needs of the user are estimated, and the potential next steps are unobtrusively predicted based on the user’s past actions.
We focus on the task of writing, in which the user is coalescing previously collected information into a text.
Our proactive system automatically recommends the user relevant background information.
The proposed system incorporates text input prediction using a long short-term memory (LSTM) network.
We present simulations, which show that the system is able to reach higher precision values in an exploratory search setting compared to both a baseline and a comparison system.

This talk will present challenges involved in building a Web search engine and will touch on questions of system and algorithm design, in particular as they involve large scale data processing and data mining.

This position paper supports the idea of the information dialog between IR systems and users during an information search task.
In order to satisfy the communication and interaction needs of humans, IR systems should explicitly support the cognitive abilities of the users.
An information dialogue which does not only support an individual query but also the complete search process is necessary.
Only in this way it is possible to satisfy an information need.

Information Retrieval techniques make use of terms that are automatically extracted from documents; these terms are used to give information access.
In this paper we propose an approach to enrich semantically this extraction by adding knowledge from thesaurus.
More specifically, the methodology we promote in this paper aims at transforming a thesaurus into a domain ontology which will then be used to semantically index documents (indexes are concepts rather than terms We also propose techniques that implement this transformation as well as an evaluation in the field of the astronomy.

s. IEEE Computer Graphics and Application, 22, 50–58.
Spainhour, S Eckstein, R 1999 Webmaster in a nutshell.
Sebastopol, CA: O’Reilly Associates.
Tijssen, R.J.W van Raan, A.F.J 1990 Net citation balances: A measure of influence between scientific journals.
Journal of the American Society for Information Science, 41, 298–304.
White, H.D McCain, K.W 1998
Visualizing a discipline: An author co-citation analysis of information science.
Journal of the American Society for Information Science, 49, 327–356.
Willekens, F.J 1983 Specification and calibration of spatial interaction models.
A contingency-table perspective and an application to intraurban migration in Rotterdam.
Tijdschrift voor Economische en Sociale Geografie, 73, 239–252.
22 JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY
January 1, 2004

IRISA participation to DeFT 2012 information retrieval and machine learning for keyword generation
This paper describes the IRISA participation to the DeFT 2012 text-mining challenge.
It consisted in the automatic attribution or generation of keywords to scientific journal articles.
Two tasks were proposed which led us to test two different strategies.
For the first task, a list of keywords was provided.
Based on that, our first strategy is to consider that as an Information Retrieval problem in wich the keyword are the queries, which are attributed to the best ranked documents.
This approach yielded very good results.
For the second task, only the articles were known; for this task, our approach is chiefly based on a term extraction system whose results are reordered by machine learning.
MOTS-CLÉS Génération de mots-clés, Extraction de termes, Recherche d’information, Boosting, arbres de décision, TermoStat.

The rising quantity of available information has constituted an enormous advance in our daily life.
However, at the same time, some problems emerge as a result from the existing difficulty to distinguish the necessary information among the high quantity of unnecessary data.
Information Retrieval has become a capital task for retrieving the useful information.
Firstly, it was mainly used for document retrieval, but lately, its use has been generalized for the retrieval of any kind of information, such as the information contained in a database, a web page, or any set of accumulated knowledge.
In particular, the so-called Vector Space Model is widely used.
Vector Space Model is based on the use of

The COSIMIR-Modell (Cognitive Similarity learning in Information Retrieval) presented in this paper, consists of of neural network based on the backpropagation algorithm.
COSIMIR learns to calculate the similarity between query and document using users‘ judgements.
Thus, it avoids the heuristic choice of a similarity function.
COSIMIR intends to model the core of the Information Retrieval process in a way cognitivly adaquate.
Numerous IR systems are already based on neural networks, however, they resemble common IR models and do not exploit all possibilities of neural networks.

August 2006 saw the third in a series of workshops held to discuss the state of the art in Geographic Information Retrieval held in conjunction with SIGIR'06.
The original call for papers for the workshop suggested the workshop should discuss further progress within the field and potential future research strands.
Topics considered relevant for the workshop included the following 8226; architectures for geographic search engines 8226; spatial indexing of documents and other media resources 8226; extraction of geographical context from documents and geo-datasets 8226; geographical annotation techniques for geo-referenced media 8226; design, construction, maintenance and access methods for geographical ontologies, gazetteers and geographical thesauri 8226; geographical query interfaces for the web and geo-spatial libraries 8226; visualising the results of geographic searches; and&#8226; relevance ranking for geographical search.

With the increasingly rich of Web resources, people need to cross-language information and knowledge sharing, cross-language information retrieval (CLIR) research on such problems.
This paper brings forward a Web cross language information retrieval model based on the domain ontology.
The model basing on the technologies of traditional information retrieval, use domain ontology to describe the relevant domain knowledge in different kinds of languages, comprehend and extend query terms, lead the retrieval rise to semantic level.
The main ideal and methods of the model are described in detail, and through experiment to validate our approach.
The experiment are designed to retrieval travel news in Chinese from Sina website with query in English, its results prove that this model can improved the average precision/recall of retrieval to certain extent.

Most information retrieval systems use stopword lists and stemming algorithms.
However, we have found that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results.
We present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding indexing terms that do not.
In two different domains, relevancy signatures produced better results than the simple indexing terms.
These experiments suggest that stopword lists and stemming algorithms may remove or conflate many words that could be used to create more effective indexing terms.

When building vision systems that predict structured objects such as image segmentations or human poses, a crucial concern is performance under task-specific evaluation measures (e.g Jaccard Index or Average Precision
An ongoing research challenge is to optimize predictions so as to maximize performance on such complex measures.
In this work, we present a simple meta-algorithm that is surprisingly effective x2013 italic>
Empirical Min Bayes Risk</italic EMBR takes as input a pre-trained model that would normally be the final product and learns three additional parameters so as to optimize performance on the complex instance-level high-order task-specific measure.
We demonstrate EMBR in several domains, taking existing state-of-the-art algorithms and improving performance up to 8 percent, simply by learning three extra parameters.
Our code is publicly available and the results presented in this paper can be replicated from our code-release.

Collaborative filtering is a technique for recommending documents to users based on how similar their tastes are to other users.
If two users tend to agree on what they like, the system will recommend the same documents to them.
The generalized vector space model of information retrieval represents a document by a vector of its similarities to all other documents.
The process of collaborative filtering is nearly identical to the process of retrieval using GVSM in a matrix of user ratings.
Using this observation, a model for filtering collaboratively using document content is possible.

The problem of spatial configuration information retrieval is a constraint satisfaction problem (CSP which can be solved using traditional CSP algorithms.
But the spatial data are generally reorganized using index techniques like R-tree to improve the efficiency of query answering in spatial information systems, especially geographic information system (GIS and the spatial data are approximated by their minimum bounding rectangles (MBRs so the spatial configuration information retrieval is actually based on the MBRs and some special techniques should be studied.
This paper studies the mapping relationships among the spatial relations for real spatial objects, the corresponding spatial relations for their MBRs and the corresponding spatial relations between the intermediate nodes and the MBRs in R-tree.
Based on these study results, three search algorithms are designed and presented.

The Web 2.0, having the user both creating and organizing content, has changed much of how one approaches to and uses the Web.
While the concept of user-submitted content is by no means new, user created organizational structure is.
The article gives an overview of the organizational means and processes that enable it.
To provide the general framework the article gives a short overview of Web 2.0.
It then centres on the collaborative tagging process as a central organizational process and means for the Web 2.0 and provides definitions for the Web 2.0 terminology used.
After describing the general process, its strengths and weaknesses and pointing out that, while useful, it cannot replace professional indexing tools and library and information science professionals the article goes on to describe collaborative tagging and its specific features in general.
Some of the more common services to use collaborative tagging are then described.

The distributed system enables multiple, simultaneous connections between clients and Inquery servers.
The different components of the system communicate using a local area network.
Each component may reside on a different host and operates independently of the others.
In this section, we describe the functionality and interaction between the clients, the connection server, and the Inquery servers.
The clients are lightweight processes that provide a user interface to the retrieval system.
Clients interact with the distributed IR system by connecting to the connection server.
The clients initiate all work in the system, but they perform very little computation.
The clients can issue the entire range of IR commands but, in this paper, we focus on inquery, document retrieval commands and query evaluation measurements.
A client sends query commands to the connection server.

Information retrieval in medical domain is now sharing major part of the web search.
Now a day’s most of the people especially adults are browsing health care and medical information at their homes using internet.
Electronic Medical Information Retrieval System (EMIRS) through search engines providing positive information to the user based on the fixed questionnaires.
In this paper we build a model for naïve users, who are having minimal knowledge to feedback the system by opting listed relevant questionnaire.
Along with the framework, we also built an Intelligent Medical Search Engine (IMSE) for searching medical information on World Wide Web (
The implementation setup of IMSE uses medical Ontology and questionnaire to facilitate naive internet users to search for medical information.
IMSE introduces and extends expert system technology into the search engine domain.
IMSE uses several key techniques to improve its usability and search result quality.

Visual attribute classification has been widely discussed due to its impact on lots of applications, such as face recognition, action recognition and scene representation.
Recently, Convolutional Neural Networks (CNNs) have demonstrated promising performance in image recognition, object detection and many other computer vision areas.
Such networks are able to automatically learn a hierarchy of discriminate features that richly describe image content.
However, dimensions of features of CNNs are usually very large.
In this paper, we propose a visual attribute classification system based on feature selection and CNNs.
Extensive experiments have been conducted using the Berkeley Attributes of People dataset.
The best overall mean average precision (mAP) is about 89.2%.

High quality relevance judgments are essential for the evaluation of information retrieval systems.
Traditional methods of collecting relevance judgments are based on collecting binary or graded nominal judgments, but such judgments are limited by factors such as inter-assessor disagreement and the arbitrariness of grades.
Previous research has shown that it is easier for assessors to make pairwise preference judgments.
However, unless the preferences collected are largely transitive, it is not clear how to combine them in order to obtain document relevance scores.
Another difficulty is that the number of pairs that need to be assessed is quadratic in the number of documents.
In this work, we consider the problem of inferring document relevance scores from pairwise preference judgments by analogy to tournaments using the Elo rating system.
We show how to combine a linear number of pairwise preference judgments from multiple assessors to compute relevance scores for every document.

The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics.
More, the semantic coherence of the topics has never been considered in this field.
We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics.
We perform a first experimental evaluation using two major TREC test collections.
Results show that retrieval performances tend to be better when using topics with higher semantic coherence.

This paper presents the 2006 Miracle team
’s approaches to the AdHoc and Geographical Information Retrieval tasks.
A first set of runs was obtained using a set of basic components.
Then, by putting together special combinations of these runs, an extended set was obtained.
With respect to previous campaigns some improvements have been introduced in our system: an entity recognition prototype is integrated in our tokenization scheme, and the performance of our indexing and retrieval engine has been improved.
For GeoCLEF, we tested retrieving using geo-entity and textual references separately, and then combining them with different approaches.

The rapid development of information technology offers people a broad information-sharing platform.
We can get relevant information through information retrieval tools.
In order to get higher search efficiency, the information retrieval technologies based on semantic grid are discussed.
Based on analysis of the technologies of grid, semantic grid and information retrieval, the architecture of grid information retrieval system is brought out and a common model of grid information retrieval is established.
The grid platform based on Globus toolkit is built which is made as the basic development platform of the system, the semantic grid information retrieval system under that platform is realized, and the designed model and algorithm are implemented and the specific experimental tests is realized.
The result shows that the system has better retrieval recall factor and pertinency factor than the conventional retrieval methods, and the feasibility of system model is verified.

Information retrieval needs to match relevant texts with a given query.
Selecting appropriate parts is useful when documents are long, and only portions are interesting to the user.
In this paper, we describe a method that extensively uses natural language techniques for text segmentation based on topic change detection.
The method requires a NLP-parser and a semantic representation in Roget-based vectors.
We have run the experiment on French documents, for which we have the appropriate tools, but the method could be transposed to any other language with the same requirements.
The article sketches an overview of the NL understanding environment functionalities, and the algorithms related to our text segmentation method.
An experiment in text segmentation is also presented and its result in an information retrieval task is shown.

The Relevant in Context retrieval task is document or article retrieval with a twist, where not only the relevant articles should be retrieved but also the relevant information within each article (captured by a set of XML elements) should be correctly identified.
Our main research question is: how to evaluate the Relevant in Context task?
We propose a generalized average precision measure that meets two main requirements: i) the score reflects the ranked list of articles inherent in the result list, and at the same time ii) the score also reflects how well the retrieved information per article (i.e the set of elements) corresponds to the relevant information.
The resulting measure was used at INEX 2006.

Samir Kumar Jalal, Subal Chandra Biswas and Parthasarathi Mukhopadhyay Assistant Librarian, Central Library, Birla Institute of Technology, Mesra, Ranchi, Jharkhand, Email: jalal_gtz@yahoo.co.in 2 Professor, Department of Library Information Science, University of Burdwan, West Bengal, Email:
scbiswas_56@yahoo.co.in 3 Sr. Lecturer, Department of Library Information Science, University of Burdwan, West Bengal, Email:
psmukhopadhyay@gmail.com

An information structure (IS) that is regarded as a formal description of a domain of discourse is proposed.
This IS is aimed at increasing the effectiveness of an information retrieval system.
It is shown how the retrieval algorithm can take into account the term dependencies that are provided by the IS.
Moreover, these term dependencies can be used by an automatic indexing procedure in order to interpret polysemic terms.
The theoretical framework of our IS has some favorable properties.
As a consequence, the construction and maintenance of such an IS is simpler than that of a thesaurus.

Termverarbeitung in der Domäne der Biowissenschaften beinhaltet für Information Retrieval, Data Mining, Information Extraction und für die Pflege wissenschaftlicher
Datenbanken eine Reihe von Herausforderungen.
Wir beschreiben diese Problematik und stellen unsere beiden Lösungsansätze vor.
Dabei handelt es sich zum einen um ein normalisiertes
Namensmatching und zum anderen um eine semantische Namensverarbeitung.

Due to the dramatically increasing amount of available data, effective and scalable solutions for data organization and search are essential.
Distributed solutions naturally provide promising alternatives to standard centralized approaches.
With the computational power of thousands or millions of computers in clusters or peer-to-peer systems, the challenges that arise are manifold, ranging from efficient resource discovery to issues in load balancing and distributed query processing.
The 2008 edition of the Workshop on Large-Scale Distributed Systems for Information Retrieval (LSDS-IR’08) provided a forum for researchers to discuss these problems and to define new directions for the work on Distributed Information Retrieval.
The Workshop program featured research contributions in the areas of similarity search, resource selection, network organization schemes, issues of data quality, result ranking techniques and query routing algorithms.

The object of our study is the Bayesian approach in solving computer vision problems.
We examine in particular i) applications of Markov random field (MRF) models to modeling spatial images
ii) MRF based statistical methods for image restoration, segmentation, texture modeling and integration of different visual cues.
Comments University of Pennsylvania Department of Computer and Information Science
Technical Report
No. MSCIS-92-29.
This technical report is available at ScholarlyCommons: http repository.upenn.edu/cis_reports/491 Markov Random Field Models:
A Bayesian Approach To Computer Vision
Problems MS-CIS-92-29 GRASP LAB

A Fuzzy Linguistic Approach Based on Semantic Web J. M. Morales-del-Castillo,1 Eduardo Peis,2 Antonio A. Ruiz,2 E. Herrera-Viedma3 Department of Library and Information Science, Carlos III University, Madrid, Spain Department of Library and Information Science, University of Granada, Granada, Spain Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain

This paper deals with Chinese, English and Japanese multilingual information retrieval.
Several merging strategies, including raw-score merging, round-robin merging, normalized-score merging, and normalizedby-top-k merging, were investigated.
Experimental results show that centralized approach is better than distributed approach.
In distributed approach, normalized-by-top-k with consideration of translation penalty outperforms the other merging strategies.

Semantic analysis and annotation of textual information with appropriate semantic entities is an essential task to enable content based search on the annotated data.
For video resources textual information is rare at first sight.
But in recent years the development of technologies for automatic extraction of textual information from audio visual content has advanced.
Additionally, video portals allow videos to be annotated with tags and comments by authors as well as users.
All this information taken together forms video metadata which is manyfold in various ways.
By making use of the characteristics of the different metadata types context can be determined to enable sound and reliable semantic analysis and to support accuracy of understanding the video’s content.
This paper proposes a description model of video metadata for semantic analysis taking into account various contextual factors.

Nowadays, language resources, such as machine readable dictionaries, WordNets, thesaurus, ontology, and semantic networks, are becoming crucial bases for many intelligent systems like Machine Translation, Information Retrieval, and Natural Language Understanding, etc.
The purpose of this work is to extract features and to analyze the manually built Korean wordnet, namely U-WIN, for the automatic construction of a feature based U-WIN.
We used ICA and CBC clustering methods and methodologies in the experiments and compared their results.
Further we have identified various issues affecting the quality of clustering score.

Text searching problem has been tackled by variety of traditional string matching approaches over the years.
This paper proposes an intelligent text searching technique for information retrieval using probabilistic approach, called mix heuristic algorithm (MHA
In the proposed algorithm, an intelligent programming approach using logical operators has been used that makes this algorithm intelligent and user friendly.
We are also presenting a short survey of well-known sequential string matching algorithms.
We test MHA implementation and present experimental results for different text matching.

In this article we investigate the expressions of collaborative activities within information seeking and retrieval processes (IS&R Generally, information seeking and retrieval is regarded as an individual and isolated process in IR research.
We assume that an IS&R situation is not merely an individual effort, but inherently involves various collaborative activities.
We present empirical results from a real-life and information-intensive setting within the patent domain, showing that the patent task performance process involves highly collaborative aspects throughout the stages of the information seeking and retrieval process.
Furthermore, we show that these activities may be categorized and related to different stages in an information seeking and retrieval process.
Therefore, the assumption that information retrieval performance is purely individual needs to be reconsidered.
Finally, we also propose a refined IR framework involving collaborative aspects.

The paper describes the use of web based content mining technology that in incorporates role of multi agent systems to perform an intelligent search.
In addition to this, an information retrieval (IR) system has been proposed that helps in deducing various inferences by interacting with agents and recording their behavior.
This multi agent system (MAS) design can be implemented by using JADE (Java Agent Development Environment)

This paper presents an approach to bilingual lexicon extraction from comparable corpora and evaluations on Cross-Language Information Retrieval.
We explore a bi-directional extraction of bilingual terminology primarily from comparable corpora.
A combined statistics-based and linguistics-based model to select best translation candidates to phrasal translation is proposed.
Evaluations using a large test collection for Japanese-English revealed the proposed combination of bi-directional comparable corpora, bilingual dictionaries and transliteration, augmented with linguistics-based pruning to be highly effective in Cross-Language Information Retrieval.

Browse with either web directories or social bookmarks is an important complementation to search by keywords in web information retrieval.
To improve users' browse experiences and facilitate the web directory construction, in this paper, we propose a novel browse system called Social Web Directory (SWD for short) by integrating web directories and social bookmarks.
In SWD 1) web pages are automatically categorized to a hierarchical structure to be retrieved efficiently, and (2) the popular web pages, hottest tags, and expert users in each category are ranked to help users find information more conveniently.
Extensive experimental results demonstrate the effectiveness of our SWD system.

Reliable H control for discrete uncertain time-delay systems with randomly occurring nonlinearities:
the output feedback case Yisha Liu
a Zidong Wang b c Wei Wang a a Research Centre of Information and Control, Dalian University of Technology,
Dalian 116023, China b School of Information Science and Technology, Donghua University, 200051 Shanghai, China c Department of Information Systems and Computing, Brunel University, Uxbridge, Middlesex, UB8 3PH, UK Version of record first published: 06 Nov 2010.

This paper presents a novel approach based on one of the signal processing tools in soft computing applied to Web information retrieval, namely wavelet transform.
The influence of two parameters, wavelet functions (Mother wavelets) and decomposition level, on feature extraction and information retrieval ability of calibration model was investigated.
The experimental results show that the proposed method performs accurate retrieval.
This work is a step towards multilingual (English, Spanish, Arabic, Chinese (Simplified and Traditional Korean, and Japanese) search engine.

Query performance prediction aims to estimate the quality of answers that a search system will return in response to a particular query.
In this paper we propose a new family of pre-retrieval predictors based on information at both the collection and document level.
retrieval predictors are important because they can be calculated from information that is available at indexing time; they are therefore more efficient than predictors that incorporate information obtained from actual search results.
Experimental evaluation of our approach shows that the new predictors give more consistent performance than previously proposed pre-retrieval methods across a variety of data types and search tasks.

Information retrieval components are currently incorporated in several types of information systems, including bibliographic retrieval systems, data base management systems and question-answering systems.
Some of the problems arising in the real-time environment in which these systems operate are briefly discussed.
Certain recent advances in information retrieval research are then mentioned, including the formulation of new probabilistic retrieval models, and the development of automatic document analysis and Boolean query processing techniques.

Researchers are aware that context affects information retrieval in general.
The health area is no exception and is particularly rich in terms of context.
To understand how context is used in health information research, we collected a sample of health information research papers that use context features.
Papers were analyzed and classified according to the type of context features and to the stage of the retrieval process into which they were incorporated.
Further, we also identified the specific context features used in each category of features and each stage of the process.
Results show a weaker use of interaction context features than we expected and, as supposed, a large use of collective features.
A considerable number of papers use context to query related activities.
We also found that research is mainly aimed at health professionals, suggesting a gap in health consumers research that should be explored.

4900 Yu Cao, Shawn Steffey, Jianbiao He, Degui Xiao, Cui Tao Ping Chen, Henning Müller 6 Department of Computer Science, The University of Massachusetts Lowell Lowell, MA 01854, USA; Email: ycao@cs.uml.edu; Phone 978)934-3628; Fax 978)934-3551 School of Information Science and Engineering, Central South University, Changsha, P.R. China 410083; Email: jbhe@mail.csu.edu.cn College of Computer Science and Electronic Engineering, Hunan University, Changsha, P.R. China 410082; Email: dgxiao@hnu.edu.cn School of Biomedical Informatics, The University of Texas, Health Science Center at Houston, Houston, TX 77030; Email: Cui.Tao@uth.tmc.edu Department of Computer Science, The University of Massachusetts Boston, Boston, MA 02125, USA; Email: Ping.Chen@umb.edu Information Systems, University of Applied Sciences Western Switzerland (HES-SO Medical Informatics, University Hospitals and University of Geneva, Switzerland; Email: henning.mueller@hevs.ch Co-corresponding Authors

The notion of theme plays a crucial role in topic-based Information Retrieval.
We discuss how topics are related to works in linguistic and discourse theories, and according to which rules they can be derived from texts.
Two experiments were devised: the first one to validate those rules with users, and the second to implement them with a collection of structured documents.
In the first experiment, participants were asked either to choose between possible themes for an expression, or to find relevant themes in a short text.
In the second experiment, a prototypal IRS was built to index the TEI Guidelines a collection of SGML documents.
Results for both experiments show the need for a new, more flexible measure of theme representativity that allow different rankings according to user or query types.

This study analyzed the information seeking behaviour of the teachers and students at College of Engineering, King Saud University, Riyadh, Kingdom of Saudi Arabia.
In this study, data collected from 150 teachers and students by administering questionnaires on their information seeking and requirements of the College of Engineering, indicates that guidance in the use of library resources and services is necessary to help teachers and students to meet their information requirements.
Found that journals, textbooks and electronic information sources are the most popular sources of information for the students’ course work.
Recommends that latest edition of textbooks and reference materials should add to the library collections.
Suggests that the CD-ROM databases of journal archives and reference books should be added and users should be guided to use the resources of the library.
Information Seeking, Information Retrieval, Users Studies, Riyadh, Saudi Arab.

Direction relations between extended spatial objects are important commonsense knowledge.
Recently, Goyal and Egenhofer proposed a formal model, called Cardinal Direction Calculus (CDC for representing direction relations between connected plane regions.
CDC is perhaps the most expressive qualitative calculus for directional information, and has attracted increasing interest from areas such as artificial intelligence, geographical information science, and image retrieval.
Given a network of CDC constraints, the consistency problem is deciding if the network is realizable by connected regions in the real plane.
This paper provides a cubic algorithm for checking consistency of basic CDC constraint networks.
As one byproduct, we also show that any consistent network of CDC constraints has a canonical realization in digital plane.
The cubic algorithm can also been adapted to cope with disconnected regions, in which case the current best algorithm is of time complexity O(n).

Information retrieval is the important work of Supply Chain Management (SCM) on the Semantic Web.
Ontology-based semantic retrieval is a hotspot of current research.
In order to achieve fuzzy semantic retrieval, this paper applies a fuzzy ontology framework to information retrieval system in SCM.
The framework includes three parts: concepts, properties of concepts and values of properties, in which propertypsilas value can be either standard data type or linguistic values of fuzzy concepts.
The semantic query expansion is constructed by order relation, equivalence relation and inclusion relation between fuzzy concepts defined in fuzzy linguistic variable ontologies.
The application to retrieve customer and product information in supply chain shows that the framework can overcome the localization of other fuzzy ontology models, and this research facilitates the semantic retrieval of information through fuzzy concepts on the Semantic Web.

Statistical language models have recently been successfully applied to many information retrieval problems.
A great deal of recent work has shown that statistical language models not only lead to superior empirical performance, but also facilitate parameter tuning and open up possibilities for modeling nontraditional retrieval problems.
In general, statistical language models provide a principled way of modeling various kinds of retrieval problems.
The purpose of this survey is to systematically and critically review the existing work in applying statistical language models to information retrieval, summarize their contributions, and point out outstanding challenges.

In this paper, we introduce AMusE, a music reasoning framework built using Abstract Data Types, describing some of the advantages of such an approach.
We illustrate this with a particular set of tools capable of tapping into the framework
’s capabilities
: a score editor with the capability to edit some early music notations, and implementations of the SIA family of Music Information Retrieval tools.
Putting these together, we discuss how the framework enables the general-purpose pattern-matching tool to operate on very specialised forms of notation.
We conclude with a discussion of further work and the need for a more logic-based design and a more outward-looking deployment.

This paper investigates the number of expansion terms to use in automatic query expansion by examining the behavior of eight retrieval systems participating in the NRRC Reliable Information Access Workshop.
The results demonstrate that current systems are able to obtain nearly all of the benefit of using a fixed number of expansion terms per topic, but significant additional improvement is possible if systems were able to accurately select the best number of expansion terms on a per topic basis.
When optimizing average effectiveness as measured by mean average precision, using a fixed number of terms increases the score a large amount for a small number of topics but has little effect for most topics.
The analysis further suggests that when a topic is helped by automatic feedback, the increase is from a set of terms that reinforce each other rather than from the system finding a single excellent term.

In this paper we take a critical look at the evaluation method of WebCLEF 2007.
The suitability of the evaluation method can be seen from two sides, namely from a participating system and a non participating system.
A participant has the advantage that the evaluation is partly based upon his output.
In this paper we will investigate if the size of the pool of snippets, the implementation of the evaluation method and the quality of the assessments is sufficient enough for reliable evaluation.
Unfortunately we have to conclude that the evaluation is not suitable.
Therefore some alternative evaluation methods will be discussed concluding in a recommendation to improve the evaluation of WebCLEF.

This poster session examines a probabilistic approach to distributed information retrieval using a Logistic Regression algorithm for estimation of collection relevance.
The algorithm is compared to other methods for distributed search using test collections developed for distributed search evaluation.

Music Information Retrieval systems are commonly built on a feature extraction stage.
For applications involving automatic classification (e.g. speech/music discrimination, music genre or mood recognition traditional approaches will consider a large set of audio features to be extracted on a large dataset.
In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efficient feature extraction.
In this paper, a new audio feature extraction software, YAAFE 1 is presented and compared to widely used libraries.
The main advantage of YAAFE is a significantly lower complexity due to the appropriate exploitation of redundancy in the feature calculation.
YAAFE remains easy to configure and each feature can be parameterized independently.
Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License.

In this paper we present the second participation of the NLP&IR group at UNED in the MediaEval Genre Tagging Task.
This categorization task was carried out applying an Information Retrieval (IR) approach considering the video collection’s textual data and query expansion techniques.
The results show that the combination of social tags and language models is useful to perform query expansion.

It a great pleasure for me to extend a welcome to all of you who are participating in SNPD 2016, the 17th International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2016 which is being held in Shanghai, China.
The conference is sponsored by the IEEE Computer Society and the International Association for Computer and Information Science (ACIS in cooperation with the Software Engineering and Information Technology Institute at Central Michigan University, USA.
It is organized by Shanghai University and co-organized by Shanghai Key Laboratory of Computer Software Testing Evaluating.

In this paper we proposed an ontology-based approach for reducing the ambiguity of bilingual translation.
A metric based on the ontological chain for co-occurrence concept evaluation has also been proposed.
Based on the ontology chain we can calculate the best translation of all possible translated terms.
The experimental results show that the proposed approach can effectively remove the irrelevant results and improve the precision.
Key-words: Cross Language Information Retrieval; Ontology; Translation Ambiguity;

Lexical ambiguity is a pervasive problem in natural language processing.
However, little quantitative information is available about the extent of the problem or about the impact that it has on information retrieval systems.
We report on an analysis of lexical ambiguity in information retrieval test collections and on experiments to determine the utility of word meanings for separating relevant from nonrelevant documents.
The experiments show that there is considerable ambiguity even in a specialized database.
Word senses provide a significant separation between relevant and nonrelevant documents, but several factors contribute to determining whether disambiguation will make an improvement in performance.
For example, resolving lexical ambiguity was found to have little impact on retrieval effectiveness for documents that have many words in common with the query.
Other uses of word sense disambiguation in an information retrieval context are discussed.

The Voorhees/Buckley swap method is useful for comparing the discrimination power of Information Retrieval (IR) and Question Answering (QA) metrics.
Given a test collection, a set of runs and an evaluation metric, it derives the swap rate, the chance of observing inconsistencies when two completely different topic sets are used for comparing a pair of runs.
Recently, however, Sanderson and Zobel claimed that the method overestimates swap rates as it samples topics without replacement.
The main question we address in this paper is whether sampling with and without replacement produce any different results for the purpose of comparing the sensitivity of different metrics.
Our IR and QA experiments show that the two methods do generally yield similar results, which suggests that the original Voorhees/Buckley method is valid.

We introduce static index pruning methods that significantly reduce the index size in information retrieval systems.
We investigate uniform and term-based methods that each remove selected entries from the index and yet have only a minor effect on retrieval results.
In uniform pruning, there is a fixed cutoff threshold, and all index entries whose contribution to relevance scores is bounded above by a given threshold are removed from the index.
In term-based pruning, the cutoff threshold is determined for each term, and thus may vary from term to term.
We give experimental evidence that for each level of compression, term-based pruning outperforms uniform pruning, under various measures of precision.
We present theoretical and experimental evidence that under our term-based pruning scheme, it is possible to prune the index greatly and still get retrieval results that are almost as good as those based on the full index.

A distinct property of personal information retrieval compared to other information retrieval processes is that the piece of information sought for is already known to the user.
It is a process where the user has seen and possibly processed the item on an earlier occasion and now needs it again for some reason.
The time span between dealing with the item and retrieving it again may vary from a few minutes to several years.
To deal with the increasing amount and types of user items, personal information retrieval systems have been developed.
In this article, I will have a brief look at some desktop search engines and more closely look at Phlat, a personal information retrieval system that aims at using all imaginable information a user can remember regarding an item to find it.

Emotions are an integral component of all human activities, including human-computer interactions.
This article reviews literature on the theories of emotions, methods for studying emotions, and their role in human information behaviour.
It also examines current research on emotions in Library and Information Science, Information Retrieval and Human-Computer Interaction, and outlines some of the challenges and directions for future work.

Textual queries, often short and ambiguous, can be insufficient when describing complex user information needs.
Since users are reluctant or unable to provide long or precise descriptions, a possible solution to the low Information Retrieval (IR) system relevance prediction capability is to exploit diverse sources of evidence which are available during the search process.
One of the open problems of the combination of diverse sources of evidence is the need of a uniform formalism which seamlessly describes the sources and the document ranking function within a single model.
To this end, this paper discusses an IR view which explicitly considers other sources in addition to the information need and the document, and proposes a methodology to exploit them to support feedback.
The IR view is described using the Entity-Relationship (ER) model which allows us to view the sources as properties of entities e.g. of the entity information need, document, or user or of their relationships.

In the area of information retrieval, the dimension of document vectors plays an important role.
Firstly, with higher dimensions index structures suffer the “curse of dimensionality” and their efficiency rapidly decreases.
Secondly, we may not use exact words when looking for a document, thus we miss some relevant documents.
LSI (Latent Semantic Indexing) is a numerical method, which discovers latent semantic in documents by creating concepts from existing terms.
However, it is hard to compute LSI.
In this article, we offer a replacement of LSI with a projection matrix created fromWordNet hierarchy and compare it with LSI.

Evaluating contributions from users of systems with large datasets is a challenge across many domains, from task assessment in crowdsourcing to document relevance in information retrieval.
This paper introduces a model for rewarding and evaluating users using retrospective validation, with only a small gold standard required to initiate the system.
A simulation of the model shows that users are rewarded appropriately for high quality responses however analysis of data from an implementation of the model in a text annotation game indicates it may not be sophisticated enough to predict user performance.

A classical information retrieval system ranks documents according to distances between texts and a user query.
The answer list is often so long that users cannot examine all the documents retrieved whereas some relevant ones are badly ranked and thus never retrieved.
To solve this problem, retrieved documents are automatically clustered.
We describe an algorithm based on hierarchical and clustering methods.
It classifies the set of documents retrieved by any IR-system.
This method is evaluated over the TREC-7 corpora and queries.
We show that it improves the results of the retrieval by providing users at least one high precision cluster.
The impact of the number of clusters and the way to browse them to build a reordered list are examined.
Over TREC corpora and queries, we show that the choice of the number of clusters according to the length of queries improves results compared with a prefixed number.

The growth of the Web has posed new challenges for Information Retrieval (IR Most of the current systems are based on traditional models, which have been developed for atomic and independents documents and are not adapted to the Web.
A promising research orientation consists of studying the impact of the Web structure on indexing.
The HyperDocument model presented in this article is based on essential aspects of information comprehension: content, composition and linear/non-linear reading.

This talk provides an overview of research opportunities in adaptive search, with particular emphasis on web search and the consequent challenges of scale.
Examples of adaptive search will be drawn from web search engines, and based in part on these examples, I will explore how adaption applies to search processes and search interfaces.
The implications of web scale for research will be considered from various viewpoints: types of user, types of search, and search engine performance.
Evaluation of adaptive web search remains a challenge, and I will briefly cover types of experiment, including access to collections and research tools.
Some concluding remarks will be made on possible research directions and activities for the information retrieval research community.
Second International Workshop on Adaptive Information Retrieval, London, UK

In addition to the frequency of terms in a document collection, the spatial distribution of terms plays an important role in determining the relevance of documents.
In this paper, a new approach for representing term positions in documents is presented.
The approach allows us to efficiently apply term-positional information at query evaluation time.
Two applications are investigated: a function-based ranking optimization representing a user-defined document region, and a query expansion technique based on overlapping the term distributions in the top-ranked documents.
Experimental results demonstrate the effectiveness of the proposed approach.

This paper describes query translation using language resources and a concept base method for Cross-language Information Retrieval (CLIR
In the proposed method, queries are translated by multiple machine translation systems on the Language Grid.
The queries are then expanded by using a bilingual dictionary to translate compound words or word phrases.
In addition, documents related to the translated query are retrieved with a TFIDF term weighting model.
The top 100 retrieved documents are re-ranked by a specificity-considered concept base with the noun phrases and compound words extracted from the query.
The reranked results are combined with the results retrieved by the probabilistic model.
For evaluation of the proposed method, we use the average precision of the non-interpolated recall and precision to compare our method with the NTCIR1 participation systems.
The proposed method achieved the highest precision.

Applying human cognition to a search engine for information retrieval is an emerging task employed to various implementations and one among them is natural language understanding by the machine in a semantic manner.
Natural language processing systems will be constructed using inference engines along with a knowledge base (KB) to store rules and facts.
High-Performance Linguistics (HPL) Scheme is an expert Knowledge base-Natural Language Processing System (K-NLP) that encompasses predicate clauses.
Principally this paper focuses about knowledge representation module of the System.
It utilizes the first-order logic which is a formal language that is used to train the semantic search engines to give effective results for natural language through the knowledge base.
Thus the machine provides relevant and accurate information for the user queries by preserving semantics of the natural language query.

In English written text, words are separated by spaces, but in written Chinese text, there are no such separators between words See Figure 1
Thus, effective information retrieval of Chinese text first requires good word segmentation.
In this paper, we investigate an efficient algorithm to discover the words and their occurrence probabilities from a corpus of unsegmented text without using a dictionary.
Using the probabilities of the words, word segmentation is done according to the maximum likelihood principle.
Comparing the segmentation output by the algorithm with the correct segmentation, recall/precision of 65.65%/71.91% is achieved.
If some simple post-processing is performed, recall/precision can be boosted up to 97.72%/91.05%.

(2) W. C. Zipperer, R. E. Steams, Jr and M. K. Park
The Integrated Subject File.
I. Data Base Characteristics J Chem.
Doc 13,92-98 (1973 3) W. C. Zipperer, M. K. Park, and J. L. Carmon
The CA Integrated Subject File.
Evaluation of Alternative Data Base Organizations J Chem.
Doc 14, 15-23 (1974 4) C. H. ODonohue Profiling, the Key to Successful Information Retrieval J. Chem.
Doc 14, 29-31 (1974 5 B. G. Prewitt
On-Line Searching of Computer Data Bases J Chem.

Much of the difficulty in Music Information Retrieval can be traced to problems of good music representations, understanding music structure, and adequate models of music perception.
In short, the central problem of Music Information Retrieval is Music Understanding, a topic that also forms the basis for much of the work in the fields of Computer Music and Music Perception.
It is important for all of these fields to communicate and share results.
With this goal in mind, the author’s work on Music Understanding in interactive systems, including computer accompaniment and style recognition, is discussed.

This paper reports preliminary work on developing methods automatically to index cases described in text so that a case-based reasoning system can reason with them.
We are employing machine learning algorithms to classify full-text legal opinions in terms of a set of predefined concepts.
These factors, representing factual strengths and weaknesses in the case, are used in the casebased argumentation module of our instructional environment CATO.
We first show empirical evidence for the conncetion between the factor model and the vector representation of texts developed in information retrieval.
In a set of hypotheses we sketch how including knowledge about the meaning of the factors, their relations and their use in the case-based reasoning system can improve learning, and discuss in what ways background knowledge about the domain can be beneficial.
The paper presents initial experiments that show the limitations of purely inductive algorithms for the task.

This paper presents a model that aims to support knowledge retrieval stored in digital repositories through domain ontologies.
In this model the ontology contains concepts and relationships which describe a specific part of the world.
The model mechanisms aim to reduce the impact of some of the main obstacles identified in the Information Retrieval process such as user specific characteristics, natural language characteristics or retrieval systems limitations.
As a result the user, by providing a query to the system, can retrieve relevant information which better meet his information need.
A prototype was developed to demonstrate the feasibility of the model using queries in the computer science domain.

The Kurdish language is an Indo-European language spoken in Kurdistan, a large geographical region in the Middle East.
Despite having a large number of speakers, Kurdish is among the less-resourced languages and has not seen much attention from the IR and NLP research communities.
This article reports on the outcomes of a project aimed at providing essential resources for processing Kurdish texts A principal output of this project is Pewan, the first standard Test Collection to evaluate Kurdish Information Retrieval systems.
The other language resources that we have built include a lightweight stemmer and a list of stopwords Our second principal contribution is using these newly-built resources to conduct a thorough experimental study on Kurdish documents.
Our experimental results show that normalization, and to a lesser extent, stemming, can greatly improve the performance of Kurdish IR systems.

E cient and accurate information retrieval is one of the main issues in image databases.
Since traditional alphanumeric indexing methods are not particularly suitable for image database indexing, new indexing methods are designed specially for image retrieval.
In this paper, we are going to discuss four partitioning methods: VP-tree, k-means, Competitive Learning (CL and Rival Penalized Competitive Learning (RPCL) for image database indexing.
Our aim is to evaluate and compare the performances of these methods for information retrieval in image database based on the performance measurements: Recall, Precision, and Speed.
From the result of some performance experiments, it is concluded that RPCL followed by CL are the most appropriate to be used for image retrieval among these four methods.

Many server selection methods suitable for distributed information retrieval applications rely, in the absence of cooperation, on the availability of unbiased samples of documents from the constituent collections.
We describe a number of sampling methods which depend only on the normal query-response mechanism of the applicable search facilities.
We evaluate these methods on a number of collections typical of a personal metasearch application.
Results demonstrate that biases exist for all methods, particularly toward longer documents, and that in some cases these biases can be reduced but not eliminated by choice of parameters.
We also introduce a new sampling technique multiple queries which produces samples of similar quality to the best current techniques but with significantly reduced cost.

For Information Retrieval, users are more concerned about the precision of top ranking documents in most practical situations.
In this paper, we propose a method to improve the precision of top N ranking documents by reordering the retrieved documents from the initial retrieval.
To reorder documents, we first automatically extract Global Key Terms from document set, then use extracted Global Key Terms to identify Local Key Terms in a single document or query topic, finally we make use of Local Key Terms in query and documents to reorder the initial ranking documents.
The experiment with NTCIR3 CLIR dataset shows that an average 10%-11% improvement and 2%-5% improvement in precision can be achieved at top 10 and 100 ranking documents

By using data abstraction concepts from database and objectoriented systems and combining them with uncertain inference, we develop a new approach for the design of information retrieval (JR) systems, thus solving major problems with regard to networked IR.
Different data types with vague predicates are required to allow for queries referring to arbitrary attributes of documents.
Physicat data independence helps in solving text search problems related to noun phrases, compound words and proper nouns.
Logical data independence allows for different views on an IR database, thus implementing e.g. data security.
Inheritance, especially on attributes, supports the creation of unified views on a set of IR databases.
Uncertain inference allows for query processing even on incompatible database schemas.

Least Common Subsumers (LCS) have been proposed in Description Logics (DL) to capture the commonalities between two or more concepts.
Since its introduction in 1992, LCS have been successfully employed as a logical tool for a variety of applications, spanning from inductive learning, to bottom-up construction of knowledge bases, information retrieval, to name a few.
The best known algorithm for computing
LCS uses structural comparison on normal forms, and the most expressive DL it is applied to is ALEN
We provide a general tableau-based calculus for computing LCS, via substitutions on concept terms containing concept variables.
We show the applicability of our method to an expressive DL (but without disjunction and full negation discuss complexity issues, and show the generality of our proposal.

A combination of information retrieval (RI) tools and structural recognition (SR) tools are used for the automatic construction of a generic model of documents.
The automatic construction takes as input a structured database of the documents which is converted into SGML, giving a logical representation of their content.
the RS tools are applied to the postscript representation of the documents to produce their physical characteristics.
We have coosen bibliographic reference database to illustrate our proposals.

In a Distributed Information Retrieval system, a user submits a query to a broker, which determines how to yield a given number of documents from all possible resource servers.
In this paper, we propose a multi-objective model for this resource selection task.
In this model, four aspects are considered simultaneously in the choice of the resource: document’s relevance to the given query, time, monetary cost, and similarity between resources.
An optimized solution is achieved by comparing the performances of all possible candidates.
Some variations of the basic model are also given, which improve the basic model’s efficiency.

1 University of Haifa, Haifa, Israel 2 ITC-irst, Trento, Italy Abstract.
Providing accurate personalized information services to the users requires knowing their interests and needs, as defined by their User Models (UMs Since the quality of the personalization depends on the richness of the UMs, services would benefit from enriching their UMs through importing and aggregating partial UMs built by other services from relatively similar domains.
The obvious question is how to determine the similarity of domains?
This paper proposes to compute inter-domain similarities by exploiting well-known Information Retrieval techniques for comparing textual contents of the Web-sites, classified under the domain nodes in Web-directories.
Initial experiments validate feasibility of the proposed approach and raise open research questions.

This paper describes the participation of RICOH in the Monolingual Information Retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2003.
We used our system with same kind of stemmer, same options and different parameters for 5 European languages to compare each result.
Total performance of the system was reasonable.
For French, German and Italian, we found some problems.

This paper present the details of participation of DEMIR (Dokuz Eylul University Multimedia Information Retrieval) research team to the ImageCLEF 2012 Medical Retrieval task.
This year, we evaluated impact of our proposed Integrated Combination method and Explicit Graded relevance feedback on the most descriptive low level features of images and best text retrieval result.
We improved results by examination of different level of integrated approach for retrieved text data and low level features.
We tested multi–modality image retrieval in ImageCLEF 2012 medical retrieval task and obtained the best rank in visual retrieval due to our experiments.
The results clearly show that proper combination of different modalities improve the overall retrieval performance.

As compared to many other techniques used in natural language processing, hidden markov models (HMMs) are an extremely flexible tool and has been successfully applied to a wide variety of information extraction tasks.
This work focus on webpage perceptive through model of Hierarchical Conditional Random Fields (i.e. HCRF) and offer results in free text segmentation and labeling.
This paper specially addresses the problem of research community of academic people integration (SIGNET-similar interest group) through perceiving the entities of them.
HMM, HCRF, Named-Entity, SIGNET

This paper is concerned with the use of linguistically motivated phrases as indexing terms in Information Retrieval applications.
Apart from the conventional noun phrases, we propose to use verb phrases as index terms for text classiication.
Techniques for phrase matching through syntactic normalization and seman-tical matching are described.
In particular, we show how to perform syntactic normalization of phrases in order to enhance recall.
Semantical normalization is based on lexico-semantical relations, taking into account certain properties of the classiication algorithms used.
The ideas described here are being implemented in the Document Routing system DORO, in which statistical learning algorithms are applied to document proles consisting of phrases.
This paper describes the rationale behind work in progress, rather than presenting nal results.

In this short paper we present three methods to valorise score relevance of some documents basing on their characteristics in order to enhance their ranking.
Our framework is an information retrieval system dedicated to children.
The valorisation methods aim to increase the relevance score of some documents by an additional value which is proportional to the number of multimedia objects included, the number of objects linked to the user particulars and the included topics.
All of the three valorization methods use fuzzy rules to identify the valorization value.

In all areas of the e-era, personalization plays an important role.
Particularly in elearning a main issue is student modeling, that is the analysis of student behavior and prediction of his/her future behavior and learning performance.
In fact, nowadays, the most prevailing issue in the e-learning environment is that it is not easy to monitor students' learning behaviors.
In this paper we have focused our attention on the system (the Profile Extractor) based on Machine Learning techniques, which allows for the discovery of preferences, needs and interests of users that have access to an e-learning system.
The automatic generation and the discovery of the user profile, to agree as simple student model based on the learning performance and the communication preferences, allow creating a personalized education environment.
Moreover, we presented an evaluation of the accuracy of the Profile Extractor system using the classical Information Retrieval metrics.

An image tells more than a thousand words Yet, while text searching on the Internet has become indispensable, with fast and scalable techniques, searching for images has not.
The purpose of this paper is to design an infrastructure for an Internet-based Collaborative Image Search System (hereafter also called CISS using existing Information Retrieval techniques and adjusting them to work play nice with multimedia.
For this purpose I will use relevance feedback, distributed indexing and distributed computing using Hadoop and HBase.

This paper describes an information retrieval system which is specifically designed to be used for storing and retrieving information about software components.
Rather than use a retrieval mechanism which is simply based on keyword descriptions, we have made use of developments in natural language research to represent component information in a form which encodes semantics as well as syntax.
We call this the component descriptor frame.
The paper describes the basic ideas which underlie our system and describes how it can be used for component information retrieval.
An example of the system in use is presented.
The version of the system described here has been fully implemented and is now being developed as part of a more general reuse support system.

Chunmin Yang Rajeev R. Raje Mikhail Auguston Barrett R. Bryant Andrew M. Olson Carol C. Burt Computer/Information Sciences Computer/Information Science Computer Science Univ.
of Alabama-Birmingham Indiana Univ.
Purdue Univ.
New Mexico State Univ.
Birmingham, AL 35294, USA Indianapolis, IN 46202, USA Las Cruces, NM 88003, USA {yangc, bryant, cburt}@cis.uab.edu {rraje, aolson}@cs.iupui.edu mikau@cs.nmsu.edu

AT&T participated in the Spoken Document Retrieval (SDR) track of TREC-7.
Our speech retrieval system uses modern Information Retrieval (IR) methods in conjunction with in-house automatic speech recognition.
The novel feature of our TREC-7 work is the use of document expansion to reduce the performance loss due to ASR errors.
Results show that retrieval from automatic transcriptions of speech is quite competitive with doing retrieval from human transcriptions.
Our experiments indicate that document expansion can be used to further improve retrieval from automatic transcripts.
This paper presents some analysis of document expansion in context of the TREC-7 SDR track task.

The Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval was held in Pittsburgh from June 27 to July 1.
This conference, with its focus on research, is the premier venue for the discussion of theoretical and practical elements of information retrieval (IR).

Marsyas is a software framework for building efficient complex audio processing systems and applications.
Although originally designed for Music Information Retrieval (MIR) tasks in the past few years it has been expanded to include any type of audio analysis or synthesis.
Complex Audio processing systems are defined hierarchically through composition using implicit patching.
Both the specification of the processing network and the control of it while data is flowing through can be performed at runtime without requiring recompilation.
Compilation is required only when new processing objects need to be defined.
Therefore the Marsyas runtime provides considerable functionality and flexibility.
In this paper we demonstrate how the Marsyas runtime can be accessed using a variety of different ways allowing non-trivial interactions with common software frameworks and environments.

The augmented adoption of XML as the standard format for representing a document structure requires the development of tools to retrieve and rank effectively elements of the XML documents.
It's known that in information retrieval, considering multiple sources of relevance improves information retrieval.
In this work some relevance features are defined and used in a learning to rank approach for XML information retrieval.
Our aim is to combine theses features to derive good ranking function and show the impact of each feature in the relevance of XML element.
Experiments on a large collection from the XML Information Retrieval evaluation campaign (INEX) showed good performance of the approach.

ACM Digital Library; Bacon’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index CSA Illumina; DBLP; GetCited; Google Scholar; IAOR Online; INSPEC; JournalTOCs; Library Information Science Abstracts (LISA MediaFinder; Norwegian Social Science Data Services (NSD SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Research Articles

With the drastic increase of available information sources on the Internet, people with different backgrounds share the same problem: locating useful information for their actual needs.
Search engines make this task easier only in certain ways; people still have to do the sifting process by themselves.
At this point, automatic summarization can complement the task of search engines.
In this paper, we consider a new summarization approach for Web information retrieval; i.e. structure-preserving and query-biased summarization.
We evaluate this approach on Turkish Web documents using TREC-like topics defined for Turkish.
The results of the task-based evaluation show that this approach has significant improvement over Google snippets and unstructured query-biased summaries in terms of f-measure using the relevance prediction approach.

Preparing the books to read every day is enjoyable for many people.
However, there are still many people who also don't like reading.
This is a problem.
But, when you can support others to start reading, it will be better.
One of the books that can be recommended for new readers is topographic mapping covering the wider field of geospatial information science technology gist.
This book is not kind of difficult book to read.
It can be read and understand by the new readers.

The conceptual model and mathematical formalism of quantum theory are employed in creating a novel framework for modeling the computational search process addressing problematic issues that restrict information retrieval research.
Mapping the mathematical formalism of search to that of quantum theory presents insightful perspectives about the nature of search.
However, differences in operational semantics of quantum theory and search restrict the utility of the mapping.
An approach is suggested for resolving these semantic differences aiming toward a sound mathematical and conceptual framework for search inspired by quantum theory.

Test collections are extensively used in the evaluation of information retrieval systems.
Crucial to their use is the degree to which results from them predict user effectiveness.
At first, past studies did not substantiate a relationship between system and user effectiveness; more recently, however, correlations have begun to emerge.
The results of this paper strengthen and extend those findings.
We introduce a novel methodology for investigating the relationship, which shows great success in establishing a significant correlation between system and user effectiveness.
It is shown that users behave differently and discern differences between pairs of systems that have a very small absolute difference in test collection effectiveness.
Our results strengthen the use of test collections in IR evaluation, confirming that users' effectiveness can be predicted successfully.

In the framework of meaning representation in Natural Language Processing (NLP we aim to develop a system that can be used for heterogeneous applications such as Machine Translation, Information Retrieval or Lexical Access.
This system is based on six hypotheses which concern meaning representation and acquisition.
In this paper, we discuss the related hypotheses that motivate the construction of a such system and how these hypotheses, together with NLP software engineering concerns, led us to conceive a distributed multi-agent system for our goals.
We present Blexisma2, a distributed multi-agent system for NLP, its conceptual properties, and an example of inter-agent collaboration.
The system is currently being tested on a Grid computing environment.

The prediction of query performance is an interesting and important issue in Information Retrieval (IR Current predictors involve the use of relevance scores, which are time-consuming to compute.
Therefore, current predictors are not very suitable for practical applications.
In this paper, we study six predictors of query performance, which can be generated prior to the retrieval process without the use of relevance scores.
As a consequence, the cost of computing these predictors is marginal.
The linear and non-parametric correlations of the proposed predictors with query performance are thoroughly assessed on the Text REtrieval Conference (TREC) disk4 and disk5 (minus CR) collection with the 249 TREC topics that were used in the recent TREC2004 Robust Track.
According to the results, some of the proposed predictors have significant correlation with query performance, showing that these predictors can be useful to infer query performance in practical applications.

Aloa-aidants.fr is an online social support application dedicated to family caregivers.
Aloa has been designed as a part of an interdisciplinary research project conducted with the cooperation of R&#x00E9;G&#x00E9;MA, a healthcare network dedicated to patients suffering from memory disorders and their family caregivers.
In this paper, we present the approach we propose to support the knowledge sharing dimension of social support.
The Open Information Retrieval (OIR) framework and socio-semantic web technologies appear of relevance for this purpose at the light of the studies of family caregivers' offline and online social support practices carried out on the framework of our research project.
Illustrations of the implemented functionalities are presented and the results of a preliminary evaluation with family caregivers lead us to discuss the relevance of our approach and future work.

Information Retrieval (IR which is also known as text or document retrieval, is the process of locating and retrieving docri)nents that are relevant to the user queries.
In hypertext environments, docuinent databases are organized as a network of nodes which are interconnected by various types of links.
This study introduces a hypertext-based text retrieval system, HypIR.
In HypIR, the sentantic relationships ainong docuinents are obtained using a clustering algorithm.
A new approach providing the advantages of system maps and history list is introduced to prevent the user fiotn being lost in the IR hivperspace.
The paper presents the underlying concepts and iinplementation details.
HypIR is based on the object-oriented paradigm and its execution platforin is HyperCard

We propose an integrated system using active learning for audio-to-score alignment.
Audio-to-score alignment is a fundamental task in music information retrieval.
Although various machine learning techniques have been applied to this task, it is not the case for active learning.
To show how beneficial active learning is in audio-to-score alignment, we demonstrate a system that integrates it with dynamic time warping, a commonly used algorithm for time series alignment.
We propose a simple parametric model for selecting queries&#x2014;a crucial step in active learning.
We evaluate the system using synthesized audio as well as real performances.
The alignment accuracy is improved with a range from 20% to 50% using only less than 10% query instances, a promising result that hopefully can inspire the creation of a collaborative framework between human and machine for audio-to-score alignment in the future.

Wir stellen eine zweischichtige Visualisierung vor, die StudentInnen helfen soll, Dokumente zu identifizieren, die für ihr Studium relevant sind.
Während die erste Visualisierung eine Übersicht bietet, zeigt die zweite eine adaptierte Perspektive:
Wir visualisieren die Beziehung des Benutzermodells zu den Dokumenten, die für die Aufgabe der BenutzerIn relevant sind; jede Benutzermodellkomponente ist durch einen Referenzpunkt repräsentiert.

One of the key challenges for creating the semantic representation of a text is mapping words found in a natural language text to their meanings.
This task, Word Sense Disambiguation (WSD is confounded by the fact that words have multiple meanings, or senses, dictated by their use in a sentence and the domain.
We present an algorithm that employs random walks over the graph structure of knowledge bases, yielding stateof-the-art results for WSD on both general and biomedical texts.
We also show that the same algorithm can be successfully applied to Word Similarity and to enrich texts with related concepts, yielding improvements in Information Retrieval.

Spotify provides users with access to a massive repository of streaming music.
While some aspects of music access are familiar to the information retrieval community (e.g. semistructured data, item recommendation nuances of the music domain require the development of new models of user understanding, intent modeling, relevance, and content understanding.
These models can be studied using the large amount of content and usage data at Spotify, allowing us to extend previous results in the music information retrieval community.
In this presentation, we will highlight the research involved in developing Spotify and outline a research program for large scale music access.

Over the past decade, the most significant evolution in organization might be the dawn of the new economy in connection with the value of intellectual assets.
In this paper, we combine Formal Concept Analysis (FCA) with Protégé in order to construct ontology-based Yoga method serving both as an example, and a knowledge-sharing platform.
This may build a picture of experts’ knowledge and understand with the general public, allow them to participate in, and be totally aware of, satisfying their requirements.
Moreover, the platform will foster to eliminate the phenomenon of information disparity between patients and physicians.
Key-Words: Formal Concept Analysis, Yoga, Information Retrieval, Ontology, Ontology-based system

In this paper, a Semantic Agent-Community-based Peer-to-Peer information retrieval method called SACP2P method is proposed for reliable community Web information sharing.
The evaluation experiment is performed and the result has shown that SACP2P method can aggregate information from different sources published through different methods (including Web content and Web services) and be effectiveness on reducing communication loads in a P2P network.

Minoru Yoshida, Kentaro Torisawa and Jun’ichi Tsujii 1 Department of Computer Science, Graduate school of Information Science and Technology, 2 School of Information Science, Japan Advanced Institute of Science and Technology 3 Information and Human Behavior, PRESTO, Japan Science and Technology Corporation CREST, JST(Japan Science and Technology Corporation)
Postal address: Department of Computer Science, Graduate school of Information Science and Technology, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-0033, Japan Telephone 81
Facsimile 81 3 5802 8872 {mino, tsujii}@is.s.u-tokyo.ac.jp, torisawa@jaist.ac.jp

Document clustering is an important technology which helps users to organize the large amount of online information, especially after the rapid growth of the Web.
This paper focuses on semantic document clustering method and its application in search engine.
We proposed a multi-agent based information retrieval system to enhance the search process.
The agents retrieve the results of Web search engine and organize the results by clustering them into different categories for a given query.
We utilized WordNet ontology and several approaches to cluster results in appropriate category according to WordNet synsets.
The experiment shows that semantic clustering work better than original clustering.

In a federated digital library system, it is too expensive to query every accessible library.
Resource selection is the task to decide to which libraries a query should be routed.
In this paper, we describe a novel technique that is used in the MIND project.
Our approach, decision-theoretic framework (DTF differs from existing algorithms like CORI in two ways:
It computes a selection which minimises the overall costs (e.g. retrieval quality, time, money) of the distributed retrieval.
And it allows for other data types beside text (e.g names, years, images whereas other resource selection techniques are restricted to text.

Music Transcription has been a core field of Music Information Retrieval.
Most of the development in MIR is dependent on how efficiently and accurately the notes are extracted.
Various methods have been used for Automatic Music Transcription.
The most effective ones have been based on Spectral factorization technique for which Non-Negative Matrix Factorization has been profoundly used.
Here we use a variant of NMF which is based on Accelerated Multiplicative Update with some predefined templates.
The method provides good results on the Disklavier Dataset.

In this paper, we propose a method which exploits the semantic proximity of words in unrestricted natural language text to retrieve relevant documents.
In order to facilitate this functionality, the system represents the documents and the query in the form of semantically relatable sets (SRS which are a group of entities demanding semantic relations when the semantic representation of the sentence is ultimately produced.
We also devise a method to augment the SRSs to further boost the performance.
WordNet is used to deal with different forms of divergence between the query and the documents.
In a series of experiments on TREC data, our semantic proximity based retrieval technique yields high precision with improved mean-average-precision in comparison to conventional retrieval techniques.

The (semi automatic detection of a user's system activity provides context that can be exploited for information retrieval, for recommendations or for adapting contents and services to this activity and thus improve task-technology-fit.
Various approaches automatically detect user activities on the level of system operations.
However, the approaches struggle with the challenge how to semantically connect these system operations with high level knowledge activities.
Yet, this link is needed to meaningfully support users engaged in knowledge activities.
This paper takes up on this challenge and maps a framework of knowledge activities to system operations on the basis of descriptions of work practices gathered from two European companies.
The framework is intended to aid the meaningful connection of automatically detected system operations with knowledge activities on varying levels of granularity.

In this paper we propose a semantic based P2P system that incorporates peer sharing policies, which allow a peer to state, for each of the concepts it deals with, the conditions under which it is available to process requests related to that concept.
The semantic routing approach, based on advertisements and peer behavior in answering previous requests, takes also into account sharing policies.

"Offene Hypertext-Systeme" sind in neueren
Arbeiten aus dem Bereich Hypertext zu einem vieldiskutierten Thema geworden.
In den Proceedings der letzten ECHT '92 z.
B. finden sich mehrere interessante
Arbeiten zu diesem
Thema, von denen eine [DHH+92] einen Kriterienkatalog für offene Hypertext-Systeme formuliert, der insbesondere auf die Unabhängigkeit von Systemplattformen und der Kommunikation mit externen Applikationsprogrammen abzielt.
Wir halten diese Eigenschaften für wichtig, sind aber der Meinung, daß
Offenheit weitergehende Konsequenzen auch auf die innere Struktur des Hypertexts haben muß.
Im folgenden werden wir einen eigenen Kriterienkatalog vorlegen, der dann an das Konstanzer Hypertext-System angelegt werden soll.
Ausführlicher dargestellt werden zwei Beispiele für die Einbindung externer Informationsquellen in Hypertext, weil uns diese
im Umfeld des Information Retrieval als besonders interessant erscheinen.

Exploring the metadata associated with documents in the Semantic Web is a way to increase the precision of information retrieval systems.
Systems have been established so far failed to overcome fully the limitations of search based on keywords.
Such systems are built from variations of classic models that represent information by keywords and work upon statistical correlations.
This work proposes an information retrieval model to find information items with similar semantic content that a given user&#x02019;s query.
The information items internal representation is based on user interest groups, called "semantic cases The model also defines a similarity measure for ordering the results based on semantic distance between semantic cases items.

Our paper discusses how active warden operates and why it is important for steganographers to understand the impending threat in which they possess.
It was a common belief that the main adversary for steganography is coming from steganalysis detection.
However, we have found in some situation, the destruction of hidden information is more easily achievable compared to the task of detecting it.
Active Wardens are attackers of steganography which aims to demolish possible hidden information within a carrier media.
If the enemy's objective is to disrupt the communication of hidden information, then the active approach is definitely a better choice compared to passive time consuming steganalysis.

Adaptive information retrieval (IR) systems based on connectionist architectures have captured the attention of researchers over the past decade.
This paper provides a review of connectionist IR research, including the major models for connectionist document and query representation, techniques to enhance query re-formulation, dynamic document routing (information filtering and connectionist techniques for document clustering.

Private Information Retrieval (PIR despite being well studied, is computationally costly and arduous to scale.
We explore lower-cost relaxations of information-theoretic PIR, based on dummy queries, sparse vectors, and compositions with an anonymity system.
We prove the security of each scheme using a flexible differentially private definition for private queries that can capture notions of imperfect privacy.
We show that basic schemes are weak, but some of them can be made arbitrarily safe by composing them with large anonymity systems.

1 Key Laboratory of Meteorological Disaster, Ministry of Education, Nanjing University of Information Science Technology, Nanjing 210044, China; 2 College of Global Change and Earth System Science, Beijing Normal University, Beijing 100875, China; 3 Applied Hydrometeorological Research Institute, Nanjing University of Information Science Technology, Nanjing 210044, China; 4 Department of Atmospheric and Environmental Sciences, University at Albany, State University of New York, Albany, NY 12222, USA

This work falls within the field of Model-Based Legal Information Retrieval.
We were brought to conceive new types of interfaces after having evaluated a legal database available on the Internet.
Although legally validated, this legal database JURISQUE-1 was considered as unsuitable to the expertise's necessities of mountains' professionals (whether legal practitioners or not) working on practical issues of responsibility in cases of avalanches.
We thus proposed to develop <i>cognitive interfaces</i> which have the peculiarity to integrate a <i>model</i> of the field into the management of various resources (software packages or knowledge) and ensure that they communicate with one another.

Information Retrieval: Common information-retrieval techniques either rely on a specific encoding of available information (e.g. fixed classification codes) or simple full-text analysis.
Both approaches suffer from severe shortcomings.
Using an ontology in order to explicate the vocabulary can help overcome some of these problems.
When used for the description of available information as well as for query formulation an ontology serves as a common basis for matching queries against potential results on a semantic level.

In this paper, we propose a novel dependency language modeling approach for information retrieval.
The approach extends the existing language modeling approach by relaxing the independence assumption.
Our goal is to build a language model in which various word relationships can be integrated.
In this work, we integrate two types of relationship extracted from WordNet and co-occurrence relationships respectively.
The integrated model has been tested on several TREC collections.
The results show that our model achieves substantial and significant improvements with respect to the models without these relationships.
These results clearly show the benefit of integrating word relationships into language models for IR.

Noun compounds are a frequently occurring yet highly ambiguous construction in natural language; their interpretation relies on extra-syntactic information.
Several statistical methods for compound disambiguation have been reported in the literature; however, a striking feature of all these approaches is that disambiguation relies on statistics derived from unambiguous compounds in training, meaning they are prone to the problem of sparse data.
Other researchers have overcome this difficulty somewhat by using manually crafted knowledge resources to collect statistics on “concepts” rather than noun tokens, but have sacrificed domain-independence by doing so.
We report here on work investigating the application of Latent Semantic Indexing [4 an Information Retrieval technique, to the task of noun compound disambiguation.
We achieved an accuracy of 84 indicating the potential of applying vectorbased distributional information measures to syntactic disambiguation.

This paper describes the approach taken to the XML Mining track at INEX 2008 by a group at the Queensland University of Technology.
We introduce the K-tree clustering algorithm in an Information Retrieval context by adapting it for document clustering.
Many large scale problems exist in document clustering.
K-tree scales well with large inputs due to its low complexity.
It offers promising results both in terms of efficiency and quality.
Document classification was completed using Support Vector Machines.

The workshop has been subsidized by the OntoWeb Consortium (www.ontoweb.org, EU Thematic Network on Semantic Web
We have received 15 papers in total.
We had experts representing both the semantic web and the IR fields as reviewers.
All the submitted papers were thoroughly reviewed by at least 4 people representing both fields.
Based on these reviews, we selected 7 very good papers dealing with various aspects of both information retrieval and semantic web.
In addition, we scheduled two invited talks, spanning two important aspects of the semantic web.
Dr Arturo Trujillo, of the Canon Research Centre Europe Ltd, has kindly chaired the workshop.
The workshop attracted an audience of more than 30 people, who were encouraged to interact with the authors and the invited speakers.

Given a database of documents and a user's query, how can we locate those documents that meet the user's information needs?
Because there is no precise deenition of which documents in the database match the user's query, uncertainty is inherent in the information retrieval process.
Therefore, probability theory is a natural tool for formalizing the retrieval task.
In this paper, we propose a Bayesian approach to one of the conventional probabilistic information retrieval models.
We discuss the motivation for such a model, describe its implementation, and present some experimental results.

This paper first took statistics on library blogs at home and abroad and listed China's major types of blogs, then analyzed the application fields of these blogs and described the mode of library information science education by blogs, and finally comprehensively analyzed the influence of library blogs on professional education in five aspects.
The conclusion was drawn at the end of the paper.
Keywords-library; blog; culture; education; impact

The performance of distributed text document re trieval systems is strongly in uenced by the organization of the inverted index This paper compares the perfor mance impact on query processing of various physical organizations for inverted lists We present a new prob abilistic model of the database and queries
Simulation experiments determine which variables most strongly in uence response time and throughput
This leads to a set of design trade o s over a range of hardware con g urations and new parallel query processing strategies

Session search is the Information Retrieval (IR) task that performs document retrieval for a search session.
During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need.
This paper proposes a novel query change retrieval model (QCM which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search.
We propose to model session search as a Markov Decision Process (MDP We consider two agents in this MDP: the user agent and the search engine agent.
The user agent's actions are query changes that we observe and the search agent's actions are proposed in this paper.
Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.

This paper describes a multiple agent system for distributed planning, specifically for intelligent network information retrieval.
The system consists of a set of reactive agents located at each information- or capability-provider site plus an agent to track each information request.
The agents negotiate with each other to assemble a distributed plan for satisfaction of each information need and each agent also interfaces with local retrieval and data processing functions.
The idea of an implicit commitment is presented as providing a way to enable automated coordination of resources (information and capabilities) localized among heterogeneous agents.
The advantages of a “shared machine” implemented by such a commitment protocol are discussed and an implementation called TRACS for retrieval and assembly of satellite image data is presented.

DO INFORMATION TECHNOLOGY UNITS HAVE MORE POWER THAN OTHER UNITS IN ACADEMIC LIBRARIES?
Sook Lim Library and Information Science, College of St. Catherine,
2004 Randolph Avenue 4125.
St. Paul, MN 55105
651 690 6888 f) 651 690 8724, slim@stkate.edu
Lim, S 2007
Do Information Technology Units Have More Power than Other Units in Academic Libraries Journal of the American Society for Information Science and Technology.
58(9 1242-1253.

The field of data structures is important in geographic information science as it is the foundation for the implementation of many operations.
In particular, the efficient execution of algorithms depends on the efficient representation of the data.
There has been much research on data structures in computer science with the most prominent work being the encyclopedic treatises of Knuth.
In this article we only review the basic data structures.

Ontology plays an important part on Semantic Web, Information Retrieval, and Intelligent Information Integration etc.
Ontology learning gets widely studied due to many problems in totally manual ontology construction.
Term extraction influences many respects of ontology learning as it's the basis of ontology learning hierarchical structure.
This paper mines topics of the corpus based on Latent Dirichlet Allocation (LDA) which uses Variational Inference and Expectation-Maximization (EM)
Algorithm to estimate model parameters.
With the help of irrelevant vocabulary, the paper provides better experimental results which show that the distribution of topics on terms reveals latent semantic features of the corpus and relevance among words.

Query is a key component of the information retrieval process.
It is a complex, multi-faceted and vague term that has been subject to a broad range of definitions.
Both user-centred and system-centred models of information retrieval stress the importance and role of query in successful information retrieval experience.
However, there is a lack of a rich, consistent and well-defined terminology for query.
This paper proposes a new faceted classification of query in the context of interactive information retrieval and conceptualizes the many facets, aspects, characteristics and features of query based on a comprehensive analysis of the literature.
The resulting classification provides a useful framework for understanding query and its various aspects for research and learning purposes.
Specifically, it provides a conceptual foundation for operationalizing the many aspects and facets of query as variables for research purposes.

T his essay reviews a portion of the books and articles about information science published over the last five years and looks for useful, relevant borrowings for documentation projects.
My own favorite window on the information-science literature is through the publications and awards of the American Society for Information Science (ASIS The rest of this essay therefore tries to show the value of that approach for documentation specialists by mentioning, and sometimes examining, recent individual journal articles, whole books, and special-topic journal issues found noteworthy by ASIS.

Product search is an emerging search application where optimization of search results relies critically on an accurate model of a user’s price preference.
In this paper, we propose a Bayesian framework for modeling a user’s price preference with a particular focus on developing a smart price filter model for inferring a user’s price preference based on the user’s selection of price filters and optimizing ranking of products accordingly.
Preliminary experiment results with product search log show promise of the framework, which opens up interesting opportunities for new research in the intersection of machine learning, information retrieval and economics.

Borda counting is a rank aggregation method widely used in the information retrieval area.
Despite many variants of Borda counting, the problem of how to take into account the prior structural information of rank lists is still open.
In this paper, we analyze the structure of a list given its AP (average precision and then propose the AP-scored Borda counting method under the information retrieval scenario.
By properly defining the elementary score function, the structural information of each list, which is inferred from its AP, is naturally incorporated into the Borda counting procedure.
Rank aggregation experiments on both simulated data and TRECVID 2004 data show promising results

Information retrieval is currently one of the most important usages of the Internet.
Scientific publishers are one of the sources of information, providing access to digital libraries on a subscription basis.
Contracts between publishers and larger organizations specify how access is to be regulated.
This paper explores the legal implications and technical considerations of the use of software agents to facilitate onand off-campus access to the ScienceDirect digital

The workshop on Medical Information Retrieval took place at SIGIR 2016 in Pisa, Italy on July 21.
The workshop programme included seven oral presentations of refereed papers, four posters and an invited keynote presentation.
This allowed time for lively discussions among the 27 participants.
These made clear the significant and diverse challenges in the area of medical information retrieval and the significant interest in developing mechanisms to go about tackling them.
Successfully addressing them will give added value to the wide variety of users that can profit from medical information search, including patients, general health professionals and specialist groups such as radiologists who mainly search for images and image-related information.

Troubleshooting search system aims at extracting relevant information to solve the problem at hand.
It is often the case that documents in troubleshooting system includes an abundant amount of domain-speci c categories.
However, the useful information about the domain-speci c categories, such as relationship between words and categories and relationship between categories, is not fully utilized in simple query search and faceted search.
In this paper, we propose an information retrieval method boosted by the domainspeci c categories.
Given a problem query and categories, the troubleshooting search system is able to retrieve the relevant information of interest with respect to the selected categories.
The experiment results show our proposal improves the recall.

In this paper, we address the problem of detecting occlusion boundaries from video sequences.
We build a bi-directed graph whose nodes are line fragments extracted from superpixels's edges.
Based on the graph, we compute a global occlusion saliency map by integrating motion, shape and topology cues into the framework of Saliency Network.
Furthermore, with the structural information generated from the network, the property of structural consistency is proposed to prune the graph and refine the saliency map.
Finally, we train a classifier to detect occlusion fragments combining the global saliency value and local edge strength.
The detector outperforms the state-of-the-art on the benchmark of Stein and Hebert[8] by improving average precision to .80.

Innovations can improve the productivity and quality of services and lead to new services.
The importance of education to such services innovation has been a focus of discussion all over the world.
The Japan Advanced Institute of Science and Technology (JAIST) has established a new education course for services innovation called MOS (management of service
The curriculum of the MOS course is based on the recognition that services in the 21st century are dependent on knowledge science, information science, business science, and transdisciplinary science and technology.
In particular, knowledge science including ethnography and brain science and information science that studies the Internet and data management can enhance service quality.
This paper describes the concept of JAIST's MOS course and outlines its curriculum.

This study focuses on the intellectual accessibility of information in indigenous languages, using Zulu, one of the main indigenous languages in South Africa, as a test case.
Both Cross-Lingual Information Retrieval (CLIR) and metadata are discussed as possible means of facilitating access and a bilateral approach combining these two methods is proposed.
Popular CLIR approaches and their resource requirements are analysed and the dictionary-based approach combined with approximate string-matching for query translation from Zulu to English are discussed in detail.
Metadata formats for knowledge representation from the Indigenous Knowledge (IK) viewpoint are discussed, in particular the advantages and limitations of the Dublin Core (DC) metadata format.

— Influence of Circuits’ Order Jun Hasegawa Fumitaka Yura Department of Computer Science, Graduate School of Information Science and Technology, University of Tokyo.
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan.
ERATO Quantum Computation and Information Project, JST.
Hongo White Building, 5-28-3 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan.
Quantum counting algorithm can estimate the number of solutions in a database.
In this paper, we analyze its behaviors in the presence of decoherence errors.
We construct two counting circuits in which the order of commutative gates are altered and show that the behaviors of circuits are different when decoherence errors occur.
On one circuit, almost correct number of solutions are obtained and on the other circuit, the estimated numbers of solutions are mostly distributed over the correct one and wrong counts zero and the number of elements.
We analyze that these phenomena result from the rotation outside the Grover space due to decoherence.

Many search engines are term-based information retrieval models.
The disadvantage of this type of model is that it does not consider word sense.
If we can represent the meanings of the terms that a user inputs, the IR system can retrieve the information the user really wants; not simply match the terms.
To represent word sense, we proposed conceptual fuzzy sets (CFSs
A CFS is a framework that represents word concepts and that changes dynamically with fuzzy sets.
In this paper, we experiment with concept retrieval for documents using conceptualized queries using CFSs.
In our experiment, we evaluated our system on a large-scale corpus consisting of 1 million newswire text data.
The experimental results showed that the performance of the IR system was improved.
It also indicated that generating conceptualized queries is effective in an IR system.

An important problem in signal analysis is to define a general purpose signal representation which is well adapted for developing pattern recognition algorithms.
In this paper we will show that such a representation can be defined from the position of the zero-crossings and the local energy values of a dyadic wavelet decomposition.
This representation is experimentally complete and admits a simple distance for pattern matching applications.
It provides a multiscale decomposition of the signal and at each scale characterizes the locations of abrupt changes in the signal.
We have developed a stereo matching algorithm to illustrate the application of this representation to pattern matching.
Comments University of Pennsylvania Department of Computer and Information Science Technical Report
. MSCIS-88-30.
This technical report is available at ScholarlyCommons:
http repository.upenn.edu/cis_reports/612 Dyadic Wavelets Energy Zero Crossings MS-CIS-88-30 GRASP LAB

The article explains why the concept of the user in Library and Information Science (LIS) user studies and information seeking behavior is theoretically inadequate and it proposes a reconceptualization of subjects, objects, and their relations according to a model of ‘double mediation Formal causation (affordances) is suggested as a substitute for mechanistic causation.
The notion of ‘affective causation’ is introduced.
The works of several psychoanalysts and continental and Anglo-American philosophers are used as tools to develop the model.

Indexing and retrieval of large quantity of multimedia data is a highly challenging and growingly important problem for the computer science research community.
Researchers in multimedia, databases, computer vision, machine learning, signal and image processing and statistics have worked on multimedia information retrieval (MIR) for over a decade.
A number of significant technological advances have been achieved in this field.
Some of the techniques have been applied to application areas such as art image retrieval, biomedical image and video retrieval, education, sensor networks, large-scale online personal and professional photo sharing communities, classification and filtering of images on the Web, scientific content, computer forensics, threat assessment and security applications more generally.

Resource selection, also called server selection, collection selection or database selection, is a foundational problem in distributed information retrieval (DIR This paper introduces a set-covering-based algorithm for resource selection in DIR, with consideration of overlapping extent between resources.
Give different document with different weight according to its position in merged results for question Q. Only results that have not appeared in some earlier selected resource are focused on in later selected resources.
The score of each resource is decided by the total weights of those merged results included in, and only the resource with max score is selected in each selecting step.
So, the selecting order is the actual rank of selected resources which are used to search the question Q&#x02019 which is similar to question Q. The approach saves big searching time due to overlapping between databases and, at the same time, enhances user's recall rate and precision.

The World Wide Web (WWW) is becoming one of the most preferred and widespread mediums of learning.
Unfortunately, most of the current Web-based learning systems are still delivering the same educational resources in the same way to learners with different profiles.
A number of past efforts have dealt with e-learning personalization, generally, relying on explicit information.
In this paper, we aim to compute on-line automatic recommendations to an active learner based on his/her recent navigation history, as well as exploiting similarities and dissimilarities among user preferences and among the contents of the learning resources.
First we start by mining learner profiles using Web usage mining techniques and content-based profiles using information retrieval techniques.
Then, we use these profiles to compute relevant links to recommend for an active learner by applying a number of different recommendation strategies.

The paper presents the use of semantically enhanced web services in the field of distributed intelligent information retrieval.
The main idea of the approach is that a web service is used as an intelligent wrapper for an information resource (IR These autonomous IRs become available for querying within distributed information system with centralized mediator through IR registration.
IR wrapper web services provide homogeneous semantically reinforced query interface for the mediator
trough the use of machine-processable ontologies.
The paper reports on the architectures of the mediator systems in two projects that exploit IR wrapper web service approach.
The architecture of the IR wrapper web service, the generic wrapper and its bindings is then presented.
The technique is evaluated by experiments with implemented tester for the web service which wraps “University Entrant” IR of Zaporozhye State University.

This paper describes the SHELLFBK system that participated in SemEval 2015 Tasks 9, 10, and 11.
Our system takes a supervised approach that builds on techniques from information retrieval.
The algorithm populates an inverted index with pseudo-documents that encode dependency parse relationships extracted from the sentences in the training set.
Each record stored in the index is annotated with the polarity and domain of the sentence it represents.
When the polarity or domain of a new sentence has to be computed, the new sentence is converted to a query that is used to retrieve the most similar sentences from the training set.
The retrieved instances are scored for relevance to the query.
The most relevant training instant is used to assign a polarity and domain label to the new sentence.
While the results on well-formed sentences are encouraging, the performance obtained on short texts like tweets demonstrate that more work is needed in this area.

The use of sampling, randomized algorithms, or training based on the unpredictable inputs of users in Information Retrieval often leads to non-deterministic outputs.
Evaluating the effectiveness of systems incorporating these methods can be challenging since each run may produce different effectiveness scores.
Current IR evaluation techniques do not address this problem.
Using the context of distributed information retrieval as a case study for our investigation, we propose a solution based on multivariate linear modeling.
We show that the approach provides a consistent and reliable method to compare the effectiveness of non-deterministic IR algorithms, and explain how statistics can safely be used to show that two IR algorithms have equivalent effectiveness.

Relevance of the new Bologna study programme at the Department of Library and Information Science and Book Studies at University of Ljubljana is demonstrated from different aspects.
The example of IS&R themes and topics which are recognized as one of LIS core areas is used to show the differences from the previous study programme in terms of content, together with coverage of Web 2.0 related themes.
In regard to teaching and learning methods it is shown how e-learning is used to support the educational process.
At the end a few insights into employability of future graduates are added.

P2P Information Retrieval (P2P-IR) is a challenging and potentially very useful area of research.
Its goal is to enable powerful, efficient, massively distributed IR.
A large fraction of current research is focused on efficiency.
However, we feel that P2P-IR needs to address the complicated combination of indexing structure and networking issues.

Troels F. Rønnow, Zhihui Wang, Joshua Job, Sergio Boixo, Sergei V. Isakov, David Wecker, John M. Martinis, Daniel A. Lidar, and Matthias Troyer∗1 Theoretische Physik, ETH Zurich, 8093 Zurich, Switzerland Department of Chemistry and Center for Quantum Information Science Technology, University of Southern California, Los Angeles, California 90089, USA Department of Physics and Center for Quantum Information Science Technology, University of Southern California, Los Angeles, California 90089, USA Google, 150 Main St, Venice Beach, CA,
90291 Google, Brandschenkestrasse 110, 8002
Zurich, Switzerland Quantum Architectures and Computation Group, Microsoft Research, Redmond, WA 98052, USA Department of Physics, University of California, Santa Barbara, CA 93106-9530, USA Departments of Electrical Engineering, Chemistry and Physics, and Center for Quantum Information Science Technology, University of Southern California, Los Angeles, California 90089, USA

In this paper, I introduce a new approach to agent-based modeling in geospatial contexts.
The novelty of the approach stems from introducing geospatial functionality as an exoskeletal wrapper around standard socio-communicative and goal-oriented agent-based AI.
Operationally, I also introduce an integrated and symbiotic tight-coupling to motion capture and Geographic Information Systems, based on space-time Geographic Information Science.
To prove the usefulness of the approach in simulation, I describe application of the model to a relatively well-understood (yet widely misrepresented) scenario involving crowd evacuation in constrained infrastructure.

This chapter presents an analytical study about methodology and methods to build ontologies and controlled vocabularies, compiled by the analysis of a literature about methodologies for building ontologies and controlled vocabularies and the international standards for software engineering.
Through theoretical and empirical research it was possible to build a comparative overview which can help as a support in the defining of methodological patterns for building ontologies, using theories from the computer science and information science.
DOI: 10.4018/978-1-61350-456-7.ch1.4

The problem of private information retrieval with private side information (PIR-PSI) is recently introduced by Kadhe et al.
In this problem, N replicated databases each store K messages, of which M are known to the user as side-information.
The identity of these M messages is unknown to the databases.
The user wishes to retrieve, as efficiently as possible, a new desired message without revealing any information about the joint indices of the desired and sideinformation messages.
We show that the capacity of PIR-PSI is 1 1 N 1 N 1 N 1
Thus, the capacity of PIR-PSI with K messages, N databases and side-information of M messages is identical to the capacity of PIR with K M messages, N databases and no sideinformation, which was found previously by Sun and Jafar.
The achievability and converse proofs are essentially inherited from the original work of Sun and Jafar.

Comparative evaluation of Information Retrieval Systems (IRSs) using publically available test collections has become an established practice in Information Retrieval (IR
By means of the popular Cranfield evaluation paradigm IR test collections enable researchers to compare new methods to existing approaches.
An important area of IR research where this strategy has not been applied to date is Personalised Information Retrieval (PIR which has generally relied on user-based evaluations.
This paper describes a method that enables the creation of publically available extended test collections to allow repeatable laboratory-based evaluation of personalised search.

Kernel Canonical Correlation Analysis (KCCA) is a method of correlating linear relationship between two variables in a kernel defined feature space.
A machine learning algorithm based on KCCA is studied for cross-language information retrieval.
We apply the algorithm in Japanese–English cross-language information retrieval.
The results are quite encouraging and are significantly better than those obtained by other state of the art methods.
Computational complexity is an important issue when applying KCCA to large dataset as in information retrieval.
We experimentally evaluate several methods to alleviate the problem of applying KCCA to large datasets.
We also investigate cross-language document classification using KCCA as well as other methods.
Our results show that it is feasible to use a classifier learned in one language to classify the documents in other languages.

Using linked data in real world applications is a hot topic in the field of Information Retrieval.
In this paper we leveraged two valuable knowledge bases in the task of information extraction.
BabelNet is used to automatically recognize and disambiguate concepts in a piece of unstructured text.
After extracting all possible concepts, DBpedia is leveraged to reason about the type of each concept using SPARQL.

Information Retrieval deals with the easy access to the information based on the user&#x02019;s request, which will be presented in the form of a query.
A dialog system that understands spoken natural language queries asks for further information if necessary and produces an answer to the speaker&#x02019;s query.
Most of the research works in Information Extraction focus only on written language processing, in which a few are devoted to the study of Spoken Language Information Extraction.
This paper discusses a novel technique for recognition of the isolated question words from Malayalam (one of the south Indian languages) speech query.
We have created and analyzed a database consisting of 500 isolated question words.
Fast Fourier Transform (FFT) and Discrete Cosine transform (DCT) is used for the feature extraction purpose and Artificial Neural Network (ANN) is used for classification and recognition.
A recognition accuracy of 85&#x025; could be achieved from this experiment

In Chinese the SVO (Subject-Verb-Object) construction is often appears in the query in information retrieval.
The SVO construction will be cut into several separate key words, if the traditional retrieval algorithm is used, it will degenerate to a normal Boolean Search, because the semantic meaning may be different when the order of the key words differs, thus the semantic implied in the SVO construction will not be exists, that is to say result of an text-retrieval algorithm will reflected by the order of the key words.
For this question: taking the advantages of the structural features of forward and inverted index, the paper gives the definition of Query Step, Document Step, analyses their reflection on the retrieval, and proposed the pre-processing algorithm to form the new retrieval model based on VSM.
Subsequently, The ontology and semantic web support was given, the procedure of translating user query into formal query for semantic search was also given in detail.

Assessing a considerable lack of systematic empirical evaluation in the field of Knowlegde Management, we give an overview of evaluative approaches in different research areas up to now.
We are especially covering those areas which are relevant for the development of Organizational Memories Informations Systems (OMIS Knowledge Engineering (including Knowledge Acquisition and Ontologies Human Computer Interaction, Information Retrieval and Software Engineering.
We report about (experimental) studies and general guidelines for evaluation from the different research fields.
Finally, we show implications for the evaluation of OMIS, propose rules of thumb for the realization of a systematic evaluative study and sketch first ideas for the evaluation of FRODO.

This paper describes the implementation of a content-based cover song identification system which has been released under an open source license.
The system is centered around the Apache Lucene text search engine library, and proves how classic techniques derived from textual Information Retrieval, in particular the bag-of-words paradigm, can successfully be adapted to music identification.
The paper focuses on extensive experimentation on the most influential system parameters, in order to find an optimal tradeoff between retrieval accuracy and speed of querying.

BACKGROUND CISMeF is a French quality-controlled health gateway that uses the MeSH thesaurus.
We introduced two new concepts, metaterms (medical specialty which has semantic links with one or more MeSH terms, subheadings and resource types) and resource types.
OBJECTIVE Evaluate precision and recall of metaterms.
METHODS We created 16 pairs of queries.
Each pair concerned the same topic, but one used metaterms and one MeSH terms.
To assess precision, each document retrieved by the query was classified as irrelevant, partly relevant or fully relevant.
The 16 queries yielded 943 documents for metaterm queries and 139 for MeSH term queries.
The recall of MeSH term queries was 0.44 (compared to 1 for metaterm queries) and the precision were identical for MeSH term and metaterm queries.
Metaconcept such as CISMeF metaterms allows a better recall with a similar precision that MeSH terms in a quality controlled health gateway.

We propose a method for recognizing such event causalities as “smoke cigarettes die of lung cancer” using background knowledge taken from web texts as well as original sentences from which candidates for the causalities were extracted.
We retrieve texts related to our event causality candidates from four billion web pages by three distinct methods, including a why-question answering system, and feed them to our multi-column convolutional neural networks.
This allows us to identify the useful background knowledge scattered in web texts and effectively exploit the identified knowledge to recognize event causalities.
We empirically show that the combination of our neural network architecture and background knowledge significantly improves average precision, while the previous state-of-the-art method gains just a small benefit from such background knowledge.

1School of Software, Dalian University of Technology, Dalian, Liaoning Province, China 2School of Information Science and Engineering, Ludong University, Yantai, Shandong Province, China 3School of Innovation Experiment, Dalian University of Technology, Dalian, Liaoning Province,
China 4Faculty of Computer and Information Sciences, Hosei University, 3-7-2, Kajino-cho, Koganei-shi Tokyo 184-8584, Japan

The objective of our work is the development of a natural language dialogue system for information retrieval with multimodal input and multimedia output.
Overall, the system consists of three phases: input analysis, information and knowledge management and output generation.
The dialogue system is designed for consulting old Mexican historical documents.
In this paper we describe the designed architecture of each of the three phases.

Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular.
Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine.
In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System
Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system.

The Information Retrieval Festival took place in April 2017 in Glasgow.
The focus of the workshop was to bring together IR researchers from the various Scottish universities and beyond in order to facilitate more awareness, increased interaction and reflection on the status of the field and its future.
The program included an industry session, research talks, demos and posters as well as two keynotes.
The first keynote was delivered by Prof. Jaana Kekalenien, who provided a historical, critical reflection of realism in Interactive Information Retrieval Experimentation, while the second keynote was delivered by Prof. Maarten de Rijke, who argued for more Artificial Intelligence usage in IR solutions and deployments.
The workshop was followed by a 226 Tour de Scotland&#226 where delegates were taken from Glasgow to Aberdeen for the European Conference in Information Retrieval (ECIR 2017)

The goal of the FID/LD Workshop on Linguistics and Information Science was to contribute to the development of a comprehensive plan that can be used to guide research activities for more effective use of linguistics in information science.
To appreciate our progress toward that goal during the.three days at Biskops-Arno in May of 1976, it is necessary to consider in some detail the stimulus that prompted the establishment of the Workshop, the composition of the group that participated, the common information we shared in the form of the materials distributed in advance to the participants, and the similarities and differences in our perceptions of the needs for linguistics in information science and of the resources that are available to work with.
This chapter is organized so that these items of background data are presented first, followed by a description of the conduct of the Workshop itself.

The latent class structure is modified by requiring the number of latent classes to be equal to the number of keywords.
This modification changes the latent structure to one that consists of matrices that are both symmetric and positive definite, therefore making a 8220;computer solution&#8221; both possible and practical.
The modified latent structure is derived, the numerical analysis considerations for a computer solution are presented, and an example illustrates the use of the latent class structure in both file organization and retrieval.

Many extensions to text-based, data-intensive knowledge management approaches, such as Information Retrieval or Data Mining, focus on integrating the impressive recent advances in language technology.
For this, they need fast, robust parsers that deliver linguistic data which is meaningful for the subsequent processing stages.
This paper introduces such a parsing system.
Its output is a hierarchical structure of syntactic relations, functional dependency structures.
Posted at the Zurich Open Repository and Archive, University of Zurich ZORA URL: https doi.org/10.5167/uzh-19099
Originally published at: Schneider, G (2003 Extracting and using trace-free functional dependencies from the penn treebank to reduce parsing complexity.
In: Treebanks and Linguistic Theories (TLT) 2003, Vxj, Sweden, 2003 2003, 153-164.
Extracting and Using Trace-Free Functional Dependencies from the Penn Treebank to Reduce Parsing Complexity

In this paper, we propose a new language model, namely, a dependency structure language model, for information retrieval to compensate for the weakness of bigram and trigram language models.
The dependency structure language model is based on the Chow Expansion theory and the dependency parse tree generated by a dependency parser.
So, long-distance dependencies can be handled by the dependency structure language model.
We carried out some experiments to verify the proposed model.
In our experiments, the dependency structure model gives a better performance than recently proposed language models, and the dependency structure is more effective than bigram in language modeling for information retrieval.

