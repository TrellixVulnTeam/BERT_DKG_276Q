task:
  # The name of the task
  task_name: conll
  # The input data directory
  #data_dir: D:\github\bishe\Text_Clustering\data
  #data_dir: /home/yjc/bishe/Text_Clustering/data
  data_dir: ../semi-data
  # bert pre-trained model directory
  #bert_model_dir: D:\github\bishe\Text_Clustering\NER_projects\pt_bert_ner\bert_model
  #bert_model_dir: /home/yjc/bishe/Text_Clustering/output_dir_lm_ai
  bert_model_dir: bert-base-cased
  # output layer, must in: SoftmaxDecoder, CRFDecoder, AttnCRFDecoder, AttnSoftmaxDecoder
  decoder: AttnCRFDecoder
  # The output directory where the model predictions and checkpoints will be written
  output_dir: ./output
  # Specify the ckeckpoint to load
  checkpoint: ~
  # Whether you are using an uncased model or not
  lower_case: false
  # The maximum total input sequence length after WordPiece tokenization
  max_seq_length: 128
  # conll03, ai, tiny
  data_type: ai
  ssl: false
train:
  # Whether do training in this run
  do: true
  # Total batch size for training
  batch_size: 16
  # The initial learning rate for Adam
  learning_rate: !!float 5e-5
  output_lr: !!float 5e-3
  # Total number of training epochs to perform
  epochs: 30
  # Proportion of training to perform linear learning rate warmup for
  warmup_proportion: 0.1
  # Number of updates steps to accumulate before performing a backward/update pass
  gradient_accumulation_steps: 1
  # Random seed for initialization
  seed: 49
dev:
  # Whether do predicting in every epoch
  do_every_epoch: true
  do_after_train: true
  # Which dataset to be predicted: train, dev or test
  dataset: dev
  # Total batch size for predicting
  batch_size: 16
test:
  # Whether do predicting in this run
  do: true
  # Which dataset to be predicted: train, dev or test
  dataset: test
  # Total batch size for predicting
  batch_size: 16
  do_every_epoch: true
  do_after_train: true
predict:
  do: true
  # Total batch size for predicting
  batch_size: 512
# Whether to use CUDA when available
use_cuda: true
n_gpu: 1
