task:
  # The name of the task
  task_name: ner
  # The input data directory
  data_dir: ../data
  # bert pre-trained model directory
  bert_model_dir: D:\github\bishe\Text_Clustering\NER_projects\bert_model
  #bert_model_dir: bert-base-cased
  # The output directory where the model predictions and checkpoints will be written
  output_dir: ./output
  # Specify the ckeckpoint to load
  checkpoint: ~
  # Whether you are using an uncased model or not
  lower_case: false
  # The maximum total input sequence length after WordPiece tokenization
  max_seq_length: 128
  # the dataset, one of conll03, ai
  data_type: conll2003
  # use tiny dataset when set true
  debug: true
  # whether use Entropy Minimization Regulazation
  ssl: false
  # whether use the BIOES format
  BIOES: true
  # Whether use the doc_level inputs.
  # If true, the input sequence consists of as more sentences as possible;
  # If false, the input sequence consists of only one sentence.
  # Both options satisfy that the total sequence length still smaller then 'max_seq_length'.
  doc_level: true
  # The type of embedding layer, BertEmbed / RandomEmbed
  embedder: BertEmbed
  # The type of middle layer, MultiAttn / BiLSTM /None
  encoder: None
  # The number of encoders
  layer_num: 1
  # Output layer, must in: SoftmaxDecoder / CRFDecoder
  decoder: SoftmaxDecoder
  # We break the word into tokens, assume the labels are 'S-PER X X X', if 'cal_X_loss' is true,
  # then the last three tokens' loss will be added to the total loss when training.
  cal_X_loss: false
train:
  # Whether do training in this run
  do: false
  # Total batch size for training
  batch_size: 32
  # The initial learning rate for Adam
  learning_rate: !!float 5e-5
  output_lr: !!float 5e-3
  # Total number of training epochs to perform
  epochs: 1
  # Proportion of training to perform linear learning rate warmup for
  warmup_proportion: 0.1
  # Number of updates steps to accumulate before performing a backward/update pass
  gradient_accumulation_steps: 1
  # Random seed for initialization
  seed: 49
dev:
  do_every_epoch: false
  do: false
  batch_size: 32
test:
  do_every_epoch: false
  do: false
  batch_size: 32
predict:
  do: true
  batch_size: 32
# Whether to use CUDA when available
use_cuda: true
n_gpu: 2
