task:
  # The name of the task
  task_name: ner
  # The input data directory
  data_dir: ../data
  # Bert pre-trained model directory
  bert_model_dir: bert-base-cased
  # The output directory where the model predictions and checkpoints will be written
  output_dir: ../output
  # Specify the checkpoint to load
  checkpoint: ~
  # Whether you are using an uncased model or not
  lower_case: false
  # The maximum total input sequence length after WordPiece tokenization
  max_seq_length: 128
  # The dataset, one of conll2003, ai
  data_type: conll2003
  # Use tiny dataset when set true
  debug: false
  # Whether use Entropy Minimization Regularization, must given `train_unlabeled.txt` when set true.
  ssl: false
  # Whether use the BIOES format
  BIOES: true
  # Whether use the doc_level inputs.
  # If true, the input sequence consists of as more sentences as possible;
  # If false, the input sequence consists of only one sentence.
  # Both options satisfy that the total sequence length still smaller then 'max_seq_length'.
  doc_level: false
  # The type of embedding layer, BertEmbed / RandomEmbed
  embedder: BertEmbed
  # The type of middle layer, MultiAttn / BiLSTM / None
  encoder: MultiAttn
  # The number of encoders
  layer_num: 1
  # Output layer, must in: SoftmaxDecoder / CRFDecoder
  decoder: CRFDecoder
  # We break the word into tokens, assume the labels are 'S-PER X X X', if 'cal_X_loss' is true,
  # then the last three tokens' loss will be added to the total loss when training.
  cal_X_loss: True
train:
  # Whether do training in this run
  do: true
  # Total batch size for training
  batch_size: 64
  # The initial learning rate for Adam
  learning_rate: !!float 5e-5
  # Total number of training epochs to perform
  epochs: 10
  # Proportion of training to perform linear learning rate warmup for
  warmup_proportion: 0.1
  # Number of updates steps to accumulate before performing a backward/update pass
  gradient_accumulation_steps: 1
  # Random seed for initialization
  seed: 49
dev:
  do_every_epoch: true
  do: true
  batch_size: 64
test:
  do_every_epoch: true
  do: true
  batch_size: 64
predict:
  do: false
  batch_size: 512
# Whether to use CUDA when available
use_cuda: true
n_gpu: 1
